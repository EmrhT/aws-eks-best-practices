
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.5.3">
    
    
      
        <title>Print Site - EKS Best Practices Guides</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.50c56a3b.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../css/print-site-enum-headings1.css">
    
      <link rel="stylesheet" href="../css/print-site-enum-headings2.css">
    
      <link rel="stylesheet" href="../css/print-site-enum-headings3.css">
    
      <link rel="stylesheet" href="../css/print-site-enum-headings4.css">
    
      <link rel="stylesheet" href="../css/print-site-enum-headings5.css">
    
      <link rel="stylesheet" href="../css/print-site-enum-headings6.css">
    
      <link rel="stylesheet" href="../css/print-site.css">
    
      <link rel="stylesheet" href="../css/print-site-material.css">
    
    <script>__md_scope=new URL("/",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function n(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],n("js",new Date),n("config","G-VQGL8B2FH8"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){this.value&&n("event","search",{search_term:this.value})}),document$.subscribe(function(){var a=document.forms.feedback;if(void 0!==a)for(var e of a.querySelectorAll("[type=submit]"))e.addEventListener("click",function(e){e.preventDefault();var t=document.location.pathname,e=this.getAttribute("data-md-value");n("event","feedback",{page:t,data:e}),a.firstElementChild.disabled=!0;e=a.querySelector(".md-feedback__note [data-md-value='"+e+"']");e&&(e.hidden=!1)}),a.hidden=!1}),location$.subscribe(function(e){n("config","G-VQGL8B2FH8",{page_path:e.pathname})})});var e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-VQGL8B2FH8",document.getElementById("__analytics").insertAdjacentElement("afterEnd",e)}</script>
  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
    
  
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#section-guides" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="EKS Best Practices Guides" class="md-header__button md-logo" aria-label="EKS Best Practices Guides" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            EKS Best Practices Guides
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Print Site
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/aws/aws-eks-best-practices" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    aws/aws-eks-best-practices
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="EKS Best Practices Guides" class="md-nav__button md-logo" aria-label="EKS Best Practices Guides" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    EKS Best Practices Guides
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/aws/aws-eks-best-practices" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    aws/aws-eks-best-practices
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1" >
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Guides
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            Guides
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Security
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Security
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../security/docs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../security/docs/iam/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Identity and Access Management
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../security/docs/pods/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Pod Security
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../security/docs/multitenancy/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Multi-tenancy
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../security/docs/detective/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Detective Controls
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../security/docs/network/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Network Security
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../security/docs/data/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data Encryption and Secrets Management
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../security/docs/runtime/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Runtime Security
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../security/docs/hosts/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Infrastructure Security
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../security/docs/compliance/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Regulatory Compliance
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../security/docs/incidents/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Incident Response and Forensics
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../security/docs/image/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Image Security
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Cluster Autoscaling
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Cluster Autoscaling
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../karpenter/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Karpenter
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cluster-autoscaling/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Cluster-Autoscaler
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Reliability
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Reliability
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reliability/docs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reliability/docs/application/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Applications
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reliability/docs/controlplane/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Control Plane
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reliability/docs/dataplane/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data Plane
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Windows Containers
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Windows Containers
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../windows/docs/ami/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    AMI Management
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../windows/docs/gmsa/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gMSA for Windows Containers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../windows/docs/hardening/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Windows Server Hardening
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../windows/docs/images/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Scanning Windows Images
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../windows/docs/licensing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Windows Versions and Licensing
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../windows/docs/logging/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Logging
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../windows/docs/monitoring/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Monitoring Windows Containers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../windows/docs/networking/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Windows Networking
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../windows/docs/oom/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Memory and Systems Management
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../windows/docs/patching/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Infrastructure Management
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../windows/docs/scheduling/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Scheduling
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../windows/docs/security/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Pod Security for Windows Containers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../windows/docs/storage/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Storage Options
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Networking
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Networking
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../networking/index/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../networking/subnets/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    VPC and Subnet Considerations
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../networking/vpc-cni/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Amazon VPC CNI
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../networking/ip-optimization-strategies/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Optimizing IP Address Utilization
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../networking/ipv6/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Running IPv6 Clusters
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../networking/custom-networking/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Custom Networking
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../networking/prefix-mode/index_linux/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Prefix Mode for Linux
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../networking/prefix-mode/index_windows/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Prefix Mode for Windows
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../networking/sgpp/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Security Groups per Pod
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../networking/loadbalancing/loadbalancing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Load Balancing
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../networking/monitoring/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Monitoring for Network performance issues
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Scalability
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Scalability
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../scalability/docs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../scalability/docs/control-plane/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Control Plane
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../scalability/docs/data-plane/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data Plane
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../scalability/docs/cluster-services/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Cluster Services
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../scalability/docs/workloads/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Workloads
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../scalability/docs/scaling_theory/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    The theory behind scaling
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../scalability/docs/kcp_monitoring/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Control plane monitoring
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../scalability/docs/node_efficiency/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Node efficiency and scaling
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../upgrades/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Cluster Upgrades
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_9" >
        
          
          <label class="md-nav__link" for="__nav_9" id="__nav_9_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Cost Optimization
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_9">
            <span class="md-nav__icon md-icon"></span>
            Cost Optimization
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cost_optimization/cfm_framework/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Cloud Financial Management Framework
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cost_optimization/cost_opt_compute/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Compute
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cost_optimization/cost_opt_networking/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Network
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cost_optimization/cost_opt_storage/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Storage
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cost_optimization/cost_opt_observability/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Observability
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#section-guides" class="md-nav__link">
    <span class="md-ellipsis">
      I. Guides
    </span>
  </a>
  
    <nav class="md-nav" aria-label="I. Guides">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#index" class="md-nav__link">
    <span class="md-ellipsis">
      1. Introduction
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-security" class="md-nav__link">
    <span class="md-ellipsis">
      II. Security
    </span>
  </a>
  
    <nav class="md-nav" aria-label="II. Security">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#security-docs" class="md-nav__link">
    <span class="md-ellipsis">
      2. Home
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#security-docs-iam" class="md-nav__link">
    <span class="md-ellipsis">
      3. Identity and Access Management
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#security-docs-pods" class="md-nav__link">
    <span class="md-ellipsis">
      4. Pod Security
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#security-docs-multitenancy" class="md-nav__link">
    <span class="md-ellipsis">
      5. Multi-tenancy
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#security-docs-detective" class="md-nav__link">
    <span class="md-ellipsis">
      6. Detective Controls
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#security-docs-network" class="md-nav__link">
    <span class="md-ellipsis">
      7. Network Security
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#security-docs-data" class="md-nav__link">
    <span class="md-ellipsis">
      8. Data Encryption and Secrets Management
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#security-docs-runtime" class="md-nav__link">
    <span class="md-ellipsis">
      9. Runtime Security
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#security-docs-hosts" class="md-nav__link">
    <span class="md-ellipsis">
      10. Infrastructure Security
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#security-docs-compliance" class="md-nav__link">
    <span class="md-ellipsis">
      11. Regulatory Compliance
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#security-docs-incidents" class="md-nav__link">
    <span class="md-ellipsis">
      12. Incident Response and Forensics
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#security-docs-image" class="md-nav__link">
    <span class="md-ellipsis">
      13. Image Security
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-cluster-autoscaling" class="md-nav__link">
    <span class="md-ellipsis">
      III. Cluster Autoscaling
    </span>
  </a>
  
    <nav class="md-nav" aria-label="III. Cluster Autoscaling">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#karpenter" class="md-nav__link">
    <span class="md-ellipsis">
      14. Karpenter
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cluster-autoscaling" class="md-nav__link">
    <span class="md-ellipsis">
      15. Cluster-Autoscaler
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-reliability" class="md-nav__link">
    <span class="md-ellipsis">
      IV. Reliability
    </span>
  </a>
  
    <nav class="md-nav" aria-label="IV. Reliability">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#reliability-docs" class="md-nav__link">
    <span class="md-ellipsis">
      16. Home
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reliability-docs-application" class="md-nav__link">
    <span class="md-ellipsis">
      17. Applications
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reliability-docs-controlplane" class="md-nav__link">
    <span class="md-ellipsis">
      18. Control Plane
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reliability-docs-dataplane" class="md-nav__link">
    <span class="md-ellipsis">
      19. Data Plane
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-windows-containers" class="md-nav__link">
    <span class="md-ellipsis">
      V. Windows Containers
    </span>
  </a>
  
    <nav class="md-nav" aria-label="V. Windows Containers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#windows-docs-ami" class="md-nav__link">
    <span class="md-ellipsis">
      20. AMI Management
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#windows-docs-gmsa" class="md-nav__link">
    <span class="md-ellipsis">
      21. gMSA for Windows Containers
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#windows-docs-hardening" class="md-nav__link">
    <span class="md-ellipsis">
      22. Windows Server Hardening
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#windows-docs-images" class="md-nav__link">
    <span class="md-ellipsis">
      23. Scanning Windows Images
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#windows-docs-licensing" class="md-nav__link">
    <span class="md-ellipsis">
      24. Windows Versions and Licensing
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#windows-docs-logging" class="md-nav__link">
    <span class="md-ellipsis">
      25. Logging
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#windows-docs-monitoring" class="md-nav__link">
    <span class="md-ellipsis">
      26. Monitoring Windows Containers
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#windows-docs-networking" class="md-nav__link">
    <span class="md-ellipsis">
      27. Windows Networking
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#windows-docs-oom" class="md-nav__link">
    <span class="md-ellipsis">
      28. Memory and Systems Management
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#windows-docs-patching" class="md-nav__link">
    <span class="md-ellipsis">
      29. Infrastructure Management
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#windows-docs-scheduling" class="md-nav__link">
    <span class="md-ellipsis">
      30. Scheduling
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#windows-docs-security" class="md-nav__link">
    <span class="md-ellipsis">
      31. Pod Security for Windows Containers
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#windows-docs-storage" class="md-nav__link">
    <span class="md-ellipsis">
      32. Storage Options
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-networking" class="md-nav__link">
    <span class="md-ellipsis">
      VI. Networking
    </span>
  </a>
  
    <nav class="md-nav" aria-label="VI. Networking">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#networking-index" class="md-nav__link">
    <span class="md-ellipsis">
      33. Home
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#networking-subnets" class="md-nav__link">
    <span class="md-ellipsis">
      34. VPC and Subnet Considerations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#networking-vpc-cni" class="md-nav__link">
    <span class="md-ellipsis">
      35. Amazon VPC CNI
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#networking-ip-optimization-strategies" class="md-nav__link">
    <span class="md-ellipsis">
      36. Optimizing IP Address Utilization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#networking-ipv6" class="md-nav__link">
    <span class="md-ellipsis">
      37. Running IPv6 Clusters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#networking-custom-networking" class="md-nav__link">
    <span class="md-ellipsis">
      38. Custom Networking
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#networking-prefix-mode-index_linux" class="md-nav__link">
    <span class="md-ellipsis">
      39. Prefix Mode for Linux
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#networking-prefix-mode-index_windows" class="md-nav__link">
    <span class="md-ellipsis">
      40. Prefix Mode for Windows
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#networking-sgpp" class="md-nav__link">
    <span class="md-ellipsis">
      41. Security Groups per Pod
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#networking-loadbalancing-loadbalancing" class="md-nav__link">
    <span class="md-ellipsis">
      42. Load Balancing
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#networking-monitoring" class="md-nav__link">
    <span class="md-ellipsis">
      43. Monitoring for Network performance issues
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-scalability" class="md-nav__link">
    <span class="md-ellipsis">
      VII. Scalability
    </span>
  </a>
  
    <nav class="md-nav" aria-label="VII. Scalability">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#scalability-docs" class="md-nav__link">
    <span class="md-ellipsis">
      44. Home
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scalability-docs-control-plane" class="md-nav__link">
    <span class="md-ellipsis">
      45. Control Plane
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scalability-docs-data-plane" class="md-nav__link">
    <span class="md-ellipsis">
      46. Data Plane
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scalability-docs-cluster-services" class="md-nav__link">
    <span class="md-ellipsis">
      47. Cluster Services
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scalability-docs-workloads" class="md-nav__link">
    <span class="md-ellipsis">
      48. Workloads
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scalability-docs-scaling_theory" class="md-nav__link">
    <span class="md-ellipsis">
      49. The theory behind scaling
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scalability-docs-kcp_monitoring" class="md-nav__link">
    <span class="md-ellipsis">
      50. Control plane monitoring
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scalability-docs-node_efficiency" class="md-nav__link">
    <span class="md-ellipsis">
      51. Node efficiency and scaling
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#upgrades" class="md-nav__link">
    <span class="md-ellipsis">
      52. Cluster Upgrades
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-cost-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      VIII. Cost Optimization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="VIII. Cost Optimization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cost_optimization-cfm_framework" class="md-nav__link">
    <span class="md-ellipsis">
      53. Cloud Financial Management Framework
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cost_optimization-cost_opt_compute" class="md-nav__link">
    <span class="md-ellipsis">
      54. Compute
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cost_optimization-cost_opt_networking" class="md-nav__link">
    <span class="md-ellipsis">
      55. Network
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cost_optimization-cost_opt_storage" class="md-nav__link">
    <span class="md-ellipsis">
      56. Storage
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cost_optimization-cost_opt_observability" class="md-nav__link">
    <span class="md-ellipsis">
      57. Observability
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<div id="print-site-page" class="print-site-enumerate-headings print-site-enumerate-figures">
        <section class="print-page">
            <div id="print-page-toc" data-toc-depth="3">
                <nav role='navigation' class='print-page-toc-nav'>
                <h1 class='print-page-toc-title'>Table of Contents</h1>
                </nav>
            </div>
        </section>
        
                        <h1 class='nav-section-title' id='section-guides'>
                            Guides <a class='headerlink' href='#section-guides' title='Permanent link'>↵</a>
                        </h1>
                        <section class="print-page" id="index"><h1 id="index-introduction">Introduction<a class="headerlink" href="#index-introduction" title="Permanent link">&para;</a></h1>
<p>Welcome to the EKS Best Practices Guides.  The primary goal of this project is to offer a set of best practices for day 2 operations for Amazon EKS. We elected to publish this guidance to GitHub so we could iterate quickly, provide timely and effective recommendations for variety of concerns, and easily incorporate suggestions from the broader community.  </p>
<p>We currently have published guides for the following topics: </p>
<ul>
<li><a href="#security-docs">Best Practices for Security</a></li>
<li><a href="#reliability-docs">Best Practices for Reliability</a></li>
<li>Best Practices for Cluster Autoscaling: <a href="#karpenter">karpenter</a>, <a href="#cluster-autoscaling">cluster-autoscaler</a></li>
<li><a href="#windows-docs-ami">Best Practices for Running Windows Containers</a></li>
<li><a href="#networking-index">Best Practices for Networking</a></li>
<li><a href="#scalability-docs">Best Practices for Scalability</a></li>
<li><a href="#upgrades">Best Practices for Cluster Upgrades</a></li>
<li><a href="#cost_optimization-cfm_framework">Best Practices for Cost Optimization</a></li>
</ul>
<p>We also open sourced a Python based CLI (Command Line Interface) called <a href="https://github.com/aws-samples/hardeneks">hardeneks</a> to check some of the recommendations from this guide.</p>
<p>In the future we will be publishing best practices guidance for performance, cost optimization, and operational excellence. </p>
<h2 id="index-related-guides">Related guides<a class="headerlink" href="#index-related-guides" title="Permanent link">&para;</a></h2>
<p>In addition to the <a href="https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html">EKS User Guide</a>, AWS has published several other guides that may help you with your implementation of EKS.</p>
<ul>
<li><a href="https://aws.github.io/aws-emr-containers-best-practices/">EMR Containers Best Practices Guides</a></li>
<li><a href="https://awslabs.github.io/data-on-eks/">Data on EKS</a></li>
<li><a href="https://aws-observability.github.io/observability-best-practices/">AWS Observability Best Practices</a></li>
<li><a href="https://aws-ia.github.io/terraform-aws-eks-blueprints/">Amazon EKS Blueprints for Terraform</a></li>
<li><a href="https://aws-quickstart.github.io/cdk-eks-blueprints/">Amazon EKS Blueprints Quick Start</a></li>
</ul>
<h2 id="index-contributing">Contributing<a class="headerlink" href="#index-contributing" title="Permanent link">&para;</a></h2>
<p>We encourage you to contribute to these guides. If you have implemented a practice that has proven to be effective, please share it with us by opening an issue or a pull request. Similarly, if you discover an error or flaw in the guidance we've already published, please submit a PR to correct it. The guidelines for submitting PRs can be found in our <a href="https://github.com/aws/aws-eks-best-practices/blob/master/CONTRIBUTING.md">Contributing Guidelines</a>.</p></section><h1 class='nav-section-title-end'>Ended: Guides</h1>
                        <h1 class='nav-section-title' id='section-security'>
                            Security <a class='headerlink' href='#section-security' title='Permanent link'>↵</a>
                        </h1>
                        <section class="print-page" id="security-docs"><h1 id="security-docs-amazon-eks-best-practices-guide-for-security">Amazon EKS Best Practices Guide for Security<a class="headerlink" href="#security-docs-amazon-eks-best-practices-guide-for-security" title="Permanent link">&para;</a></h1>
<p>This guide provides advice about protecting information, systems, and assets that are reliant on EKS while delivering business value through risk assessments and mitigation strategies. The guidance herein is part of a series of best practices guides that AWS is publishing to help customers implement EKS in accordance with best practices. Guides for Performance, Operational Excellence, Cost Optimization, and Reliability will be available in the coming months. </p>
<h2 id="security-docs-how-to-use-this-guide">How to use this guide<a class="headerlink" href="#security-docs-how-to-use-this-guide" title="Permanent link">&para;</a></h2>
<p>This guide is meant for security practitioners who are responsible for implementing and monitoring the effectiveness of security controls for EKS clusters and the workloads they support. The guide is organized into different topic areas for easier consumption. Each topic starts with a brief overview, followed by a list of recommendations and best practices for securing your EKS clusters. The topics do not need to be read in a particular order. </p>
<h2 id="security-docs-understanding-the-shared-responsibility-model">Understanding the Shared Responsibility Model<a class="headerlink" href="#security-docs-understanding-the-shared-responsibility-model" title="Permanent link">&para;</a></h2>
<p>Security and compliance are considered shared responsibilities when using a managed service like EKS. Generally speaking, AWS is responsible for security "of" the cloud whereas you, the customer, are responsible for security "in" the cloud. With EKS, AWS is responsible for managing of the EKS managed Kubernetes control plane. This includes the Kubernetes control plane nodes, the ETCD database, and other infrastructure necessary for AWS to deliver a secure and reliable service. As a consumer of EKS, you are largely responsible for the topics in this guide, e.g. IAM, pod security, runtime security, network security, and so forth. </p>
<p>When it comes to infrastructure security, AWS will assume additional responsibilities as you move from self-managed workers, to managed node groups, to Fargate. For example, with Fargate, AWS becomes responsible for securing the underlying instance/runtime used to run your Pods. </p>
<p><img alt="Shared Responsibility Model - Fargate" src="../security/docs/images/SRM-EKS.jpg" /></p>
<p>AWS will also assume responsibility of keeping the EKS optimized AMI up to date with Kubernetes patch versions and security patches. Customers using Managed Node Groups (MNG) are responsible for upgrading their Nodegroups to the latest AMI via EKS API, CLI, Cloudformation or AWS Console. Also unlike Fargate, MNGs will not automatically scale your infrastructure/cluster.  That can be handled by the <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md">cluster-autoscaler</a> or other technologies such as <a href="https://karpenter.sh/">Karpenter</a>, native AWS autoscaling, SpotInst's <a href="https://spot.io/solutions/kubernetes-2/">Ocean</a>, or Atlassian's <a href="https://github.com/atlassian/escalator">Escalator</a>. </p>
<p><img alt="Shared Responsibility Model - MNG" src="../security/docs/images/SRM-MNG.jpg" /></p>
<p>Before designing your system, it is important to know where the line of demarcation is between your responsibilities and the provider of the service (AWS).</p>
<p>For additional information about the shared responsibility model, see <a href="https://aws.amazon.com/compliance/shared-responsibility-model/">https://aws.amazon.com/compliance/shared-responsibility-model/</a></p>
<h2 id="security-docs-introduction">Introduction<a class="headerlink" href="#security-docs-introduction" title="Permanent link">&para;</a></h2>
<p>There are several security best practice areas that are pertinent when using a managed Kubernetes service like EKS:</p>
<ul>
<li>Identity and Access Management </li>
<li>Pod Security</li>
<li>Runtime Security</li>
<li>Network Security</li>
<li>Multi-tenancy</li>
<li>Detective Controls</li>
<li>Infrastructure Security</li>
<li>Data Encryption and Secrets Management</li>
<li>Regulatory Compliance</li>
<li>Incident Response and Forensics</li>
<li>Image Security</li>
</ul>
<p>As part of designing any system, you need to think about its security implications and the practices that can affect your security posture. For example, you need to control who can perform actions against a set of resources. You also need the ability to quickly identify security incidents, protect your systems and services from unauthorized access, and maintain the confidentiality and integrity of data through data protection. Having a well-defined and rehearsed set of processes for responding to security incidents will improve your security posture too. These tools and techniques are important because they support objectives such as preventing financial loss or complying with regulatory obligations.</p>
<p>AWS helps organizations achieve their security and compliance goals by offering a rich set of security services that have evolved based on feedback from a broad set of security conscious customers. By offering a highly secure foundation, customers can spend less time on “undifferentiated heavy lifting” and more time on achieving their business objectives. </p>
<h2 id="security-docs-feedback">Feedback<a class="headerlink" href="#security-docs-feedback" title="Permanent link">&para;</a></h2>
<p>This guide is being released on GitHub so as to collect direct feedback and suggestions from the broader EKS/Kubernetes community. If you have a best practice that you feel we ought to include in the guide, please file an issue or submit a PR in the GitHub repository. Our intention is to update the guide periodically as new features are added to the service or when a new best practice evolves. </p>
<h2 id="security-docs-further-reading">Further Reading<a class="headerlink" href="#security-docs-further-reading" title="Permanent link">&para;</a></h2>
<p><a href="https://github.com/kubernetes/sig-security/blob/main/sig-security-external-audit/security-audit-2019/findings/Kubernetes%20White%20Paper.pdf">Kubernetes Security Whitepaper</a>, sponsored by the Security Audit Working Group, this Whitepaper describes key aspects of the Kubernetes attack surface and security architecture with the aim of helping security practitioners make sound design and implementation decisions.</p>
<p>The CNCF published also a <a href="https://github.com/cncf/tag-security/blob/efb183dc4f19a1bf82f967586c9dfcb556d87534/security-whitepaper/v2/CNCF_cloud-native-security-whitepaper-May2022-v2.pdf">white paper</a> on cloud native security. The paper examines how the technology landscape has evolved and advocates for the adoption of security practices that align with DevOps processes and agile methodologies.</p></section><section class="print-page" id="security-docs-iam"><h1 id="security-docs-iam-identity-and-access-management">Identity and Access Management<a class="headerlink" href="#security-docs-iam-identity-and-access-management" title="Permanent link">&para;</a></h1>
<p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html">Identity and Access Management</a> (IAM) is an AWS service that performs two essential functions: Authentication and Authorization.  Authentication involves the verification of a identity whereas authorization governs the actions that can be performed by AWS resources.  Within AWS, a resource can be another AWS service, e.g. EC2, or an AWS <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/intro-structure.html#intro-structure-principal">principal</a> such as an <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html#id_iam-users">IAM User</a> or <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html#id_iam-roles">Role</a>.  The rules governing the actions that a resource is allowed to perform are expressed as <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html">IAM policies</a>.</p>
<h2 id="security-docs-iam-controlling-access-to-eks-clusters">Controlling Access to EKS Clusters<a class="headerlink" href="#security-docs-iam-controlling-access-to-eks-clusters" title="Permanent link">&para;</a></h2>
<p>The Kubernetes project supports a variety of different strategies to authenticate requests to the kube-apiserver service, e.g. Bearer Tokens, X.509 certificates, OIDC, etc. EKS currently has native support for <a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#webhook-token-authentication">webhook token authentication</a>, <a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#service-account-tokens">service account tokens</a>, and as of February 21, 2021, OIDC authentication.</p>
<p>The webhook authentication strategy calls a webhook that verifies bearer tokens. On EKS, these bearer tokens are generated by the AWS CLI or the <a href="https://github.com/kubernetes-sigs/aws-iam-authenticator">aws-iam-authenticator</a> client when you run <code>kubectl</code> commands. As you execute commands, the token is passed to the kube-apiserver which forwards it to the authentication webhook.  If the request is well-formed, the webhook calls a pre-signed URL embedded in the token's body. This URL validates the request's signature and returns information about the user, e.g. the user's account, Arn, and UserId to the kube-apiserver.</p>
<p>To manually generate a authentication token, type the following command in a terminal window:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#security-docs-iam-__codelineno-0-1"></a>aws<span class="w"> </span>eks<span class="w"> </span>get-token<span class="w"> </span>--cluster-name<span class="w"> </span>&lt;cluster_name&gt;
</code></pre></div>
<p>You can also get a token programmatically. Below is an example written in Go:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#security-docs-iam-__codelineno-1-1"></a><span class="kn">package</span><span class="w"> </span><span class="nx">main</span>
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#security-docs-iam-__codelineno-1-2"></a>
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#security-docs-iam-__codelineno-1-3"></a><span class="kn">import</span><span class="w"> </span><span class="p">(</span>
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#security-docs-iam-__codelineno-1-4"></a><span class="w">  </span><span class="s">&quot;fmt&quot;</span>
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#security-docs-iam-__codelineno-1-5"></a><span class="w">  </span><span class="s">&quot;log&quot;</span>
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#security-docs-iam-__codelineno-1-6"></a><span class="w">  </span><span class="s">&quot;sigs.k8s.io/aws-iam-authenticator/pkg/token&quot;</span>
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#security-docs-iam-__codelineno-1-7"></a><span class="p">)</span>
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#security-docs-iam-__codelineno-1-8"></a>
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#security-docs-iam-__codelineno-1-9"></a><span class="kd">func</span><span class="w"> </span><span class="nx">main</span><span class="p">()</span><span class="w">  </span><span class="p">{</span>
<a id="__codelineno-1-10" name="__codelineno-1-10" href="#security-docs-iam-__codelineno-1-10"></a><span class="w">  </span><span class="nx">g</span><span class="p">,</span><span class="w"> </span><span class="nx">_</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">token</span><span class="p">.</span><span class="nx">NewGenerator</span><span class="p">(</span><span class="kc">false</span><span class="p">,</span><span class="w"> </span><span class="kc">false</span><span class="p">)</span>
<a id="__codelineno-1-11" name="__codelineno-1-11" href="#security-docs-iam-__codelineno-1-11"></a><span class="w">  </span><span class="nx">tk</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">g</span><span class="p">.</span><span class="nx">Get</span><span class="p">(</span><span class="s">&quot;&lt;cluster_name&gt;&quot;</span><span class="p">)</span>
<a id="__codelineno-1-12" name="__codelineno-1-12" href="#security-docs-iam-__codelineno-1-12"></a><span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span>
<a id="__codelineno-1-13" name="__codelineno-1-13" href="#security-docs-iam-__codelineno-1-13"></a><span class="w">    </span><span class="nx">log</span><span class="p">.</span><span class="nx">Fatal</span><span class="p">(</span><span class="nx">err</span><span class="p">)</span>
<a id="__codelineno-1-14" name="__codelineno-1-14" href="#security-docs-iam-__codelineno-1-14"></a><span class="w">  </span><span class="p">}</span>
<a id="__codelineno-1-15" name="__codelineno-1-15" href="#security-docs-iam-__codelineno-1-15"></a><span class="w">  </span><span class="nx">fmt</span><span class="p">.</span><span class="nx">Println</span><span class="p">(</span><span class="nx">tk</span><span class="p">)</span>
<a id="__codelineno-1-16" name="__codelineno-1-16" href="#security-docs-iam-__codelineno-1-16"></a><span class="p">}</span>
</code></pre></div>
<p>The output should resemble this:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#security-docs-iam-__codelineno-2-1"></a><span class="p">{</span>
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#security-docs-iam-__codelineno-2-2"></a><span class="w">  </span><span class="nt">&quot;kind&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;ExecCredential&quot;</span><span class="p">,</span>
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#security-docs-iam-__codelineno-2-3"></a><span class="w">  </span><span class="nt">&quot;apiVersion&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;client.authentication.k8s.io/v1alpha1&quot;</span><span class="p">,</span>
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#security-docs-iam-__codelineno-2-4"></a><span class="w">  </span><span class="nt">&quot;spec&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span>
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#security-docs-iam-__codelineno-2-5"></a><span class="w">  </span><span class="nt">&quot;status&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#security-docs-iam-__codelineno-2-6"></a><span class="w">    </span><span class="nt">&quot;expirationTimestamp&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;2020-02-19T16:08:27Z&quot;</span><span class="p">,</span>
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#security-docs-iam-__codelineno-2-7"></a><span class="w">    </span><span class="nt">&quot;token&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;k8s-aws-v1.aHR0cHM6Ly9zdHMuYW1hem9uYXdzLmNvbS8_QWN0aW9uPUdldENhbGxlcklkZW50aXR5JlZlcnNpb249MjAxMS0wNi0xNSZYLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFKTkdSSUxLTlNSQzJXNVFBJTJGMjAyMDAyMTklMkZ1cy1lYXN0LTElMkZzdHMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDIwMDIxOVQxNTU0MjdaJlgtQW16LUV4cGlyZXM9NjAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JTNCeC1rOHMtYXdzLWlkJlgtQW16LVNpZ25hdHVyZT0yMjBmOGYzNTg1ZTMyMGRkYjVlNjgzYTVjOWE0MDUzMDFhZDc2NTQ2ZjI0ZjI4MTExZmRhZDA5Y2Y2NDhhMzkz&quot;</span>
<a id="__codelineno-2-8" name="__codelineno-2-8" href="#security-docs-iam-__codelineno-2-8"></a><span class="w">  </span><span class="p">}</span>
<a id="__codelineno-2-9" name="__codelineno-2-9" href="#security-docs-iam-__codelineno-2-9"></a><span class="p">}</span>
</code></pre></div>
<p>Each token starts with <code>k8s-aws-v1.</code> followed by a base64 encoded string. The string, when decoded, should resemble to something similar to this:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#security-docs-iam-__codelineno-3-1"></a>https://sts.amazonaws.com/?Action<span class="o">=</span>GetCallerIdentity<span class="p">&amp;</span><span class="nv">Version</span><span class="o">=</span><span class="m">2011</span>-06-15<span class="p">&amp;</span>X-Amz-Algorithm<span class="o">=</span>AWS4-HMAC-SHA256<span class="p">&amp;</span>X-Amz-Credential<span class="o">=</span>XXXXJPFRILKNSRC2W5QA%2F20200219%2Fus-xxxx-1%2Fsts%2Faws4_request<span class="p">&amp;</span>X-Amz-Date<span class="o">=</span>20200219T155427Z<span class="p">&amp;</span>X-Amz-Expires<span class="o">=</span><span class="m">60</span><span class="p">&amp;</span>X-Amz-SignedHeaders<span class="o">=</span>host%3Bx-k8s-aws-id<span class="p">&amp;</span>X-Amz-Signature<span class="o">=</span>XXXf8f3285e320ddb5e683a5c9a405301ad76546f24f28111fdad09cf648a393
</code></pre></div>
<p>The token consists of a pre-signed URL that includes an Amazon credential and signature. For additional details see <a href="https://docs.aws.amazon.com/STS/latest/APIReference/API_GetCallerIdentity.html">https://docs.aws.amazon.com/STS/latest/APIReference/API_GetCallerIdentity.html</a>.</p>
<p>The token has a time to live (TTL) of 15 minutes after which a new token will need to be generated. This is handled automatically when you use a client like <code>kubectl</code>, however, if you're using the Kubernetes dashboard, you will need to generate a new token and re-authenticate each time the token expires.</p>
<p>Once the user's identity has been authenticated by the AWS IAM service, the kube-apiserver reads the <code>aws-auth</code> ConfigMap in the <code>kube-system</code> Namespace to determine the RBAC group to associate with the user.  The <code>aws-auth</code> ConfigMap is used to create a static mapping between IAM principals, i.e. IAM Users and Roles, and Kubernetes RBAC groups. RBAC groups can be referenced in Kubernetes RoleBindings or ClusterRoleBindings. They are similar to IAM Roles in that they define a set of actions (verbs) that can be performed against a collection of Kubernetes resources (objects).</p>
<h3 id="security-docs-iam-the-aws-auth-configmap">The <code>aws-auth</code> ConfigMap<a class="headerlink" href="#security-docs-iam-the-aws-auth-configmap" title="Permanent link">&para;</a></h3>
<p>One way Kubernetes integration with AWS authentication can be done is via the <code>aws-auth</code> ConfigMap, which resides in the <code>kube-system</code> Namespace. It's is responsible for mapping the AWS IAM Identities (Users, Groups, and Roles) authentication, to Kubernates role-based access control (RBAC) authorization. The <code>aws-auth</code> ConfigMap is automatically created in your Amazon EKS cluster during its provisioning phase. It was initially created to allow nodes to join your cluster, but as mentioned you can also use this ConfigMap to add RBACs access to IAM principals.</p>
<p>To check your cluster's <code>aws-auth</code> ConfigMap, you can use the following command.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#security-docs-iam-__codelineno-4-1"></a>kubectl<span class="w"> </span>-n<span class="w"> </span>kube-system<span class="w"> </span>get<span class="w"> </span>configmap<span class="w"> </span>aws-auth<span class="w"> </span>-o<span class="w"> </span>yaml
</code></pre></div>
<p>This is a sample of a default configuration of the <code>aws-auth</code> ConfigMap.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#security-docs-iam-__codelineno-5-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<a id="__codelineno-5-2" name="__codelineno-5-2" href="#security-docs-iam-__codelineno-5-2"></a><span class="nt">data</span><span class="p">:</span>
<a id="__codelineno-5-3" name="__codelineno-5-3" href="#security-docs-iam-__codelineno-5-3"></a><span class="w">  </span><span class="nt">mapRoles</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<a id="__codelineno-5-4" name="__codelineno-5-4" href="#security-docs-iam-__codelineno-5-4"></a><span class="w">    </span><span class="no">- groups:</span>
<a id="__codelineno-5-5" name="__codelineno-5-5" href="#security-docs-iam-__codelineno-5-5"></a><span class="w">      </span><span class="no">- system:bootstrappers</span>
<a id="__codelineno-5-6" name="__codelineno-5-6" href="#security-docs-iam-__codelineno-5-6"></a><span class="w">      </span><span class="no">- system:nodes</span>
<a id="__codelineno-5-7" name="__codelineno-5-7" href="#security-docs-iam-__codelineno-5-7"></a><span class="w">      </span><span class="no">- system:node-proxier</span>
<a id="__codelineno-5-8" name="__codelineno-5-8" href="#security-docs-iam-__codelineno-5-8"></a><span class="w">      </span><span class="no">rolearn: arn:aws:iam::&lt;AWS_ACCOUNT_ID&gt;:role/kube-system-&lt;SELF_GENERATED_UUID&gt;</span>
<a id="__codelineno-5-9" name="__codelineno-5-9" href="#security-docs-iam-__codelineno-5-9"></a><span class="w">      </span><span class="no">username: system:node:{{SessionName}}</span>
<a id="__codelineno-5-10" name="__codelineno-5-10" href="#security-docs-iam-__codelineno-5-10"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ConfigMap</span>
<a id="__codelineno-5-11" name="__codelineno-5-11" href="#security-docs-iam-__codelineno-5-11"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-5-12" name="__codelineno-5-12" href="#security-docs-iam-__codelineno-5-12"></a><span class="w">  </span><span class="nt">creationTimestamp</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;2023-10-22T18:19:30Z&quot;</span>
<a id="__codelineno-5-13" name="__codelineno-5-13" href="#security-docs-iam-__codelineno-5-13"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">aws-auth</span>
<a id="__codelineno-5-14" name="__codelineno-5-14" href="#security-docs-iam-__codelineno-5-14"></a><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">kube-system</span>
</code></pre></div>
<p>The main session of this ConfigMap, is under <code>data</code> in the <code>mapRoles</code> block, which is basically composed by 3 parameters.</p>
<ul>
<li><strong>groups:</strong> The Kubernetes group(s) to map the IAM Role to. This can be a default group, or a custom group specified in a <code>clusterrolebinding</code> or <code>rolebinding</code>. In the above example we have just system groups declared.</li>
<li><strong>rolearn:</strong> The ARN of the AWS IAM Role be mapped to the Kubernetes group(s) add, using the following format <code>arn:&lt;PARTITION&gt;:iam::&lt;AWS_ACCOUNT_ID&gt;:role/role-name</code>.</li>
<li><strong>username:</strong> The username within Kubernetes to map to the AWS IAM role. This can be any custom name.</li>
</ul>
<blockquote>
<p>It is also possible to map permissions for AWS IAM Users, defining a new configuration block for <code>mapUsers</code>, under <code>data</code> in the <code>aws-auth</code> ConfigMap, replacing the <strong>rolearn</strong> parameter for <strong>userarn</strong>, however as a <strong>Best Practice</strong> it's always recommended to user <code>mapRoles</code> instead.</p>
</blockquote>
<p>To manage permissions, you can edit the <code>aws-auth</code> ConfigMap adding or removing access to your Amazon EKS cluster. Although it's possible to edit the <code>aws-auth</code> ConfigMap manually, it's recommended using tools like <code>eksctl</code>, since this is a very senstitive configuration, and an inaccurate configuration can lock you outside your Amazon EKS Cluster. Check the subsection <a href="https://aws.github.io/aws-eks-best-practices/security/docs/iam/#use-tools-to-make-changes-to-the-aws-auth-configmap">Use tools to make changes to the aws-auth ConfigMap</a> below for more details.</p>
<h3 id="security-docs-iam-cluster-access-manager">Cluster Access Manager<a class="headerlink" href="#security-docs-iam-cluster-access-manager" title="Permanent link">&para;</a></h3>
<p>Cluster Access Manager, now the preferred way to manage access of AWS IAM principals to Amazon EKS clusters, is a functionality of the AWS API as is an opt-in feature for EKS v1.23 and later clusters (new or existing). It simplifies identity mapping between AWS IAM and Kubernetes RBACs, eliminating the need to switch between AWS and Kubernetes APIs or editing the the <code>aws-auth</code> ConfigMap for access management, reducing operational overhead, and helping address misconfigurations. The tool also enables cluster administrators to revoke or refine <code>cluster-admin</code> permissions automatically granted to the AWS IAM principal used to create the cluster.</p>
<p>This API relies on two concepts:</p>
<ul>
<li><strong>Access Entries:</strong> A cluster identity directly linked to an AWS IAM principal (user or role) allowed to authenticate to an Amazon EKS cluster.</li>
<li><strong>Access Policies:</strong> Are Amazon EKS specific policies that provides the authorization for an Access Entry to perform actions in the Amazon EKS cluster.</li>
</ul>
<blockquote>
<p>At launch Amazon EKS supports only predefined and AWS managed policies. Access policies are not IAM entities and are defined and managed by Amazon EKS.</p>
</blockquote>
<p>Cluster Access Manager allows the combiniation of upstream RBAC with Access Policies supporting allow and pass (but not deny) on Kubernetes AuthZ decisions regarding API server requests. A deny descision will happen when both, the upstream RBAC and Amazon EKS authorizers can't determine the outcome of a request evaluation.</p>
<p>With this feature, Amazon EKS support three modes of authentication:</p>
<ol>
<li><code>CONFIG_MAP</code> to continue using <code>aws-auth</code> configMap exclusively.</li>
<li><code>API_AND_CONFIG_MAP</code> to source authenticated IAM principals from both EKS Access Entry APIs and the <code>aws-auth</code> configMap, prioritizing the Access Entries. Ideal to migrate existing <code>aws-auth</code> permissions to Access Entries.</li>
<li><code>API</code> to exclusively rely on EKS Access Entry APIs. Being this one the new <strong>recommended approach</strong>.</li>
</ol>
<p>To get started, cluster administrators can create or update Amazon EKS clusters, setting the preferred authentication to <code>API_AND_CONFIG_MAP</code> or <code>API</code> method and define Access Entries to grant access the desired AWS IAM principals.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#security-docs-iam-__codelineno-6-1"></a>$<span class="w"> </span>aws<span class="w"> </span>eks<span class="w"> </span>create-cluster<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-6-2" name="__codelineno-6-2" href="#security-docs-iam-__codelineno-6-2"></a><span class="w">    </span>--name<span class="w"> </span>&lt;CLUSTER_NAME&gt;<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-6-3" name="__codelineno-6-3" href="#security-docs-iam-__codelineno-6-3"></a><span class="w">    </span>--role-arn<span class="w"> </span>&lt;CLUSTER_ROLE_ARN&gt;<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-6-4" name="__codelineno-6-4" href="#security-docs-iam-__codelineno-6-4"></a><span class="w">    </span>--resources-vpc-config<span class="w"> </span><span class="nv">subnetIds</span><span class="o">=</span>&lt;value&gt;,endpointPublicAccess<span class="o">=</span>true,endpointPrivateAccess<span class="o">=</span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<a id="__codelineno-6-5" name="__codelineno-6-5" href="#security-docs-iam-__codelineno-6-5"></a><span class="w">    </span>--logging<span class="w"> </span><span class="s1">&#39;{&quot;clusterLogging&quot;:[{&quot;types&quot;:[&quot;api&quot;,&quot;audit&quot;,&quot;authenticator&quot;,&quot;controllerManager&quot;,&quot;scheduler&quot;],&quot;enabled&quot;:true}]}&#39;</span><span class="w"> </span><span class="se">\</span>
<a id="__codelineno-6-6" name="__codelineno-6-6" href="#security-docs-iam-__codelineno-6-6"></a><span class="w">    </span>--access-config<span class="w"> </span><span class="nv">authenticationMode</span><span class="o">=</span>API_AND_CONFIG_MAP,bootstrapClusterCreatorAdminPermissions<span class="o">=</span><span class="nb">false</span>
</code></pre></div>
<p>The above command is an example to create an Amazon EKS cluster already without the admin permissions of the cluster creator.</p>
<p>It is possible to update Amazon EKS clusters configuration to enable <code>API</code> authenticationMode using the <code>update-cluster-config</code> command, to do that on existing clusters using <code>CONFIG_MAP</code> you will have to first update to <code>API_AND_CONFIG_MAP</code> and then to <code>API</code>. <strong>These operations cannot be reverted</strong>, meaning that's not possible to switch from <code>API</code> to <code>API_AND_CONFIG_MAP</code> or <code>CONFIG_MAP</code>, and also from <code>API_AND_CONFIG_MAP</code> to <code>CONFIG_MAP</code>.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#security-docs-iam-__codelineno-7-1"></a>$<span class="w"> </span>aws<span class="w"> </span>eks<span class="w"> </span>update-cluster-config<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-7-2" name="__codelineno-7-2" href="#security-docs-iam-__codelineno-7-2"></a><span class="w">    </span>--name<span class="w"> </span>&lt;CLUSTER_NAME&gt;<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-7-3" name="__codelineno-7-3" href="#security-docs-iam-__codelineno-7-3"></a><span class="w">    </span>--access-config<span class="w"> </span><span class="nv">authenticationMode</span><span class="o">=</span>API
</code></pre></div>
<p>The API support commands to add and revoke access to the cluster, as well as validate the existing Access Policies and Access Entries for the specified cluster. The default policies are created to match Kubernets RBACs as follows.</p>
<table>
<thead>
<tr>
<th>EKS Access Policy</th>
<th>Kubernetes RBAC</th>
</tr>
</thead>
<tbody>
<tr>
<td>AmazonEKSClusterAdminPolicy</td>
<td>cluster-admin</td>
</tr>
<tr>
<td>AmazonEKSAdminPolicy</td>
<td>admin</td>
</tr>
<tr>
<td>AmazonEKSEditPolicy</td>
<td>edit</td>
</tr>
<tr>
<td>AmazonEKSViewPolicy</td>
<td>view</td>
</tr>
</tbody>
</table>
<div class="highlight"><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1" href="#security-docs-iam-__codelineno-8-1"></a>$<span class="w"> </span>aws<span class="w"> </span>eks<span class="w"> </span>list-access-policies
<a id="__codelineno-8-2" name="__codelineno-8-2" href="#security-docs-iam-__codelineno-8-2"></a><span class="o">{</span>
<a id="__codelineno-8-3" name="__codelineno-8-3" href="#security-docs-iam-__codelineno-8-3"></a><span class="w">    </span><span class="s2">&quot;accessPolicies&quot;</span>:<span class="w"> </span><span class="o">[</span>
<a id="__codelineno-8-4" name="__codelineno-8-4" href="#security-docs-iam-__codelineno-8-4"></a><span class="w">        </span><span class="o">{</span>
<a id="__codelineno-8-5" name="__codelineno-8-5" href="#security-docs-iam-__codelineno-8-5"></a><span class="w">            </span><span class="s2">&quot;name&quot;</span>:<span class="w"> </span><span class="s2">&quot;AmazonEKSAdminPolicy&quot;</span>,
<a id="__codelineno-8-6" name="__codelineno-8-6" href="#security-docs-iam-__codelineno-8-6"></a><span class="w">            </span><span class="s2">&quot;arn&quot;</span>:<span class="w"> </span><span class="s2">&quot;arn:aws:eks::aws:cluster-access-policy/AmazonEKSAdminPolicy&quot;</span>
<a id="__codelineno-8-7" name="__codelineno-8-7" href="#security-docs-iam-__codelineno-8-7"></a><span class="w">        </span><span class="o">}</span>,
<a id="__codelineno-8-8" name="__codelineno-8-8" href="#security-docs-iam-__codelineno-8-8"></a><span class="w">        </span><span class="o">{</span>
<a id="__codelineno-8-9" name="__codelineno-8-9" href="#security-docs-iam-__codelineno-8-9"></a><span class="w">            </span><span class="s2">&quot;name&quot;</span>:<span class="w"> </span><span class="s2">&quot;AmazonEKSClusterAdminPolicy&quot;</span>,
<a id="__codelineno-8-10" name="__codelineno-8-10" href="#security-docs-iam-__codelineno-8-10"></a><span class="w">            </span><span class="s2">&quot;arn&quot;</span>:<span class="w"> </span><span class="s2">&quot;arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy&quot;</span>
<a id="__codelineno-8-11" name="__codelineno-8-11" href="#security-docs-iam-__codelineno-8-11"></a><span class="w">        </span><span class="o">}</span>,
<a id="__codelineno-8-12" name="__codelineno-8-12" href="#security-docs-iam-__codelineno-8-12"></a><span class="w">        </span><span class="o">{</span>
<a id="__codelineno-8-13" name="__codelineno-8-13" href="#security-docs-iam-__codelineno-8-13"></a><span class="w">            </span><span class="s2">&quot;name&quot;</span>:<span class="w"> </span><span class="s2">&quot;AmazonEKSEditPolicy&quot;</span>,
<a id="__codelineno-8-14" name="__codelineno-8-14" href="#security-docs-iam-__codelineno-8-14"></a><span class="w">            </span><span class="s2">&quot;arn&quot;</span>:<span class="w"> </span><span class="s2">&quot;arn:aws:eks::aws:cluster-access-policy/AmazonEKSEditPolicy&quot;</span>
<a id="__codelineno-8-15" name="__codelineno-8-15" href="#security-docs-iam-__codelineno-8-15"></a><span class="w">        </span><span class="o">}</span>,
<a id="__codelineno-8-16" name="__codelineno-8-16" href="#security-docs-iam-__codelineno-8-16"></a><span class="w">        </span><span class="o">{</span>
<a id="__codelineno-8-17" name="__codelineno-8-17" href="#security-docs-iam-__codelineno-8-17"></a><span class="w">            </span><span class="s2">&quot;name&quot;</span>:<span class="w"> </span><span class="s2">&quot;AmazonEKSViewPolicy&quot;</span>,
<a id="__codelineno-8-18" name="__codelineno-8-18" href="#security-docs-iam-__codelineno-8-18"></a><span class="w">            </span><span class="s2">&quot;arn&quot;</span>:<span class="w"> </span><span class="s2">&quot;arn:aws:eks::aws:cluster-access-policy/AmazonEKSViewPolicy&quot;</span>
<a id="__codelineno-8-19" name="__codelineno-8-19" href="#security-docs-iam-__codelineno-8-19"></a><span class="w">        </span><span class="o">}</span>
<a id="__codelineno-8-20" name="__codelineno-8-20" href="#security-docs-iam-__codelineno-8-20"></a><span class="w">    </span><span class="o">]</span>
<a id="__codelineno-8-21" name="__codelineno-8-21" href="#security-docs-iam-__codelineno-8-21"></a><span class="o">}</span>
<a id="__codelineno-8-22" name="__codelineno-8-22" href="#security-docs-iam-__codelineno-8-22"></a>
<a id="__codelineno-8-23" name="__codelineno-8-23" href="#security-docs-iam-__codelineno-8-23"></a>$<span class="w"> </span>aws<span class="w"> </span>eks<span class="w"> </span>list-access-entries<span class="w"> </span>--cluster-name<span class="w"> </span>&lt;CLUSTER_NAME&gt;
<a id="__codelineno-8-24" name="__codelineno-8-24" href="#security-docs-iam-__codelineno-8-24"></a>
<a id="__codelineno-8-25" name="__codelineno-8-25" href="#security-docs-iam-__codelineno-8-25"></a><span class="o">{</span>
<a id="__codelineno-8-26" name="__codelineno-8-26" href="#security-docs-iam-__codelineno-8-26"></a><span class="w">    </span><span class="s2">&quot;accessEntries&quot;</span>:<span class="w"> </span><span class="o">[]</span>
<a id="__codelineno-8-27" name="__codelineno-8-27" href="#security-docs-iam-__codelineno-8-27"></a><span class="o">}</span>
</code></pre></div>
<blockquote>
<p>No Access Entries are available when the cluster is created without the cluster creator admin permission, which is the only entry created by default.</p>
</blockquote>
<h2 id="security-docs-iam-cluster-access-recommendations">Cluster Access Recommendations<a class="headerlink" href="#security-docs-iam-cluster-access-recommendations" title="Permanent link">&para;</a></h2>
<h3 id="security-docs-iam-make-the-eks-cluster-endpoint-private">Make the EKS Cluster Endpoint private<a class="headerlink" href="#security-docs-iam-make-the-eks-cluster-endpoint-private" title="Permanent link">&para;</a></h3>
<p>By default when you provision an EKS cluster, the API cluster endpoint is set to public, i.e. it can be accessed from the Internet. Despite being accessible from the Internet, the endpoint is still considered secure because it requires all API requests to be authenticated by IAM and then authorized by Kubernetes RBAC. That said, if your corporate security policy mandates that you restrict access to the API from the Internet or prevents you from routing traffic outside the cluster VPC, you can:</p>
<ul>
<li>Configure the EKS cluster endpoint to be private. See <a href="https://docs.aws.amazon.com/eks/latest/userguide/cluster-endpoint.html">Modifying Cluster Endpoint Access</a> for further information on this topic.</li>
<li>Leave the cluster endpoint public and specify which CIDR blocks can communicate with the cluster endpoint. The blocks are effectively a whitelisted set of public IP addresses that are allowed to access the cluster endpoint.</li>
<li>Configure public access with a set of whitelisted CIDR blocks and set private endpoint access to enabled. This will allow public access from a specific range of public IPs while forcing all network traffic between the kubelets (workers) and the Kubernetes API through the cross-account ENIs that get provisioned into the cluster VPC when the control plane is provisioned.</li>
</ul>
<h3 id="security-docs-iam-dont-use-a-service-account-token-for-authentication">Don't use a service account token for authentication<a class="headerlink" href="#security-docs-iam-dont-use-a-service-account-token-for-authentication" title="Permanent link">&para;</a></h3>
<p>A service account token is a long-lived, static credential. If it is compromised, lost, or stolen, an attacker may be able to perform all the actions associated with that token until the service account is deleted. At times, you may need to grant an exception for applications that have to consume the Kubernetes API from outside the cluster, e.g. a CI/CD pipeline application. If such applications run on AWS infrastructure, like EC2 instances, consider using an instance profile and mapping that to a Kubernetes RBAC role.</p>
<h3 id="security-docs-iam-employ-least-privileged-access-to-aws-resources">Employ least privileged access to AWS Resources<a class="headerlink" href="#security-docs-iam-employ-least-privileged-access-to-aws-resources" title="Permanent link">&para;</a></h3>
<p>An IAM User does not need to be assigned privileges to AWS resources to access the Kubernetes API. If you need to grant an IAM user access to an EKS cluster, create an entry in the <code>aws-auth</code> ConfigMap for that user that maps to a specific Kubernetes RBAC group.</p>
<h3 id="security-docs-iam-remove-the-cluster-admin-permissions-from-the-cluster-creator-principal">Remove the cluster-admin permissions from the cluster creator principal<a class="headerlink" href="#security-docs-iam-remove-the-cluster-admin-permissions-from-the-cluster-creator-principal" title="Permanent link">&para;</a></h3>
<p>By default Amazon EKS clusters are created with a permanent <code>cluster-admin</code> permission bound to the cluster creator principal. With the Cluster Access Manager API, it's possible to create clusters without this permission setting the <code>--access-config bootstrapClusterCreatorAdminPermissions</code> to <code>false</code>, when using <code>API_AND_CONFIG_MAP</code> or <code>API</code> authentication mode. Revoke this access considered a best practice to avoid any unwanted changes to the cluster configuration. The process to revoke this access, follows the same process to revoke any other access to the cluster.</p>
<p>The API gives you flexibility to only disassociate an IAM principal from an Access Policy, in this case the <code>AmazonEKSClusterAdminPolicy</code>.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-9-1" name="__codelineno-9-1" href="#security-docs-iam-__codelineno-9-1"></a>$<span class="w"> </span>aws<span class="w"> </span>eks<span class="w"> </span>list-associated-access-policies<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-9-2" name="__codelineno-9-2" href="#security-docs-iam-__codelineno-9-2"></a><span class="w">    </span>--cluster-name<span class="w"> </span>&lt;CLUSTER_NAME&gt;<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-9-3" name="__codelineno-9-3" href="#security-docs-iam-__codelineno-9-3"></a><span class="w">    </span>--principal-arn<span class="w"> </span>&lt;IAM_PRINCIPAL_ARN&gt;
<a id="__codelineno-9-4" name="__codelineno-9-4" href="#security-docs-iam-__codelineno-9-4"></a>
<a id="__codelineno-9-5" name="__codelineno-9-5" href="#security-docs-iam-__codelineno-9-5"></a>$<span class="w"> </span>aws<span class="w"> </span>eks<span class="w"> </span>disassociate-access-policy<span class="w"> </span>--cluster-name<span class="w"> </span>&lt;CLUSTER_NAME&gt;<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-9-6" name="__codelineno-9-6" href="#security-docs-iam-__codelineno-9-6"></a><span class="w">    </span>--principal-arn<span class="w"> </span>&lt;IAM_PRINCIPAL_ARN.<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-9-7" name="__codelineno-9-7" href="#security-docs-iam-__codelineno-9-7"></a><span class="w">    </span>--policy-arn<span class="w"> </span>arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy
</code></pre></div>
<p>Or completeley removing the Access Entry associated with the <code>cluster-admin</code> permission.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-10-1" name="__codelineno-10-1" href="#security-docs-iam-__codelineno-10-1"></a>$<span class="w"> </span>aws<span class="w"> </span>eks<span class="w"> </span>list-access-entries<span class="w"> </span>--cluster-name<span class="w"> </span>&lt;CLUSTER_NAME&gt;
<a id="__codelineno-10-2" name="__codelineno-10-2" href="#security-docs-iam-__codelineno-10-2"></a>
<a id="__codelineno-10-3" name="__codelineno-10-3" href="#security-docs-iam-__codelineno-10-3"></a><span class="o">{</span>
<a id="__codelineno-10-4" name="__codelineno-10-4" href="#security-docs-iam-__codelineno-10-4"></a><span class="w">    </span><span class="s2">&quot;accessEntries&quot;</span>:<span class="w"> </span><span class="o">[]</span>
<a id="__codelineno-10-5" name="__codelineno-10-5" href="#security-docs-iam-__codelineno-10-5"></a><span class="o">}</span>
<a id="__codelineno-10-6" name="__codelineno-10-6" href="#security-docs-iam-__codelineno-10-6"></a>
<a id="__codelineno-10-7" name="__codelineno-10-7" href="#security-docs-iam-__codelineno-10-7"></a>$<span class="w"> </span>aws<span class="w"> </span>eks<span class="w"> </span>delete-access-entry<span class="w"> </span>--cluster-name<span class="w"> </span>&lt;CLUSTER_NAME&gt;<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-10-8" name="__codelineno-10-8" href="#security-docs-iam-__codelineno-10-8"></a><span class="w">  </span>--principal-arn<span class="w"> </span>&lt;IAM_PRINCIPAL_ARN&gt;
</code></pre></div>
<blockquote>
<p>This access can be granted again if needed during an incident, emergency or break glass scenario where the cluster is otherwise inaccessible.</p>
</blockquote>
<p>If the cluster still configured with the <code>CONFIG_MAP</code> authentication method, all additional users should be granted access to the cluster through the <code>aws-auth</code> ConfigMap, and after <code>aws-auth</code> ConfigMap is configured, the role assigned to the entity that created the cluster, can be deleted and only recreated in case of an incident, emergency or break glass scenario, or where the <code>aws-auth</code> ConfigMap is corrupted and the cluster is otherwise inaccessible. This can be particularly useful in production clusters.</p>
<h3 id="security-docs-iam-use-iam-roles-when-multiple-users-need-identical-access-to-the-cluster">Use IAM Roles when multiple users need identical access to the cluster<a class="headerlink" href="#security-docs-iam-use-iam-roles-when-multiple-users-need-identical-access-to-the-cluster" title="Permanent link">&para;</a></h3>
<p>Rather than creating an entry for each individual IAM User, allow those users to assume an IAM Role and map that role to a Kubernetes RBAC group.  This will be easier to maintain, especially as the number of users that require access grows.</p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>When accessing the EKS cluster with the IAM entity mapped by <code>aws-auth</code> ConfigMap, the username described is recorded in the user field of the Kubernetes audit log. If you're using an IAM role, the actual users who assume that role aren't recorded and can't be audited.</p>
</div>
<p>If still using the <code>aws-auth</code> configMap as the authentication method, when assigning K8s RBAC permissions to an IAM role, you should include {{SessionName}} in your username. That way, the audit log will record the session name so you can track who the actual user assume this role along with the CloudTrail log.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-11-1" name="__codelineno-11-1" href="#security-docs-iam-__codelineno-11-1"></a><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">rolearn</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">arn:aws:iam::XXXXXXXXXXXX:role/testRole</span>
<a id="__codelineno-11-2" name="__codelineno-11-2" href="#security-docs-iam-__codelineno-11-2"></a><span class="w">  </span><span class="nt">username</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">testRole:{{SessionName}}</span>
<a id="__codelineno-11-3" name="__codelineno-11-3" href="#security-docs-iam-__codelineno-11-3"></a><span class="w">  </span><span class="nt">groups</span><span class="p">:</span>
<a id="__codelineno-11-4" name="__codelineno-11-4" href="#security-docs-iam-__codelineno-11-4"></a><span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">system:masters</span>
</code></pre></div>
<blockquote>
<p>In Kubernetes 1.20 and above, this change is no longer required, since <code>user.extra.sessionName.0</code> was added to the Kubernetes audit log.</p>
</blockquote>
<h3 id="security-docs-iam-employ-least-privileged-access-when-creating-rolebindings-and-clusterrolebindings">Employ least privileged access when creating RoleBindings and ClusterRoleBindings<a class="headerlink" href="#security-docs-iam-employ-least-privileged-access-when-creating-rolebindings-and-clusterrolebindings" title="Permanent link">&para;</a></h3>
<p>Like the earlier point about granting access to AWS Resources, RoleBindings and ClusterRoleBindings should only include the set of permissions necessary to perform a specific function. Avoid using <code>["*"]</code> in your Roles and ClusterRoles unless it's absolutely necessary. If you're unsure what permissions to assign, consider using a tool like <a href="https://github.com/liggitt/audit2rbac">audit2rbac</a> to automatically generate Roles and binding based on the observed API calls in the Kubernetes Audit Log.</p>
<h3 id="security-docs-iam-create-cluster-using-an-automated-process">Create cluster using an automated process<a class="headerlink" href="#security-docs-iam-create-cluster-using-an-automated-process" title="Permanent link">&para;</a></h3>
<p>As seen in earlier steps, when creating an Amazon EKS cluster, if not using the using <code>API_AND_CONFIG_MAP</code> or <code>API</code> authentication mode, and not opting out to delegate <code>cluster-admin</code> permissions to the cluster creator, the IAM entity user or role, such as a federated user that creates the cluster, is automatically granted <code>system:masters</code> permissions in the cluster's RBAC configuration. Even being a best practice to remove this permission, as described <a href="#security-docs-iam-rremove-the-cluster-admin-permissions-from-the-cluster-creator-principal">here</a> if using the <code>CONFIG_MAP</code> authentication method, relying on <code>aws-auth</code> ConfigMap, this access cannot be revoked. Therefore it is a good idea to create the cluster with an infrastructure automation pipeline tied to dedicated IAM role, with no permissions to be assumed by other users or entities and regularly audit this role permissions, policies, and who have access to the trigger the pipeline. Also, this role should not be used to perform routine actions on the cluster, and be exclusively used to cluster level actions triggered by the pipeline, via SCM code changes for example.</p>
<h3 id="security-docs-iam-regularly-audit-access-to-the-cluster">Regularly audit access to the cluster<a class="headerlink" href="#security-docs-iam-regularly-audit-access-to-the-cluster" title="Permanent link">&para;</a></h3>
<p>Who requires access is likely to change over time. Plan to periodically audit the <code>aws-auth</code> ConfigMap to see who has been granted access and the rights they've been assigned. You can also use open source tooling like <a href="https://github.com/aquasecurity/kubectl-who-can">kubectl-who-can</a>, or <a href="https://github.com/FairwindsOps/rbac-lookup">rbac-lookup</a> to examine the roles bound to a particular service account, user, or group. We'll explore this topic further when we get to the section on <a href="#security-docs-detective">auditing</a>.  Additional ideas can be found in this <a href="https://www.nccgroup.trust/us/about-us/newsroom-and-events/blog/2019/august/tools-and-methods-for-auditing-kubernetes-rbac-policies/?mkt_tok=eyJpIjoiWWpGa056SXlNV1E0WWpRNSIsInQiOiJBT1hyUTRHYkg1TGxBV0hTZnRibDAyRUZ0VzBxbndnRzNGbTAxZzI0WmFHckJJbWlKdE5WWDdUQlBrYVZpMnNuTFJ1R3hacVYrRCsxYWQ2RTRcL2pMN1BtRVA1ZFZcL0NtaEtIUDdZV3pENzNLcE1zWGVwUndEXC9Pb2tmSERcL1pUaGUifQ%3D%3D">article</a> from NCC Group.</p>
<h3 id="security-docs-iam-if-relying-on-aws-auth-configmap-use-tools-to-make-changes">If relying on <code>aws-auth</code> configMap use tools to make changes<a class="headerlink" href="#security-docs-iam-if-relying-on-aws-auth-configmap-use-tools-to-make-changes" title="Permanent link">&para;</a></h3>
<p>An improperly formatted aws-auth ConfigMap may cause you to lose access to the cluster. If you need to make changes to the ConfigMap, use a tool.</p>
<p><strong>eksctl</strong>
The <code>eksctl</code> CLI includes a command for adding identity mappings to the aws-auth
ConfigMap.</p>
<p>View CLI Help:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-12-1" name="__codelineno-12-1" href="#security-docs-iam-__codelineno-12-1"></a>$<span class="w"> </span>eksctl<span class="w"> </span>create<span class="w"> </span>iamidentitymapping<span class="w"> </span>--help
<a id="__codelineno-12-2" name="__codelineno-12-2" href="#security-docs-iam-__codelineno-12-2"></a>...
</code></pre></div>
<p>Check the identities mapped to your Amazon EKS Cluster.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-13-1" name="__codelineno-13-1" href="#security-docs-iam-__codelineno-13-1"></a>$<span class="w"> </span>eksctl<span class="w"> </span>get<span class="w"> </span>iamidentitymapping<span class="w"> </span>--cluster<span class="w"> </span><span class="nv">$CLUSTER_NAME</span><span class="w"> </span>--region<span class="w"> </span><span class="nv">$AWS_REGION</span>
<a id="__codelineno-13-2" name="__codelineno-13-2" href="#security-docs-iam-__codelineno-13-2"></a>ARN<span class="w">                                                                   </span>USERNAME<span class="w">                        </span>GROUPS<span class="w">                                                  </span>ACCOUNT
<a id="__codelineno-13-3" name="__codelineno-13-3" href="#security-docs-iam-__codelineno-13-3"></a>arn:aws:iam::788355785855:role/kube-system-&lt;SELF_GENERATED_UUID&gt;<span class="w">      </span>system:node:<span class="o">{{</span>SessionName<span class="o">}}</span><span class="w">     </span>system:bootstrappers,system:nodes,system:node-proxier<span class="w">  </span>
</code></pre></div>
<p>Make an IAM Role a Cluster Admin:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-14-1" name="__codelineno-14-1" href="#security-docs-iam-__codelineno-14-1"></a>$<span class="w"> </span>eksctl<span class="w"> </span>create<span class="w"> </span>iamidentitymapping<span class="w"> </span>--cluster<span class="w">  </span>&lt;CLUSTER_NAME&gt;<span class="w"> </span>--region<span class="o">=</span>&lt;region&gt;<span class="w"> </span>--arn<span class="w"> </span>arn:aws:iam::123456:role/testing<span class="w"> </span>--group<span class="w"> </span>system:masters<span class="w"> </span>--username<span class="w"> </span>admin
<a id="__codelineno-14-2" name="__codelineno-14-2" href="#security-docs-iam-__codelineno-14-2"></a>...
</code></pre></div>
<p>For more information, review <a href="https://eksctl.io/usage/iam-identity-mappings/"><code>eksctl</code> docs</a></p>
<p><strong><a href="https://github.com/keikoproj/aws-auth">aws-auth</a> by keikoproj</strong></p>
<p><code>aws-auth</code> by keikoproj includes both a cli and a go library.</p>
<p>Download and view help CLI help:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-15-1" name="__codelineno-15-1" href="#security-docs-iam-__codelineno-15-1"></a>$<span class="w"> </span>go<span class="w"> </span>get<span class="w"> </span>github.com/keikoproj/aws-auth
<a id="__codelineno-15-2" name="__codelineno-15-2" href="#security-docs-iam-__codelineno-15-2"></a>...
<a id="__codelineno-15-3" name="__codelineno-15-3" href="#security-docs-iam-__codelineno-15-3"></a>$<span class="w"> </span>aws-auth<span class="w"> </span><span class="nb">help</span>
<a id="__codelineno-15-4" name="__codelineno-15-4" href="#security-docs-iam-__codelineno-15-4"></a>...
</code></pre></div>
<p>Alternatively, install <code>aws-auth</code> with the <a href="https://krew.sigs.k8s.io">krew plugin manager</a> for kubectl.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-16-1" name="__codelineno-16-1" href="#security-docs-iam-__codelineno-16-1"></a>$<span class="w"> </span>kubectl<span class="w"> </span>krew<span class="w"> </span>install<span class="w"> </span>aws-auth
<a id="__codelineno-16-2" name="__codelineno-16-2" href="#security-docs-iam-__codelineno-16-2"></a>...
<a id="__codelineno-16-3" name="__codelineno-16-3" href="#security-docs-iam-__codelineno-16-3"></a>$<span class="w"> </span>kubectl<span class="w"> </span>aws-auth
<a id="__codelineno-16-4" name="__codelineno-16-4" href="#security-docs-iam-__codelineno-16-4"></a>...
</code></pre></div>
<p><a href="https://github.com/keikoproj/aws-auth/blob/master/README.md">Review the aws-auth docs on GitHub</a> for more information, including the go library.</p>
<p><strong><a href="https://github.com/kubernetes-sigs/aws-iam-authenticator/tree/master/cmd/aws-iam-authenticator">AWS IAM Authenticator CLI</a></strong></p>
<p>The <code>aws-iam-authenticator</code> project includes a CLI for updating the ConfigMap.</p>
<p><a href="https://github.com/kubernetes-sigs/aws-iam-authenticator/releases">Download a release</a> on GitHub.</p>
<p>Add cluster permissions to an IAM Role:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-17-1" name="__codelineno-17-1" href="#security-docs-iam-__codelineno-17-1"></a>$<span class="w"> </span>./aws-iam-authenticator<span class="w"> </span>add<span class="w"> </span>role<span class="w"> </span>--rolearn<span class="w"> </span>arn:aws:iam::185309785115:role/lil-dev-role-cluster<span class="w"> </span>--username<span class="w"> </span>lil-dev-user<span class="w"> </span>--groups<span class="w"> </span>system:masters<span class="w"> </span>--kubeconfig<span class="w"> </span>~/.kube/config
<a id="__codelineno-17-2" name="__codelineno-17-2" href="#security-docs-iam-__codelineno-17-2"></a>...
</code></pre></div>
<h3 id="security-docs-iam-alternative-approaches-to-authentication-and-access-management">Alternative Approaches to Authentication and Access Management<a class="headerlink" href="#security-docs-iam-alternative-approaches-to-authentication-and-access-management" title="Permanent link">&para;</a></h3>
<p>While IAM is the preferred way to authenticate users who need access to an EKS cluster, it is possible to use an OIDC identity provider such as GitHub using an authentication proxy and Kubernetes <a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#user-impersonation">impersonation</a>. Posts for two such solutions have been published on the AWS Open Source blog:</p>
<ul>
<li><a href="https://aws.amazon.com/blogs/opensource/authenticating-eks-github-credentials-teleport/">Authenticating to EKS Using GitHub Credentials with Teleport</a></li>
<li><a href="https://aws.amazon.com/blogs/opensource/consistent-oidc-authentication-across-multiple-eks-clusters-using-kube-oidc-proxy/">Consistent OIDC authentication across multiple EKS clusters using kube-oidc-proxy</a></li>
</ul>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>EKS natively supports OIDC authentication without using a proxy. For further information, please read the launch blog, <a href="https://aws.amazon.com/blogs/containers/introducing-oidc-identity-provider-authentication-amazon-eks/">Introducing OIDC identity provider authentication for Amazon EKS</a>. For an example showing how to configure EKS with Dex, a popular open source OIDC provider with connectors for a variety of different authention methods, see <a href="https://aws.amazon.com/blogs/containers/using-dex-dex-k8s-authenticator-to-authenticate-to-amazon-eks/">Using Dex &amp; dex-k8s-authenticator to authenticate to Amazon EKS</a>. As described in the blogs, the username/group of users authenticated by an OIDC provider will appear in the Kubernetes audit log.</p>
</div>
<p>You can also use <a href="https://docs.aws.amazon.com/singlesignon/latest/userguide/what-is.html">AWS SSO</a> to federate AWS with an external identity provider, e.g. Azure AD. If you decide to use this, the AWS CLI v2.0 includes an option to create a named profile that makes it easy to associate an SSO session with your current CLI session and assume an IAM role. Know that you must assume a role <em>prior</em> to running <code>kubectl</code> as the IAM role is used to determine the user's Kubernetes RBAC group.</p>
<h3 id="security-docs-iam-additional-resources">Additional Resources<a class="headerlink" href="#security-docs-iam-additional-resources" title="Permanent link">&para;</a></h3>
<p><a href="https://github.com/mhausenblas/rbac.dev">rbac.dev</a> A list of additional resources, including blogs and tools, for Kubernetes RBAC</p>
<h2 id="security-docs-iam-pods-identities">Pods Identities<a class="headerlink" href="#security-docs-iam-pods-identities" title="Permanent link">&para;</a></h2>
<p>Certain applications that run within a Kubernetes cluster need permission to call the Kubernetes API to function properly. For example, the <a href="https://github.com/kubernetes-sigs/aws-load-balancer-controller">AWS Load Balancer Controller</a> needs to be able to list a Service's Endpoints. The controller also needs to be able to invoke AWS APIs to provision and configure an ALB.  In this section we will explore the best practices for assigning rights and privileges to Pods.</p>
<h3 id="security-docs-iam-kubernetes-service-accounts">Kubernetes Service Accounts<a class="headerlink" href="#security-docs-iam-kubernetes-service-accounts" title="Permanent link">&para;</a></h3>
<p>A service account is a special type of object that allows you to assign a Kubernetes RBAC role to a pod.  A default service account is created automatically for each Namespace within a cluster. When you deploy a pod into a Namespace without referencing a specific service account, the default service account for that Namespace will automatically get assigned to the Pod and the Secret, i.e. the service account (JWT) token for that service account, will get mounted to the pod as a volume at <code>/var/run/secrets/kubernetes.io/serviceaccount</code>. Decoding the service account token in that directory will reveal the following metadata:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-18-1" name="__codelineno-18-1" href="#security-docs-iam-__codelineno-18-1"></a><span class="p">{</span>
<a id="__codelineno-18-2" name="__codelineno-18-2" href="#security-docs-iam-__codelineno-18-2"></a><span class="w">  </span><span class="nt">&quot;iss&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;kubernetes/serviceaccount&quot;</span><span class="p">,</span>
<a id="__codelineno-18-3" name="__codelineno-18-3" href="#security-docs-iam-__codelineno-18-3"></a><span class="w">  </span><span class="nt">&quot;kubernetes.io/serviceaccount/namespace&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;default&quot;</span><span class="p">,</span>
<a id="__codelineno-18-4" name="__codelineno-18-4" href="#security-docs-iam-__codelineno-18-4"></a><span class="w">  </span><span class="nt">&quot;kubernetes.io/serviceaccount/secret.name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;default-token-5pv4z&quot;</span><span class="p">,</span>
<a id="__codelineno-18-5" name="__codelineno-18-5" href="#security-docs-iam-__codelineno-18-5"></a><span class="w">  </span><span class="nt">&quot;kubernetes.io/serviceaccount/service-account.name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;default&quot;</span><span class="p">,</span>
<a id="__codelineno-18-6" name="__codelineno-18-6" href="#security-docs-iam-__codelineno-18-6"></a><span class="w">  </span><span class="nt">&quot;kubernetes.io/serviceaccount/service-account.uid&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;3b36ddb5-438c-11ea-9438-063a49b60fba&quot;</span><span class="p">,</span>
<a id="__codelineno-18-7" name="__codelineno-18-7" href="#security-docs-iam-__codelineno-18-7"></a><span class="w">  </span><span class="nt">&quot;sub&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;system:serviceaccount:default:default&quot;</span>
<a id="__codelineno-18-8" name="__codelineno-18-8" href="#security-docs-iam-__codelineno-18-8"></a><span class="p">}</span>
</code></pre></div>
<p>The default service account has the following permissions to the Kubernetes API.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-19-1" name="__codelineno-19-1" href="#security-docs-iam-__codelineno-19-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">rbac.authorization.k8s.io/v1</span>
<a id="__codelineno-19-2" name="__codelineno-19-2" href="#security-docs-iam-__codelineno-19-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ClusterRole</span>
<a id="__codelineno-19-3" name="__codelineno-19-3" href="#security-docs-iam-__codelineno-19-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-19-4" name="__codelineno-19-4" href="#security-docs-iam-__codelineno-19-4"></a><span class="w">  </span><span class="nt">annotations</span><span class="p">:</span>
<a id="__codelineno-19-5" name="__codelineno-19-5" href="#security-docs-iam-__codelineno-19-5"></a><span class="w">    </span><span class="nt">rbac.authorization.kubernetes.io/autoupdate</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;true&quot;</span>
<a id="__codelineno-19-6" name="__codelineno-19-6" href="#security-docs-iam-__codelineno-19-6"></a><span class="w">  </span><span class="nt">creationTimestamp</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;2020-01-30T18:13:25Z&quot;</span>
<a id="__codelineno-19-7" name="__codelineno-19-7" href="#security-docs-iam-__codelineno-19-7"></a><span class="w">  </span><span class="nt">labels</span><span class="p">:</span>
<a id="__codelineno-19-8" name="__codelineno-19-8" href="#security-docs-iam-__codelineno-19-8"></a><span class="w">    </span><span class="nt">kubernetes.io/bootstrapping</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">rbac-defaults</span>
<a id="__codelineno-19-9" name="__codelineno-19-9" href="#security-docs-iam-__codelineno-19-9"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">system:discovery</span>
<a id="__codelineno-19-10" name="__codelineno-19-10" href="#security-docs-iam-__codelineno-19-10"></a><span class="w">  </span><span class="nt">resourceVersion</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;43&quot;</span>
<a id="__codelineno-19-11" name="__codelineno-19-11" href="#security-docs-iam-__codelineno-19-11"></a><span class="w">  </span><span class="nt">selfLink</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Adiscovery</span>
<a id="__codelineno-19-12" name="__codelineno-19-12" href="#security-docs-iam-__codelineno-19-12"></a><span class="w">  </span><span class="nt">uid</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">350d2ab8-438c-11ea-9438-063a49b60fba</span>
<a id="__codelineno-19-13" name="__codelineno-19-13" href="#security-docs-iam-__codelineno-19-13"></a><span class="nt">rules</span><span class="p">:</span>
<a id="__codelineno-19-14" name="__codelineno-19-14" href="#security-docs-iam-__codelineno-19-14"></a><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">nonResourceURLs</span><span class="p">:</span>
<a id="__codelineno-19-15" name="__codelineno-19-15" href="#security-docs-iam-__codelineno-19-15"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/api</span>
<a id="__codelineno-19-16" name="__codelineno-19-16" href="#security-docs-iam-__codelineno-19-16"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/api/*</span>
<a id="__codelineno-19-17" name="__codelineno-19-17" href="#security-docs-iam-__codelineno-19-17"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/apis</span>
<a id="__codelineno-19-18" name="__codelineno-19-18" href="#security-docs-iam-__codelineno-19-18"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/apis/*</span>
<a id="__codelineno-19-19" name="__codelineno-19-19" href="#security-docs-iam-__codelineno-19-19"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/healthz</span>
<a id="__codelineno-19-20" name="__codelineno-19-20" href="#security-docs-iam-__codelineno-19-20"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/openapi</span>
<a id="__codelineno-19-21" name="__codelineno-19-21" href="#security-docs-iam-__codelineno-19-21"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/openapi/*</span>
<a id="__codelineno-19-22" name="__codelineno-19-22" href="#security-docs-iam-__codelineno-19-22"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/version</span>
<a id="__codelineno-19-23" name="__codelineno-19-23" href="#security-docs-iam-__codelineno-19-23"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/version/</span>
<a id="__codelineno-19-24" name="__codelineno-19-24" href="#security-docs-iam-__codelineno-19-24"></a><span class="w">  </span><span class="nt">verbs</span><span class="p">:</span>
<a id="__codelineno-19-25" name="__codelineno-19-25" href="#security-docs-iam-__codelineno-19-25"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">get</span>
</code></pre></div>
<p>This role authorizes unauthenticated and authenticated users to read API information and is deemed safe to be publicly accessible.</p>
<p>When an application running within a Pod calls the Kubernetes APIs, the Pod needs to be assigned a service account that explicitly grants it permission to call those APIs.  Similar to guidelines for user access, the Role or ClusterRole bound to a service account should be restricted to the API resources and methods that the application needs to function and nothing else. To use a non-default service account simply set the <code>spec.serviceAccountName</code> field of a Pod to the name of the service account you wish to use. For additional information about creating service accounts, see <a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/#service-account-permissions">https://kubernetes.io/docs/reference/access-authn-authz/rbac/#service-account-permissions</a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Prior to Kubernetes 1.24, Kubernetes would automatically create a secret for each a service account. This secret was mounted to the pod at /var/run/secrets/kubernetes.io/serviceaccount and would be used by the pod to authenticate to the Kubernetes API server. In Kubernetes 1.24, a service account token is dynamically generated when the pod runs and is only valid for an hour by default. A secret for the service account will not be created. If you have an application that runs outside the cluster that needs to authenticate to the Kubernetes API, e.g. Jenkins, you will need to create a secret of type <code>kubernetes.io/service-account-token</code> along with an annotation that references the service account such as <code>metadata.annotations.kubernetes.io/service-account.name: &lt;SERVICE_ACCOUNT_NAME&gt;</code>. Secrets created in this way do not expire.</p>
</div>
<h3 id="security-docs-iam-iam-roles-for-service-accounts-irsa">IAM Roles for Service Accounts (IRSA)<a class="headerlink" href="#security-docs-iam-iam-roles-for-service-accounts-irsa" title="Permanent link">&para;</a></h3>
<p>IRSA is a feature that allows you to assign an IAM role to a Kubernetes service account. It works by leveraging a Kubernetes feature known as <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection">Service Account Token Volume Projection</a>. When Pods are configured with a Service Account that references an IAM Role, the Kubernetes API server will call the public OIDC discovery endpoint for the cluster on startup. The endpoint cryptographically signs the OIDC token issued by Kubernetes and the resulting token mounted as a volume. This signed token allows the Pod to call the AWS APIs associated IAM role. When an AWS API is invoked, the AWS SDKs calls <code>sts:AssumeRoleWithWebIdentity</code>. After validating the token's signature, IAM exchanges the Kubernetes issued token for a temporary AWS role credential.</p>
<p>Decoding the (JWT) token for IRSA will produce output similar to the example you see below:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-20-1" name="__codelineno-20-1" href="#security-docs-iam-__codelineno-20-1"></a><span class="p">{</span>
<a id="__codelineno-20-2" name="__codelineno-20-2" href="#security-docs-iam-__codelineno-20-2"></a><span class="w">  </span><span class="nt">&quot;aud&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<a id="__codelineno-20-3" name="__codelineno-20-3" href="#security-docs-iam-__codelineno-20-3"></a><span class="w">    </span><span class="s2">&quot;sts.amazonaws.com&quot;</span>
<a id="__codelineno-20-4" name="__codelineno-20-4" href="#security-docs-iam-__codelineno-20-4"></a><span class="w">  </span><span class="p">],</span>
<a id="__codelineno-20-5" name="__codelineno-20-5" href="#security-docs-iam-__codelineno-20-5"></a><span class="w">  </span><span class="nt">&quot;exp&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1582306514</span><span class="p">,</span>
<a id="__codelineno-20-6" name="__codelineno-20-6" href="#security-docs-iam-__codelineno-20-6"></a><span class="w">  </span><span class="nt">&quot;iat&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1582220114</span><span class="p">,</span>
<a id="__codelineno-20-7" name="__codelineno-20-7" href="#security-docs-iam-__codelineno-20-7"></a><span class="w">  </span><span class="nt">&quot;iss&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;https://oidc.eks.us-west-2.amazonaws.com/id/D43CF17C27A865933144EA99A26FB128&quot;</span><span class="p">,</span>
<a id="__codelineno-20-8" name="__codelineno-20-8" href="#security-docs-iam-__codelineno-20-8"></a><span class="w">  </span><span class="nt">&quot;kubernetes.io&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<a id="__codelineno-20-9" name="__codelineno-20-9" href="#security-docs-iam-__codelineno-20-9"></a><span class="w">    </span><span class="nt">&quot;namespace&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;default&quot;</span><span class="p">,</span>
<a id="__codelineno-20-10" name="__codelineno-20-10" href="#security-docs-iam-__codelineno-20-10"></a><span class="w">    </span><span class="nt">&quot;pod&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<a id="__codelineno-20-11" name="__codelineno-20-11" href="#security-docs-iam-__codelineno-20-11"></a><span class="w">      </span><span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;alpine-57b5664646-rf966&quot;</span><span class="p">,</span>
<a id="__codelineno-20-12" name="__codelineno-20-12" href="#security-docs-iam-__codelineno-20-12"></a><span class="w">      </span><span class="nt">&quot;uid&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;5a20f883-5407-11ea-a85c-0e62b7a4a436&quot;</span>
<a id="__codelineno-20-13" name="__codelineno-20-13" href="#security-docs-iam-__codelineno-20-13"></a><span class="w">    </span><span class="p">},</span>
<a id="__codelineno-20-14" name="__codelineno-20-14" href="#security-docs-iam-__codelineno-20-14"></a><span class="w">    </span><span class="nt">&quot;serviceaccount&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<a id="__codelineno-20-15" name="__codelineno-20-15" href="#security-docs-iam-__codelineno-20-15"></a><span class="w">      </span><span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;s3-read-only&quot;</span><span class="p">,</span>
<a id="__codelineno-20-16" name="__codelineno-20-16" href="#security-docs-iam-__codelineno-20-16"></a><span class="w">      </span><span class="nt">&quot;uid&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;a720ba5c-5406-11ea-9438-063a49b60fba&quot;</span>
<a id="__codelineno-20-17" name="__codelineno-20-17" href="#security-docs-iam-__codelineno-20-17"></a><span class="w">    </span><span class="p">}</span>
<a id="__codelineno-20-18" name="__codelineno-20-18" href="#security-docs-iam-__codelineno-20-18"></a><span class="w">  </span><span class="p">},</span>
<a id="__codelineno-20-19" name="__codelineno-20-19" href="#security-docs-iam-__codelineno-20-19"></a><span class="w">  </span><span class="nt">&quot;nbf&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1582220114</span><span class="p">,</span>
<a id="__codelineno-20-20" name="__codelineno-20-20" href="#security-docs-iam-__codelineno-20-20"></a><span class="w">  </span><span class="nt">&quot;sub&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;system:serviceaccount:default:s3-read-only&quot;</span>
<a id="__codelineno-20-21" name="__codelineno-20-21" href="#security-docs-iam-__codelineno-20-21"></a><span class="p">}</span>
</code></pre></div>
<p>This particular token grants the Pod view-only privileges to S3. When the application attempts to read from S3, the token is exchanged for a temporary set of IAM credentials that resembles this:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-21-1" name="__codelineno-21-1" href="#security-docs-iam-__codelineno-21-1"></a><span class="p">{</span>
<a id="__codelineno-21-2" name="__codelineno-21-2" href="#security-docs-iam-__codelineno-21-2"></a><span class="w">    </span><span class="nt">&quot;AssumedRoleUser&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<a id="__codelineno-21-3" name="__codelineno-21-3" href="#security-docs-iam-__codelineno-21-3"></a><span class="w">        </span><span class="nt">&quot;AssumedRoleId&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;AROA36C6WWEJULFUYMPB6:abc&quot;</span><span class="p">,</span>
<a id="__codelineno-21-4" name="__codelineno-21-4" href="#security-docs-iam-__codelineno-21-4"></a><span class="w">        </span><span class="nt">&quot;Arn&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;arn:aws:sts::123456789012:assumed-role/eksctl-winterfell-addon-iamserviceaccount-de-Role1-1D61LT75JH3MB/abc&quot;</span>
<a id="__codelineno-21-5" name="__codelineno-21-5" href="#security-docs-iam-__codelineno-21-5"></a><span class="w">    </span><span class="p">},</span>
<a id="__codelineno-21-6" name="__codelineno-21-6" href="#security-docs-iam-__codelineno-21-6"></a><span class="w">    </span><span class="nt">&quot;Audience&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;sts.amazonaws.com&quot;</span><span class="p">,</span>
<a id="__codelineno-21-7" name="__codelineno-21-7" href="#security-docs-iam-__codelineno-21-7"></a><span class="w">    </span><span class="nt">&quot;Provider&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-west-2.amazonaws.com/id/D43CF17C27A865933144EA99A26FB128&quot;</span><span class="p">,</span>
<a id="__codelineno-21-8" name="__codelineno-21-8" href="#security-docs-iam-__codelineno-21-8"></a><span class="w">    </span><span class="nt">&quot;SubjectFromWebIdentityToken&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;system:serviceaccount:default:s3-read-only&quot;</span><span class="p">,</span>
<a id="__codelineno-21-9" name="__codelineno-21-9" href="#security-docs-iam-__codelineno-21-9"></a><span class="w">    </span><span class="nt">&quot;Credentials&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<a id="__codelineno-21-10" name="__codelineno-21-10" href="#security-docs-iam-__codelineno-21-10"></a><span class="w">        </span><span class="nt">&quot;SecretAccessKey&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;ORJ+8Adk+wW+nU8FETq7+mOqeA8Z6jlPihnV8hX1&quot;</span><span class="p">,</span>
<a id="__codelineno-21-11" name="__codelineno-21-11" href="#security-docs-iam-__codelineno-21-11"></a><span class="w">        </span><span class="nt">&quot;SessionToken&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;FwoGZXIvYXdzEGMaDMLxAZkuLpmSwYXShiL9A1S0X87VBC1mHCrRe/pB2oes+l1eXxUYnPJyC9ayOoXMvqXQsomq0xs6OqZ3vaa5Iw1HIyA4Cv1suLaOCoU3hNvOIJ6C94H1vU0siQYk7DIq9Av5RZe+uE2FnOctNBvYLd3i0IZo1ajjc00yRK3v24VRq9nQpoPLuqyH2jzlhCEjXuPScPbi5KEVs9fNcOTtgzbVf7IG2gNiwNs5aCpN4Bv/Zv2A6zp5xGz9cWj2f0aD9v66vX4bexOs5t/YYhwuwAvkkJPSIGvxja0xRThnceHyFHKtj0H+bi/PWAtlI8YJcDX69cM30JAHDdQH+ltm/4scFptW1hlvMaP+WReCAaCrsHrAT+yka7ttw5YlUyvZ8EPog+j6fwHlxmrXM9h1BqdikomyJU00gm1++FJelfP+1zAwcyrxCnbRl3ARFrAt8hIlrT6Vyu8WvWtLxcI8KcLcJQb/LgkW+sCTGlYcY8z3zkigJMbYn07ewTL5Ss7LazTJJa758I7PZan/v3xQHd5DEc5WBneiV3iOznDFgup0VAMkIviVjVCkszaPSVEdK2NU7jtrh6Jfm7bU/3P6ZG+CkyDLIa8MBn9KPXeJd/y+jTk5Ii+fIwO/+mDpGNUribg6TPxhzZ8b/XdZO1kS1gVgqjXyVC+M+BRBh6C4H21w/eMzjCtDIpoxt5rGKL6Nu/IFMipoC4fgx6LIIHwtGYMG7SWQi7OsMAkiwZRg0n68/RqWgLzBt/4pfjSRYuk=&quot;</span><span class="p">,</span>
<a id="__codelineno-21-12" name="__codelineno-21-12" href="#security-docs-iam-__codelineno-21-12"></a><span class="w">        </span><span class="nt">&quot;Expiration&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;2020-02-20T18:49:50Z&quot;</span><span class="p">,</span>
<a id="__codelineno-21-13" name="__codelineno-21-13" href="#security-docs-iam-__codelineno-21-13"></a><span class="w">        </span><span class="nt">&quot;AccessKeyId&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;XXXX36C6WWEJUMHA3L7Z&quot;</span>
<a id="__codelineno-21-14" name="__codelineno-21-14" href="#security-docs-iam-__codelineno-21-14"></a><span class="w">    </span><span class="p">}</span>
<a id="__codelineno-21-15" name="__codelineno-21-15" href="#security-docs-iam-__codelineno-21-15"></a><span class="p">}</span>
</code></pre></div>
<p>A mutating webhook that runs as part of the EKS control plane injects the AWS Role ARN and the path to a web identity token file into the Pod as environment variables. These values can also be supplied manually.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-22-1" name="__codelineno-22-1" href="#security-docs-iam-__codelineno-22-1"></a><span class="nv">AWS_ROLE_ARN</span><span class="o">=</span>arn:aws:iam::AWS_ACCOUNT_ID:role/IAM_ROLE_NAME
<a id="__codelineno-22-2" name="__codelineno-22-2" href="#security-docs-iam-__codelineno-22-2"></a><span class="nv">AWS_WEB_IDENTITY_TOKEN_FILE</span><span class="o">=</span>/var/run/secrets/eks.amazonaws.com/serviceaccount/token
</code></pre></div>
<p>The kubelet will automatically rotate the projected token when it is older than 80% of its total TTL, or after 24 hours. The AWS SDKs are responsible for reloading the token when it rotates. For further information about IRSA, see <a href="https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts-technical-overview.html">https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts-technical-overview.html</a>.</p>
<h2 id="security-docs-iam-pod-identity-recommendations">Pod Identity Recommendations<a class="headerlink" href="#security-docs-iam-pod-identity-recommendations" title="Permanent link">&para;</a></h2>
<h3 id="security-docs-iam-update-the-aws-node-daemonset-to-use-irsa">Update the aws-node daemonset to use IRSA<a class="headerlink" href="#security-docs-iam-update-the-aws-node-daemonset-to-use-irsa" title="Permanent link">&para;</a></h3>
<p>At present, the aws-node daemonset is configured to use a role assigned to the EC2 instances to assign IPs to pods.  This role includes several AWS managed policies, e.g. AmazonEKS_CNI_Policy and EC2ContainerRegistryReadOnly that effectively allow <strong>all</strong> pods running on a node to attach/detach ENIs, assign/unassign IP addresses, or pull images from ECR. Since this presents a risk to your cluster, it is recommended that you update the aws-node daemonset to use IRSA. A script for doing this can be found in the <a href="https://github.com/aws/aws-eks-best-practices/tree/master/projects/enable-irsa/src">repository</a> for this guide.</p>
<h3 id="security-docs-iam-restrict-access-to-the-instance-profile-assigned-to-the-worker-node">Restrict access to the instance profile assigned to the worker node<a class="headerlink" href="#security-docs-iam-restrict-access-to-the-instance-profile-assigned-to-the-worker-node" title="Permanent link">&para;</a></h3>
<p>When you use IRSA, it updates the credential chain of the pod to use the IRSA token, however, the pod <em>can still inherit the rights of the instance profile assigned to the worker node</em>. When using IRSA, it is <strong>strongly</strong> recommended that you block access <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/configuring-instance-metadata-service.html">instance metadata</a> to minimize the blast radius of a breach.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>Blocking access to instance metadata will prevent pods that do not use IRSA from inheriting the role assigned to the worker node.</p>
</div>
<p>You can block access to instance metadata by requiring the instance to use IMDSv2 only and updating the hop count to 1 as in the example below. You can also include these settings in the node group's launch template. Do <strong>not</strong> disable instance metadata as this will prevent components like the node termination handler and other things that rely on instance metadata from working properly.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-23-1" name="__codelineno-23-1" href="#security-docs-iam-__codelineno-23-1"></a>$<span class="w"> </span>aws<span class="w"> </span>ec2<span class="w"> </span>modify-instance-metadata-options<span class="w"> </span>--instance-id<span class="w"> </span>&lt;value&gt;<span class="w"> </span>--http-tokens<span class="w"> </span>required<span class="w"> </span>--http-put-response-hop-limit<span class="w"> </span><span class="m">1</span>
<a id="__codelineno-23-2" name="__codelineno-23-2" href="#security-docs-iam-__codelineno-23-2"></a>...
</code></pre></div>
<p>If you are using Terraform to create launch templates for use with Managed Node Groups, add the metadata block to configure the hop count as seen in this code snippet:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-24-1" name="__codelineno-24-1" href="#security-docs-iam-__codelineno-24-1"></a><span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_launch_template&quot;</span><span class="w"> </span><span class="nv">&quot;foo&quot;</span><span class="w"> </span><span class="p">{</span>
<a id="__codelineno-24-2" name="__codelineno-24-2" href="#security-docs-iam-__codelineno-24-2"></a><span class="w">  </span><span class="na">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;foo&quot;</span>
<a id="__codelineno-24-3" name="__codelineno-24-3" href="#security-docs-iam-__codelineno-24-3"></a><span class="w">  </span><span class="p">...</span>
<a id="__codelineno-24-4" name="__codelineno-24-4" href="#security-docs-iam-__codelineno-24-4"></a><span class="w">    </span><span class="nb">metadata_options</span><span class="w"> </span><span class="p">{</span>
<a id="__codelineno-24-5" name="__codelineno-24-5" href="#security-docs-iam-__codelineno-24-5"></a><span class="w">    </span><span class="na">http_endpoint</span><span class="w">               </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;enabled&quot;</span>
<a id="__codelineno-24-6" name="__codelineno-24-6" href="#security-docs-iam-__codelineno-24-6"></a><span class="w">    </span><span class="na">http_tokens</span><span class="w">                 </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;required&quot;</span>
<a id="__codelineno-24-7" name="__codelineno-24-7" href="#security-docs-iam-__codelineno-24-7"></a><span class="hll"><span class="w">    </span><span class="na">http_put_response_hop_limit</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span>
</span><a id="__codelineno-24-8" name="__codelineno-24-8" href="#security-docs-iam-__codelineno-24-8"></a><span class="w">    </span><span class="na">instance_metadata_tags</span><span class="w">      </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;enabled&quot;</span>
<a id="__codelineno-24-9" name="__codelineno-24-9" href="#security-docs-iam-__codelineno-24-9"></a><span class="w">  </span><span class="p">}</span>
<a id="__codelineno-24-10" name="__codelineno-24-10" href="#security-docs-iam-__codelineno-24-10"></a><span class="w">  </span><span class="p">...</span>
</code></pre></div>
<p>You can also block a pod's access to EC2 metadata by manipulating iptables on the node. For further information about this method, see <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html#instance-metadata-limiting-access">Limiting access to the instance metadata service</a>.</p>
<p>If you have an application that is using an older version of the AWS SDK that doesn't support IRSA, you should update the SDK version.</p>
<h3 id="security-docs-iam-scope-the-iam-role-trust-policy-for-irsa-to-the-service-account-name">Scope the IAM Role trust policy for IRSA to the service account name<a class="headerlink" href="#security-docs-iam-scope-the-iam-role-trust-policy-for-irsa-to-the-service-account-name" title="Permanent link">&para;</a></h3>
<p>The trust policy can be scoped to a Namespace or a specific service account within a Namespace. When using IRSA it's best to make the role trust policy as explicit as possible by including the service account name. This will effectively prevent other Pods within the same Namespace from assuming the role. The CLI <code>eksctl</code> will do this automatically when you use it to create service accounts/IAM roles. See <a href="https://eksctl.io/usage/iamserviceaccounts/">https://eksctl.io/usage/iamserviceaccounts/</a> for further information.</p>
<h3 id="security-docs-iam-when-your-application-needs-access-to-imds-use-imdsv2-and-increase-the-hop-limit-on-ec2-instances-to-2">When your application needs access to IMDS, use IMDSv2 and increase the hop limit on EC2 instances to 2<a class="headerlink" href="#security-docs-iam-when-your-application-needs-access-to-imds-use-imdsv2-and-increase-the-hop-limit-on-ec2-instances-to-2" title="Permanent link">&para;</a></h3>
<p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/configuring-instance-metadata-service.html">IMDSv2</a> requires you use a PUT request to get a session token.  The initial PUT request has to include a TTL for the session token.  Newer versions of the AWS SDKs will handle this and the renewal of said token automatically. It's also important to be aware that the default hop limit on EC2 instances is intentionally set to 1 to prevent IP forwarding. As a consequence, Pods that request a session token that are run on EC2 instances may eventually time out and fallback to using the IMDSv1 data flow. EKS adds support IMDSv2 by <em>enabling</em> both v1 and v2 and changing the hop limit to 2 on nodes provisioned by eksctl or with the official CloudFormation templates.</p>
<h3 id="security-docs-iam-disable-auto-mounting-of-service-account-tokens">Disable auto-mounting of service account tokens<a class="headerlink" href="#security-docs-iam-disable-auto-mounting-of-service-account-tokens" title="Permanent link">&para;</a></h3>
<p>If your application doesn't need to call the Kubernetes API set the <code>automountServiceAccountToken</code> attribute to <code>false</code> in the PodSpec for your application or patch the default service account in each namespace so that it's no longer mounted to pods automatically. For example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-25-1" name="__codelineno-25-1" href="#security-docs-iam-__codelineno-25-1"></a>kubectl<span class="w"> </span>patch<span class="w"> </span>serviceaccount<span class="w"> </span>default<span class="w"> </span>-p<span class="w"> </span><span class="s1">$&#39;automountServiceAccountToken: false&#39;</span>
</code></pre></div>
<h3 id="security-docs-iam-use-dedicated-service-accounts-for-each-application">Use dedicated service accounts for each application<a class="headerlink" href="#security-docs-iam-use-dedicated-service-accounts-for-each-application" title="Permanent link">&para;</a></h3>
<p>Each application should have its own dedicated service account.  This applies to service accounts for the Kubernetes API as well as IRSA.</p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>If you employ a blue/green approach to cluster upgrades instead of performing an in-place cluster upgrade, you will need to update the trust policy of each of the IRSA IAM roles with the OIDC endpoint of the new cluster. A blue/green cluster upgrade is where you create a cluster running a newer version of Kubernetes alongside the old cluster and use a load balancer or a service mesh to seamlessly shift traffic from services running on the old cluster to the new cluster.</p>
</div>
<h3 id="security-docs-iam-run-the-application-as-a-non-root-user">Run the application as a non-root user<a class="headerlink" href="#security-docs-iam-run-the-application-as-a-non-root-user" title="Permanent link">&para;</a></h3>
<p>Containers run as root by default. While this allows them to read the web identity token file, running a container as root is not considered a best practice. As an alternative, consider adding the <code>spec.securityContext.runAsUser</code> attribute to the PodSpec.  The value of <code>runAsUser</code> is arbitrary value.</p>
<p>In the following example, all processes within the Pod will run under the user ID specified in the <code>runAsUser</code> field.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-26-1" name="__codelineno-26-1" href="#security-docs-iam-__codelineno-26-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<a id="__codelineno-26-2" name="__codelineno-26-2" href="#security-docs-iam-__codelineno-26-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Pod</span>
<a id="__codelineno-26-3" name="__codelineno-26-3" href="#security-docs-iam-__codelineno-26-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-26-4" name="__codelineno-26-4" href="#security-docs-iam-__codelineno-26-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">security-context-demo</span>
<a id="__codelineno-26-5" name="__codelineno-26-5" href="#security-docs-iam-__codelineno-26-5"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-26-6" name="__codelineno-26-6" href="#security-docs-iam-__codelineno-26-6"></a><span class="w">  </span><span class="nt">securityContext</span><span class="p">:</span>
<a id="__codelineno-26-7" name="__codelineno-26-7" href="#security-docs-iam-__codelineno-26-7"></a><span class="w">    </span><span class="nt">runAsUser</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1000</span>
<a id="__codelineno-26-8" name="__codelineno-26-8" href="#security-docs-iam-__codelineno-26-8"></a><span class="w">    </span><span class="nt">runAsGroup</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">3000</span>
<a id="__codelineno-26-9" name="__codelineno-26-9" href="#security-docs-iam-__codelineno-26-9"></a><span class="w">  </span><span class="nt">containers</span><span class="p">:</span>
<a id="__codelineno-26-10" name="__codelineno-26-10" href="#security-docs-iam-__codelineno-26-10"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">sec-ctx-demo</span>
<a id="__codelineno-26-11" name="__codelineno-26-11" href="#security-docs-iam-__codelineno-26-11"></a><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">busybox</span>
<a id="__codelineno-26-12" name="__codelineno-26-12" href="#security-docs-iam-__codelineno-26-12"></a><span class="w">    </span><span class="nt">command</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="w"> </span><span class="s">&quot;sh&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;-c&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;sleep</span><span class="nv"> </span><span class="s">1h&quot;</span><span class="w"> </span><span class="p p-Indicator">]</span>
</code></pre></div>
<p>When you run a container as a non-root user, it prevents the container from reading the IRSA service account token because the token is assigned 0600 [root] permissions by default. If you update the securityContext for your container to include fsgroup=65534 [Nobody] it will allow the container to read the token.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-27-1" name="__codelineno-27-1" href="#security-docs-iam-__codelineno-27-1"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-27-2" name="__codelineno-27-2" href="#security-docs-iam-__codelineno-27-2"></a><span class="w">  </span><span class="nt">securityContext</span><span class="p">:</span>
<a id="__codelineno-27-3" name="__codelineno-27-3" href="#security-docs-iam-__codelineno-27-3"></a><span class="w">    </span><span class="nt">fsGroup</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">65534</span>
</code></pre></div>
<p>In Kubernetes 1.19 and above, this change is no longer required.</p>
<h3 id="security-docs-iam-grant-least-privileged-access-to-applications">Grant least privileged access to applications<a class="headerlink" href="#security-docs-iam-grant-least-privileged-access-to-applications" title="Permanent link">&para;</a></h3>
<p><a href="https://github.com/princespaghetti/actionhero">Action Hero</a> is a utility that you can run alongside your application to identify the AWS API calls and corresponding IAM permissions your application needs to function properly.  It is similar to <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_access-advisor.html">IAM Access Advisor</a> in that it helps you gradually limit the scope of IAM roles assigned to applications. Consult the documentation on granting <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#grant-least-privilege">least privileged access</a> to AWS resources for further information.</p>
<h3 id="security-docs-iam-review-and-revoke-unnecessary-anonymous-access">Review and revoke unnecessary anonymous access<a class="headerlink" href="#security-docs-iam-review-and-revoke-unnecessary-anonymous-access" title="Permanent link">&para;</a></h3>
<p>Ideally anonymous access should be disabled for all API actions. Anonymous access is granted by creating a RoleBinding or ClusterRoleBinding for the Kubernetes built-in user system:anonymous. You can use the <a href="https://github.com/FairwindsOps/rbac-lookup">rbac-lookup</a> tool to identify permissions that system:anonymous user has on your cluster:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-28-1" name="__codelineno-28-1" href="#security-docs-iam-__codelineno-28-1"></a>./rbac-lookup<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>-P<span class="w"> </span><span class="s1">&#39;system:(anonymous)|(unauthenticated)&#39;</span>
<a id="__codelineno-28-2" name="__codelineno-28-2" href="#security-docs-iam-__codelineno-28-2"></a>system:anonymous<span class="w">               </span>cluster-wide<span class="w">        </span>ClusterRole/system:discovery
<a id="__codelineno-28-3" name="__codelineno-28-3" href="#security-docs-iam-__codelineno-28-3"></a>system:unauthenticated<span class="w">         </span>cluster-wide<span class="w">        </span>ClusterRole/system:discovery
<a id="__codelineno-28-4" name="__codelineno-28-4" href="#security-docs-iam-__codelineno-28-4"></a>system:unauthenticated<span class="w">         </span>cluster-wide<span class="w">        </span>ClusterRole/system:public-info-viewer
</code></pre></div>
<p>Any role or ClusterRole other than system:public-info-viewer should not be bound to system:anonymous user or system:unauthenticated group.</p>
<p>There may be some legitimate reasons to enable anonymous access on specific APIs. If this is the case for your cluster ensure that only those specific APIs are accessible by anonymous user and exposing those APIs without authentication doesn’t make your cluster vulnerable.</p>
<p>Prior to Kubernetes/EKS Version 1.14, system:unauthenticated group was associated to system:discovery and system:basic-user ClusterRoles by default.  Note that even if you have updated your cluster to version 1.14 or higher, these permissions may still be enabled on your cluster, since cluster updates do not revoke these permissions.
To check which ClusterRoles have "system:unauthenticated" except system:public-info-viewer you can run the following command (requires jq util):</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-29-1" name="__codelineno-29-1" href="#security-docs-iam-__codelineno-29-1"></a>kubectl<span class="w"> </span>get<span class="w"> </span>ClusterRoleBinding<span class="w"> </span>-o<span class="w"> </span>json<span class="w"> </span><span class="p">|</span><span class="w"> </span>jq<span class="w"> </span>-r<span class="w"> </span><span class="s1">&#39;.items[] | select(.subjects[]?.name ==&quot;system:unauthenticated&quot;) | select(.metadata.name != &quot;system:public-info-viewer&quot;) | .metadata.name&#39;</span>
</code></pre></div>
<p>And "system:unauthenticated" can be removed from all the roles except "system:public-info-viewer" using:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-30-1" name="__codelineno-30-1" href="#security-docs-iam-__codelineno-30-1"></a>kubectl<span class="w"> </span>get<span class="w"> </span>ClusterRoleBinding<span class="w"> </span>-o<span class="w"> </span>json<span class="w"> </span><span class="p">|</span><span class="w"> </span>jq<span class="w"> </span>-r<span class="w"> </span><span class="s1">&#39;.items[] | select(.subjects[]?.name ==&quot;system:unauthenticated&quot;) | select(.metadata.name != &quot;system:public-info-viewer&quot;) | del(.subjects[] | select(.name ==&quot;system:unauthenticated&quot;))&#39;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>kubectl<span class="w"> </span>apply<span class="w"> </span>-f<span class="w"> </span>-
</code></pre></div>
<p>Alternatively, you can check and remove it manually by kubectl describe and kubectl edit. To check if system:unauthenticated group has system:discovery permissions on your cluster run the following command:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-31-1" name="__codelineno-31-1" href="#security-docs-iam-__codelineno-31-1"></a>kubectl<span class="w"> </span>describe<span class="w"> </span>clusterrolebindings<span class="w"> </span>system:discovery
<a id="__codelineno-31-2" name="__codelineno-31-2" href="#security-docs-iam-__codelineno-31-2"></a>
<a id="__codelineno-31-3" name="__codelineno-31-3" href="#security-docs-iam-__codelineno-31-3"></a>Name:<span class="w">         </span>system:discovery
<a id="__codelineno-31-4" name="__codelineno-31-4" href="#security-docs-iam-__codelineno-31-4"></a>Labels:<span class="w">       </span>kubernetes.io/bootstrapping<span class="o">=</span>rbac-defaults
<a id="__codelineno-31-5" name="__codelineno-31-5" href="#security-docs-iam-__codelineno-31-5"></a>Annotations:<span class="w">  </span>rbac.authorization.kubernetes.io/autoupdate:<span class="w"> </span><span class="nb">true</span>
<a id="__codelineno-31-6" name="__codelineno-31-6" href="#security-docs-iam-__codelineno-31-6"></a>Role:
<a id="__codelineno-31-7" name="__codelineno-31-7" href="#security-docs-iam-__codelineno-31-7"></a><span class="w">  </span>Kind:<span class="w">  </span>ClusterRole
<a id="__codelineno-31-8" name="__codelineno-31-8" href="#security-docs-iam-__codelineno-31-8"></a><span class="w">  </span>Name:<span class="w">  </span>system:discovery
<a id="__codelineno-31-9" name="__codelineno-31-9" href="#security-docs-iam-__codelineno-31-9"></a>Subjects:
<a id="__codelineno-31-10" name="__codelineno-31-10" href="#security-docs-iam-__codelineno-31-10"></a><span class="w">  </span>Kind<span class="w">   </span>Name<span class="w">                    </span>Namespace
<a id="__codelineno-31-11" name="__codelineno-31-11" href="#security-docs-iam-__codelineno-31-11"></a><span class="w">  </span>----<span class="w">   </span>----<span class="w">                    </span>---------
<a id="__codelineno-31-12" name="__codelineno-31-12" href="#security-docs-iam-__codelineno-31-12"></a><span class="w">  </span>Group<span class="w">  </span>system:authenticated
<a id="__codelineno-31-13" name="__codelineno-31-13" href="#security-docs-iam-__codelineno-31-13"></a><span class="w">  </span>Group<span class="w">  </span>system:unauthenticated
</code></pre></div>
<p>To check if system:unauthenticated group has system:basic-user permission on your cluster run the following command:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-32-1" name="__codelineno-32-1" href="#security-docs-iam-__codelineno-32-1"></a>kubectl<span class="w"> </span>describe<span class="w"> </span>clusterrolebindings<span class="w"> </span>system:basic-user
<a id="__codelineno-32-2" name="__codelineno-32-2" href="#security-docs-iam-__codelineno-32-2"></a>
<a id="__codelineno-32-3" name="__codelineno-32-3" href="#security-docs-iam-__codelineno-32-3"></a>Name:<span class="w">         </span>system:basic-user
<a id="__codelineno-32-4" name="__codelineno-32-4" href="#security-docs-iam-__codelineno-32-4"></a>Labels:<span class="w">       </span>kubernetes.io/bootstrapping<span class="o">=</span>rbac-defaults
<a id="__codelineno-32-5" name="__codelineno-32-5" href="#security-docs-iam-__codelineno-32-5"></a>Annotations:<span class="w">  </span>rbac.authorization.kubernetes.io/autoupdate:<span class="w"> </span><span class="nb">true</span>
<a id="__codelineno-32-6" name="__codelineno-32-6" href="#security-docs-iam-__codelineno-32-6"></a>Role:
<a id="__codelineno-32-7" name="__codelineno-32-7" href="#security-docs-iam-__codelineno-32-7"></a><span class="w">  </span>Kind:<span class="w">  </span>ClusterRole
<a id="__codelineno-32-8" name="__codelineno-32-8" href="#security-docs-iam-__codelineno-32-8"></a><span class="w">  </span>Name:<span class="w">  </span>system:basic-user
<a id="__codelineno-32-9" name="__codelineno-32-9" href="#security-docs-iam-__codelineno-32-9"></a>Subjects:
<a id="__codelineno-32-10" name="__codelineno-32-10" href="#security-docs-iam-__codelineno-32-10"></a><span class="w">  </span>Kind<span class="w">   </span>Name<span class="w">                    </span>Namespace
<a id="__codelineno-32-11" name="__codelineno-32-11" href="#security-docs-iam-__codelineno-32-11"></a><span class="w">  </span>----<span class="w">   </span>----<span class="w">                    </span>---------
<a id="__codelineno-32-12" name="__codelineno-32-12" href="#security-docs-iam-__codelineno-32-12"></a><span class="w">  </span>Group<span class="w">  </span>system:authenticated
<a id="__codelineno-32-13" name="__codelineno-32-13" href="#security-docs-iam-__codelineno-32-13"></a><span class="w">  </span>Group<span class="w">  </span>system:unauthenticated
</code></pre></div>
<p>If system:unauthenticated group is bound to system:discovery and/or system:basic-user ClusterRoles on your cluster, you should disassociate these roles from system:unauthenticated group. Edit system:discovery ClusterRoleBinding using the following command:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-33-1" name="__codelineno-33-1" href="#security-docs-iam-__codelineno-33-1"></a>kubectl<span class="w"> </span>edit<span class="w"> </span>clusterrolebindings<span class="w"> </span>system:discovery
</code></pre></div>
<p>The above command will open the current definition of system:discovery ClusterRoleBinding in an editor as shown below:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-34-1" name="__codelineno-34-1" href="#security-docs-iam-__codelineno-34-1"></a><span class="c1"># Please edit the object below. Lines beginning with a &#39;#&#39; will be ignored,</span>
<a id="__codelineno-34-2" name="__codelineno-34-2" href="#security-docs-iam-__codelineno-34-2"></a><span class="c1"># and an empty file will abort the edit. If an error occurs while saving this file will be</span>
<a id="__codelineno-34-3" name="__codelineno-34-3" href="#security-docs-iam-__codelineno-34-3"></a><span class="c1"># reopened with the relevant failures.</span>
<a id="__codelineno-34-4" name="__codelineno-34-4" href="#security-docs-iam-__codelineno-34-4"></a><span class="c1">#</span>
<a id="__codelineno-34-5" name="__codelineno-34-5" href="#security-docs-iam-__codelineno-34-5"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">rbac.authorization.k8s.io/v1</span>
<a id="__codelineno-34-6" name="__codelineno-34-6" href="#security-docs-iam-__codelineno-34-6"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ClusterRoleBinding</span>
<a id="__codelineno-34-7" name="__codelineno-34-7" href="#security-docs-iam-__codelineno-34-7"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-34-8" name="__codelineno-34-8" href="#security-docs-iam-__codelineno-34-8"></a><span class="w">  </span><span class="nt">annotations</span><span class="p">:</span>
<a id="__codelineno-34-9" name="__codelineno-34-9" href="#security-docs-iam-__codelineno-34-9"></a><span class="w">    </span><span class="nt">rbac.authorization.kubernetes.io/autoupdate</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;true&quot;</span>
<a id="__codelineno-34-10" name="__codelineno-34-10" href="#security-docs-iam-__codelineno-34-10"></a><span class="w">  </span><span class="nt">creationTimestamp</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;2021-06-17T20:50:49Z&quot;</span>
<a id="__codelineno-34-11" name="__codelineno-34-11" href="#security-docs-iam-__codelineno-34-11"></a><span class="w">  </span><span class="nt">labels</span><span class="p">:</span>
<a id="__codelineno-34-12" name="__codelineno-34-12" href="#security-docs-iam-__codelineno-34-12"></a><span class="w">    </span><span class="nt">kubernetes.io/bootstrapping</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">rbac-defaults</span>
<a id="__codelineno-34-13" name="__codelineno-34-13" href="#security-docs-iam-__codelineno-34-13"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">system:discovery</span>
<a id="__codelineno-34-14" name="__codelineno-34-14" href="#security-docs-iam-__codelineno-34-14"></a><span class="w">  </span><span class="nt">resourceVersion</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;24502985&quot;</span>
<a id="__codelineno-34-15" name="__codelineno-34-15" href="#security-docs-iam-__codelineno-34-15"></a><span class="w">  </span><span class="nt">selfLink</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system%3Adiscovery</span>
<a id="__codelineno-34-16" name="__codelineno-34-16" href="#security-docs-iam-__codelineno-34-16"></a><span class="w">  </span><span class="nt">uid</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">b7936268-5043-431a-a0e1-171a423abeb6</span>
<a id="__codelineno-34-17" name="__codelineno-34-17" href="#security-docs-iam-__codelineno-34-17"></a><span class="nt">roleRef</span><span class="p">:</span>
<a id="__codelineno-34-18" name="__codelineno-34-18" href="#security-docs-iam-__codelineno-34-18"></a><span class="w">  </span><span class="nt">apiGroup</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">rbac.authorization.k8s.io</span>
<a id="__codelineno-34-19" name="__codelineno-34-19" href="#security-docs-iam-__codelineno-34-19"></a><span class="w">  </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ClusterRole</span>
<a id="__codelineno-34-20" name="__codelineno-34-20" href="#security-docs-iam-__codelineno-34-20"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">system:discovery</span>
<a id="__codelineno-34-21" name="__codelineno-34-21" href="#security-docs-iam-__codelineno-34-21"></a><span class="nt">subjects</span><span class="p">:</span>
<a id="__codelineno-34-22" name="__codelineno-34-22" href="#security-docs-iam-__codelineno-34-22"></a><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">apiGroup</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">rbac.authorization.k8s.io</span>
<a id="__codelineno-34-23" name="__codelineno-34-23" href="#security-docs-iam-__codelineno-34-23"></a><span class="w">  </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Group</span>
<a id="__codelineno-34-24" name="__codelineno-34-24" href="#security-docs-iam-__codelineno-34-24"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">system:authenticated</span>
<a id="__codelineno-34-25" name="__codelineno-34-25" href="#security-docs-iam-__codelineno-34-25"></a><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">apiGroup</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">rbac.authorization.k8s.io</span>
<a id="__codelineno-34-26" name="__codelineno-34-26" href="#security-docs-iam-__codelineno-34-26"></a><span class="w">  </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Group</span>
<a id="__codelineno-34-27" name="__codelineno-34-27" href="#security-docs-iam-__codelineno-34-27"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">system:unauthenticated</span>
</code></pre></div>
<p>Delete the entry for system:unauthenticated group from the “subjects” section in the above editor screen.</p>
<p>Repeat the same steps for system:basic-user ClusterRoleBinding.</p>
<h3 id="security-docs-iam-alternative-approaches">Alternative approaches<a class="headerlink" href="#security-docs-iam-alternative-approaches" title="Permanent link">&para;</a></h3>
<p>While IRSA is the <em>preferred way</em> to assign an AWS "identity" to a pod, it requires that you include recent version of the AWS SDKs in your application. For a complete listing of the SDKs that currently support IRSA, see <a href="https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts-minimum-sdk.html">https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts-minimum-sdk.html</a>. If you have an application that you can't immediately update with a IRSA-compatible SDK, there are several community-built solutions available for assigning IAM roles to Kubernetes pods, including <a href="https://github.com/jtblin/kube2iam">kube2iam</a> and <a href="https://github.com/uswitch/kiam">kiam</a>.  Although AWS doesn't endorse or condone the use of these solutions, they are frequently used by the community at large to achieve similar results as IRSA.</p></section><section class="print-page" id="security-docs-pods"><h1 id="security-docs-pods-pod-security">Pod Security<a class="headerlink" href="#security-docs-pods-pod-security" title="Permanent link">&para;</a></h1>
<p>The pod specification includes a variety of different attributes that can strengthen or weaken your overall security posture.  As a Kubernetes practitioner your chief concern should be preventing a process that’s running in a container from escaping the isolation boundaries of the container runtime and gaining access to the underlying host.</p>
<h3 id="security-docs-pods-linux-capabilities">Linux Capabilities<a class="headerlink" href="#security-docs-pods-linux-capabilities" title="Permanent link">&para;</a></h3>
<p>The processes that run within a container run under the context of the [Linux] root user by default.  Although the actions of root within a container are partially constrained by the set of Linux capabilities that the container runtime assigns to the containers, these default privileges could allow an attacker to escalate their privileges and/or gain access to sensitive information bound to the host, including Secrets and ConfigMaps.  Below is a list of the default capabilities assigned to containers.  For additional information about each capability, see <a href="http://man7.org/linux/man-pages/man7/capabilities.7.html">http://man7.org/linux/man-pages/man7/capabilities.7.html</a>.</p>
<p><code>CAP_AUDIT_WRITE, CAP_CHOWN, CAP_DAC_OVERRIDE, CAP_FOWNER, CAP_FSETID, CAP_KILL, CAP_MKNOD, CAP_NET_BIND_SERVICE, CAP_NET_RAW, CAP_SETGID, CAP_SETUID, CAP_SETFCAP, CAP_SETPCAP, CAP_SYS_CHROOT</code></p>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>EC2 and Fargate pods are assigned the aforementioned capabilities by default. Additionally, Linux capabilities can only be dropped from Fargate pods. </p>
</div>
<p>Pods that are run as privileged, inherit <em>all</em> of the Linux capabilities associated with root on the host. This should be avoided if possible.</p>
<h3 id="security-docs-pods-node-authorization">Node Authorization<a class="headerlink" href="#security-docs-pods-node-authorization" title="Permanent link">&para;</a></h3>
<p>All Kubernetes worker nodes use an authorization mode called <a href="https://kubernetes.io/docs/reference/access-authn-authz/node/">Node Authorization</a>. Node Authorization authorizes all API requests that originate from the kubelet and allows nodes to perform the following actions: </p>
<p>Read operations:</p>
<ul>
<li>services</li>
<li>endpoints</li>
<li>nodes</li>
<li>pods</li>
<li>secrets, configmaps, persistent volume claims and persistent volumes related to pods bound to the kubelet’s node</li>
</ul>
<p>Write operations:</p>
<ul>
<li>nodes and node status (enable the <code>NodeRestriction</code> admission plugin to limit a kubelet to modify its own node)</li>
<li>pods and pod status (enable the <code>NodeRestriction</code> admission plugin to limit a kubelet to modify pods bound to itself)</li>
<li>events</li>
</ul>
<p>Auth-related operations:</p>
<ul>
<li>Read/write access to the CertificateSigningRequest (CSR) API for TLS bootstrapping</li>
<li>the ability to create TokenReview and SubjectAccessReview for delegated authentication/authorization checks</li>
</ul>
<p>EKS uses the <a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#noderestriction">node restriction admission controller</a> which only allows the node to modify a limited set of node attributes and pod objects that are bound to the node.   Nevertheless, an attacker who manages to get access to the host will still be able to glean sensitive information about the environment from the Kubernetes API that could allow them to move laterally within the cluster.</p>
<h2 id="security-docs-pods-pod-security-solutions">Pod Security Solutions<a class="headerlink" href="#security-docs-pods-pod-security-solutions" title="Permanent link">&para;</a></h2>
<h3 id="security-docs-pods-pod-security-policy-psp">Pod Security Policy (PSP)<a class="headerlink" href="#security-docs-pods-pod-security-policy-psp" title="Permanent link">&para;</a></h3>
<p>In the past, <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/">Pod Security Policy (PSP)</a> resources were used to specify a set of requirements that pods had to meet before they could be created. As of Kubernetes version 1.21, PSP have been deprecated. They are scheduled for removal in Kubernetes version 1.25. </p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p><a href="https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/">PSPs are deprecated</a> in Kubernetes version 1.21. You will have until version 1.25 or roughly 2 years to transition to an alternative. This <a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-auth/2579-psp-replacement/README.md#motivation">document</a> explains the motivation for this deprecation.</p>
</div>
<h3 id="security-docs-pods-migrating-to-a-new-pod-security-solution">Migrating to a new pod security solution<a class="headerlink" href="#security-docs-pods-migrating-to-a-new-pod-security-solution" title="Permanent link">&para;</a></h3>
<p>Since PSPs have been removed as of Kubernetes v1.25, cluster administrators and operators must replace those security controls. Two solutions can fill this need:</p>
<ul>
<li>Policy-as-code (PAC) solutions from the Kubernetes ecosystem</li>
<li>Kubernetes <a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/">Pod Security Standards (PSS)</a></li>
</ul>
<p>Both the PAC and PSS solutions can coexist with PSP; they can be used in clusters before PSP is removed. This eases adoption when migrating from PSP. Please see this <a href="https://kubernetes.io/docs/tasks/configure-pod-container/migrate-from-psp/">document</a> when considering migrating from PSP to PSS.</p>
<p>Kyverno, one of the PAC solutions outlined below, has specific guidance outlined in a <a href="https://kyverno.io/blog/2023/05/24/podsecuritypolicy-migration-with-kyverno/">blog post</a> when migrating from PSPs to its solution including analogous policies, feature comparisons, and a migration procedure. Additional information and guidance on migration to Kyverno with respect to Pod Security Admission (PSA) has been published on the AWS blog <a href="https://aws.amazon.com/blogs/containers/managing-pod-security-on-amazon-eks-with-kyverno/">here</a>.</p>
<h3 id="security-docs-pods-policy-as-code-pac">Policy-as-code (PAC)<a class="headerlink" href="#security-docs-pods-policy-as-code-pac" title="Permanent link">&para;</a></h3>
<p>Policy-as-code (PAC) solutions provide guardrails to guide cluster users, and prevent unwanted behaviors, through prescribed and automated controls. PAC uses <a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/">Kubernetes Dynamic Admission Controllers</a> to intercept the Kubernetes API server request flow, via a webhook call, and mutate and validate request payloads, based on policies written and stored as code. Mutation and validation happens before the API server request results in a change to the cluster. PAC solutions use policies to match and act on API server request payloads, based on taxonomy and values.</p>
<p>There are several open source PAC solutions available for Kubernetes. These solutions are not part of the Kubernetes project; they are sourced from the Kubernetes ecosystem. Some PAC solutions are listed below.</p>
<ul>
<li><a href="https://open-policy-agent.github.io/gatekeeper/website/docs/">OPA/Gatekeeper</a></li>
<li><a href="https://www.openpolicyagent.org/">Open Policy Agent (OPA)</a></li>
<li><a href="https://kyverno.io/">Kyverno</a></li>
<li><a href="https://www.kubewarden.io/">Kubewarden</a></li>
<li><a href="https://www.jspolicy.com/">jsPolicy</a></li>
</ul>
<p>For further information about PAC solutions and how to help you select the appropriate solution for your needs, see the links below.</p>
<ul>
<li><a href="https://aws.amazon.com/blogs/containers/policy-based-countermeasures-for-kubernetes-part-1/">Policy-based countermeasures for Kubernetes – Part 1</a></li>
<li><a href="https://aws.amazon.com/blogs/containers/policy-based-countermeasures-for-kubernetes-part-2/">Policy-based countermeasures for Kubernetes – Part 2</a></li>
</ul>
<h3 id="security-docs-pods-pod-security-standards-pss-and-pod-security-admission-psa">Pod Security Standards (PSS) and Pod Security Admission (PSA)<a class="headerlink" href="#security-docs-pods-pod-security-standards-pss-and-pod-security-admission-psa" title="Permanent link">&para;</a></h3>
<p>In response to the PSP deprecation and the ongoing need to control pod security out-of-the-box, with a built-in Kubernetes solution, the Kubernetes <a href="https://github.com/kubernetes/community/tree/master/sig-auth">Auth Special Interest Group</a> created the <a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/">Pod Security Standards (PSS)</a> and <a href="https://kubernetes.io/docs/concepts/security/pod-security-admission/">Pod Security Admission (PSA)</a>. The PSA effort includes an <a href="https://github.com/kubernetes/pod-security-admission#pod-security-admission">admission controller webhook project</a> that implements the controls defined in the PSS. This admission controller approach resembles that used in the PAC solutions.</p>
<p>According to the Kubernetes documentation, the PSS <em>"define three different policies to broadly cover the security spectrum. These policies are cumulative and range from highly-permissive to highly-restrictive."</em> </p>
<p>These policies are defined as:</p>
<ul>
<li>
<p><strong>Privileged:</strong> Unrestricted (unsecure) policy, providing the widest possible level of permissions. This policy allows for known privilege escalations. It is the absence of a policy. This is good for applications such as logging agents, CNIs, storage drivers, and other system wide applications that need privileged access.</p>
</li>
<li>
<p><strong>Baseline:</strong> Minimally restrictive policy which prevents known privilege escalations. Allows the default (minimally specified) Pod configuration. The baseline policy prohibits use of hostNetwork, hostPID, hostIPC, hostPath, hostPort, the inability to add Linux capabilities, along with several other restrictions. </p>
</li>
<li>
<p><strong>Restricted:</strong> Heavily restricted policy, following current Pod hardening best practices.  This policy inherits from the baseline and adds further restrictions such as the inability to run as root or a root-group. Restricted policies may impact an application's ability to function. They are primarily targeted at running security critical applications.</p>
</li>
</ul>
<p>These policies define <a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/#profile-details">profiles for pod execution</a>, arranged into three levels of privileged vs. restricted access.</p>
<p>To implement the controls defined by the PSS, PSA operates in three modes:</p>
<ul>
<li>
<p><strong>enforce:</strong> Policy violations will cause the pod to be rejected.</p>
</li>
<li>
<p><strong>audit:</strong> Policy violations will trigger the addition of an audit annotation to the event recorded in the audit log, but are otherwise allowed.</p>
</li>
<li>
<p><strong>warn:</strong> Policy violations will trigger a user-facing warning, but are otherwise allowed.</p>
</li>
</ul>
<p>These modes and the profile (restriction) levels are configured at the Kubernetes Namespace level, using labels, as seen in the below example.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#security-docs-pods-__codelineno-0-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#security-docs-pods-__codelineno-0-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Namespace</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#security-docs-pods-__codelineno-0-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#security-docs-pods-__codelineno-0-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">policy-test</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#security-docs-pods-__codelineno-0-5"></a><span class="w">  </span><span class="nt">labels</span><span class="p">:</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#security-docs-pods-__codelineno-0-6"></a><span class="w">    </span><span class="nt">pod-security.kubernetes.io/enforce</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">restricted</span>
</code></pre></div>
<p>When used independently, these operational modes have different responses that result in different user experiences. The <em>enforce</em> mode will prevent pods from being created if respective podSpecs violate the configured restriction level. However, in this mode, non-pod Kubernetes objects that create pods, such as Deployments, will not be prevented from being applied to the cluster, even if the podSpec therein violates the applied PSS. In this case the Deployment will be applied, while the pod(s) will be prevented from being applied.</p>
<p>This is a difficult user experience, as there is no immediate indication that the successfully applied Deployment object belies failed pod creation. The offending podSpecs will not create pods. Inspecting the Deployment resource with <code>kubectl get deploy &lt;DEPLOYMENT_NAME&gt; -oyaml</code> will expose the message from the failed pod(s) <code>.status.conditions</code> element, as seen below.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#security-docs-pods-__codelineno-1-1"></a><span class="nn">...</span>
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#security-docs-pods-__codelineno-1-2"></a><span class="nt">status</span><span class="p">:</span>
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#security-docs-pods-__codelineno-1-3"></a><span class="w">  </span><span class="nt">conditions</span><span class="p">:</span>
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#security-docs-pods-__codelineno-1-4"></a><span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">lastTransitionTime</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;2022-01-20T01:02:08Z&quot;</span>
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#security-docs-pods-__codelineno-1-5"></a><span class="w">      </span><span class="nt">lastUpdateTime</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;2022-01-20T01:02:08Z&quot;</span>
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#security-docs-pods-__codelineno-1-6"></a><span class="w">      </span><span class="nt">message</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;pods</span><span class="nv"> </span><span class="s">&quot;test-688f68dc87-tw587&quot;</span><span class="nv"> </span><span class="s">is</span><span class="nv"> </span><span class="s">forbidden:</span><span class="nv"> </span><span class="s">violates</span><span class="nv"> </span><span class="s">PodSecurity</span><span class="nv"> </span><span class="s">&quot;restricted:latest&quot;:</span>
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#security-docs-pods-__codelineno-1-7"></a><span class="w">        </span><span class="s">allowPrivilegeEscalation</span><span class="nv"> </span><span class="s">!=</span><span class="nv"> </span><span class="s">false</span><span class="nv"> </span><span class="s">(container</span><span class="nv"> </span><span class="s">&quot;test&quot;</span><span class="nv"> </span><span class="s">must</span><span class="nv"> </span><span class="s">set</span><span class="nv"> </span><span class="s">securityContext.allowPrivilegeEscalation=false),</span>
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#security-docs-pods-__codelineno-1-8"></a><span class="w">        </span><span class="s">unrestricted</span><span class="nv"> </span><span class="s">capabilities</span><span class="nv"> </span><span class="s">(container</span><span class="nv"> </span><span class="s">&quot;test&quot;</span><span class="nv"> </span><span class="s">must</span><span class="nv"> </span><span class="s">set</span><span class="nv"> </span><span class="s">securityContext.capabilities.drop=[&quot;ALL&quot;]),</span>
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#security-docs-pods-__codelineno-1-9"></a><span class="w">        </span><span class="s">runAsNonRoot</span><span class="nv"> </span><span class="s">!=</span><span class="nv"> </span><span class="s">true</span><span class="nv"> </span><span class="s">(pod</span><span class="nv"> </span><span class="s">or</span><span class="nv"> </span><span class="s">container</span><span class="nv"> </span><span class="s">&quot;test&quot;</span><span class="nv"> </span><span class="s">must</span><span class="nv"> </span><span class="s">set</span><span class="nv"> </span><span class="s">securityContext.runAsNonRoot=true),</span>
<a id="__codelineno-1-10" name="__codelineno-1-10" href="#security-docs-pods-__codelineno-1-10"></a><span class="w">        </span><span class="s">seccompProfile</span><span class="nv"> </span><span class="s">(pod</span><span class="nv"> </span><span class="s">or</span><span class="nv"> </span><span class="s">container</span><span class="nv"> </span><span class="s">&quot;test&quot;</span><span class="nv"> </span><span class="s">must</span><span class="nv"> </span><span class="s">set</span><span class="nv"> </span><span class="s">securityContext.seccompProfile.type</span>
<a id="__codelineno-1-11" name="__codelineno-1-11" href="#security-docs-pods-__codelineno-1-11"></a><span class="w">        </span><span class="s">to</span><span class="nv"> </span><span class="s">&quot;RuntimeDefault&quot;</span><span class="nv"> </span><span class="s">or</span><span class="nv"> </span><span class="s">&quot;Localhost&quot;)&#39;</span>
<a id="__codelineno-1-12" name="__codelineno-1-12" href="#security-docs-pods-__codelineno-1-12"></a><span class="w">      </span><span class="nt">reason</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">FailedCreate</span>
<a id="__codelineno-1-13" name="__codelineno-1-13" href="#security-docs-pods-__codelineno-1-13"></a><span class="w">      </span><span class="nt">status</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;True&quot;</span>
<a id="__codelineno-1-14" name="__codelineno-1-14" href="#security-docs-pods-__codelineno-1-14"></a><span class="w">      </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ReplicaFailure</span>
<a id="__codelineno-1-15" name="__codelineno-1-15" href="#security-docs-pods-__codelineno-1-15"></a><span class="nn">...</span>
</code></pre></div>
<p>In both the <em>audit</em> and <em>warn</em> modes, the pod restrictions do not prevent violating pods from being created and started. However, in these modes audit annotations on API server audit log events and warnings to API server clients, such as <em>kubectl</em>, are triggered, respectively, when pods, as well as objects that create pods, contain podSpecs with violations. A <code>kubectl</code> <em>Warning</em> message is seen below.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#security-docs-pods-__codelineno-2-1"></a>Warning:<span class="w"> </span>would<span class="w"> </span>violate<span class="w"> </span>PodSecurity<span class="w"> </span><span class="s2">&quot;restricted:latest&quot;</span>:<span class="w"> </span>allowPrivilegeEscalation<span class="w"> </span>!<span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="w"> </span><span class="o">(</span>container<span class="w"> </span><span class="s2">&quot;test&quot;</span><span class="w"> </span>must<span class="w"> </span><span class="nb">set</span><span class="w"> </span>securityContext.allowPrivilegeEscalation<span class="o">=</span><span class="nb">false</span><span class="o">)</span>,<span class="w"> </span>unrestricted<span class="w"> </span>capabilities<span class="w"> </span><span class="o">(</span>container<span class="w"> </span><span class="s2">&quot;test&quot;</span><span class="w"> </span>must<span class="w"> </span><span class="nb">set</span><span class="w"> </span>securityContext.capabilities.drop<span class="o">=[</span><span class="s2">&quot;ALL&quot;</span><span class="o">])</span>,<span class="w"> </span>runAsNonRoot<span class="w"> </span>!<span class="o">=</span><span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="o">(</span>pod<span class="w"> </span>or<span class="w"> </span>container<span class="w"> </span><span class="s2">&quot;test&quot;</span><span class="w"> </span>must<span class="w"> </span><span class="nb">set</span><span class="w"> </span>securityContext.runAsNonRoot<span class="o">=</span><span class="nb">true</span><span class="o">)</span>,<span class="w"> </span>seccompProfile<span class="w"> </span><span class="o">(</span>pod<span class="w"> </span>or<span class="w"> </span>container<span class="w"> </span><span class="s2">&quot;test&quot;</span><span class="w"> </span>must<span class="w"> </span><span class="nb">set</span><span class="w"> </span>securityContext.seccompProfile.type<span class="w"> </span>to<span class="w"> </span><span class="s2">&quot;RuntimeDefault&quot;</span><span class="w"> </span>or<span class="w"> </span><span class="s2">&quot;Localhost&quot;</span><span class="o">)</span>
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#security-docs-pods-__codelineno-2-2"></a>deployment.apps/test<span class="w"> </span>created
</code></pre></div>
<p>The PSA <em>audit</em> and <em>warn</em> modes are useful when introducing the PSS without negatively impacting cluster operations.</p>
<p>The PSA operational modes are not mutually exclusive, and can be used in a cumulative manner. As seen below, the multiple modes can be configured in a single namespace.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#security-docs-pods-__codelineno-3-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#security-docs-pods-__codelineno-3-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Namespace</span>
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#security-docs-pods-__codelineno-3-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-3-4" name="__codelineno-3-4" href="#security-docs-pods-__codelineno-3-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">policy-test</span>
<a id="__codelineno-3-5" name="__codelineno-3-5" href="#security-docs-pods-__codelineno-3-5"></a><span class="w">  </span><span class="nt">labels</span><span class="p">:</span>
<a id="__codelineno-3-6" name="__codelineno-3-6" href="#security-docs-pods-__codelineno-3-6"></a><span class="w">    </span><span class="nt">pod-security.kubernetes.io/audit</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">restricted</span>
<a id="__codelineno-3-7" name="__codelineno-3-7" href="#security-docs-pods-__codelineno-3-7"></a><span class="w">    </span><span class="nt">pod-security.kubernetes.io/enforce</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">restricted</span>
<a id="__codelineno-3-8" name="__codelineno-3-8" href="#security-docs-pods-__codelineno-3-8"></a><span class="w">    </span><span class="nt">pod-security.kubernetes.io/warn</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">restricted</span>
</code></pre></div>
<p>In the above example, the user-friendly warnings and audit annotations are provided when applying Deployments, while the enforce of violations are also provided at the pod level. In fact multiple PSA labels can use different profile levels, as seen below.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#security-docs-pods-__codelineno-4-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<a id="__codelineno-4-2" name="__codelineno-4-2" href="#security-docs-pods-__codelineno-4-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Namespace</span>
<a id="__codelineno-4-3" name="__codelineno-4-3" href="#security-docs-pods-__codelineno-4-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-4-4" name="__codelineno-4-4" href="#security-docs-pods-__codelineno-4-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">policy-test</span>
<a id="__codelineno-4-5" name="__codelineno-4-5" href="#security-docs-pods-__codelineno-4-5"></a><span class="w">  </span><span class="nt">labels</span><span class="p">:</span>
<a id="__codelineno-4-6" name="__codelineno-4-6" href="#security-docs-pods-__codelineno-4-6"></a><span class="w">    </span><span class="nt">pod-security.kubernetes.io/enforce</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">baseline</span>
<a id="__codelineno-4-7" name="__codelineno-4-7" href="#security-docs-pods-__codelineno-4-7"></a><span class="w">    </span><span class="nt">pod-security.kubernetes.io/warn</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">restricted</span>
</code></pre></div>
<p>In the above example, PSA is configured to allow the creation of all pods that satisfy the <em>baseline</em> profile level, and then <em>warn</em> on pods (and objects that create pods) that violate the <em>restricted</em> profile level. This is a useful approach to determine the possible impacts when changing from the <em>baseline</em> to <em>restricted</em> profiles.</p>
<h4 id="security-docs-pods-existing-pods">Existing Pods<a class="headerlink" href="#security-docs-pods-existing-pods" title="Permanent link">&para;</a></h4>
<p>If a namespace with existing pods is modified to use a more restrictive PSS profile, the <em>audit</em> and <em>warn</em> modes will produce appropriate messages; however, <em>enforce</em> mode will not delete the pods. The warning messages are seen below.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#security-docs-pods-__codelineno-5-1"></a>Warning:<span class="w"> </span>existing<span class="w"> </span>pods<span class="w"> </span><span class="k">in</span><span class="w"> </span>namespace<span class="w"> </span><span class="s2">&quot;policy-test&quot;</span><span class="w"> </span>violate<span class="w"> </span>the<span class="w"> </span>new<span class="w"> </span>PodSecurity<span class="w"> </span>enforce<span class="w"> </span>level<span class="w"> </span><span class="s2">&quot;restricted:latest&quot;</span>
<a id="__codelineno-5-2" name="__codelineno-5-2" href="#security-docs-pods-__codelineno-5-2"></a>Warning:<span class="w"> </span>test-688f68dc87-htm8x:<span class="w"> </span>allowPrivilegeEscalation<span class="w"> </span>!<span class="o">=</span><span class="w"> </span>false,<span class="w"> </span>unrestricted<span class="w"> </span>capabilities,<span class="w"> </span>runAsNonRoot<span class="w"> </span>!<span class="o">=</span><span class="w"> </span>true,<span class="w"> </span>seccompProfile
<a id="__codelineno-5-3" name="__codelineno-5-3" href="#security-docs-pods-__codelineno-5-3"></a>namespace/policy-test<span class="w"> </span>configured
</code></pre></div>
<h4 id="security-docs-pods-exemptions">Exemptions<a class="headerlink" href="#security-docs-pods-exemptions" title="Permanent link">&para;</a></h4>
<p>PSA uses <em>Exemptions</em> to exclude enforcement of violations against pods that would have otherwise been applied. These exemptions are listed below.</p>
<ul>
<li>
<p><strong>Usernames:</strong> requests from users with an exempt authenticated (or impersonated) username are ignored.</p>
</li>
<li>
<p><strong>RuntimeClassNames:</strong> pods and workload resources specifying an exempt runtime class name are ignored.</p>
</li>
<li>
<p><strong>Namespaces:</strong> pods and workload resources in an exempt namespace are ignored.</p>
</li>
</ul>
<p>These exemptions are applied statically in the <a href="https://kubernetes.io/docs/tasks/configure-pod-container/enforce-standards-admission-controller/#configure-the-admission-controller">PSA admission controller configuration</a> as part of the API server configuration.</p>
<p>In the <em>Validating Webhook</em> implementation the exemptions can be configured within a Kubernetes <a href="https://github.com/kubernetes/pod-security-admission/blob/master/webhook/manifests/20-configmap.yaml">ConfigMap</a> resource that gets mounted as a volume into the <a href="https://github.com/kubernetes/pod-security-admission/blob/master/webhook/manifests/50-deployment.yaml">pod-security-webhook</a> container.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#security-docs-pods-__codelineno-6-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<a id="__codelineno-6-2" name="__codelineno-6-2" href="#security-docs-pods-__codelineno-6-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ConfigMap</span>
<a id="__codelineno-6-3" name="__codelineno-6-3" href="#security-docs-pods-__codelineno-6-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-6-4" name="__codelineno-6-4" href="#security-docs-pods-__codelineno-6-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pod-security-webhook</span>
<a id="__codelineno-6-5" name="__codelineno-6-5" href="#security-docs-pods-__codelineno-6-5"></a><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pod-security-webhook</span>
<a id="__codelineno-6-6" name="__codelineno-6-6" href="#security-docs-pods-__codelineno-6-6"></a><span class="nt">data</span><span class="p">:</span>
<a id="__codelineno-6-7" name="__codelineno-6-7" href="#security-docs-pods-__codelineno-6-7"></a><span class="w">  </span><span class="nt">podsecurityconfiguration.yaml</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<a id="__codelineno-6-8" name="__codelineno-6-8" href="#security-docs-pods-__codelineno-6-8"></a><span class="w">    </span><span class="no">apiVersion: pod-security.admission.config.k8s.io/v1</span>
<a id="__codelineno-6-9" name="__codelineno-6-9" href="#security-docs-pods-__codelineno-6-9"></a><span class="w">    </span><span class="no">kind: PodSecurityConfiguration</span>
<a id="__codelineno-6-10" name="__codelineno-6-10" href="#security-docs-pods-__codelineno-6-10"></a><span class="w">    </span><span class="no">defaults:</span>
<a id="__codelineno-6-11" name="__codelineno-6-11" href="#security-docs-pods-__codelineno-6-11"></a><span class="w">      </span><span class="no">enforce: &quot;restricted&quot;</span>
<a id="__codelineno-6-12" name="__codelineno-6-12" href="#security-docs-pods-__codelineno-6-12"></a><span class="w">      </span><span class="no">enforce-version: &quot;latest&quot;</span>
<a id="__codelineno-6-13" name="__codelineno-6-13" href="#security-docs-pods-__codelineno-6-13"></a><span class="w">      </span><span class="no">audit: &quot;restricted&quot;</span>
<a id="__codelineno-6-14" name="__codelineno-6-14" href="#security-docs-pods-__codelineno-6-14"></a><span class="w">      </span><span class="no">audit-version: &quot;latest&quot;</span>
<a id="__codelineno-6-15" name="__codelineno-6-15" href="#security-docs-pods-__codelineno-6-15"></a><span class="w">      </span><span class="no">warn: &quot;restricted&quot;</span>
<a id="__codelineno-6-16" name="__codelineno-6-16" href="#security-docs-pods-__codelineno-6-16"></a><span class="w">      </span><span class="no">warn-version: &quot;latest&quot;</span>
<a id="__codelineno-6-17" name="__codelineno-6-17" href="#security-docs-pods-__codelineno-6-17"></a><span class="w">    </span><span class="no">exemptions:</span>
<a id="__codelineno-6-18" name="__codelineno-6-18" href="#security-docs-pods-__codelineno-6-18"></a><span class="w">      </span><span class="no"># Array of authenticated usernames to exempt.</span>
<a id="__codelineno-6-19" name="__codelineno-6-19" href="#security-docs-pods-__codelineno-6-19"></a><span class="w">      </span><span class="no">usernames: []</span>
<a id="__codelineno-6-20" name="__codelineno-6-20" href="#security-docs-pods-__codelineno-6-20"></a><span class="w">      </span><span class="no"># Array of runtime class names to exempt.</span>
<a id="__codelineno-6-21" name="__codelineno-6-21" href="#security-docs-pods-__codelineno-6-21"></a><span class="w">      </span><span class="no">runtimeClasses: []</span>
<a id="__codelineno-6-22" name="__codelineno-6-22" href="#security-docs-pods-__codelineno-6-22"></a><span class="w">      </span><span class="no"># Array of namespaces to exempt.</span>
<a id="__codelineno-6-23" name="__codelineno-6-23" href="#security-docs-pods-__codelineno-6-23"></a><span class="w">      </span><span class="no">namespaces: [&quot;kube-system&quot;,&quot;policy-test1&quot;]</span>
</code></pre></div>
<p>As seen in the above ConfigMap YAML the cluster-wide default PSS level has been set to <em>restricted</em> for all PSA modes, <em>audit</em>, <em>enforce</em>, and <em>warn</em>. This affects all namespaces, except those exempted: <code>namespaces: ["kube-system","policy-test1"]</code>. Additionally, in the <em>ValidatingWebhookConfiguration</em> resource, seen below, the <em>pod-security-webhook</em> namespace is also exempted from configured PSS.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#security-docs-pods-__codelineno-7-1"></a><span class="nn">...</span>
<a id="__codelineno-7-2" name="__codelineno-7-2" href="#security-docs-pods-__codelineno-7-2"></a><span class="nt">webhooks</span><span class="p">:</span>
<a id="__codelineno-7-3" name="__codelineno-7-3" href="#security-docs-pods-__codelineno-7-3"></a><span class="w">  </span><span class="c1"># Audit annotations will be prefixed with this name</span>
<a id="__codelineno-7-4" name="__codelineno-7-4" href="#security-docs-pods-__codelineno-7-4"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;pod-security-webhook.kubernetes.io&quot;</span>
<a id="__codelineno-7-5" name="__codelineno-7-5" href="#security-docs-pods-__codelineno-7-5"></a><span class="w">    </span><span class="c1"># Fail-closed admission webhooks can present operational challenges.</span>
<a id="__codelineno-7-6" name="__codelineno-7-6" href="#security-docs-pods-__codelineno-7-6"></a><span class="w">    </span><span class="c1"># You may want to consider using a failure policy of Ignore, but should </span>
<a id="__codelineno-7-7" name="__codelineno-7-7" href="#security-docs-pods-__codelineno-7-7"></a><span class="w">    </span><span class="c1"># consider the security tradeoffs.</span>
<a id="__codelineno-7-8" name="__codelineno-7-8" href="#security-docs-pods-__codelineno-7-8"></a><span class="w">    </span><span class="nt">failurePolicy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Fail</span>
<a id="__codelineno-7-9" name="__codelineno-7-9" href="#security-docs-pods-__codelineno-7-9"></a><span class="w">    </span><span class="nt">namespaceSelector</span><span class="p">:</span>
<a id="__codelineno-7-10" name="__codelineno-7-10" href="#security-docs-pods-__codelineno-7-10"></a><span class="w">      </span><span class="c1"># Exempt the webhook itself to avoid a circular dependency.</span>
<a id="__codelineno-7-11" name="__codelineno-7-11" href="#security-docs-pods-__codelineno-7-11"></a><span class="w">      </span><span class="nt">matchExpressions</span><span class="p">:</span>
<a id="__codelineno-7-12" name="__codelineno-7-12" href="#security-docs-pods-__codelineno-7-12"></a><span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">kubernetes.io/metadata.name</span>
<a id="__codelineno-7-13" name="__codelineno-7-13" href="#security-docs-pods-__codelineno-7-13"></a><span class="w">          </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">NotIn</span>
<a id="__codelineno-7-14" name="__codelineno-7-14" href="#security-docs-pods-__codelineno-7-14"></a><span class="w">          </span><span class="nt">values</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;pod-security-webhook&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-7-15" name="__codelineno-7-15" href="#security-docs-pods-__codelineno-7-15"></a><span class="nn">...</span>
</code></pre></div>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Pod Security Admissions graduated to stable in Kubernetes v1.25. If you wanted to use the Pod Security Admission feature prior to it being enabled by default, you needed to install the dynamic admission controller (mutating webhook). The instructions for installing and configuring the webhook can be found <a href="https://github.com/kubernetes/pod-security-admission/tree/master/webhook">here</a>.</p>
</div>
<h3 id="security-docs-pods-choosing-between-policy-as-code-and-pod-security-standards">Choosing between policy-as-code and Pod Security Standards<a class="headerlink" href="#security-docs-pods-choosing-between-policy-as-code-and-pod-security-standards" title="Permanent link">&para;</a></h3>
<p>The Pod Security Standards (PSS) were developed to replace the Pod Security Policy (PSP), by providing a solution that was built-in to Kubernetes and did not require solutions from the Kubernetes ecosystem. That being said, policy-as-code (PAC) solutions are considerably more flexible. </p>
<p>The following list of Pros and Cons is designed help you make a more informed decision about your pod security solution.</p>
<p><strong>Policy-as-code (as compared to Pod Security Standards)</strong></p>
<p>Pros:</p>
<ul>
<li>More flexible and more granular (down to attributes of resources if need be)</li>
<li>Not just focused on pods, can be used against different resources and actions</li>
<li>Not just applied at the namespace level</li>
<li>More mature than the Pod Security Standards</li>
<li>Decisions can be based on anything in the API server request payload, as well as existing cluster resources and external data (solution dependent)</li>
<li>Supports mutating API server requests before validation (solution dependent)</li>
<li>Can generate complementary policies and Kubernetes resources (solution dependent - From pod policies, Kyverno can <a href="https://kyverno.io/docs/writing-policies/autogen/">auto-gen</a> policies for higher-level controllers, such as Deployments. Kyverno can also generate additional Kubernetes resources <em>"when a new resource is created or when the source is updated"</em> by using <a href="https://kyverno.io/docs/writing-policies/generate/">Generate Rules</a>.)</li>
<li>Can be used to shift left, into CICD pipelines, before making calls to the Kubernetes API server (solution dependent)</li>
<li>Can be used to implement behaviors that are not necessarily security related, such as best practices, organizational standards, etc.</li>
<li>Can be used in non-Kubernetes use cases (solution dependent)</li>
<li>Because of flexibility, the user experience can be tuned to users' needs</li>
</ul>
<p>Cons:</p>
<ul>
<li>Not built into Kubernetes</li>
<li>More complex to learn, configure, and support</li>
<li>Policy authoring may require new skills/languages/capabilities</li>
</ul>
<p><strong>Pod Security Admission (as compared to policy-as-code)</strong></p>
<p>Pros:</p>
<ul>
<li>Built into Kubernetes</li>
<li>Simpler to configure</li>
<li>No new languages to use or policies to author</li>
<li>If the cluster default admission level is configured to <em>privileged</em>, namespace labels can be used to opt namespaces into the pod security profiles.</li>
</ul>
<p>Cons:</p>
<ul>
<li>Not as flexible or granular as policy-as-code</li>
<li>Only 3 levels of restrictions</li>
<li>Primarily focused on pods</li>
</ul>
<h4 id="security-docs-pods-summary">Summary<a class="headerlink" href="#security-docs-pods-summary" title="Permanent link">&para;</a></h4>
<p>If you currently do not have a pod security solution, beyond PSP, and your required pod security posture fits the model defined in the Pod Security Standards (PSS), then an easier path may be to adopt the PSS, in lieu of a policy-as-code solution. However, if your pod security posture does not fit the PSS model, or you envision adding additional controls, beyond that defined by PSS, then a policy-as-code solution would seem a better fit.</p>
<h2 id="security-docs-pods-recommendations">Recommendations<a class="headerlink" href="#security-docs-pods-recommendations" title="Permanent link">&para;</a></h2>
<h3 id="security-docs-pods-use-multiple-pod-security-admission-psa-modes-for-a-better-user-experience">Use multiple Pod Security Admission (PSA) modes for a better user experience<a class="headerlink" href="#security-docs-pods-use-multiple-pod-security-admission-psa-modes-for-a-better-user-experience" title="Permanent link">&para;</a></h3>
<p>As mentioned earlier, PSA <em>enforce</em> mode prevents pods with PSS violations from being applied, but does not stop higher-level controllers, such as Deployments. In fact, the Deployment will be applied successfully without any indication that the pods failed to be applied. While you can use <em>kubectl</em> to inspect the Deployment object, and discover the failed pods message from the PSA, the user experience could be better. To make the user experience better, multiple PSA modes (audit, enforce, warn) should be used.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1" href="#security-docs-pods-__codelineno-8-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<a id="__codelineno-8-2" name="__codelineno-8-2" href="#security-docs-pods-__codelineno-8-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Namespace</span>
<a id="__codelineno-8-3" name="__codelineno-8-3" href="#security-docs-pods-__codelineno-8-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-8-4" name="__codelineno-8-4" href="#security-docs-pods-__codelineno-8-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">policy-test</span>
<a id="__codelineno-8-5" name="__codelineno-8-5" href="#security-docs-pods-__codelineno-8-5"></a><span class="w">  </span><span class="nt">labels</span><span class="p">:</span>
<a id="__codelineno-8-6" name="__codelineno-8-6" href="#security-docs-pods-__codelineno-8-6"></a><span class="w">    </span><span class="nt">pod-security.kubernetes.io/audit</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">restricted</span>
<a id="__codelineno-8-7" name="__codelineno-8-7" href="#security-docs-pods-__codelineno-8-7"></a><span class="w">    </span><span class="nt">pod-security.kubernetes.io/enforce</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">restricted</span>
<a id="__codelineno-8-8" name="__codelineno-8-8" href="#security-docs-pods-__codelineno-8-8"></a><span class="w">    </span><span class="nt">pod-security.kubernetes.io/warn</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">restricted</span>
</code></pre></div>
<p>In the above example, with <em>enforce</em> mode defined, when a Deployment manifest with PSS violations in the respective podSpec is attempted to be applied to the Kubernetes API server, the Deployment will be successfully applied, but the pods will not. And, since the <em>audit</em> and <em>warn</em> modes are also enabled, the API server client will receive a warning message and the API server audit log event will be annotated with a message as well.</p>
<h3 id="security-docs-pods-restrict-the-containers-that-can-run-as-privileged">Restrict the containers that can run as privileged<a class="headerlink" href="#security-docs-pods-restrict-the-containers-that-can-run-as-privileged" title="Permanent link">&para;</a></h3>
<p>As mentioned, containers that run as privileged inherit all of the Linux capabilities assigned to root on the host.  Seldom do containers need these types of privileges to function properly.  There are multiple methods that can be used to restrict the permissions and capabilities of containers.</p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Fargate is a launch type that enables you to run "serverless" container(s) where the containers of a pod are run on infrastructure that AWS manages. With Fargate, you cannot run a privileged container or configure your pod to use hostNetwork or hostPort.</p>
</div>
<h3 id="security-docs-pods-do-not-run-processes-in-containers-as-root">Do not run processes in containers as root<a class="headerlink" href="#security-docs-pods-do-not-run-processes-in-containers-as-root" title="Permanent link">&para;</a></h3>
<p>All containers run as root by default.  This could be problematic if an attacker is able to exploit a vulnerability in the application and get shell access to the running container.  You can mitigate this risk a variety of ways.  First, by removing the shell from the container image.  Second, adding the USER directive to your Dockerfile or running the containers in the pod as a non-root user.  The Kubernetes podSpec includes a set of fields, under <code>spec.securityContext</code>, that let you specify the user and/or group under which to run your application.  These fields are <code>runAsUser</code> and <code>runAsGroup</code> respectively.  </p>
<p>To enforce the use of the <code>spec.securityContext</code>, and its associated elements, within the Kubernetes podSpec, policy-as-code or Pod Security Standards can be added to clusters. These solutions allow you to write and/or use policies or profiles that can validate inbound Kubernetes API server request payloads, before they are persisted into etcd. Furthermore, policy-as-code solutions can mutate inbound requests, and in some cases, generate new requests.</p>
<h3 id="security-docs-pods-never-run-docker-in-docker-or-mount-the-socket-in-the-container">Never run Docker in Docker or mount the socket in the container<a class="headerlink" href="#security-docs-pods-never-run-docker-in-docker-or-mount-the-socket-in-the-container" title="Permanent link">&para;</a></h3>
<p>While this conveniently lets you to build/run images in Docker containers, you're basically relinquishing complete control of the node to the process running in the container. If you need to build container images on Kubernetes use <a href="https://github.com/GoogleContainerTools/kaniko">Kaniko</a>, <a href="https://github.com/containers/buildah">buildah</a>, or a build service like <a href="https://docs.aws.amazon.com/codebuild/latest/userguide/welcome.html">CodeBuild</a> instead. </p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Kubernetes clusters used for CICD processing, such as building container images, should be isolated from clusters running more generalized workloads.</p>
</div>
<h3 id="security-docs-pods-restrict-the-use-of-hostpath-or-if-hostpath-is-necessary-restrict-which-prefixes-can-be-used-and-configure-the-volume-as-read-only">Restrict the use of hostPath or if hostPath is necessary restrict which prefixes can be used and configure the volume as read-only<a class="headerlink" href="#security-docs-pods-restrict-the-use-of-hostpath-or-if-hostpath-is-necessary-restrict-which-prefixes-can-be-used-and-configure-the-volume-as-read-only" title="Permanent link">&para;</a></h3>
<p><code>hostPath</code> is a volume that mounts a directory from the host directly to the container.  Rarely will pods need this type of access, but if they do, you need to be aware of the risks.  By default pods that run as root will have write access to the file system exposed by hostPath.  This could allow an attacker to modify the kubelet settings, create symbolic links to directories or files not directly exposed by the hostPath, e.g. /etc/shadow, install ssh keys, read secrets mounted to the host, and other malicious things. To mitigate the risks from hostPath, configure the <code>spec.containers.volumeMounts</code> as <code>readOnly</code>, for example: </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-9-1" name="__codelineno-9-1" href="#security-docs-pods-__codelineno-9-1"></a><span class="nt">volumeMounts</span><span class="p">:</span>
<a id="__codelineno-9-2" name="__codelineno-9-2" href="#security-docs-pods-__codelineno-9-2"></a><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">hostPath-volume</span>
<a id="__codelineno-9-3" name="__codelineno-9-3" href="#security-docs-pods-__codelineno-9-3"></a><span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">readOnly</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<a id="__codelineno-9-4" name="__codelineno-9-4" href="#security-docs-pods-__codelineno-9-4"></a><span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">mountPath</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/host-path</span>
</code></pre></div>
<p>You should also use policy-as-code solutions to restrict the directories that can be used by <code>hostPath</code> volumes, or prevent <code>hostPath</code> usage altogether.  You can use the Pod Security Standards <em>Baseline</em> or <em>Restricted</em> policies to prevent the use of <code>hostPath</code>.</p>
<p>For further information about the dangers of privileged escalation, read Seth Art's blog <a href="https://labs.bishopfox.com/tech-blog/bad-pods-kubernetes-pod-privilege-escalation">Bad Pods: Kubernetes Pod Privilege Escalation</a>.</p>
<h3 id="security-docs-pods-set-requests-and-limits-for-each-container-to-avoid-resource-contention-and-dos-attacks">Set requests and limits for each container to avoid resource contention and DoS attacks<a class="headerlink" href="#security-docs-pods-set-requests-and-limits-for-each-container-to-avoid-resource-contention-and-dos-attacks" title="Permanent link">&para;</a></h3>
<p>A pod without requests or limits can theoretically consume all of the resources available on a host.  As additional pods are scheduled onto a node, the node may experience CPU or memory pressure which can cause the Kubelet to terminate or evict pods from the node.  While you can’t prevent this from happening all together, setting requests and limits will help minimize resource contention and mitigate the risk from poorly written applications that consume an excessive amount of resources. </p>
<p>The <code>podSpec</code> allows you to specify requests and limits for CPU and memory.  CPU is considered a compressible resource because it can be oversubscribed.  Memory is incompressible, i.e. it cannot be shared among multiple containers.  </p>
<p>When you specify <em>requests</em> for CPU or memory, you’re essentially designating the amount of <em>memory</em> that containers are guaranteed to get.  Kubernetes aggregates the requests of all the containers in a pod to determine which node to schedule the pod onto.  If a container exceeds the requested amount of memory it may be subject to termination if there’s memory pressure on the node. </p>
<p><em>Limits</em> are the maximum amount of CPU and memory resources that a container is allowed to consume and directly corresponds to the <code>memory.limit_in_bytes</code> value of the cgroup created for the container.  A container that exceeds the memory limit will be OOM killed. If a container exceeds its CPU limit, it will be throttled. </p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>When using container <code>resources.limits</code> it is strongly recommended that container resource usage (a.k.a. Resource Footprints) be data-driven and accurate, based on load testing. Absent an accurate and trusted resource footprint, container <code>resources.limits</code> can be padded. For example, <code>resources.limits.memory</code> could be padded 20-30% higher than observable maximums, to account for potential memory resource limit inaccuracies.</p>
</div>
<p>Kubernetes uses three Quality of Service (QoS) classes to prioritize the workloads running on a node.  These include: </p>
<ul>
<li>guaranteed</li>
<li>burstable</li>
<li>best-effort</li>
</ul>
<p>If limits and requests are not set, the pod is configured as <em>best-effort</em> (lowest priority).  Best-effort pods are the first to get killed when there is insufficient memory.  If limits are set on <em>all</em> containers within the pod, or if the requests and limits are set to the same values and not equal to 0, the pod is configured as <em>guaranteed</em> (highest priority).  Guaranteed pods will not be killed unless they exceed their configured memory limits. If the limits and requests are configured with different values and not equal to 0, or one container within the pod sets limits and the others don’t or have limits set for different resources, the pods are configured as <em>burstable</em> (medium priority). These pods have some resource guarantees, but can be killed once they exceed their requested memory. </p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Requests don't affect the <code>memory_limit_in_bytes</code> value of the container's cgroup; the cgroup limit is set to the amount of memory available on the host. Nevertheless, setting the requests value too low could cause the pod to be targeted for termination by the kubelet if the node undergoes memory pressure. </p>
</div>
<table>
<thead>
<tr>
<th style="text-align: left;">Class</th>
<th style="text-align: left;">Priority</th>
<th style="text-align: left;">Condition</th>
<th style="text-align: left;">Kill Condition</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Guaranteed</td>
<td style="text-align: left;">highest</td>
<td style="text-align: left;">limit = request != 0</td>
<td style="text-align: left;">Only exceed memory limits</td>
</tr>
<tr>
<td style="text-align: left;">Burstable</td>
<td style="text-align: left;">medium</td>
<td style="text-align: left;">limit != request != 0</td>
<td style="text-align: left;">Can be killed if exceed request memory</td>
</tr>
<tr>
<td style="text-align: left;">Best-Effort</td>
<td style="text-align: left;">lowest</td>
<td style="text-align: left;">limit &amp; request Not Set</td>
<td style="text-align: left;">First to get killed when there's insufficient memory</td>
</tr>
</tbody>
</table>
<p>For additional information about resource QoS, please refer to the <a href="https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/">Kubernetes documentation</a>.</p>
<p>You can force the use of requests and limits by setting a <a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/">resource quota</a> on a namespace or by creating a <a href="https://kubernetes.io/docs/concepts/policy/limit-range/">limit range</a>.  A resource quota allows you to specify the total amount of resources, e.g. CPU and RAM, allocated to a namespace.  When it’s applied to a namespace, it forces you to specify requests and limits for all containers deployed into that namespace. By contrast, limit ranges give you more granular control of the allocation of resources. With limit ranges you can min/max for CPU and memory resources per pod or per container within a namespace.  You can also use them to set default request/limit values if none are provided.</p>
<p>Policy-as-code solutions can be used enforce requests and limits. or to even create the resource quotas and limit ranges when namespaces are created.</p>
<h3 id="security-docs-pods-do-not-allow-privileged-escalation">Do not allow privileged escalation<a class="headerlink" href="#security-docs-pods-do-not-allow-privileged-escalation" title="Permanent link">&para;</a></h3>
<p>Privileged escalation allows a process to change the security context under which its running.  Sudo is a good example of this as are binaries with the SUID or SGID bit.  Privileged escalation is basically a way for users to execute a file with the permissions of another user or group.  You can prevent a container from using privileged escalation by implementing a policy-as-code mutating policy that sets <code>allowPrivilegeEscalation</code> to <code>false</code> or by setting <code>securityContext.allowPrivilegeEscalation</code> in the <code>podSpec</code>. Policy-as-code policies can also be used to prevent API server requests from succeeding if incorrect settings are detected. Pod Security Standards can also be used to prevent pods from using privilege escalation.</p>
<h3 id="security-docs-pods-disable-serviceaccount-token-mounts">Disable ServiceAccount token mounts<a class="headerlink" href="#security-docs-pods-disable-serviceaccount-token-mounts" title="Permanent link">&para;</a></h3>
<p>For pods that do not need to access the Kubernetes API, you can disable the
automatic mounting of a ServiceAccount token on a pod spec, or for all pods that
use a particular ServiceAccount.</p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Disabling ServiceAccount mounting does not prevent a pod from having network
access to the Kubernetes API. To prevent a pod from having any network
access to the Kubernetes API, you will need to modify the <a href="https://docs.aws.amazon.com/eks/latest/userguide/cluster-endpoint.html">EKS cluster
endpoint access</a> and use
<a href="#security-docs-network-network-policy">NetworkPolicy</a> to block pod access</p>
</div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-10-1" name="__codelineno-10-1" href="#security-docs-pods-__codelineno-10-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<a id="__codelineno-10-2" name="__codelineno-10-2" href="#security-docs-pods-__codelineno-10-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Pod</span>
<a id="__codelineno-10-3" name="__codelineno-10-3" href="#security-docs-pods-__codelineno-10-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-10-4" name="__codelineno-10-4" href="#security-docs-pods-__codelineno-10-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pod-no-automount</span>
<a id="__codelineno-10-5" name="__codelineno-10-5" href="#security-docs-pods-__codelineno-10-5"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-10-6" name="__codelineno-10-6" href="#security-docs-pods-__codelineno-10-6"></a><span class="w">  </span><span class="nt">automountServiceAccountToken</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-11-1" name="__codelineno-11-1" href="#security-docs-pods-__codelineno-11-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<a id="__codelineno-11-2" name="__codelineno-11-2" href="#security-docs-pods-__codelineno-11-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ServiceAccount</span>
<a id="__codelineno-11-3" name="__codelineno-11-3" href="#security-docs-pods-__codelineno-11-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-11-4" name="__codelineno-11-4" href="#security-docs-pods-__codelineno-11-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">sa-no-automount</span>
<a id="__codelineno-11-5" name="__codelineno-11-5" href="#security-docs-pods-__codelineno-11-5"></a><span class="nt">automountServiceAccountToken</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
</code></pre></div>
<h3 id="security-docs-pods-disable-service-discovery">Disable service discovery<a class="headerlink" href="#security-docs-pods-disable-service-discovery" title="Permanent link">&para;</a></h3>
<p>For pods that do not need to lookup or call in-cluster services, you can
reduce the amount of information given to a pod. You can set the Pod's DNS
policy to not use CoreDNS, and not expose services in the pod's namespace as
environment variables. See the <a href="https://kubernetes.io/docs/concepts/services-networking/service/#environment-variables">Kubernetes docs on environment
variables</a> for more information on service links. The default
value for a pod's DNS policy is "ClusterFirst" which uses in-cluster DNS, while
the non-default value "Default" uses the underlying node's DNS resolution. See
the <a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-s-dns-policy">Kubernetes docs on Pod DNS policy</a> for more information.</p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Disabling service links and changing the pod's DNS policy does not prevent a
pod from having network access to the in-cluster DNS service. An attacker
can still enumerate services in a cluster by reaching the in-cluster DNS
service. (ex: <code>dig SRV *.*.svc.cluster.local @$CLUSTER_DNS_IP</code>) To prevent
in-cluster service discovery, use <a href="#security-docs-network-network-policy">NetworkPolicy</a>
to block pod access</p>
</div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-12-1" name="__codelineno-12-1" href="#security-docs-pods-__codelineno-12-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<a id="__codelineno-12-2" name="__codelineno-12-2" href="#security-docs-pods-__codelineno-12-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Pod</span>
<a id="__codelineno-12-3" name="__codelineno-12-3" href="#security-docs-pods-__codelineno-12-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-12-4" name="__codelineno-12-4" href="#security-docs-pods-__codelineno-12-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pod-no-service-info</span>
<a id="__codelineno-12-5" name="__codelineno-12-5" href="#security-docs-pods-__codelineno-12-5"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-12-6" name="__codelineno-12-6" href="#security-docs-pods-__codelineno-12-6"></a><span class="w">    </span><span class="nt">dnsPolicy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Default</span><span class="w"> </span><span class="c1"># &quot;Default&quot; is not the true default value</span>
<a id="__codelineno-12-7" name="__codelineno-12-7" href="#security-docs-pods-__codelineno-12-7"></a><span class="w">    </span><span class="nt">enableServiceLinks</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
</code></pre></div>
<h3 id="security-docs-pods-configure-your-images-with-read-only-root-file-system">Configure your images with read-only root file system<a class="headerlink" href="#security-docs-pods-configure-your-images-with-read-only-root-file-system" title="Permanent link">&para;</a></h3>
<p>Configuring your images with a read-only root file system prevents an attacker from overwriting a binary on the file system that your application uses. If your application has to write to the file system, consider writing to a temporary directory or attach and mount a volume. You can enforce this by setting the pod's SecurityContext as follows:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-13-1" name="__codelineno-13-1" href="#security-docs-pods-__codelineno-13-1"></a><span class="nn">...</span>
<a id="__codelineno-13-2" name="__codelineno-13-2" href="#security-docs-pods-__codelineno-13-2"></a><span class="nt">securityContext</span><span class="p">:</span>
<a id="__codelineno-13-3" name="__codelineno-13-3" href="#security-docs-pods-__codelineno-13-3"></a><span class="w">  </span><span class="nt">readOnlyRootFilesystem</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<a id="__codelineno-13-4" name="__codelineno-13-4" href="#security-docs-pods-__codelineno-13-4"></a><span class="nn">...</span>
</code></pre></div>
<p>Policy-as-code and Pod Security Standards can be used to enforce this behavior.</p>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>As per <a href="https://kubernetes.io/docs/concepts/windows/intro/">Windows containers in Kubernetes</a> <code>securityContext.readOnlyRootFilesystem</code> cannot be set to
<code>true</code> for a container running on Windows as write access is required for registry and system processes to run inside the container.</p>
</div>
<h2 id="security-docs-pods-tools-and-resources">Tools and Resources<a class="headerlink" href="#security-docs-pods-tools-and-resources" title="Permanent link">&para;</a></h2>
<ul>
<li><a href="https://github.com/open-policy-agent/gatekeeper-library">open-policy-agent/gatekeeper-library: The OPA Gatekeeper policy library</a> a library of OPA/Gatekeeper policies that you can use as a substitute for PSPs.</li>
<li><a href="https://kyverno.io/policies/">Kyverno Policy Library</a></li>
<li>A collection of common OPA and Kyverno <a href="https://github.com/aws/aws-eks-best-practices/tree/master/policies">policies</a> for EKS.</li>
<li><a href="https://aws.amazon.com/blogs/containers/policy-based-countermeasures-for-kubernetes-part-1/">Policy based countermeasures: part 1</a></li>
<li><a href="https://aws.amazon.com/blogs/containers/policy-based-countermeasures-for-kubernetes-part-2/">Policy based countermeasures: part 2</a></li>
<li><a href="https://appvia.github.io/psp-migration/">Pod Security Policy Migrator</a> a tool that converts PSPs to OPA/Gatekeeper, KubeWarden, or Kyverno policies</li>
</ul></section><section class="print-page" id="security-docs-multitenancy"><h1 id="security-docs-multitenancy-tenant-isolation">Tenant Isolation<a class="headerlink" href="#security-docs-multitenancy-tenant-isolation" title="Permanent link">&para;</a></h1>
<p>When we think of multi-tenancy, we often want to isolate a user or application from other users or applications running on a shared infrastructure. </p>
<p>Kubernetes is a <em>single tenant orchestrator</em>, i.e. a single instance of the control plane is shared among all the tenants  within a cluster. There are, however, various Kubernetes objects that you can use to create the semblance of multi-tenancy. For example, Namespaces and Role-based access controls (RBAC) can be implemented to logically isolate tenants from each other. Similarly, Quotas and Limit Ranges can be used to control the amount of cluster resources each tenant can consume. Nevertheless, the cluster is the only construct that provides a strong security boundary. This is because an attacker that manages to gain access to a host within the cluster can retrieve <em>all</em> Secrets, ConfigMaps, and Volumes, mounted on that host. They could also impersonate the Kubelet which would allow them to manipulate the attributes of the node and/or move laterally within the cluster.</p>
<p>The following sections will explain how to implement tenant isolation while mitigating the risks of using a single tenant orchestrator like Kubernetes.</p>
<h2 id="security-docs-multitenancy-soft-multi-tenancy">Soft multi-tenancy<a class="headerlink" href="#security-docs-multitenancy-soft-multi-tenancy" title="Permanent link">&para;</a></h2>
<p>With soft multi-tenancy, you use native Kubernetes constructs, e.g. namespaces, roles and role bindings, and network policies, to create logical separation between tenants. RBAC, for example, can prevent tenants from accessing or manipulate each other's resources. Quotas and limit ranges control the amount of cluster resources each tenant can consume while network policies can help prevent applications deployed into different namespaces from communicating with each other.</p>
<p>None of these controls, however, prevent pods from different tenants from sharing a node. If stronger isolation is required, you can use a node selector, anti-affinity rules, and/or taints and tolerations to force pods from different tenants to be scheduled onto separate nodes; often referred to as <em>sole tenant nodes</em>. This could get rather complicated, and cost prohibitive, in an environment with many tenants. </p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Soft multi-tenancy implemented with Namespaces does not allow you to provide tenants with a filtered list of Namespaces because Namespaces are a globally scoped Type. If a tenant has the ability to view a particular Namespace, it can view all Namespaces within the cluster. </p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>With soft-multi-tenancy, tenants retain the ability to query CoreDNS for all services that run within the cluster by default. An attacker could exploit this by running dig SRV <em>.</em>.svc.cluster.local from any pod in the cluster.  If you need to restrict access to DNS records of services that run within your clusters, consider using the Firewall or Policy plugins for CoreDNS. For additional information, see <a href="https://github.com/coredns/policy#kubernetes-metadata-multi-tenancy-policy">https://github.com/coredns/policy#kubernetes-metadata-multi-tenancy-policy</a>. </p>
</div>
<p><a href="https://github.com/kiosk-sh/kiosk">Kiosk</a> is an open source project that can aid in the implementation of soft multi-tenancy.  It is implemented as a series of CRDs and controllers that provide the following capabilities: </p>
<ul>
<li><strong>Accounts &amp; Account Users</strong> to separate tenants in a shared Kubernetes cluster</li>
<li><strong>Self-Service Namespace Provisioning</strong> for account users</li>
<li><strong>Account Limits</strong> to ensure quality of service and fairness when sharing a cluster</li>
<li><strong>Namespace Templates</strong> for secure tenant isolation and self-service namespace initialization</li>
</ul>
<p><a href="https://loft.sh">Loft</a> is a commercial offering from the maintainers of Kiosk and <a href="https://github.com/devspace-cloud/devspace">DevSpace</a> that adds the following capabilities:</p>
<ul>
<li><strong>Multi-cluster access</strong> for granting access to spaces in different clusters </li>
<li><strong>Sleep mode</strong> scales down deployments in a space during periods of inactivity</li>
<li><strong>Single sign-on</strong> with OIDC authentication providers like GitHub</li>
</ul>
<p>There are three primary use cases that can be addressed by soft multi-tenancy.</p>
<h3 id="security-docs-multitenancy-enterprise-setting">Enterprise Setting<a class="headerlink" href="#security-docs-multitenancy-enterprise-setting" title="Permanent link">&para;</a></h3>
<p>The first is in an Enterprise setting where the "tenants" are semi-trusted in that they are employees, contractors, or are otherwise authorized by the organization. Each tenant will typically align to an administrative division such as a department or team. </p>
<p>In this type of setting, a cluster administrator will usually be responsible for creating namespaces and managing policies. They may also implement a delegated administration model where certain individuals are given oversight of a namespace, allowing them to perform CRUD operations for non-policy related objects like deployments, services, pods, jobs, etc.</p>
<p>The isolation provided by a container runtime may be acceptable within this setting or it may need to be augmented with additional controls for pod security. It may also be necessary to restrict communication between services in different namespaces if stricter isolation is required.</p>
<h3 id="security-docs-multitenancy-kubernetes-as-a-service">Kubernetes as a Service<a class="headerlink" href="#security-docs-multitenancy-kubernetes-as-a-service" title="Permanent link">&para;</a></h3>
<p>By contrast, soft multi-tenancy can be used in settings where you want to offer Kubernetes as a service (KaaS). With KaaS, your application is hosted in a shared cluster along with a collection of controllers and CRDs that provide a set of PaaS services.  Tenants interact directly with the Kubernetes API server and are permitted to perform CRUD operations on non-policy objects. There is also an element of self-service in that tenants may be allowed to create and manage their own namespaces. In this type of environment, tenants are assumed to be running untrusted code.</p>
<p>To isolate tenants in this type of environment, you will likely need to implement strict network policies as well as <em>pod sandboxing</em>. Sandboxing is where you run the containers of a pod inside a micro VM like Firecracker or in a user-space kernel.  Today, you can create sandboxed pods with EKS Fargate.</p>
<h3 id="security-docs-multitenancy-software-as-a-service-saas">Software as a Service (SaaS)<a class="headerlink" href="#security-docs-multitenancy-software-as-a-service-saas" title="Permanent link">&para;</a></h3>
<p>The final use case for soft multi-tenancy is in a Software-as-a-Service (SaaS) setting.  In this environment, each tenant is associated with a particular <em>instance</em> of an application that's running within the cluster.  Each instance often has its own data and uses separate access controls that are usually independent of Kubernetes RBAC.</p>
<p>Unlike the other use cases, the tenant in a SaaS setting does not directly interface with the Kubernetes API.  Instead, the SaaS application is responsible for interfacing with the Kubernetes API to create the necessary objects to support each tenant.</p>
<h2 id="security-docs-multitenancy-kubernetes-constructs">Kubernetes Constructs<a class="headerlink" href="#security-docs-multitenancy-kubernetes-constructs" title="Permanent link">&para;</a></h2>
<p>In each of these instances the following constructs are used to isolate tenants from each other: </p>
<h3 id="security-docs-multitenancy-namespaces">Namespaces<a class="headerlink" href="#security-docs-multitenancy-namespaces" title="Permanent link">&para;</a></h3>
<p>Namespaces are fundamental to implementing soft multi-tenancy. They allow you to divide the cluster into logical partitions. Quotas, network policies, service accounts, and other objects needed to implement multi-tenancy are scoped to a namespace.</p>
<h3 id="security-docs-multitenancy-network-policies">Network policies<a class="headerlink" href="#security-docs-multitenancy-network-policies" title="Permanent link">&para;</a></h3>
<p>By default, all pods in a Kubernetes cluster are allowed to communicate with each other. This behavior can be altered using network policies.</p>
<p>Network policies restrict communication between pods using labels or IP address ranges. In a multi-tenant environment where strict network isolation between tenants is required, we recommend starting with a default rule that denies communication between pods, and another rule that allows all pods to query the DNS server for name resolution. With that in place, you can begin adding more permissive rules that allow for communication within a namespace. This can be further refined as required. </p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Amazon <a href="https://aws.amazon.com/blogs/containers/amazon-vpc-cni-now-supports-kubernetes-network-policies/">VPC CNI now supports Kubernetes Network Policies</a> to create policies that can isolate sensitive workloads and protect them from unauthorized access when running Kubernetes on AWS. This means that you can use all the capabilities of the Network Policy API within your Amazon EKS cluster. This level of granular control enables you to implement the principle of least privilege, which ensures that only authorized pods are allowed to communicate with each other.</p>
</div>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Network policies are necessary but not sufficient. The enforcement of network policies requires a policy engine such as Calico or Cilium.</p>
</div>
<h3 id="security-docs-multitenancy-role-based-access-control-rbac">Role-based access control (RBAC)<a class="headerlink" href="#security-docs-multitenancy-role-based-access-control-rbac" title="Permanent link">&para;</a></h3>
<p>Roles and role bindings are the Kubernetes objects used to enforce role-based access control (RBAC) in Kubernetes. <strong>Roles</strong> contain lists of actions that can be performed against objects in your cluster. <strong>Role bindings</strong> specify the individuals or groups to whom the roles apply.  In the enterprise and KaaS settings, RBAC can be used to permit administration of objects by selected groups or individuals.</p>
<h3 id="security-docs-multitenancy-quotas">Quotas<a class="headerlink" href="#security-docs-multitenancy-quotas" title="Permanent link">&para;</a></h3>
<p>Quotas are used to define limits on workloads hosted in your cluster. With quotas, you can specify the maximum amount of CPU and memory that a pod can consume, or you can limit the number of resources that can be allocated in a cluster or namespace. <strong>Limit ranges</strong> allow you to declare minimum, maximum, and default values for each limit.</p>
<p>Overcommitting resources in a shared cluster is often beneficial because it allows you maximize your resources.  However, unbounded access to a cluster can cause resource starvation, which can lead to performance degradation and loss of application availability. If a pod's requests are set too low and the actual resource utilization exceeds the capacity of the node, the node will begin to experience CPU or memory pressure.  When this happens, pods may be restarted and/or evicted from the node.</p>
<p>To prevent this from happening, you should plan to impose quotas on namespaces in a multi-tenant environment to force tenants to specify requests and limits when scheduling their pods on the cluster.  It will also mitigate a potential denial of service by constraining the amount of resources a pod can consume.</p>
<p>You can also use quotas to apportion the cluster's resources to align with a tenant's spend.  This is particularly useful in the KaaS scenario.</p>
<h3 id="security-docs-multitenancy-pod-priority-and-preemption">Pod priority and preemption<a class="headerlink" href="#security-docs-multitenancy-pod-priority-and-preemption" title="Permanent link">&para;</a></h3>
<p>Pod priority and preemption can be useful when you want to provide more importance to a Pod relative to other Pods.  For example, with pod priority you can configure pods from customer A to run at a higher priority than customer B. When there's insufficient capacity available, the scheduler will evict the lower-priority pods from customer B to accommodate the higher-priority pods from customer A.  This can be especially handy in a SaaS environment where customers willing to pay a premium receive a higher priority.</p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Pods priority can have an undesired effect on other Pods with lower priority. For example, although the victim pods are terminated gracefully but the PodDisruptionBudget is not guaranteed, which could break a application with lower priority that relies on a quorum of Pods, see <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#limitations-of-preemption">Limitations of preemption</a>.</p>
</div>
<h2 id="security-docs-multitenancy-mitigating-controls">Mitigating controls<a class="headerlink" href="#security-docs-multitenancy-mitigating-controls" title="Permanent link">&para;</a></h2>
<p>Your chief concern as an administrator of a multi-tenant environment is preventing an attacker from gaining access to the underlying host. The following controls should be considered to mitigate this risk: </p>
<h3 id="security-docs-multitenancy-sandboxed-execution-environments-for-containers">Sandboxed execution environments for containers<a class="headerlink" href="#security-docs-multitenancy-sandboxed-execution-environments-for-containers" title="Permanent link">&para;</a></h3>
<p>Sandboxing is a technique by which each container is run in its own isolated virtual machine. Technologies that perform pod sandboxing include <a href="https://firecracker-microvm.github.io/">Firecracker</a> and Weave's <a href="https://www.weave.works/blog/firekube-fast-and-secure-kubernetes-clusters-using-weave-ignite">Firekube</a>.</p>
<p>For additional information about the effort to make Firecracker a supported runtime for EKS, see
<a href="https://threadreaderapp.com/thread/1238496944684597248.html">https://threadreaderapp.com/thread/1238496944684597248.html</a>. </p>
<h3 id="security-docs-multitenancy-open-policy-agent-opa-gatekeeper">Open Policy Agent (OPA) &amp; Gatekeeper<a class="headerlink" href="#security-docs-multitenancy-open-policy-agent-opa-gatekeeper" title="Permanent link">&para;</a></h3>
<p><a href="https://github.com/open-policy-agent/gatekeeper">Gatekeeper</a> is a Kubernetes admission controller that enforces policies created with <a href="https://www.openpolicyagent.org/">OPA</a>. With OPA you can create a policy that runs pods from tenants on separate instances or at a higher priority than other tenants. A collection of common OPA policies can be found in the GitHub <a href="https://github.com/aws/aws-eks-best-practices/tree/master/policies/opa">repository</a> for this project. </p>
<p>There is also an experimental <a href="https://github.com/coredns/coredns-opa">OPA plugin for CoreDNS</a> that allows you to use OPA to filter/control the records returned by CoreDNS. </p>
<h3 id="security-docs-multitenancy-kyverno">Kyverno<a class="headerlink" href="#security-docs-multitenancy-kyverno" title="Permanent link">&para;</a></h3>
<p><a href="https://kyverno.io">Kyverno</a> is a Kubernetes native policy engine that can validate, mutate, and generate configurations with policies as Kubernetes resources. Kyverno uses Kustomize-style overlays for validation, supports JSON Patch and strategic merge patch for mutation, and can clone resources across namespaces based on flexible triggers.</p>
<p>You can use Kyverno to isolate namespaces, enforce pod security and other best practices, and generate default configurations such as network policies.  Several examples are included in the GitHub <a href="https://github.com/aws/aws-eks-best-practices/tree/master/policies/kyverno">repository</a> for this project. Many others are included in the <a href="https://kyverno.io/policies/">policy library</a> on the Kyverno website.</p>
<h3 id="security-docs-multitenancy-isolating-tenant-workloads-to-specific-nodes">Isolating tenant workloads to specific nodes<a class="headerlink" href="#security-docs-multitenancy-isolating-tenant-workloads-to-specific-nodes" title="Permanent link">&para;</a></h3>
<p>Restricting tenant workloads to run on specific nodes can be used to increase isolation in the soft multi-tenancy model. With this approach, tenant-specific workloads are only run on nodes provisioned for the respective tenants. To achieve this isolation, native Kubernetes properties (node affinity, and taints and tolerations) are used to target specific nodes for pod scheduling, and prevent pods, from other tenants, from being scheduled on the tenant-specific nodes.</p>
<h4 id="security-docs-multitenancy-part-1-node-affinity">Part 1 - Node affinity<a class="headerlink" href="#security-docs-multitenancy-part-1-node-affinity" title="Permanent link">&para;</a></h4>
<p>Kubernetes <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity">node affinity</a> is used to target nodes for scheduling, based on node <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/">labels</a>. With node affinity rules, the pods are attracted to specific nodes that match the selector terms. In the below pod specification, the <code>requiredDuringSchedulingIgnoredDuringExecution</code> node affinity is applied to the respective pod. The result is that the pod will target nodes that are labeled with the following key/value: <code>node-restriction.kubernetes.io/tenant: tenants-x</code>.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#security-docs-multitenancy-__codelineno-0-1"></a><span class="nn">...</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#security-docs-multitenancy-__codelineno-0-2"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#security-docs-multitenancy-__codelineno-0-3"></a><span class="w">  </span><span class="nt">affinity</span><span class="p">:</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#security-docs-multitenancy-__codelineno-0-4"></a><span class="w">    </span><span class="nt">nodeAffinity</span><span class="p">:</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#security-docs-multitenancy-__codelineno-0-5"></a><span class="w">      </span><span class="nt">requiredDuringSchedulingIgnoredDuringExecution</span><span class="p">:</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#security-docs-multitenancy-__codelineno-0-6"></a><span class="w">        </span><span class="nt">nodeSelectorTerms</span><span class="p">:</span>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#security-docs-multitenancy-__codelineno-0-7"></a><span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">matchExpressions</span><span class="p">:</span>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#security-docs-multitenancy-__codelineno-0-8"></a><span class="w">          </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">node-restriction.kubernetes.io/tenant</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#security-docs-multitenancy-__codelineno-0-9"></a><span class="w">            </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">In</span>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#security-docs-multitenancy-__codelineno-0-10"></a><span class="w">            </span><span class="nt">values</span><span class="p">:</span>
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#security-docs-multitenancy-__codelineno-0-11"></a><span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">tenants-x</span>
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#security-docs-multitenancy-__codelineno-0-12"></a><span class="nn">...</span>
</code></pre></div>
<p>With this node affinity, the label is required during scheduling, but not during execution; if the underlying nodes' labels change, the pods will not be evicted due solely to that label change. However, future scheduling could be impacted.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The label prefix of <code>node-restriction.kubernetes.io/</code> has special meaning in Kubernetes. <a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#noderestriction">NodeRestriction</a> which is enabled for EKS clusters prevents <code>kubelet</code> from adding/removing/updating labels with this prefix. Attackers aren't able to use the <code>kubelet</code>'s credentials to update the node object or modify the system setup to pass these labels into <code>kubelet</code> as <code>kubelet</code> isn't allowed to modify these labels. If this prefix is used for all pod to node scheduling, it prevents scenarios where an attacker may want to attract a different set of workloads to a node by modifying the node labels.</p>
</div>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>Instead of node affinity, we could have used the <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector">node selector</a>. However, node affinity is more expressive and allows for more conditions to be considered during pod scheduling. For additional information about the differences and more advanced scheduling choices, please see this CNCF blog post on <a href="https://www.cncf.io/blog/2021/07/27/advanced-kubernetes-pod-to-node-scheduling/">Advanced Kubernetes pod to node scheduling</a>.</p>
</div>
<h4 id="security-docs-multitenancy-part-2-taints-and-tolerations">Part 2 - Taints and tolerations<a class="headerlink" href="#security-docs-multitenancy-part-2-taints-and-tolerations" title="Permanent link">&para;</a></h4>
<p>Attracting pods to nodes is just the first part of this three-part approach. For this approach to work, we must repel pods from scheduling onto nodes for which the pods are not authorized. To repel unwanted or unauthorized pods, Kubernetes uses node <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">taints</a>. Taints are used to place conditions on nodes that prevent pods from being scheduled. The below taint uses a key-value pair of <code>tenant: tenants-x</code>.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#security-docs-multitenancy-__codelineno-1-1"></a><span class="nn">...</span>
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#security-docs-multitenancy-__codelineno-1-2"></a><span class="w">    </span><span class="nt">taints</span><span class="p">:</span>
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#security-docs-multitenancy-__codelineno-1-3"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">tenant</span>
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#security-docs-multitenancy-__codelineno-1-4"></a><span class="w">        </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">tenants-x</span>
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#security-docs-multitenancy-__codelineno-1-5"></a><span class="w">        </span><span class="nt">effect</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">NoSchedule</span>
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#security-docs-multitenancy-__codelineno-1-6"></a><span class="nn">...</span>
</code></pre></div>
<p>Given the above node <code>taint</code>, only pods that <em>tolerate</em> the taint will be allowed to be scheduled on the node. To allow authorized pods to be scheduled onto the node, the respective pod specifications must include a <code>toleration</code> to the taint, as seen below.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#security-docs-multitenancy-__codelineno-2-1"></a><span class="nn">...</span>
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#security-docs-multitenancy-__codelineno-2-2"></a><span class="w">  </span><span class="nt">tolerations</span><span class="p">:</span>
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#security-docs-multitenancy-__codelineno-2-3"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">effect</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">NoSchedule</span>
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#security-docs-multitenancy-__codelineno-2-4"></a><span class="w">    </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">tenant</span>
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#security-docs-multitenancy-__codelineno-2-5"></a><span class="w">    </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Equal</span>
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#security-docs-multitenancy-__codelineno-2-6"></a><span class="w">    </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">tenants-x</span>
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#security-docs-multitenancy-__codelineno-2-7"></a><span class="nn">...</span>
</code></pre></div>
<p>Pods with the above <code>toleration</code> will not be stopped from scheduling on the node, at least not because of that specific taint. Taints are also used by Kubernetes to temporarily stop pod scheduling during certain conditions, like node resource pressure. With node affinity, and taints and tolerations, we can effectively attract the desired pods to specific nodes and repel unwanted pods.</p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Certain Kubernetes pods are required to run on all nodes. Examples of these pods are those started by the <a href="https://github.com/containernetworking/cni">Container Network Interface (CNI)</a> and <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/">kube-proxy</a> <a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/">daemonsets</a>. To that end, the specifications for these pods contain very permissive tolerations, to tolerate different taints. Care should be taken to not change these tolerations. Changing these tolerations could result in incorrect cluster operation. Additionally, policy-management tools, such as <a href="https://github.com/open-policy-agent/gatekeeper">OPA/Gatekeeper</a> and <a href="https://kyverno.io/">Kyverno</a> can be used to write validating policies that prevent unauthorized pods from using these permissive tolerations.</p>
</div>
<h4 id="security-docs-multitenancy-part-3-policy-based-management-for-node-selection">Part 3 - Policy-based management for node selection<a class="headerlink" href="#security-docs-multitenancy-part-3-policy-based-management-for-node-selection" title="Permanent link">&para;</a></h4>
<p>There are several tools that can be used to help manage the node affinity and tolerations of pod specifications, including enforcement of rules in CICD pipelines. However, enforcement of isolation should also be done at the Kubernetes cluster level. For this purpose, policy-management tools can be used to <em>mutate</em> inbound Kubernetes API server requests, based on request payloads, to apply the respective node affinity rules and tolerations mentioned above.</p>
<p>For example, pods destined for the <em>tenants-x</em> namespace can be <em>stamped</em> with the correct node affinity and toleration to permit scheduling on the <em>tenants-x</em> nodes. Utilizing policy-management tools configured using the Kubernetes <a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook">Mutating Admission Webhook</a>, policies can be used to mutate the inbound pod specifications. The mutations add the needed elements to allow desired scheduling. An example OPA/Gatekeeper policy that adds a node affinity is seen below.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#security-docs-multitenancy-__codelineno-3-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">mutations.gatekeeper.sh/v1alpha1</span>
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#security-docs-multitenancy-__codelineno-3-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Assign</span>
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#security-docs-multitenancy-__codelineno-3-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-3-4" name="__codelineno-3-4" href="#security-docs-multitenancy-__codelineno-3-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">mutator-add-nodeaffinity-pod</span>
<a id="__codelineno-3-5" name="__codelineno-3-5" href="#security-docs-multitenancy-__codelineno-3-5"></a><span class="w">  </span><span class="nt">annotations</span><span class="p">:</span>
<a id="__codelineno-3-6" name="__codelineno-3-6" href="#security-docs-multitenancy-__codelineno-3-6"></a><span class="w">    </span><span class="nt">aws-eks-best-practices/description</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">&gt;-</span>
<a id="__codelineno-3-7" name="__codelineno-3-7" href="#security-docs-multitenancy-__codelineno-3-7"></a><span class="w">      </span><span class="no">Adds Node affinity - https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity</span>
<a id="__codelineno-3-8" name="__codelineno-3-8" href="#security-docs-multitenancy-__codelineno-3-8"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-3-9" name="__codelineno-3-9" href="#security-docs-multitenancy-__codelineno-3-9"></a><span class="w">  </span><span class="nt">applyTo</span><span class="p">:</span>
<a id="__codelineno-3-10" name="__codelineno-3-10" href="#security-docs-multitenancy-__codelineno-3-10"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">groups</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-3-11" name="__codelineno-3-11" href="#security-docs-multitenancy-__codelineno-3-11"></a><span class="w">    </span><span class="nt">kinds</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;Pod&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-3-12" name="__codelineno-3-12" href="#security-docs-multitenancy-__codelineno-3-12"></a><span class="w">    </span><span class="nt">versions</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;v1&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-3-13" name="__codelineno-3-13" href="#security-docs-multitenancy-__codelineno-3-13"></a><span class="w">  </span><span class="nt">match</span><span class="p">:</span>
<a id="__codelineno-3-14" name="__codelineno-3-14" href="#security-docs-multitenancy-__codelineno-3-14"></a><span class="w">    </span><span class="nt">namespaces</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;tenants-x&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-3-15" name="__codelineno-3-15" href="#security-docs-multitenancy-__codelineno-3-15"></a><span class="w">  </span><span class="nt">location</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms&quot;</span>
<a id="__codelineno-3-16" name="__codelineno-3-16" href="#security-docs-multitenancy-__codelineno-3-16"></a><span class="w">  </span><span class="nt">parameters</span><span class="p">:</span>
<a id="__codelineno-3-17" name="__codelineno-3-17" href="#security-docs-multitenancy-__codelineno-3-17"></a><span class="w">    </span><span class="nt">assign</span><span class="p">:</span>
<a id="__codelineno-3-18" name="__codelineno-3-18" href="#security-docs-multitenancy-__codelineno-3-18"></a><span class="w">      </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span>
<a id="__codelineno-3-19" name="__codelineno-3-19" href="#security-docs-multitenancy-__codelineno-3-19"></a><span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">matchExpressions</span><span class="p">:</span>
<a id="__codelineno-3-20" name="__codelineno-3-20" href="#security-docs-multitenancy-__codelineno-3-20"></a><span class="w">          </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;tenant&quot;</span>
<a id="__codelineno-3-21" name="__codelineno-3-21" href="#security-docs-multitenancy-__codelineno-3-21"></a><span class="w">            </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">In</span>
<a id="__codelineno-3-22" name="__codelineno-3-22" href="#security-docs-multitenancy-__codelineno-3-22"></a><span class="w">            </span><span class="nt">values</span><span class="p">:</span>
<a id="__codelineno-3-23" name="__codelineno-3-23" href="#security-docs-multitenancy-__codelineno-3-23"></a><span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&quot;tenants-x&quot;</span>
</code></pre></div>
<p>The above policy is applied to a Kubernetes API server request, to apply a pod to the <em>tenants-x</em> namespace. The policy adds the <code>requiredDuringSchedulingIgnoredDuringExecution</code> node affinity rule, so that pods are attracted to nodes with the <code>tenant: tenants-x</code> label.</p>
<p>A second policy, seen below, adds the toleration to the same pod specification, using the same matching criteria of target namespace and groups, kinds, and versions.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#security-docs-multitenancy-__codelineno-4-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">mutations.gatekeeper.sh/v1alpha1</span>
<a id="__codelineno-4-2" name="__codelineno-4-2" href="#security-docs-multitenancy-__codelineno-4-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Assign</span>
<a id="__codelineno-4-3" name="__codelineno-4-3" href="#security-docs-multitenancy-__codelineno-4-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-4-4" name="__codelineno-4-4" href="#security-docs-multitenancy-__codelineno-4-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">mutator-add-toleration-pod</span>
<a id="__codelineno-4-5" name="__codelineno-4-5" href="#security-docs-multitenancy-__codelineno-4-5"></a><span class="w">  </span><span class="nt">annotations</span><span class="p">:</span>
<a id="__codelineno-4-6" name="__codelineno-4-6" href="#security-docs-multitenancy-__codelineno-4-6"></a><span class="w">    </span><span class="nt">aws-eks-best-practices/description</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">&gt;-</span>
<a id="__codelineno-4-7" name="__codelineno-4-7" href="#security-docs-multitenancy-__codelineno-4-7"></a><span class="w">      </span><span class="no">Adds toleration - https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/</span>
<a id="__codelineno-4-8" name="__codelineno-4-8" href="#security-docs-multitenancy-__codelineno-4-8"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-4-9" name="__codelineno-4-9" href="#security-docs-multitenancy-__codelineno-4-9"></a><span class="w">  </span><span class="nt">applyTo</span><span class="p">:</span>
<a id="__codelineno-4-10" name="__codelineno-4-10" href="#security-docs-multitenancy-__codelineno-4-10"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">groups</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-4-11" name="__codelineno-4-11" href="#security-docs-multitenancy-__codelineno-4-11"></a><span class="w">    </span><span class="nt">kinds</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;Pod&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-4-12" name="__codelineno-4-12" href="#security-docs-multitenancy-__codelineno-4-12"></a><span class="w">    </span><span class="nt">versions</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;v1&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-4-13" name="__codelineno-4-13" href="#security-docs-multitenancy-__codelineno-4-13"></a><span class="w">  </span><span class="nt">match</span><span class="p">:</span>
<a id="__codelineno-4-14" name="__codelineno-4-14" href="#security-docs-multitenancy-__codelineno-4-14"></a><span class="w">    </span><span class="nt">namespaces</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;tenants-x&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-4-15" name="__codelineno-4-15" href="#security-docs-multitenancy-__codelineno-4-15"></a><span class="w">  </span><span class="nt">location</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;spec.tolerations&quot;</span>
<a id="__codelineno-4-16" name="__codelineno-4-16" href="#security-docs-multitenancy-__codelineno-4-16"></a><span class="w">  </span><span class="nt">parameters</span><span class="p">:</span>
<a id="__codelineno-4-17" name="__codelineno-4-17" href="#security-docs-multitenancy-__codelineno-4-17"></a><span class="w">    </span><span class="nt">assign</span><span class="p">:</span>
<a id="__codelineno-4-18" name="__codelineno-4-18" href="#security-docs-multitenancy-__codelineno-4-18"></a><span class="w">      </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span>
<a id="__codelineno-4-19" name="__codelineno-4-19" href="#security-docs-multitenancy-__codelineno-4-19"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;tenant&quot;</span>
<a id="__codelineno-4-20" name="__codelineno-4-20" href="#security-docs-multitenancy-__codelineno-4-20"></a><span class="w">        </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Equal&quot;</span>
<a id="__codelineno-4-21" name="__codelineno-4-21" href="#security-docs-multitenancy-__codelineno-4-21"></a><span class="w">        </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;tenants-x&quot;</span>
<a id="__codelineno-4-22" name="__codelineno-4-22" href="#security-docs-multitenancy-__codelineno-4-22"></a><span class="w">        </span><span class="nt">effect</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;NoSchedule&quot;</span>
</code></pre></div>
<p>The above policies are specific to pods; this is due to the paths to the mutated elements in the policies' <code>location</code> elements. Additional policies could be written to handle resources that create pods, like Deployment and Job resources. The listed policies and other examples can been seen in the companion <a href="https://github.com/aws/aws-eks-best-practices/tree/master/policies/opa/gatekeeper/node-selector">GitHub project</a> for this guide.</p>
<p>The result of these two mutations is that pods are attracted to the desired node, while at the same time, not repelled by the specific node taint. To verify this, we can see the snippets of output from two <code>kubectl</code> calls to get the nodes labeled with <code>tenant=tenants-x</code>, and get the pods in the <code>tenants-x</code> namespace.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#security-docs-multitenancy-__codelineno-5-1"></a>kubectl<span class="w"> </span>get<span class="w"> </span>nodes<span class="w"> </span>-l<span class="w"> </span><span class="nv">tenant</span><span class="o">=</span>tenants-x
<a id="__codelineno-5-2" name="__codelineno-5-2" href="#security-docs-multitenancy-__codelineno-5-2"></a>NAME<span class="w">                                        </span>
<a id="__codelineno-5-3" name="__codelineno-5-3" href="#security-docs-multitenancy-__codelineno-5-3"></a>ip-10-0-11-255...
<a id="__codelineno-5-4" name="__codelineno-5-4" href="#security-docs-multitenancy-__codelineno-5-4"></a>ip-10-0-28-81...
<a id="__codelineno-5-5" name="__codelineno-5-5" href="#security-docs-multitenancy-__codelineno-5-5"></a>ip-10-0-43-107...
<a id="__codelineno-5-6" name="__codelineno-5-6" href="#security-docs-multitenancy-__codelineno-5-6"></a>
<a id="__codelineno-5-7" name="__codelineno-5-7" href="#security-docs-multitenancy-__codelineno-5-7"></a>kubectl<span class="w"> </span>-n<span class="w"> </span>tenants-x<span class="w"> </span>get<span class="w"> </span>pods<span class="w"> </span>-owide
<a id="__codelineno-5-8" name="__codelineno-5-8" href="#security-docs-multitenancy-__codelineno-5-8"></a>NAME<span class="w">                                  </span>READY<span class="w">   </span>STATUS<span class="w">    </span>RESTARTS<span class="w">   </span>AGE<span class="w">   </span>IP<span class="w">            </span>NODE
<a id="__codelineno-5-9" name="__codelineno-5-9" href="#security-docs-multitenancy-__codelineno-5-9"></a>tenant-test-deploy-58b895ff87-2q7xw<span class="w">   </span><span class="m">1</span>/1<span class="w">     </span>Running<span class="w">   </span><span class="m">0</span><span class="w">          </span>13s<span class="w">   </span><span class="m">10</span>.0.42.143<span class="w">   </span>ip-10-0-43-107...
<a id="__codelineno-5-10" name="__codelineno-5-10" href="#security-docs-multitenancy-__codelineno-5-10"></a>tenant-test-deploy-58b895ff87-9b6hg<span class="w">   </span><span class="m">1</span>/1<span class="w">     </span>Running<span class="w">   </span><span class="m">0</span><span class="w">          </span>13s<span class="w">   </span><span class="m">10</span>.0.18.145<span class="w">   </span>ip-10-0-28-81...
<a id="__codelineno-5-11" name="__codelineno-5-11" href="#security-docs-multitenancy-__codelineno-5-11"></a>tenant-test-deploy-58b895ff87-nxvw5<span class="w">   </span><span class="m">1</span>/1<span class="w">     </span>Running<span class="w">   </span><span class="m">0</span><span class="w">          </span>13s<span class="w">   </span><span class="m">10</span>.0.30.117<span class="w">   </span>ip-10-0-28-81...
<a id="__codelineno-5-12" name="__codelineno-5-12" href="#security-docs-multitenancy-__codelineno-5-12"></a>tenant-test-deploy-58b895ff87-vw796<span class="w">   </span><span class="m">1</span>/1<span class="w">     </span>Running<span class="w">   </span><span class="m">0</span><span class="w">          </span>13s<span class="w">   </span><span class="m">10</span>.0.3.113<span class="w">    </span>ip-10-0-11-255...
<a id="__codelineno-5-13" name="__codelineno-5-13" href="#security-docs-multitenancy-__codelineno-5-13"></a>tenant-test-pod<span class="w">                       </span><span class="m">1</span>/1<span class="w">     </span>Running<span class="w">   </span><span class="m">0</span><span class="w">          </span>13s<span class="w">   </span><span class="m">10</span>.0.35.83<span class="w">    </span>ip-10-0-43-107...
</code></pre></div>
<p>As we can see from the above outputs, all the pods are scheduled on the nodes labeled with <code>tenant=tenants-x</code>. Simply put, the pods will only run on the desired nodes, and the other pods (without the required affinity and tolerations) will not. The tenant workloads are effectively isolated.</p>
<p>An example mutated pod specification is seen below.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#security-docs-multitenancy-__codelineno-6-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<a id="__codelineno-6-2" name="__codelineno-6-2" href="#security-docs-multitenancy-__codelineno-6-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Pod</span>
<a id="__codelineno-6-3" name="__codelineno-6-3" href="#security-docs-multitenancy-__codelineno-6-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-6-4" name="__codelineno-6-4" href="#security-docs-multitenancy-__codelineno-6-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">tenant-test-pod</span>
<a id="__codelineno-6-5" name="__codelineno-6-5" href="#security-docs-multitenancy-__codelineno-6-5"></a><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">tenants-x</span>
<a id="__codelineno-6-6" name="__codelineno-6-6" href="#security-docs-multitenancy-__codelineno-6-6"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-6-7" name="__codelineno-6-7" href="#security-docs-multitenancy-__codelineno-6-7"></a><span class="w">  </span><span class="nt">affinity</span><span class="p">:</span>
<a id="__codelineno-6-8" name="__codelineno-6-8" href="#security-docs-multitenancy-__codelineno-6-8"></a><span class="w">    </span><span class="nt">nodeAffinity</span><span class="p">:</span>
<a id="__codelineno-6-9" name="__codelineno-6-9" href="#security-docs-multitenancy-__codelineno-6-9"></a><span class="w">      </span><span class="nt">requiredDuringSchedulingIgnoredDuringExecution</span><span class="p">:</span>
<a id="__codelineno-6-10" name="__codelineno-6-10" href="#security-docs-multitenancy-__codelineno-6-10"></a><span class="w">        </span><span class="nt">nodeSelectorTerms</span><span class="p">:</span>
<a id="__codelineno-6-11" name="__codelineno-6-11" href="#security-docs-multitenancy-__codelineno-6-11"></a><span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">matchExpressions</span><span class="p">:</span>
<a id="__codelineno-6-12" name="__codelineno-6-12" href="#security-docs-multitenancy-__codelineno-6-12"></a><span class="w">          </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">tenant</span>
<a id="__codelineno-6-13" name="__codelineno-6-13" href="#security-docs-multitenancy-__codelineno-6-13"></a><span class="w">            </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">In</span>
<a id="__codelineno-6-14" name="__codelineno-6-14" href="#security-docs-multitenancy-__codelineno-6-14"></a><span class="w">            </span><span class="nt">values</span><span class="p">:</span>
<a id="__codelineno-6-15" name="__codelineno-6-15" href="#security-docs-multitenancy-__codelineno-6-15"></a><span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">tenants-x</span>
<a id="__codelineno-6-16" name="__codelineno-6-16" href="#security-docs-multitenancy-__codelineno-6-16"></a><span class="nn">...</span>
<a id="__codelineno-6-17" name="__codelineno-6-17" href="#security-docs-multitenancy-__codelineno-6-17"></a><span class="w">  </span><span class="nt">tolerations</span><span class="p">:</span>
<a id="__codelineno-6-18" name="__codelineno-6-18" href="#security-docs-multitenancy-__codelineno-6-18"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">effect</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">NoSchedule</span>
<a id="__codelineno-6-19" name="__codelineno-6-19" href="#security-docs-multitenancy-__codelineno-6-19"></a><span class="w">    </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">tenant</span>
<a id="__codelineno-6-20" name="__codelineno-6-20" href="#security-docs-multitenancy-__codelineno-6-20"></a><span class="w">    </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Equal</span>
<a id="__codelineno-6-21" name="__codelineno-6-21" href="#security-docs-multitenancy-__codelineno-6-21"></a><span class="w">    </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">tenants-x</span>
<a id="__codelineno-6-22" name="__codelineno-6-22" href="#security-docs-multitenancy-__codelineno-6-22"></a><span class="nn">...</span>
</code></pre></div>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Policy-management tools that are integrated to the Kubernetes API server request flow, using mutating and validating admission webhooks, are designed to respond to the API server's request within a specified timeframe. This is usually 3 seconds or less. If the webhook call fails to return a response within the configured time, the mutation and/or validation of the inbound API sever request may or may not occur. This behavior is based on whether the admission webhook configurations are set to <a href="https://open-policy-agent.github.io/gatekeeper/website/docs/#admission-webhook-fail-open-by-default">Fail Open or Fail Close</a>.</p>
</div>
<p>In the above examples, we used policies written for OPA/Gatekeeper. However, there are other policy management tools that handle our node-selection use case as well. For example, this <a href="https://kyverno.io/policies/other/add_node_affinity/add_node_affinity/">Kyverno policy</a> could be used to handle the node affinity mutation.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If operating correctly, mutating policies will effect the desired changes to inbound API server request payloads. However, validating policies should also be included to verify that the desired changes occur, before changes are allowed to persist. This is especially important when using these policies for tenant-to-node isolation. It is also a good idea to include <em>Audit</em> policies to routinely check your cluster for unwanted configurations.</p>
</div>
<h3 id="security-docs-multitenancy-references">References<a class="headerlink" href="#security-docs-multitenancy-references" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><a href="https://github.com/cruise-automation/k-rail">k-rail</a> Designed to help you secure a multi-tenant environment through the enforcement of certain policies. </p>
</li>
<li>
<p><a href="https://d1.awsstatic.com/whitepapers/security-practices-for-multi-tenant-saas-apps-using-eks.pdf">Security Practices for MultiTenant SaaS Applications using Amazon EKS</a></p>
</li>
</ul>
<h2 id="security-docs-multitenancy-hard-multi-tenancy">Hard multi-tenancy<a class="headerlink" href="#security-docs-multitenancy-hard-multi-tenancy" title="Permanent link">&para;</a></h2>
<p>Hard multi-tenancy can be implemented by provisioning separate clusters for each tenant.  While this provides very strong isolation between tenants, it has several drawbacks.</p>
<p>First, when you have many tenants, this approach can quickly become expensive. Not only will you have to pay for the control plane costs for each cluster, you will not be able to share compute resources between clusters.  This will eventually cause fragmentation where a subset of your clusters are underutilized while others are overutilized.</p>
<p>Second, you will likely need to buy or build special tooling to manage all of these clusters.  In time, managing hundreds or thousands of clusters may simply become too unwieldy.</p>
<p>Finally, creating a cluster per tenant will be slow relative to a creating a namespace. Nevertheless, a hard-tenancy approach may be necessary in highly-regulated industries or in SaaS environments where strong isolation is required.</p>
<h2 id="security-docs-multitenancy-future-directions">Future directions<a class="headerlink" href="#security-docs-multitenancy-future-directions" title="Permanent link">&para;</a></h2>
<p>The Kubernetes community has recognized the current shortcomings of soft multi-tenancy and the challenges with hard multi-tenancy. The <a href="https://github.com/kubernetes-sigs/multi-tenancy">Multi-Tenancy Special Interest Group (SIG)</a> is attempting to address these shortcomings through several incubation projects, including Hierarchical Namespace Controller (HNC) and Virtual Cluster.</p>
<p>The HNC proposal (KEP) describes a way to create parent-child relationships between namespaces with [policy] object inheritance along with an ability for tenant administrators to create sub-namespaces.</p>
<p>The Virtual Cluster proposal describes a mechanism for creating separate instances of the control plane services, including the API server, the controller manager, and scheduler, for each tenant within the cluster (also known as "Kubernetes on Kubernetes").</p>
<p>The <a href="https://github.com/kubernetes-sigs/multi-tenancy/blob/master/benchmarks/README.md">Multi-Tenancy Benchmarks</a> proposal provides guidelines for sharing clusters using namespaces for isolation and segmentation, and a command line tool <a href="https://github.com/kubernetes-sigs/multi-tenancy/blob/master/benchmarks/kubectl-mtb/README.md">kubectl-mtb</a> to validate conformance to the guidelines.</p>
<h2 id="security-docs-multitenancy-multi-cluster-management-resources">Multi-cluster management resources<a class="headerlink" href="#security-docs-multitenancy-multi-cluster-management-resources" title="Permanent link">&para;</a></h2>
<ul>
<li><a href="https://banzaicloud.com/">Banzai Cloud</a></li>
<li><a href="https://d2iq.com/solutions/ksphere/kommander">Kommander</a></li>
<li><a href="https://github.com/lensapp/lens">Lens</a></li>
<li><a href="https://nirmata.com">Nirmata</a></li>
<li><a href="https://rafay.co/">Rafay</a></li>
<li><a href="https://rancher.com/products/rancher/">Rancher</a></li>
<li><a href="https://www.weave.works/oss/flux/">Weave Flux</a></li>
</ul></section><section class="print-page" id="security-docs-detective"><h1 id="security-docs-detective-auditing-and-logging">Auditing and logging<a class="headerlink" href="#security-docs-detective-auditing-and-logging" title="Permanent link">&para;</a></h1>
<p>Collecting and analyzing [audit] logs is useful for a variety of different reasons.  Logs can help with root cause analysis and attribution, i.e. ascribing a change to a particular user. When enough logs have been collected, they can be used to detect anomalous behaviors too. On EKS, the audit logs are sent to Amazon Cloudwatch Logs. The audit policy for EKS is as follows: </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#security-docs-detective-__codelineno-0-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">audit.k8s.io/v1beta1</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#security-docs-detective-__codelineno-0-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Policy</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#security-docs-detective-__codelineno-0-3"></a><span class="nt">rules</span><span class="p">:</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#security-docs-detective-__codelineno-0-4"></a><span class="w">  </span><span class="c1"># Log aws-auth configmap changes</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#security-docs-detective-__codelineno-0-5"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">level</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">RequestResponse</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#security-docs-detective-__codelineno-0-6"></a><span class="w">    </span><span class="nt">namespaces</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;kube-system&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#security-docs-detective-__codelineno-0-7"></a><span class="w">    </span><span class="nt">verbs</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;update&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;patch&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;delete&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#security-docs-detective-__codelineno-0-8"></a><span class="w">    </span><span class="nt">resources</span><span class="p">:</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#security-docs-detective-__codelineno-0-9"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="w"> </span><span class="c1"># core</span>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#security-docs-detective-__codelineno-0-10"></a><span class="w">        </span><span class="nt">resources</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;configmaps&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#security-docs-detective-__codelineno-0-11"></a><span class="w">        </span><span class="nt">resourceNames</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;aws-auth&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#security-docs-detective-__codelineno-0-12"></a><span class="w">    </span><span class="nt">omitStages</span><span class="p">:</span>
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#security-docs-detective-__codelineno-0-13"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&quot;RequestReceived&quot;</span>
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#security-docs-detective-__codelineno-0-14"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">level</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">None</span>
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#security-docs-detective-__codelineno-0-15"></a><span class="w">    </span><span class="nt">users</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;system:kube-proxy&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#security-docs-detective-__codelineno-0-16"></a><span class="w">    </span><span class="nt">verbs</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;watch&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-0-17" name="__codelineno-0-17" href="#security-docs-detective-__codelineno-0-17"></a><span class="w">    </span><span class="nt">resources</span><span class="p">:</span>
<a id="__codelineno-0-18" name="__codelineno-0-18" href="#security-docs-detective-__codelineno-0-18"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="w"> </span><span class="c1"># core</span>
<a id="__codelineno-0-19" name="__codelineno-0-19" href="#security-docs-detective-__codelineno-0-19"></a><span class="w">        </span><span class="nt">resources</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;endpoints&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;services&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;services/status&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-0-20" name="__codelineno-0-20" href="#security-docs-detective-__codelineno-0-20"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">level</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">None</span>
<a id="__codelineno-0-21" name="__codelineno-0-21" href="#security-docs-detective-__codelineno-0-21"></a><span class="w">    </span><span class="nt">users</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;kubelet&quot;</span><span class="p p-Indicator">]</span><span class="w"> </span><span class="c1"># legacy kubelet identity</span>
<a id="__codelineno-0-22" name="__codelineno-0-22" href="#security-docs-detective-__codelineno-0-22"></a><span class="w">    </span><span class="nt">verbs</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;get&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-0-23" name="__codelineno-0-23" href="#security-docs-detective-__codelineno-0-23"></a><span class="w">    </span><span class="nt">resources</span><span class="p">:</span>
<a id="__codelineno-0-24" name="__codelineno-0-24" href="#security-docs-detective-__codelineno-0-24"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="w"> </span><span class="c1"># core</span>
<a id="__codelineno-0-25" name="__codelineno-0-25" href="#security-docs-detective-__codelineno-0-25"></a><span class="w">        </span><span class="nt">resources</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;nodes&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;nodes/status&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-0-26" name="__codelineno-0-26" href="#security-docs-detective-__codelineno-0-26"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">level</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">None</span>
<a id="__codelineno-0-27" name="__codelineno-0-27" href="#security-docs-detective-__codelineno-0-27"></a><span class="w">    </span><span class="nt">userGroups</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;system:nodes&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-0-28" name="__codelineno-0-28" href="#security-docs-detective-__codelineno-0-28"></a><span class="w">    </span><span class="nt">verbs</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;get&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-0-29" name="__codelineno-0-29" href="#security-docs-detective-__codelineno-0-29"></a><span class="w">    </span><span class="nt">resources</span><span class="p">:</span>
<a id="__codelineno-0-30" name="__codelineno-0-30" href="#security-docs-detective-__codelineno-0-30"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="w"> </span><span class="c1"># core</span>
<a id="__codelineno-0-31" name="__codelineno-0-31" href="#security-docs-detective-__codelineno-0-31"></a><span class="w">        </span><span class="nt">resources</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;nodes&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;nodes/status&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-0-32" name="__codelineno-0-32" href="#security-docs-detective-__codelineno-0-32"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">level</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">None</span>
<a id="__codelineno-0-33" name="__codelineno-0-33" href="#security-docs-detective-__codelineno-0-33"></a><span class="w">    </span><span class="nt">users</span><span class="p">:</span>
<a id="__codelineno-0-34" name="__codelineno-0-34" href="#security-docs-detective-__codelineno-0-34"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">system:kube-controller-manager</span>
<a id="__codelineno-0-35" name="__codelineno-0-35" href="#security-docs-detective-__codelineno-0-35"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">system:kube-scheduler</span>
<a id="__codelineno-0-36" name="__codelineno-0-36" href="#security-docs-detective-__codelineno-0-36"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">system:serviceaccount:kube-system:endpoint-controller</span>
<a id="__codelineno-0-37" name="__codelineno-0-37" href="#security-docs-detective-__codelineno-0-37"></a><span class="w">    </span><span class="nt">verbs</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;get&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;update&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-0-38" name="__codelineno-0-38" href="#security-docs-detective-__codelineno-0-38"></a><span class="w">    </span><span class="nt">namespaces</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;kube-system&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-0-39" name="__codelineno-0-39" href="#security-docs-detective-__codelineno-0-39"></a><span class="w">    </span><span class="nt">resources</span><span class="p">:</span>
<a id="__codelineno-0-40" name="__codelineno-0-40" href="#security-docs-detective-__codelineno-0-40"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="w"> </span><span class="c1"># core</span>
<a id="__codelineno-0-41" name="__codelineno-0-41" href="#security-docs-detective-__codelineno-0-41"></a><span class="w">        </span><span class="nt">resources</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;endpoints&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-0-42" name="__codelineno-0-42" href="#security-docs-detective-__codelineno-0-42"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">level</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">None</span>
<a id="__codelineno-0-43" name="__codelineno-0-43" href="#security-docs-detective-__codelineno-0-43"></a><span class="w">    </span><span class="nt">users</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;system:apiserver&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-0-44" name="__codelineno-0-44" href="#security-docs-detective-__codelineno-0-44"></a><span class="w">    </span><span class="nt">verbs</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;get&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-0-45" name="__codelineno-0-45" href="#security-docs-detective-__codelineno-0-45"></a><span class="w">    </span><span class="nt">resources</span><span class="p">:</span>
<a id="__codelineno-0-46" name="__codelineno-0-46" href="#security-docs-detective-__codelineno-0-46"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="w"> </span><span class="c1"># core</span>
<a id="__codelineno-0-47" name="__codelineno-0-47" href="#security-docs-detective-__codelineno-0-47"></a><span class="w">        </span><span class="nt">resources</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;namespaces&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;namespaces/status&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;namespaces/finalize&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-0-48" name="__codelineno-0-48" href="#security-docs-detective-__codelineno-0-48"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">level</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">None</span>
<a id="__codelineno-0-49" name="__codelineno-0-49" href="#security-docs-detective-__codelineno-0-49"></a><span class="w">    </span><span class="nt">users</span><span class="p">:</span>
<a id="__codelineno-0-50" name="__codelineno-0-50" href="#security-docs-detective-__codelineno-0-50"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">system:kube-controller-manager</span>
<a id="__codelineno-0-51" name="__codelineno-0-51" href="#security-docs-detective-__codelineno-0-51"></a><span class="w">    </span><span class="nt">verbs</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;get&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;list&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-0-52" name="__codelineno-0-52" href="#security-docs-detective-__codelineno-0-52"></a><span class="w">    </span><span class="nt">resources</span><span class="p">:</span>
<a id="__codelineno-0-53" name="__codelineno-0-53" href="#security-docs-detective-__codelineno-0-53"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;metrics.k8s.io&quot;</span>
<a id="__codelineno-0-54" name="__codelineno-0-54" href="#security-docs-detective-__codelineno-0-54"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">level</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">None</span>
<a id="__codelineno-0-55" name="__codelineno-0-55" href="#security-docs-detective-__codelineno-0-55"></a><span class="w">    </span><span class="nt">nonResourceURLs</span><span class="p">:</span>
<a id="__codelineno-0-56" name="__codelineno-0-56" href="#security-docs-detective-__codelineno-0-56"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/healthz*</span>
<a id="__codelineno-0-57" name="__codelineno-0-57" href="#security-docs-detective-__codelineno-0-57"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/version</span>
<a id="__codelineno-0-58" name="__codelineno-0-58" href="#security-docs-detective-__codelineno-0-58"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/swagger*</span>
<a id="__codelineno-0-59" name="__codelineno-0-59" href="#security-docs-detective-__codelineno-0-59"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">level</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">None</span>
<a id="__codelineno-0-60" name="__codelineno-0-60" href="#security-docs-detective-__codelineno-0-60"></a><span class="w">    </span><span class="nt">resources</span><span class="p">:</span>
<a id="__codelineno-0-61" name="__codelineno-0-61" href="#security-docs-detective-__codelineno-0-61"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="w"> </span><span class="c1"># core</span>
<a id="__codelineno-0-62" name="__codelineno-0-62" href="#security-docs-detective-__codelineno-0-62"></a><span class="w">        </span><span class="nt">resources</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;events&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-0-63" name="__codelineno-0-63" href="#security-docs-detective-__codelineno-0-63"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">level</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Request</span>
<a id="__codelineno-0-64" name="__codelineno-0-64" href="#security-docs-detective-__codelineno-0-64"></a><span class="w">    </span><span class="nt">users</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;kubelet&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;system:node-problem-detector&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;system:serviceaccount:kube-system:node-problem-detector&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-0-65" name="__codelineno-0-65" href="#security-docs-detective-__codelineno-0-65"></a><span class="w">    </span><span class="nt">verbs</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;update&quot;</span><span class="p p-Indicator">,</span><span class="s">&quot;patch&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-0-66" name="__codelineno-0-66" href="#security-docs-detective-__codelineno-0-66"></a><span class="w">    </span><span class="nt">resources</span><span class="p">:</span>
<a id="__codelineno-0-67" name="__codelineno-0-67" href="#security-docs-detective-__codelineno-0-67"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="w"> </span><span class="c1"># core</span>
<a id="__codelineno-0-68" name="__codelineno-0-68" href="#security-docs-detective-__codelineno-0-68"></a><span class="w">        </span><span class="nt">resources</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;nodes/status&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;pods/status&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-0-69" name="__codelineno-0-69" href="#security-docs-detective-__codelineno-0-69"></a><span class="w">    </span><span class="nt">omitStages</span><span class="p">:</span>
<a id="__codelineno-0-70" name="__codelineno-0-70" href="#security-docs-detective-__codelineno-0-70"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&quot;RequestReceived&quot;</span>
<a id="__codelineno-0-71" name="__codelineno-0-71" href="#security-docs-detective-__codelineno-0-71"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">level</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Request</span>
<a id="__codelineno-0-72" name="__codelineno-0-72" href="#security-docs-detective-__codelineno-0-72"></a><span class="w">    </span><span class="nt">userGroups</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;system:nodes&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-0-73" name="__codelineno-0-73" href="#security-docs-detective-__codelineno-0-73"></a><span class="w">    </span><span class="nt">verbs</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;update&quot;</span><span class="p p-Indicator">,</span><span class="s">&quot;patch&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-0-74" name="__codelineno-0-74" href="#security-docs-detective-__codelineno-0-74"></a><span class="w">    </span><span class="nt">resources</span><span class="p">:</span>
<a id="__codelineno-0-75" name="__codelineno-0-75" href="#security-docs-detective-__codelineno-0-75"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="w"> </span><span class="c1"># core</span>
<a id="__codelineno-0-76" name="__codelineno-0-76" href="#security-docs-detective-__codelineno-0-76"></a><span class="w">        </span><span class="nt">resources</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;nodes/status&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;pods/status&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-0-77" name="__codelineno-0-77" href="#security-docs-detective-__codelineno-0-77"></a><span class="w">    </span><span class="nt">omitStages</span><span class="p">:</span>
<a id="__codelineno-0-78" name="__codelineno-0-78" href="#security-docs-detective-__codelineno-0-78"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&quot;RequestReceived&quot;</span>
<a id="__codelineno-0-79" name="__codelineno-0-79" href="#security-docs-detective-__codelineno-0-79"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">level</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Request</span>
<a id="__codelineno-0-80" name="__codelineno-0-80" href="#security-docs-detective-__codelineno-0-80"></a><span class="w">    </span><span class="nt">users</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;system:serviceaccount:kube-system:namespace-controller&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-0-81" name="__codelineno-0-81" href="#security-docs-detective-__codelineno-0-81"></a><span class="w">    </span><span class="nt">verbs</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;deletecollection&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-0-82" name="__codelineno-0-82" href="#security-docs-detective-__codelineno-0-82"></a><span class="w">    </span><span class="nt">omitStages</span><span class="p">:</span>
<a id="__codelineno-0-83" name="__codelineno-0-83" href="#security-docs-detective-__codelineno-0-83"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&quot;RequestReceived&quot;</span>
<a id="__codelineno-0-84" name="__codelineno-0-84" href="#security-docs-detective-__codelineno-0-84"></a><span class="w">  </span><span class="c1"># Secrets, ConfigMaps, and TokenReviews can contain sensitive &amp; binary data,</span>
<a id="__codelineno-0-85" name="__codelineno-0-85" href="#security-docs-detective-__codelineno-0-85"></a><span class="w">  </span><span class="c1"># so only log at the Metadata level.</span>
<a id="__codelineno-0-86" name="__codelineno-0-86" href="#security-docs-detective-__codelineno-0-86"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">level</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Metadata</span>
<a id="__codelineno-0-87" name="__codelineno-0-87" href="#security-docs-detective-__codelineno-0-87"></a><span class="w">    </span><span class="nt">resources</span><span class="p">:</span>
<a id="__codelineno-0-88" name="__codelineno-0-88" href="#security-docs-detective-__codelineno-0-88"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="w"> </span><span class="c1"># core</span>
<a id="__codelineno-0-89" name="__codelineno-0-89" href="#security-docs-detective-__codelineno-0-89"></a><span class="w">        </span><span class="nt">resources</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;secrets&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;configmaps&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-0-90" name="__codelineno-0-90" href="#security-docs-detective-__codelineno-0-90"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">authentication.k8s.io</span>
<a id="__codelineno-0-91" name="__codelineno-0-91" href="#security-docs-detective-__codelineno-0-91"></a><span class="w">        </span><span class="nt">resources</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;tokenreviews&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-0-92" name="__codelineno-0-92" href="#security-docs-detective-__codelineno-0-92"></a><span class="w">    </span><span class="nt">omitStages</span><span class="p">:</span>
<a id="__codelineno-0-93" name="__codelineno-0-93" href="#security-docs-detective-__codelineno-0-93"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&quot;RequestReceived&quot;</span>
<a id="__codelineno-0-94" name="__codelineno-0-94" href="#security-docs-detective-__codelineno-0-94"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">level</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Request</span>
<a id="__codelineno-0-95" name="__codelineno-0-95" href="#security-docs-detective-__codelineno-0-95"></a><span class="w">    </span><span class="nt">resources</span><span class="p">:</span>
<a id="__codelineno-0-96" name="__codelineno-0-96" href="#security-docs-detective-__codelineno-0-96"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;&quot;</span>
<a id="__codelineno-0-97" name="__codelineno-0-97" href="#security-docs-detective-__codelineno-0-97"></a><span class="w">        </span><span class="nt">resources</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;serviceaccounts/token&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-0-98" name="__codelineno-0-98" href="#security-docs-detective-__codelineno-0-98"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">level</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Request</span>
<a id="__codelineno-0-99" name="__codelineno-0-99" href="#security-docs-detective-__codelineno-0-99"></a><span class="w">    </span><span class="nt">verbs</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;get&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;list&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;watch&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-0-100" name="__codelineno-0-100" href="#security-docs-detective-__codelineno-0-100"></a><span class="w">    </span><span class="nt">resources</span><span class="p">:</span><span class="w"> </span>
<a id="__codelineno-0-101" name="__codelineno-0-101" href="#security-docs-detective-__codelineno-0-101"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="w"> </span><span class="c1"># core</span>
<a id="__codelineno-0-102" name="__codelineno-0-102" href="#security-docs-detective-__codelineno-0-102"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;admissionregistration.k8s.io&quot;</span>
<a id="__codelineno-0-103" name="__codelineno-0-103" href="#security-docs-detective-__codelineno-0-103"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;apiextensions.k8s.io&quot;</span>
<a id="__codelineno-0-104" name="__codelineno-0-104" href="#security-docs-detective-__codelineno-0-104"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;apiregistration.k8s.io&quot;</span>
<a id="__codelineno-0-105" name="__codelineno-0-105" href="#security-docs-detective-__codelineno-0-105"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;apps&quot;</span>
<a id="__codelineno-0-106" name="__codelineno-0-106" href="#security-docs-detective-__codelineno-0-106"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;authentication.k8s.io&quot;</span>
<a id="__codelineno-0-107" name="__codelineno-0-107" href="#security-docs-detective-__codelineno-0-107"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;authorization.k8s.io&quot;</span>
<a id="__codelineno-0-108" name="__codelineno-0-108" href="#security-docs-detective-__codelineno-0-108"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;autoscaling&quot;</span>
<a id="__codelineno-0-109" name="__codelineno-0-109" href="#security-docs-detective-__codelineno-0-109"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;batch&quot;</span>
<a id="__codelineno-0-110" name="__codelineno-0-110" href="#security-docs-detective-__codelineno-0-110"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;certificates.k8s.io&quot;</span>
<a id="__codelineno-0-111" name="__codelineno-0-111" href="#security-docs-detective-__codelineno-0-111"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;extensions&quot;</span>
<a id="__codelineno-0-112" name="__codelineno-0-112" href="#security-docs-detective-__codelineno-0-112"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;metrics.k8s.io&quot;</span>
<a id="__codelineno-0-113" name="__codelineno-0-113" href="#security-docs-detective-__codelineno-0-113"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;networking.k8s.io&quot;</span>
<a id="__codelineno-0-114" name="__codelineno-0-114" href="#security-docs-detective-__codelineno-0-114"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;policy&quot;</span>
<a id="__codelineno-0-115" name="__codelineno-0-115" href="#security-docs-detective-__codelineno-0-115"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;rbac.authorization.k8s.io&quot;</span>
<a id="__codelineno-0-116" name="__codelineno-0-116" href="#security-docs-detective-__codelineno-0-116"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;scheduling.k8s.io&quot;</span>
<a id="__codelineno-0-117" name="__codelineno-0-117" href="#security-docs-detective-__codelineno-0-117"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;settings.k8s.io&quot;</span>
<a id="__codelineno-0-118" name="__codelineno-0-118" href="#security-docs-detective-__codelineno-0-118"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;storage.k8s.io&quot;</span>
<a id="__codelineno-0-119" name="__codelineno-0-119" href="#security-docs-detective-__codelineno-0-119"></a><span class="w">    </span><span class="nt">omitStages</span><span class="p">:</span>
<a id="__codelineno-0-120" name="__codelineno-0-120" href="#security-docs-detective-__codelineno-0-120"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&quot;RequestReceived&quot;</span>
<a id="__codelineno-0-121" name="__codelineno-0-121" href="#security-docs-detective-__codelineno-0-121"></a><span class="w">  </span><span class="c1"># Default level for known APIs</span>
<a id="__codelineno-0-122" name="__codelineno-0-122" href="#security-docs-detective-__codelineno-0-122"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">level</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">RequestResponse</span>
<a id="__codelineno-0-123" name="__codelineno-0-123" href="#security-docs-detective-__codelineno-0-123"></a><span class="w">    </span><span class="nt">resources</span><span class="p">:</span><span class="w"> </span>
<a id="__codelineno-0-124" name="__codelineno-0-124" href="#security-docs-detective-__codelineno-0-124"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="w"> </span><span class="c1"># core</span>
<a id="__codelineno-0-125" name="__codelineno-0-125" href="#security-docs-detective-__codelineno-0-125"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;admissionregistration.k8s.io&quot;</span>
<a id="__codelineno-0-126" name="__codelineno-0-126" href="#security-docs-detective-__codelineno-0-126"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;apiextensions.k8s.io&quot;</span>
<a id="__codelineno-0-127" name="__codelineno-0-127" href="#security-docs-detective-__codelineno-0-127"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;apiregistration.k8s.io&quot;</span>
<a id="__codelineno-0-128" name="__codelineno-0-128" href="#security-docs-detective-__codelineno-0-128"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;apps&quot;</span>
<a id="__codelineno-0-129" name="__codelineno-0-129" href="#security-docs-detective-__codelineno-0-129"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;authentication.k8s.io&quot;</span>
<a id="__codelineno-0-130" name="__codelineno-0-130" href="#security-docs-detective-__codelineno-0-130"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;authorization.k8s.io&quot;</span>
<a id="__codelineno-0-131" name="__codelineno-0-131" href="#security-docs-detective-__codelineno-0-131"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;autoscaling&quot;</span>
<a id="__codelineno-0-132" name="__codelineno-0-132" href="#security-docs-detective-__codelineno-0-132"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;batch&quot;</span>
<a id="__codelineno-0-133" name="__codelineno-0-133" href="#security-docs-detective-__codelineno-0-133"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;certificates.k8s.io&quot;</span>
<a id="__codelineno-0-134" name="__codelineno-0-134" href="#security-docs-detective-__codelineno-0-134"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;extensions&quot;</span>
<a id="__codelineno-0-135" name="__codelineno-0-135" href="#security-docs-detective-__codelineno-0-135"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;metrics.k8s.io&quot;</span>
<a id="__codelineno-0-136" name="__codelineno-0-136" href="#security-docs-detective-__codelineno-0-136"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;networking.k8s.io&quot;</span>
<a id="__codelineno-0-137" name="__codelineno-0-137" href="#security-docs-detective-__codelineno-0-137"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;policy&quot;</span>
<a id="__codelineno-0-138" name="__codelineno-0-138" href="#security-docs-detective-__codelineno-0-138"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;rbac.authorization.k8s.io&quot;</span>
<a id="__codelineno-0-139" name="__codelineno-0-139" href="#security-docs-detective-__codelineno-0-139"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;scheduling.k8s.io&quot;</span>
<a id="__codelineno-0-140" name="__codelineno-0-140" href="#security-docs-detective-__codelineno-0-140"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;settings.k8s.io&quot;</span>
<a id="__codelineno-0-141" name="__codelineno-0-141" href="#security-docs-detective-__codelineno-0-141"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;storage.k8s.io&quot;</span>
<a id="__codelineno-0-142" name="__codelineno-0-142" href="#security-docs-detective-__codelineno-0-142"></a><span class="w">    </span><span class="nt">omitStages</span><span class="p">:</span>
<a id="__codelineno-0-143" name="__codelineno-0-143" href="#security-docs-detective-__codelineno-0-143"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&quot;RequestReceived&quot;</span>
<a id="__codelineno-0-144" name="__codelineno-0-144" href="#security-docs-detective-__codelineno-0-144"></a><span class="w">  </span><span class="c1"># Default level for all other requests.</span>
<a id="__codelineno-0-145" name="__codelineno-0-145" href="#security-docs-detective-__codelineno-0-145"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">level</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Metadata</span>
<a id="__codelineno-0-146" name="__codelineno-0-146" href="#security-docs-detective-__codelineno-0-146"></a><span class="w">    </span><span class="nt">omitStages</span><span class="p">:</span>
<a id="__codelineno-0-147" name="__codelineno-0-147" href="#security-docs-detective-__codelineno-0-147"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&quot;RequestReceived&quot;</span>
</code></pre></div>
<h2 id="security-docs-detective-recommendations">Recommendations<a class="headerlink" href="#security-docs-detective-recommendations" title="Permanent link">&para;</a></h2>
<h3 id="security-docs-detective-enable-audit-logs">Enable audit logs<a class="headerlink" href="#security-docs-detective-enable-audit-logs" title="Permanent link">&para;</a></h3>
<p>The audit logs are part of the EKS managed Kubernetes control plane logs that are managed by EKS.  Instructions for enabling/disabling the control plane logs, which includes the logs for the Kubernetes API server, the controller manager, and the scheduler, along with the audit log, can be found here, <a href="https://docs.aws.amazon.com/eks/latest/userguide/control-plane-logs.html#enabling-control-plane-log-export">https://docs.aws.amazon.com/eks/latest/userguide/control-plane-logs.html#enabling-control-plane-log-export</a>. </p>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>When you enable control plane logging, you will incur <a href="https://aws.amazon.com/cloudwatch/pricing/">costs</a> for storing the logs in CloudWatch. This raises a broader issue about the ongoing cost of security. Ultimately you will have to weigh those costs against the cost of a security breach, e.g. financial loss, damage to your reputation, etc. You may find that you can adequately secure your environment by implementing only some of the recommendations in this guide. </p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The maximum size for a CloudWatch Logs entry is <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/cloudwatch_limits_cwl.html">256KB</a> whereas the maximum Kubernetes API request size is 1.5MiB. Log entries greater than 256KB will either be truncated or only include the request metadata. </p>
</div>
<h3 id="security-docs-detective-utilize-audit-metadata">Utilize audit metadata<a class="headerlink" href="#security-docs-detective-utilize-audit-metadata" title="Permanent link">&para;</a></h3>
<p>Kubernetes audit logs include two annotations that indicate whether or not a request was authorized <code>authorization.k8s.io/decision</code> and the reason for the decision <code>authorization.k8s.io/reason</code>.  Use these attributes to ascertain why a particular API call was allowed. </p>
<h3 id="security-docs-detective-create-alarms-for-suspicious-events">Create alarms for suspicious events<a class="headerlink" href="#security-docs-detective-create-alarms-for-suspicious-events" title="Permanent link">&para;</a></h3>
<p>Create an alarm to automatically alert you where there is an increase in 403 Forbidden and 401 Unauthorized responses, and then use attributes like <code>host</code>, <code>sourceIPs</code>, and <code>k8s_user.username</code> to find out where those requests are coming from.</p>
<h3 id="security-docs-detective-analyze-logs-with-log-insights">Analyze logs with Log Insights<a class="headerlink" href="#security-docs-detective-analyze-logs-with-log-insights" title="Permanent link">&para;</a></h3>
<p>Use CloudWatch Log Insights to monitor changes to RBAC objects, e.g. Roles, RoleBindings, ClusterRoles, and ClusterRoleBindings.  A few sample queries appear below: </p>
<p>Lists updates to the <code>aws-auth</code> ConfigMap:
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#security-docs-detective-__codelineno-1-1"></a>fields @timestamp, @message
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#security-docs-detective-__codelineno-1-2"></a>| filter @logStream like &quot;kube-apiserver-audit&quot;
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#security-docs-detective-__codelineno-1-3"></a>| filter verb in [&quot;update&quot;, &quot;patch&quot;]
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#security-docs-detective-__codelineno-1-4"></a>| filter objectRef.resource = &quot;configmaps&quot; and objectRef.name = &quot;aws-auth&quot; and objectRef.namespace = &quot;kube-system&quot;
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#security-docs-detective-__codelineno-1-5"></a>| sort @timestamp desc
</code></pre></div>
Lists creation of new or changes to validation webhooks:
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#security-docs-detective-__codelineno-2-1"></a>fields @timestamp, @message
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#security-docs-detective-__codelineno-2-2"></a>| filter @logStream like &quot;kube-apiserver-audit&quot;
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#security-docs-detective-__codelineno-2-3"></a>| filter verb in [&quot;create&quot;, &quot;update&quot;, &quot;patch&quot;] and responseStatus.code = 201
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#security-docs-detective-__codelineno-2-4"></a>| filter objectRef.resource = &quot;validatingwebhookconfigurations&quot;
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#security-docs-detective-__codelineno-2-5"></a>| sort @timestamp desc
</code></pre></div>
Lists create, update, delete operations to Roles:
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#security-docs-detective-__codelineno-3-1"></a>fields @timestamp, @message
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#security-docs-detective-__codelineno-3-2"></a>| sort @timestamp desc
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#security-docs-detective-__codelineno-3-3"></a>| limit 100
<a id="__codelineno-3-4" name="__codelineno-3-4" href="#security-docs-detective-__codelineno-3-4"></a>| filter objectRef.resource=&quot;roles&quot; and verb in [&quot;create&quot;, &quot;update&quot;, &quot;patch&quot;, &quot;delete&quot;]
</code></pre></div>
Lists create, update, delete operations to RoleBindings:
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#security-docs-detective-__codelineno-4-1"></a>fields @timestamp, @message
<a id="__codelineno-4-2" name="__codelineno-4-2" href="#security-docs-detective-__codelineno-4-2"></a>| sort @timestamp desc
<a id="__codelineno-4-3" name="__codelineno-4-3" href="#security-docs-detective-__codelineno-4-3"></a>| limit 100
<a id="__codelineno-4-4" name="__codelineno-4-4" href="#security-docs-detective-__codelineno-4-4"></a>| filter objectRef.resource=&quot;rolebindings&quot; and verb in [&quot;create&quot;, &quot;update&quot;, &quot;patch&quot;, &quot;delete&quot;]
</code></pre></div>
Lists create, update, delete operations to ClusterRoles:
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#security-docs-detective-__codelineno-5-1"></a>fields @timestamp, @message
<a id="__codelineno-5-2" name="__codelineno-5-2" href="#security-docs-detective-__codelineno-5-2"></a>| sort @timestamp desc
<a id="__codelineno-5-3" name="__codelineno-5-3" href="#security-docs-detective-__codelineno-5-3"></a>| limit 100
<a id="__codelineno-5-4" name="__codelineno-5-4" href="#security-docs-detective-__codelineno-5-4"></a>| filter objectRef.resource=&quot;clusterroles&quot; and verb in [&quot;create&quot;, &quot;update&quot;, &quot;patch&quot;, &quot;delete&quot;]
</code></pre></div>
Lists create, update, delete operations to ClusterRoleBindings:
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#security-docs-detective-__codelineno-6-1"></a>fields @timestamp, @message
<a id="__codelineno-6-2" name="__codelineno-6-2" href="#security-docs-detective-__codelineno-6-2"></a>| sort @timestamp desc
<a id="__codelineno-6-3" name="__codelineno-6-3" href="#security-docs-detective-__codelineno-6-3"></a>| limit 100
<a id="__codelineno-6-4" name="__codelineno-6-4" href="#security-docs-detective-__codelineno-6-4"></a>| filter objectRef.resource=&quot;clusterrolebindings&quot; and verb in [&quot;create&quot;, &quot;update&quot;, &quot;patch&quot;, &quot;delete&quot;]
</code></pre></div>
Plots unauthorized read operations against Secrets:
<div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#security-docs-detective-__codelineno-7-1"></a>fields @timestamp, @message
<a id="__codelineno-7-2" name="__codelineno-7-2" href="#security-docs-detective-__codelineno-7-2"></a>| sort @timestamp desc
<a id="__codelineno-7-3" name="__codelineno-7-3" href="#security-docs-detective-__codelineno-7-3"></a>| limit 100
<a id="__codelineno-7-4" name="__codelineno-7-4" href="#security-docs-detective-__codelineno-7-4"></a>| filter objectRef.resource=&quot;secrets&quot; and verb in [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;] and responseStatus.code=&quot;401&quot;
<a id="__codelineno-7-5" name="__codelineno-7-5" href="#security-docs-detective-__codelineno-7-5"></a>| stats count() by bin(1m)
</code></pre></div>
List of failed anonymous requests:
<div class="highlight"><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1" href="#security-docs-detective-__codelineno-8-1"></a>fields @timestamp, @message, sourceIPs.0
<a id="__codelineno-8-2" name="__codelineno-8-2" href="#security-docs-detective-__codelineno-8-2"></a>| sort @timestamp desc
<a id="__codelineno-8-3" name="__codelineno-8-3" href="#security-docs-detective-__codelineno-8-3"></a>| limit 100
<a id="__codelineno-8-4" name="__codelineno-8-4" href="#security-docs-detective-__codelineno-8-4"></a>| filter user.username=&quot;system:anonymous&quot; and responseStatus.code in [&quot;401&quot;, &quot;403&quot;]
</code></pre></div></p>
<h3 id="security-docs-detective-audit-your-cloudtrail-logs">Audit your CloudTrail logs<a class="headerlink" href="#security-docs-detective-audit-your-cloudtrail-logs" title="Permanent link">&para;</a></h3>
<p>AWS APIs called by pods that are utilizing IAM Roles for Service Accounts (IRSA) are automatically logged to CloudTrail along with the name of the service account. If the name of a service account that wasn't explicitly authorized to call an API appears in the log, it may be an indication that the IAM role's trust policy was misconfigured. Generally speaking, Cloudtrail is a great way to ascribe AWS API calls to specific IAM principals. </p>
<h3 id="security-docs-detective-use-cloudtrail-insights-to-unearth-suspicious-activity">Use CloudTrail Insights to unearth suspicious activity<a class="headerlink" href="#security-docs-detective-use-cloudtrail-insights-to-unearth-suspicious-activity" title="Permanent link">&para;</a></h3>
<p>CloudTrail insights automatically analyzes write management events from CloudTrail trails and alerts you of unusual activity. This can help you identify when there's an increase in call volume on write APIs in your AWS account, including from pods that use IRSA to assume an IAM role. See <a href="https://aws.amazon.com/blogs/aws/announcing-cloudtrail-insights-identify-and-respond-to-unusual-api-activity/">Announcing CloudTrail Insights: Identify and Response to Unusual API Activity</a> for further information.</p>
<h3 id="security-docs-detective-additional-resources">Additional resources<a class="headerlink" href="#security-docs-detective-additional-resources" title="Permanent link">&para;</a></h3>
<p>As the volume of logs increases, parsing and filtering them with Log Insights or another log analysis tool may become ineffective.  As an alternative, you might want to consider running <a href="https://github.com/falcosecurity/falco">Sysdig Falco</a> and <a href="https://github.com/sysdiglabs/ekscloudwatch">ekscloudwatch</a>. Falco analyzes audit logs and flags anomalies or abuse over an extended period of time. The ekscloudwatch project forwards audit log events from CloudWatch to Falco for analysis. Falco provides a set of <a href="https://github.com/falcosecurity/plugins/blob/master/plugins/k8saudit/rules/k8s_audit_rules.yaml">default audit rules</a> along with the ability to add your own. </p>
<p>Yet another option might be to store the audit logs in S3 and use the SageMaker <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/randomcutforest.html">Random Cut Forest</a> algorithm to anomalous behaviors that warrant further investigation.</p>
<h2 id="security-docs-detective-tooling">Tooling<a class="headerlink" href="#security-docs-detective-tooling" title="Permanent link">&para;</a></h2>
<p>The following commercial and open source projects can be used to assess your cluster's alignment with established best practices:</p>
<ul>
<li><a href="https://github.com/Shopify/kubeaudit">kubeaudit</a></li>
<li><a href="https://github.com/octarinesec/kube-scan">kube-scan</a> Assigns a risk score to the workloads running in your cluster in accordance with the Kubernetes Common Configuration Scoring System framework</li>
<li><a href="https://kubesec.io/">kubesec.io</a></li>
<li><a href="https://github.com/FairwindsOps/polaris">polaris</a></li>
<li><a href="https://github.com/aquasecurity/starboard">Starboard</a></li>
<li><a href="https://support.snyk.io/hc/en-us/articles/360003916138-Kubernetes-integration-overview">Snyk</a></li>
<li><a href="https://github.com/kubescape/kubescape">Kubescape</a> Kubescape is an open source kubernetes security tool that scans clusters, YAML files, and Helm charts. It detects misconfigurations according to multiple frameworks (including <a href="https://www.armosec.io/blog/kubernetes-hardening-guidance-summary-by-armo/?utm_source=github&amp;utm_medium=repository">NSA-CISA</a> and <a href="https://www.microsoft.com/security/blog/2021/03/23/secure-containerized-environments-with-updated-threat-matrix-for-kubernetes/">MITRE ATT&amp;CK®</a>.)</li>
</ul></section><section class="print-page" id="security-docs-network"><h1 id="security-docs-network-network-security">Network security<a class="headerlink" href="#security-docs-network-network-security" title="Permanent link">&para;</a></h1>
<p>Network security has several facets.  The first involves the application of rules which restrict the flow of network traffic between services.  The second involves the encryption of traffic while it is in transit.  The mechanisms to implement these security measures on EKS are varied but often include the following items:</p>
<h4 id="security-docs-network-traffic-control">Traffic control<a class="headerlink" href="#security-docs-network-traffic-control" title="Permanent link">&para;</a></h4>
<ul>
<li>Network Policies</li>
<li>Security Groups</li>
</ul>
<h4 id="security-docs-network-encryption-in-transit">Encryption in transit<a class="headerlink" href="#security-docs-network-encryption-in-transit" title="Permanent link">&para;</a></h4>
<ul>
<li>Service Mesh</li>
<li>Container Network Interfaces (CNIs)</li>
<li>Ingress Controllers and Load Balancers</li>
<li>Nitro Instances</li>
<li>ACM Private CA with cert-manager</li>
</ul>
<h2 id="security-docs-network-network-policy">Network policy<a class="headerlink" href="#security-docs-network-network-policy" title="Permanent link">&para;</a></h2>
<p>Within a Kubernetes cluster, all Pod to Pod communication is allowed by default. While this flexibility may help promote experimentation, it is not considered secure. Kubernetes network policies give you a mechanism to restrict network traffic between Pods (often referred to as East/West traffic) as well as between Pods and external services. Kubernetes network policies operate at layers 3 and 4 of the OSI model. Network policies use pod, namespace selectors and labels to identify source and destination pods, but can also include IP addresses, port numbers, protocols, or a combination of these. Network Policies can be applied to both Inbound or Outbound connections to the pod, often called Ingress and Egress rules. </p>
<p>With native network policy support of Amazon VPC CNI Plugin, you can implement network policies to secure network traffic in kubernetes clusters. This integrates with the upstream Kubernetes Network Policy API, ensuring compatibility and adherence to Kubernetes standards. You can define policies using different <a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/">identifiers</a> supported by the upstream API. By default, all ingress and egress traffic is allowed to a pod. When a network policy with a policyType Ingress is specified, only allowed connections into the pod are those from the pod's node and those allowed by the ingress rules. Same applies for egress rules.  If multiple rules are defined, then union of all rules are taken into account when making the decision. Thus, order of evaluation does not affect the policy result.</p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>When you first provision an EKS cluster, VPC CNI Network Policy functionality is not enabled by default. Ensure you deployed supported VPC CNI Add-on version and set <code>ENABLE_NETWORK_POLICY</code> flag to <code>true</code> on the vpc-cni add-on to enable this. Refer <a href="https://docs.aws.amazon.com/eks/latest/userguide/managing-vpc-cni.html">Amazon EKS User guide</a> for detailed instructions.</p>
</div>
<h2 id="security-docs-network-recommendations">Recommendations<a class="headerlink" href="#security-docs-network-recommendations" title="Permanent link">&para;</a></h2>
<h3 id="security-docs-network-getting-started-with-network-policies-follow-principle-of-least-privilege">Getting Started with Network Policies - Follow Principle of Least Privilege<a class="headerlink" href="#security-docs-network-getting-started-with-network-policies-follow-principle-of-least-privilege" title="Permanent link">&para;</a></h3>
<h4 id="security-docs-network-create-a-default-deny-policy">Create a default deny policy<a class="headerlink" href="#security-docs-network-create-a-default-deny-policy" title="Permanent link">&para;</a></h4>
<p>As with RBAC policies, it is recommended to follow least privileged access principles with network policies. Start by creating a deny all policy that restricts all inbound and outbound traffic with in a namespace.</p>
<p><em>Kubernetes network policy</em>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#security-docs-network-__codelineno-0-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">networking.k8s.io/v1</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#security-docs-network-__codelineno-0-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">NetworkPolicy</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#security-docs-network-__codelineno-0-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#security-docs-network-__codelineno-0-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">default-deny</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#security-docs-network-__codelineno-0-5"></a><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">default</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#security-docs-network-__codelineno-0-6"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#security-docs-network-__codelineno-0-7"></a><span class="w">  </span><span class="nt">podSelector</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">{}</span>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#security-docs-network-__codelineno-0-8"></a><span class="w">  </span><span class="nt">policyTypes</span><span class="p">:</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#security-docs-network-__codelineno-0-9"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Ingress</span>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#security-docs-network-__codelineno-0-10"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Egress</span>
</code></pre></div></p>
<p><img alt="" src="../security/docs/images/default-deny.jpg" /></p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The image above was created by the network policy viewer from <a href="https://orca.tufin.io/netpol/">Tufin</a>.</p>
</div>
<h4 id="security-docs-network-create-a-rule-to-allow-dns-queries">Create a rule to allow DNS queries<a class="headerlink" href="#security-docs-network-create-a-rule-to-allow-dns-queries" title="Permanent link">&para;</a></h4>
<p>Once you have the default deny all rule in place, you can begin layering on additional rules, such as a rule that allows pods to query CoreDNS for name resolution.</p>
<p><em>Kubernetes network policy</em>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#security-docs-network-__codelineno-1-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">networking.k8s.io/v1</span>
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#security-docs-network-__codelineno-1-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">NetworkPolicy</span>
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#security-docs-network-__codelineno-1-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#security-docs-network-__codelineno-1-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">allow-dns-access</span>
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#security-docs-network-__codelineno-1-5"></a><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">default</span>
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#security-docs-network-__codelineno-1-6"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#security-docs-network-__codelineno-1-7"></a><span class="w">  </span><span class="nt">podSelector</span><span class="p">:</span>
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#security-docs-network-__codelineno-1-8"></a><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">{}</span>
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#security-docs-network-__codelineno-1-9"></a><span class="w">  </span><span class="nt">policyTypes</span><span class="p">:</span>
<a id="__codelineno-1-10" name="__codelineno-1-10" href="#security-docs-network-__codelineno-1-10"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Egress</span>
<a id="__codelineno-1-11" name="__codelineno-1-11" href="#security-docs-network-__codelineno-1-11"></a><span class="w">  </span><span class="nt">egress</span><span class="p">:</span>
<a id="__codelineno-1-12" name="__codelineno-1-12" href="#security-docs-network-__codelineno-1-12"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">to</span><span class="p">:</span>
<a id="__codelineno-1-13" name="__codelineno-1-13" href="#security-docs-network-__codelineno-1-13"></a><span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">namespaceSelector</span><span class="p">:</span>
<a id="__codelineno-1-14" name="__codelineno-1-14" href="#security-docs-network-__codelineno-1-14"></a><span class="w">        </span><span class="nt">matchLabels</span><span class="p">:</span>
<a id="__codelineno-1-15" name="__codelineno-1-15" href="#security-docs-network-__codelineno-1-15"></a><span class="w">          </span><span class="nt">kubernetes.io/metadata.name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">kube-system</span>
<a id="__codelineno-1-16" name="__codelineno-1-16" href="#security-docs-network-__codelineno-1-16"></a><span class="w">      </span><span class="nt">podSelector</span><span class="p">:</span>
<a id="__codelineno-1-17" name="__codelineno-1-17" href="#security-docs-network-__codelineno-1-17"></a><span class="w">        </span><span class="nt">matchLabels</span><span class="p">:</span>
<a id="__codelineno-1-18" name="__codelineno-1-18" href="#security-docs-network-__codelineno-1-18"></a><span class="w">          </span><span class="nt">k8s-app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">kube-dns</span>
<a id="__codelineno-1-19" name="__codelineno-1-19" href="#security-docs-network-__codelineno-1-19"></a><span class="w">    </span><span class="nt">ports</span><span class="p">:</span>
<a id="__codelineno-1-20" name="__codelineno-1-20" href="#security-docs-network-__codelineno-1-20"></a><span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">UDP</span>
<a id="__codelineno-1-21" name="__codelineno-1-21" href="#security-docs-network-__codelineno-1-21"></a><span class="w">      </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">53</span>
</code></pre></div></p>
<p><img alt="" src="../security/docs/images/allow-dns-access.jpg" /></p>
<h4 id="security-docs-network-incrementally-add-rules-to-selectively-allow-the-flow-of-traffic-between-namespacespods">Incrementally add rules to selectively allow the flow of traffic between namespaces/pods<a class="headerlink" href="#security-docs-network-incrementally-add-rules-to-selectively-allow-the-flow-of-traffic-between-namespacespods" title="Permanent link">&para;</a></h4>
<p>Understand the application requirements and create fine-grained ingress and egress rules as needed. Below example shows how to restrict ingress traffic on port 80 to <code>app-one</code> from <code>client-one</code>. This helps minimize the attack surface and reduces the risk of unauthorized access.</p>
<p><em>Kubernetes network policy</em>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#security-docs-network-__codelineno-2-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">networking.k8s.io/v1</span>
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#security-docs-network-__codelineno-2-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">NetworkPolicy</span>
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#security-docs-network-__codelineno-2-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#security-docs-network-__codelineno-2-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">allow-ingress-app-one</span>
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#security-docs-network-__codelineno-2-5"></a><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">default</span>
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#security-docs-network-__codelineno-2-6"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#security-docs-network-__codelineno-2-7"></a><span class="w">  </span><span class="nt">podSelector</span><span class="p">:</span>
<a id="__codelineno-2-8" name="__codelineno-2-8" href="#security-docs-network-__codelineno-2-8"></a><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span>
<a id="__codelineno-2-9" name="__codelineno-2-9" href="#security-docs-network-__codelineno-2-9"></a><span class="w">      </span><span class="nt">k8s-app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">app-one</span>
<a id="__codelineno-2-10" name="__codelineno-2-10" href="#security-docs-network-__codelineno-2-10"></a><span class="w">  </span><span class="nt">policyTypes</span><span class="p">:</span>
<a id="__codelineno-2-11" name="__codelineno-2-11" href="#security-docs-network-__codelineno-2-11"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Ingress</span>
<a id="__codelineno-2-12" name="__codelineno-2-12" href="#security-docs-network-__codelineno-2-12"></a><span class="w">  </span><span class="nt">ingress</span><span class="p">:</span>
<a id="__codelineno-2-13" name="__codelineno-2-13" href="#security-docs-network-__codelineno-2-13"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">from</span><span class="p">:</span>
<a id="__codelineno-2-14" name="__codelineno-2-14" href="#security-docs-network-__codelineno-2-14"></a><span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">podSelector</span><span class="p">:</span>
<a id="__codelineno-2-15" name="__codelineno-2-15" href="#security-docs-network-__codelineno-2-15"></a><span class="w">        </span><span class="nt">matchLabels</span><span class="p">:</span>
<a id="__codelineno-2-16" name="__codelineno-2-16" href="#security-docs-network-__codelineno-2-16"></a><span class="w">          </span><span class="nt">k8s-app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">client-one</span>
<a id="__codelineno-2-17" name="__codelineno-2-17" href="#security-docs-network-__codelineno-2-17"></a><span class="w">    </span><span class="nt">ports</span><span class="p">:</span>
<a id="__codelineno-2-18" name="__codelineno-2-18" href="#security-docs-network-__codelineno-2-18"></a><span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">TCP</span>
<a id="__codelineno-2-19" name="__codelineno-2-19" href="#security-docs-network-__codelineno-2-19"></a><span class="w">      </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">80</span>
</code></pre></div></p>
<p><img alt="" src="../security/docs/images/allow-ingress-app-one.png" /></p>
<h3 id="security-docs-network-monitoring-network-policy-enforcement">Monitoring network policy enforcement<a class="headerlink" href="#security-docs-network-monitoring-network-policy-enforcement" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Use Network Policy editor</strong><ul>
<li><a href="https://networkpolicy.io/">Network policy editor</a> helps with visualizations, security score, autogenerates from network flow logs</li>
<li>Build network policies in an interactive way</li>
</ul>
</li>
<li><strong>Audit Logs</strong><ul>
<li>Regularly review audit logs of your EKS cluster</li>
<li>Audit logs provide wealth of information about what actions have been performed on your cluster including changes to network policies</li>
<li>Use this information to track changes to your network policies over time and detect any unauthorized or unexpected changes</li>
</ul>
</li>
<li><strong>Automated testing</strong><ul>
<li>Implement automated testing by creating a test environment that mirrors your production environment and periodically deploy workloads that attempt to violate your network policies.</li>
</ul>
</li>
<li><strong>Monitoring metrics</strong><ul>
<li>Configure your observability agents to scrape the prometheus metrics from the VPC CNI node agents, that allows to monitor the agent health, and sdk errors.</li>
</ul>
</li>
<li><strong>Audit Network Policies regularly</strong><ul>
<li>Periodically audit your Network Policies to make sure that they meet your current application requirements. As your application evolves, an audit gives you the opportunity to remove redundant ingress, egress rules and make sure that your applications don’t have excessive permissions.</li>
</ul>
</li>
<li><strong>Ensure Network Policies exists using Open Policy Agent (OPA)</strong><ul>
<li>Use OPA Policy like shown below to ensure Network Policy always exists before onboarding application pods. This policy denies onboarding k8s pods with a label <code>k8s-app: sample-app</code> if corresponding network policy does not exist.</li>
</ul>
</li>
</ul>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#security-docs-network-__codelineno-3-1"></a><span class="kr">package</span><span class="w"> </span><span class="nx">kubernetes</span><span class="p">.</span><span class="nx">admission</span>
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#security-docs-network-__codelineno-3-2"></a><span class="k">import</span><span class="w"> </span><span class="nx">data</span><span class="p">.</span><span class="nx">kubernetes</span><span class="p">.</span><span class="nx">networkpolicies</span>
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#security-docs-network-__codelineno-3-3"></a>
<a id="__codelineno-3-4" name="__codelineno-3-4" href="#security-docs-network-__codelineno-3-4"></a><span class="nx">deny</span><span class="p">[</span><span class="nx">msg</span><span class="p">]</span><span class="w"> </span><span class="p">{</span>
<a id="__codelineno-3-5" name="__codelineno-3-5" href="#security-docs-network-__codelineno-3-5"></a><span class="w">    </span><span class="nx">input</span><span class="p">.</span><span class="nx">request</span><span class="p">.</span><span class="nx">kind</span><span class="p">.</span><span class="nx">kind</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s2">&quot;Pod&quot;</span>
<a id="__codelineno-3-6" name="__codelineno-3-6" href="#security-docs-network-__codelineno-3-6"></a><span class="w">    </span><span class="nx">pod_label_value</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="p">{</span><span class="nx">v</span><span class="p">[</span><span class="s2">&quot;k8s-app&quot;</span><span class="p">]</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="nx">v</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">input</span><span class="p">.</span><span class="nx">request</span><span class="p">.</span><span class="nx">object</span><span class="p">.</span><span class="nx">metadata</span><span class="p">.</span><span class="nx">labels</span><span class="p">}</span>
<a id="__codelineno-3-7" name="__codelineno-3-7" href="#security-docs-network-__codelineno-3-7"></a><span class="w">    </span><span class="nx">contains_label</span><span class="p">(</span><span class="nx">pod_label_value</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;sample-app&quot;</span><span class="p">)</span>
<a id="__codelineno-3-8" name="__codelineno-3-8" href="#security-docs-network-__codelineno-3-8"></a><span class="w">    </span><span class="nx">np_label_value</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="p">{</span><span class="nx">v</span><span class="p">[</span><span class="s2">&quot;k8s-app&quot;</span><span class="p">]</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="nx">v</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">networkpolicies</span><span class="p">[</span><span class="nx">_</span><span class="p">].</span><span class="nx">spec</span><span class="p">.</span><span class="nx">podSelector</span><span class="p">.</span><span class="nx">matchLabels</span><span class="p">}</span>
<a id="__codelineno-3-9" name="__codelineno-3-9" href="#security-docs-network-__codelineno-3-9"></a><span class="w">    </span><span class="nx">not</span><span class="w"> </span><span class="nx">contains_label</span><span class="p">(</span><span class="nx">np_label_value</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;sample-app&quot;</span><span class="p">)</span>
<a id="__codelineno-3-10" name="__codelineno-3-10" href="#security-docs-network-__codelineno-3-10"></a><span class="w">    </span><span class="nx">msg</span><span class="o">:=</span><span class="w"> </span><span class="nx">sprintf</span><span class="p">(</span><span class="s2">&quot;The Pod %v could not be created because it is missing an associated Network Policy.&quot;</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="nx">input</span><span class="p">.</span><span class="nx">request</span><span class="p">.</span><span class="nx">object</span><span class="p">.</span><span class="nx">metadata</span><span class="p">.</span><span class="nx">name</span><span class="p">])</span>
<a id="__codelineno-3-11" name="__codelineno-3-11" href="#security-docs-network-__codelineno-3-11"></a><span class="p">}</span>
<a id="__codelineno-3-12" name="__codelineno-3-12" href="#security-docs-network-__codelineno-3-12"></a><span class="nx">contains_label</span><span class="p">(</span><span class="nx">arr</span><span class="p">,</span><span class="w"> </span><span class="nx">val</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<a id="__codelineno-3-13" name="__codelineno-3-13" href="#security-docs-network-__codelineno-3-13"></a><span class="w">    </span><span class="nx">arr</span><span class="p">[</span><span class="nx">_</span><span class="p">]</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nx">val</span>
<a id="__codelineno-3-14" name="__codelineno-3-14" href="#security-docs-network-__codelineno-3-14"></a><span class="p">}</span>
</code></pre></div>
<h3 id="security-docs-network-troubleshooting">Troubleshooting<a class="headerlink" href="#security-docs-network-troubleshooting" title="Permanent link">&para;</a></h3>
<h4 id="security-docs-network-monitor-the-vpc-network-policy-controller-node-agent-logs">Monitor the vpc-network-policy-controller, node-agent logs<a class="headerlink" href="#security-docs-network-monitor-the-vpc-network-policy-controller-node-agent-logs" title="Permanent link">&para;</a></h4>
<p>Enable the EKS Control plane controller manager logs to diagnose the network policy functionality.  You can stream the control plane logs to a CloudWatch log group and use <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html">CloudWatch Log insights</a> to perform advanced queries. From the logs, you can view what pod endpoint objects are resolved to a Network Policy, reconcilation status of the policies, and debug if the policy is working as expected.</p>
<p>In addition, Amazon VPC CNI allows you to enable the collection and export of policy enforcement logs to <a href="https://aws.amazon.com/cloudwatch/">Amazon Cloudwatch</a> from the EKS worker nodes. Once enabled, you can leverage <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/ContainerInsights.html">CloudWatch Container Insights</a> to provide insights on your usage related to Network Policies. </p>
<p>Amazon VPC CNI also ships an SDK that provides an interface to interact with eBPF programs on the node. The SDK is installed when the <code>aws-node</code> is deployed onto the nodes. You can find the SDK binary installed under <code>/opt/cni/bin</code> directory on the node. At launch, the SDK provides support for fundamental functionalities such as inspecting eBPF programs and maps.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#security-docs-network-__codelineno-4-1"></a>sudo<span class="w"> </span>/opt/cni/bin/aws-eks-na-cli<span class="w"> </span>ebpf<span class="w"> </span>progs
</code></pre></div>
<h4 id="security-docs-network-log-network-traffic-metadata">Log network traffic metadata<a class="headerlink" href="#security-docs-network-log-network-traffic-metadata" title="Permanent link">&para;</a></h4>
<p><a href="https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html">AWS VPC Flow Logs</a> captures metadata about the traffic flowing through a VPC, such as source and destination IP address and port along with accepted/dropped packets. This information could be analyzed to look for suspicious or unusual activity between resources within the VPC, including Pods.  However, since the IP addresses of pods frequently change as they are replaced, Flow Logs may not be sufficient on its own.  Calico Enterprise extends the Flow Logs with pod labels and other metadata, making it easier to decipher the traffic flows between pods.</p>
<h2 id="security-docs-network-security-groups">Security groups<a class="headerlink" href="#security-docs-network-security-groups" title="Permanent link">&para;</a></h2>
<p>EKS uses <a href="https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html">AWS VPC Security Groups</a> (SGs) to control the traffic between the Kubernetes control plane and the cluster's worker nodes. Security groups are also used to control the traffic between worker nodes, and other VPC resources, and external IP addresses.  When you provision an EKS cluster (with Kubernetes version 1.14-eks.3 or greater), a cluster security group is automatically created for you.  This security group allows unfettered communication between the EKS control plane and the nodes from managed node groups. For simplicity, it is recommended that you add the cluster SG to all node groups, including unmanaged node groups.</p>
<p>Prior to Kubernetes version 1.14 and EKS version eks.3, there were separate security groups configured for the EKS control plane and node groups. The minimum and suggested rules for the control plane and node group security groups can be found at <a href="https://docs.aws.amazon.com/eks/latest/userguide/sec-group-reqs.html">https://docs.aws.amazon.com/eks/latest/userguide/sec-group-reqs.html</a>.  The minimum rules for the <em>control plane security group</em> allows port 443 inbound from the worker node SG. This rule is what allows the kubelets to communicate with the Kubernetes API server.  It also includes port 10250 for outbound traffic to the worker node SG; 10250 is the port that the kubelets listen on. Similarly, the minimum <em>node group</em> rules allow port 10250 inbound from the control plane SG and 443 outbound to the control plane SG.  Finally there is a rule that allows unfettered communication between nodes within a node group.</p>
<p>If you need to control communication between services that run within the cluster and service the run outside the cluster such as an RDS database, consider <a href="https://docs.aws.amazon.com/eks/latest/userguide/security-groups-for-pods.html">security groups for pods</a>. With security groups for pods, you can assign an <strong>existing</strong> security group to a collection of pods.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you reference a security group that does not exist prior to the creation of the pods, the pods will not get scheduled.</p>
</div>
<p>You can control which pods are assigned to a security group by creating a <code>SecurityGroupPolicy</code> object and specifying a <code>PodSelector</code> or a <code>ServiceAccountSelector</code>. Setting the selectors to <code>{}</code> will assign the SGs referenced in the <code>SecurityGroupPolicy</code> to all pods in a namespace or all Service Accounts in a namespace. Be sure you've familiarized yourself with all the <a href="https://docs.aws.amazon.com/eks/latest/userguide/security-groups-for-pods.html#security-groups-pods-considerations">considerations</a> before implementing security groups for pods.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>If you use SGs for pods you <strong>must</strong> create SGs that allow port 53 outbound to the cluster security group.  Similarly, you <strong>must</strong> update the cluster security group to accept port 53 inbound traffic from the pod security group.</p>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>The <a href="https://docs.aws.amazon.com/vpc/latest/userguide/amazon-vpc-limits.html#vpc-limits-security-groups">limits for security groups</a> still apply when using security groups for pods so use them judiciously.</p>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>You <strong>must</strong> create rules for inbound traffic from the cluster security group (kubelet) for all of the probes configured for pod.</p>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Security groups for pods relies on a feature known as <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-instance-eni.html">ENI trunking</a> which was created to increase the ENI density of an EC2 instance. When a pod is assigned to an SG, a VPC controller associates a branch ENI from the node group with the pod. If there aren't enough branch ENIs available in a node group at the time the pod is scheduled, the pod will stay in pending state. The number of branch ENIs an instance can support varies by instance type/family. See <a href="https://docs.aws.amazon.com/eks/latest/userguide/security-groups-for-pods.html#supported-instance-types">https://docs.aws.amazon.com/eks/latest/userguide/security-groups-for-pods.html#supported-instance-types</a> for further details.</p>
</div>
<p>While security groups for pods offers an AWS-native way to control network traffic within and outside of your cluster without the overhead of a policy daemon, other options are available. For example, the Cilium policy engine allows you to reference a DNS name in a network policy. Calico Enterprise includes an option for mapping network policies to AWS security groups. If you've implemented a service mesh like Istio, you can use an egress gateway to restrict network egress to specific, fully qualified domains or IP addresses. For further information about this option, read the three part series on <a href="https://istio.io/blog/2019/egress-traffic-control-in-istio-part-1/">egress traffic control in Istio</a>.</p>
<h2 id="security-docs-network-when-to-use-network-policy-vs-security-group-for-pods">When to use Network Policy vs Security Group for Pods?<a class="headerlink" href="#security-docs-network-when-to-use-network-policy-vs-security-group-for-pods" title="Permanent link">&para;</a></h2>
<h3 id="security-docs-network-when-to-use-kubernetes-network-policy">When to use Kubernetes network policy:<a class="headerlink" href="#security-docs-network-when-to-use-kubernetes-network-policy" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Controlling pod-to-pod traffic</strong><ul>
<li>Suitable for controlling network traffic between pods inside a cluster (east-west traffic)</li>
</ul>
</li>
<li><strong>Control traffic at the IP address or port level (OSI layer 3 or 4)</strong></li>
</ul>
<h3 id="security-docs-network-when-to-use-aws-security-groups-for-pods-sgp">When to use AWS Security groups for pods (SGP):<a class="headerlink" href="#security-docs-network-when-to-use-aws-security-groups-for-pods-sgp" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Leverage existing AWS configurations</strong><ul>
<li>If you already have complex set of EC2 security groups that manage access to AWS services and you are migrating applications from EC2 instances to EKS, SGPs can be a very good choice allowing you to reuse security group resources and apply them to your pods.</li>
</ul>
</li>
<li><strong>Control access to AWS services</strong><ul>
<li>Your applications running within an EKS cluster wants to communicate with other AWS services (RDS database), use SGPs as an efficient mechanism to control the traffic from the pods to AWS services.</li>
</ul>
</li>
<li><strong>Isolation of Pod &amp; Node traffic</strong><ul>
<li>If you want to completely separate pod traffic from the rest of the node traffic, use SGP in <code>POD_SECURITY_GROUP_ENFORCING_MODE=strict</code> mode.</li>
</ul>
</li>
</ul>
<h3 id="security-docs-network-best-practices-using-security-groups-for-pods-and-network-policy">Best practices using <code>Security groups for pods</code> and <code>Network Policy</code><a class="headerlink" href="#security-docs-network-best-practices-using-security-groups-for-pods-and-network-policy" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Layered security</strong><ul>
<li>Use a combination of SGP and kubernetes network policy for a layered security approach</li>
<li>Use SGPs to limit network level access to AWS services that are not part of a cluster, while kubernetes network policies can restrict network traffic between pods inside the cluster</li>
</ul>
</li>
<li><strong>Principle of least privilege</strong><ul>
<li>Only allow necessary traffic between pods or namespaces</li>
</ul>
</li>
<li><strong>Segment your applications</strong><ul>
<li>Wherever possible, segment applications by the network policy to reduce the blast radius if an application is compromised</li>
</ul>
</li>
<li><strong>Keep policies simple and clear</strong><ul>
<li>Kubernetes network policies can be quite granular and complex, its best to keep them as simple as possible to reduce the risk of misconfiguration and ease the management overhead</li>
</ul>
</li>
<li><strong>Reduce the attack surface</strong><ul>
<li>Minimize the attack surface by limiting the exposure of your applications</li>
</ul>
</li>
</ul>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Security Groups for pods provides two enforcing modes: <code>strict</code> and <code>standard</code>. You must use <code>standard</code> mode when using both Network Policy and Security Groups for pods features in an EKS cluster.</p>
</div>
<p>When it comes to network security, a layered approach is often the most effective solution. Using kubernetes network policy and SGP in combination can provide a robust defense-in-depth strategy for your applications running in EKS.</p>
<h2 id="security-docs-network-service-mesh-policy-enforcement-or-kubernetes-network-policy">Service Mesh Policy Enforcement or Kubernetes network policy<a class="headerlink" href="#security-docs-network-service-mesh-policy-enforcement-or-kubernetes-network-policy" title="Permanent link">&para;</a></h2>
<p>A <code>service mesh</code> is a dedicated infrastructure layer that you can add to your applications. It allows you to transparently add capabilities like observability, traffic management, and security, without adding them to your own code. </p>
<p>Service mesh enforces policies at Layer 7 (application) of OSI model whereas kubernetes network policies operate at Layer 3 (network) and Layer 4 (transport). There are many offerings in this space like AWS AppMesh, Istio, Linkerd, etc.,</p>
<h3 id="security-docs-network-when-to-use-service-mesh-for-policy-enforcement">When to use Service mesh for policy enforcement:<a class="headerlink" href="#security-docs-network-when-to-use-service-mesh-for-policy-enforcement" title="Permanent link">&para;</a></h3>
<ul>
<li>Have existing investment in a service mesh</li>
<li>Need more advanced capabilities like traffic management, observability &amp; security <ul>
<li>Traffic control, load balancing, circuit breaking, rate limiting, timeouts etc.</li>
<li>Detailed insights into how your services are performing (latency, error rates, requests per second, request volumes etc.)</li>
<li>You want to implement and leverage service mesh for security features like mTLS</li>
</ul>
</li>
</ul>
<h3 id="security-docs-network-choose-kubernetes-network-policy-for-simpler-use-cases">Choose Kubernetes network policy for simpler use cases<a class="headerlink" href="#security-docs-network-choose-kubernetes-network-policy-for-simpler-use-cases" title="Permanent link">&para;</a></h3>
<ul>
<li>Limit which pods can communicate with each other</li>
<li>Network policies require fewer resources than a service mesh making them a good fit for simpler use cases or for smaller clusters where the overhead of running and managing a service mesh might not be justified</li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Network policies and Service mesh can also be used together. Use network policies to provide a baseline level of security and isolation between your pods and then use a service mesh to add additional capabilities like traffic management, observability and security.</p>
</div>
<h2 id="security-docs-network-thirdparty-network-policy-engines">ThirdParty Network Policy Engines<a class="headerlink" href="#security-docs-network-thirdparty-network-policy-engines" title="Permanent link">&para;</a></h2>
<p>Consider a Third Party Network Policy Engine when you have advanced policy requirements like Global Network Policies, support for DNS Hostname based rules, Layer 7 rules, ServiceAccount based rules, and explicit deny/log actions, etc., <a href="https://docs.projectcalico.org/introduction/">Calico</a>, is an open source policy engine from <a href="https://tigera.io">Tigera</a> that works well with EKS. In addition to implementing the full set of Kubernetes network policy features, Calico supports extended network polices with a richer set of features, including support for layer 7 rules, e.g. HTTP, when integrated with Istio. Calico policies can be scoped to Namespaces, Pods, service accounts, or globally.  When policies are scoped to a service account, it associates a set of ingress/egress rules with that service account.  With the proper RBAC rules in place, you can prevent teams from overriding these rules, allowing IT security professionals to safely delegate administration of namespaces. Isovalent, the maintainers of <a href="https://cilium.readthedocs.io/en/stable/intro/">Cilium</a>, have also extended the network policies to include partial support for layer 7 rules, e.g. HTTP.  Cilium also has support for DNS hostnames which can be useful for restricting traffic between Kubernetes Services/Pods and resources that run within or outside of your VPC. By contrast, Calico Enterprise includes a feature that allows you to map a Kubernetes network policy to an AWS security group, as well as DNS hostnames. </p>
<p>You can find a list of common Kubernetes network policies at <a href="https://github.com/ahmetb/kubernetes-network-policy-recipes">https://github.com/ahmetb/kubernetes-network-policy-recipes</a>.  A similar set of rules for Calico are available at <a href="https://docs.projectcalico.org/security/calico-network-policy">https://docs.projectcalico.org/security/calico-network-policy</a>.</p>
<h3 id="security-docs-network-migration-to-amazon-vpc-cni-network-policy-engine">Migration to Amazon VPC CNI Network Policy Engine<a class="headerlink" href="#security-docs-network-migration-to-amazon-vpc-cni-network-policy-engine" title="Permanent link">&para;</a></h3>
<p>To maintain consistency and avoid unexpected pod communication behavior, it is recommended to deploy only one Network Policy Engine in your cluster. If you want to migrate from 3P to VPC CNI Network Policy Engine, we recommend converting your existing 3P NetworkPolicy CRDs to the Kubernetes NetworkPolicy resources before enabling VPC CNI network policy support. And, test the migrated policies in a separate test cluster before applying them in you production environment. This allows you to identify and address any potential issues or inconsistencies in pod communication behavior.</p>
<h4 id="security-docs-network-migration-tool">Migration Tool<a class="headerlink" href="#security-docs-network-migration-tool" title="Permanent link">&para;</a></h4>
<p>To assist in your migration process, we have developed a tool called <a href="https://github.com/awslabs/k8s-network-policy-migrator">K8s Network Policy Migrator</a> that converts your existing Calico/Cilium network policy CRDs to Kubernetes native network policies. After conversion you can directly test the converted network policies on your new clusters running VPC CNI network policy controller. The tool is designed to help you streamline the migration process and ensure a smooth transition.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Migration tool will only convert 3P policies that are compatible with native kubernetes network policy api. If you are using advanced network policy features offered by 3P plugins, Migration tool will skip and report them.</p>
</div>
<p>Please note that migration tool is currently not supported by AWS VPC CNI Network policy engineering team, it is made available to customers on a best-effort basis. We encourage you to utilize this tool to facilitate your migration process. In the event that you encounter any issues or bugs with the tool, we kindly ask you create a <a href="https://github.com/awslabs/k8s-network-policy-migrator/issues">GitHub issue</a>. Your feedback is invaluable to us and will assist in the continuous improvement of our services.</p>
<h3 id="security-docs-network-additional-resources">Additional Resources<a class="headerlink" href="#security-docs-network-additional-resources" title="Permanent link">&para;</a></h3>
<ul>
<li><a href="https://youtu.be/lEY2WnRHYpg">Kubernetes &amp; Tigera: Network Policies, Security, and Audit</a></li>
<li><a href="https://www.tigera.io/tigera-products/calico-enterprise/">Calico Enterprise</a></li>
<li><a href="https://cilium.readthedocs.io/en/stable/intro/">Cilium</a></li>
<li><a href="https://cilium.io/blog/2021/02/10/network-policy-editor">NetworkPolicy Editor</a> an interactive policy editor from Cilium</li>
<li><a href="https://www.inspektor-gadget.io/docs/latest/gadgets/advise/network-policy/">Inspektor Gadget advise network-policy gadget</a> Suggests network policies based on an analysis of network traffic</li>
</ul>
<h2 id="security-docs-network-encryption-in-transit_1">Encryption in transit<a class="headerlink" href="#security-docs-network-encryption-in-transit_1" title="Permanent link">&para;</a></h2>
<p>Applications that need to conform to PCI, HIPAA, or other regulations may need to encrypt data while it is in transit.  Nowadays TLS is the de facto choice for encrypting traffic on the wire.  TLS, like it's predecessor SSL, provides secure communications over a network using cryptographic protocols.  TLS uses symmetric encryption where the keys to encrypt the data are generated based on a shared secret that is negotiated at the beginning of the session. The following are a few ways that you can encrypt data in a Kubernetes environment.</p>
<h3 id="security-docs-network-nitro-instances">Nitro Instances<a class="headerlink" href="#security-docs-network-nitro-instances" title="Permanent link">&para;</a></h3>
<p>Traffic exchanged between the following Nitro instance types, e.g. C5n, G4, I3en, M5dn, M5n, P3dn, R5dn, and R5n, is automatically encrypted by default.  When there's an intermediate hop, like a transit gateway or a load balancer, the traffic is not encrypted. See <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/data-protection.html#encryption-transit">Encryption in transit</a> for further details on encryption in transit as well as the complete list of instances types that support network encryption by default.</p>
<h3 id="security-docs-network-container-network-interfaces-cnis">Container Network Interfaces (CNIs)<a class="headerlink" href="#security-docs-network-container-network-interfaces-cnis" title="Permanent link">&para;</a></h3>
<p><a href="https://www.weave.works/oss/net/">WeaveNet</a> can be configured to automatically encrypt all traffic using NaCl encryption for sleeve traffic, and IPsec ESP for fast datapath traffic.</p>
<h3 id="security-docs-network-service-mesh">Service Mesh<a class="headerlink" href="#security-docs-network-service-mesh" title="Permanent link">&para;</a></h3>
<p>Encryption in transit can also be implemented with a service mesh like App Mesh, Linkerd v2, and Istio. AppMesh supports <a href="https://docs.aws.amazon.com/app-mesh/latest/userguide/mutual-tls.html">mTLS</a> with X.509 certificates or Envoy's Secret Discovery Service(SDS). Linkerd and Istio both have support for mTLS.</p>
<p>The <a href="https://github.com/aws/aws-app-mesh-examples">aws-app-mesh-examples</a> GitHub repository provides walkthroughs for configuring mTLS using X.509 certificates and SPIRE as SDS provider with your Envoy container:</p>
<ul>
<li><a href="https://github.com/aws/aws-app-mesh-examples/tree/main/walkthroughs/howto-k8s-mtls-file-based">Configuring mTLS using X.509 certificates</a></li>
<li><a href="https://github.com/aws/aws-app-mesh-examples/tree/main/walkthroughs/howto-k8s-mtls-sds-based">Configuring TLS using SPIRE (SDS)</a></li>
</ul>
<p>App Mesh also supports <a href="https://docs.aws.amazon.com/app-mesh/latest/userguide/virtual-node-tls.html">TLS encryption</a> with a private certificate issued by <a href="https://docs.aws.amazon.com/acm/latest/userguide/acm-overview.html">AWS Certificate Manager</a> (ACM) or a certificate stored on the local file system of the virtual node.</p>
<p>The <a href="https://github.com/aws/aws-app-mesh-examples">aws-app-mesh-examples</a> GitHub repository provides walkthroughs for configuring TLS using certificates issued by ACM and certificates that are packaged with your Envoy container:
+ <a href="https://github.com/aws/aws-app-mesh-examples/tree/master/walkthroughs/howto-tls-file-provided">Configuring TLS with File Provided TLS Certificates</a>
+ <a href="https://github.com/aws/aws-app-mesh-examples/tree/master/walkthroughs/tls-with-acm">Configuring TLS with AWS Certificate Manager</a></p>
<h3 id="security-docs-network-ingress-controllers-and-load-balancers">Ingress Controllers and Load Balancers<a class="headerlink" href="#security-docs-network-ingress-controllers-and-load-balancers" title="Permanent link">&para;</a></h3>
<p>Ingress controllers are a way for you to intelligently route HTTP/S traffic that emanates from outside the cluster to services running inside the cluster. Oftentimes, these Ingresses are fronted by a layer 4 load balancer, like the Classic Load Balancer or the Network Load Balancer (NLB). Encrypted traffic can be terminated at different places within the network, e.g. at the load balancer, at the ingress resource, or the Pod. How and where you terminate your SSL connection will ultimately be dictated by your organization's network security policy. For instance, if you have a policy that requires end-to-end encryption, you will have to decrypt the traffic at the Pod. This will place additional burden on your Pod as it will have to spend cycles establishing the initial handshake. Overall SSL/TLS processing is very CPU intensive. Consequently, if you have the flexibility, try performing the SSL offload at the Ingress or the load balancer.</p>
<h4 id="security-docs-network-use-encryption-with-aws-elastic-load-balancers">Use encryption with AWS Elastic load balancers<a class="headerlink" href="#security-docs-network-use-encryption-with-aws-elastic-load-balancers" title="Permanent link">&para;</a></h4>
<p>The <a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html">AWS Application Load Balancer</a> (ALB) and <a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html">Network Load Balancer</a> (NLB) both have support for transport encryption (SSL and TLS).  The <code>alb.ingress.kubernetes.io/certificate-arn</code> annotation for the ALB lets you to specify which certificates to add to the ALB.  If you omit the annotation the controller will attempt to add certificates to listeners that require it by matching the available <a href="https://docs.aws.amazon.com/acm/latest/userguide/acm-overview.html">AWS Certificate Manager (ACM)</a> certificates using the host field. Starting with EKS v1.15 you can use the <code>service.beta.kubernetes.io/aws-load-balancer-ssl-cert</code> annotation with the NLB as shown in the example below.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#security-docs-network-__codelineno-5-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<a id="__codelineno-5-2" name="__codelineno-5-2" href="#security-docs-network-__codelineno-5-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Service</span>
<a id="__codelineno-5-3" name="__codelineno-5-3" href="#security-docs-network-__codelineno-5-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-5-4" name="__codelineno-5-4" href="#security-docs-network-__codelineno-5-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">demo-app</span>
<a id="__codelineno-5-5" name="__codelineno-5-5" href="#security-docs-network-__codelineno-5-5"></a><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">default</span>
<a id="__codelineno-5-6" name="__codelineno-5-6" href="#security-docs-network-__codelineno-5-6"></a><span class="w">  </span><span class="nt">labels</span><span class="p">:</span>
<a id="__codelineno-5-7" name="__codelineno-5-7" href="#security-docs-network-__codelineno-5-7"></a><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">demo-app</span>
<a id="__codelineno-5-8" name="__codelineno-5-8" href="#security-docs-network-__codelineno-5-8"></a><span class="w">  </span><span class="nt">annotations</span><span class="p">:</span>
<a id="__codelineno-5-9" name="__codelineno-5-9" href="#security-docs-network-__codelineno-5-9"></a><span class="w">     </span><span class="nt">service.beta.kubernetes.io/aws-load-balancer-type</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;nlb&quot;</span>
<a id="__codelineno-5-10" name="__codelineno-5-10" href="#security-docs-network-__codelineno-5-10"></a><span class="w">     </span><span class="nt">service.beta.kubernetes.io/aws-load-balancer-ssl-cert</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;&lt;certificate</span><span class="nv"> </span><span class="s">ARN&gt;&quot;</span>
<a id="__codelineno-5-11" name="__codelineno-5-11" href="#security-docs-network-__codelineno-5-11"></a><span class="w">     </span><span class="nt">service.beta.kubernetes.io/aws-load-balancer-ssl-ports</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;443&quot;</span>
<a id="__codelineno-5-12" name="__codelineno-5-12" href="#security-docs-network-__codelineno-5-12"></a><span class="w">     </span><span class="nt">service.beta.kubernetes.io/aws-load-balancer-backend-protocol</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;http&quot;</span>
<a id="__codelineno-5-13" name="__codelineno-5-13" href="#security-docs-network-__codelineno-5-13"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-5-14" name="__codelineno-5-14" href="#security-docs-network-__codelineno-5-14"></a><span class="w">  </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">LoadBalancer</span>
<a id="__codelineno-5-15" name="__codelineno-5-15" href="#security-docs-network-__codelineno-5-15"></a><span class="w">  </span><span class="nt">ports</span><span class="p">:</span>
<a id="__codelineno-5-16" name="__codelineno-5-16" href="#security-docs-network-__codelineno-5-16"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">443</span>
<a id="__codelineno-5-17" name="__codelineno-5-17" href="#security-docs-network-__codelineno-5-17"></a><span class="w">    </span><span class="nt">targetPort</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">80</span>
<a id="__codelineno-5-18" name="__codelineno-5-18" href="#security-docs-network-__codelineno-5-18"></a><span class="w">    </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">TCP</span>
<a id="__codelineno-5-19" name="__codelineno-5-19" href="#security-docs-network-__codelineno-5-19"></a><span class="w">  </span><span class="nt">selector</span><span class="p">:</span>
<a id="__codelineno-5-20" name="__codelineno-5-20" href="#security-docs-network-__codelineno-5-20"></a><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">demo-app</span>
<a id="__codelineno-5-21" name="__codelineno-5-21" href="#security-docs-network-__codelineno-5-21"></a><span class="nn">---</span>
<a id="__codelineno-5-22" name="__codelineno-5-22" href="#security-docs-network-__codelineno-5-22"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deployment</span>
<a id="__codelineno-5-23" name="__codelineno-5-23" href="#security-docs-network-__codelineno-5-23"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">apps/v1</span>
<a id="__codelineno-5-24" name="__codelineno-5-24" href="#security-docs-network-__codelineno-5-24"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-5-25" name="__codelineno-5-25" href="#security-docs-network-__codelineno-5-25"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nginx</span>
<a id="__codelineno-5-26" name="__codelineno-5-26" href="#security-docs-network-__codelineno-5-26"></a><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">default</span>
<a id="__codelineno-5-27" name="__codelineno-5-27" href="#security-docs-network-__codelineno-5-27"></a><span class="w">  </span><span class="nt">labels</span><span class="p">:</span>
<a id="__codelineno-5-28" name="__codelineno-5-28" href="#security-docs-network-__codelineno-5-28"></a><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">demo-app</span>
<a id="__codelineno-5-29" name="__codelineno-5-29" href="#security-docs-network-__codelineno-5-29"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-5-30" name="__codelineno-5-30" href="#security-docs-network-__codelineno-5-30"></a><span class="w">  </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<a id="__codelineno-5-31" name="__codelineno-5-31" href="#security-docs-network-__codelineno-5-31"></a><span class="w">  </span><span class="nt">selector</span><span class="p">:</span>
<a id="__codelineno-5-32" name="__codelineno-5-32" href="#security-docs-network-__codelineno-5-32"></a><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span>
<a id="__codelineno-5-33" name="__codelineno-5-33" href="#security-docs-network-__codelineno-5-33"></a><span class="w">      </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">demo-app</span>
<a id="__codelineno-5-34" name="__codelineno-5-34" href="#security-docs-network-__codelineno-5-34"></a><span class="w">  </span><span class="nt">template</span><span class="p">:</span>
<a id="__codelineno-5-35" name="__codelineno-5-35" href="#security-docs-network-__codelineno-5-35"></a><span class="w">    </span><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-5-36" name="__codelineno-5-36" href="#security-docs-network-__codelineno-5-36"></a><span class="w">      </span><span class="nt">labels</span><span class="p">:</span>
<a id="__codelineno-5-37" name="__codelineno-5-37" href="#security-docs-network-__codelineno-5-37"></a><span class="w">        </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">demo-app</span>
<a id="__codelineno-5-38" name="__codelineno-5-38" href="#security-docs-network-__codelineno-5-38"></a><span class="w">    </span><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-5-39" name="__codelineno-5-39" href="#security-docs-network-__codelineno-5-39"></a><span class="w">      </span><span class="nt">containers</span><span class="p">:</span>
<a id="__codelineno-5-40" name="__codelineno-5-40" href="#security-docs-network-__codelineno-5-40"></a><span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nginx</span>
<a id="__codelineno-5-41" name="__codelineno-5-41" href="#security-docs-network-__codelineno-5-41"></a><span class="w">          </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nginx</span>
<a id="__codelineno-5-42" name="__codelineno-5-42" href="#security-docs-network-__codelineno-5-42"></a><span class="w">          </span><span class="nt">ports</span><span class="p">:</span>
<a id="__codelineno-5-43" name="__codelineno-5-43" href="#security-docs-network-__codelineno-5-43"></a><span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">443</span>
<a id="__codelineno-5-44" name="__codelineno-5-44" href="#security-docs-network-__codelineno-5-44"></a><span class="w">              </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">TCP</span>
<a id="__codelineno-5-45" name="__codelineno-5-45" href="#security-docs-network-__codelineno-5-45"></a><span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">80</span>
<a id="__codelineno-5-46" name="__codelineno-5-46" href="#security-docs-network-__codelineno-5-46"></a><span class="w">              </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">TCP</span>
</code></pre></div>
<p>Following are additional examples for SSL/TLS termination.</p>
<ul>
<li><a href="https://aws.amazon.com/blogs/containers/securing-eks-ingress-contour-lets-encrypt-gitops/">Securing EKS Ingress With Contour And Let’s Encrypt The GitOps Way</a></li>
<li><a href="https://aws.amazon.com/premiumsupport/knowledge-center/terminate-https-traffic-eks-acm/">How do I terminate HTTPS traffic on Amazon EKS workloads with ACM?</a></li>
</ul>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Some Ingresses, like the AWS LB controller, implement the SSL/TLS using Annotations instead of as part of the Ingress Spec.</p>
</div>
<h3 id="security-docs-network-acm-private-ca-with-cert-manager">ACM Private CA with cert-manager<a class="headerlink" href="#security-docs-network-acm-private-ca-with-cert-manager" title="Permanent link">&para;</a></h3>
<p>You can enable TLS and mTLS to secure your EKS application workloads at the ingress, on the pod, and between pods using ACM Private Certificate Authority (CA) and <a href="https://cert-manager.io/">cert-manager</a>, a popular Kubernetes add-on to distribute, renew, and revoke certificates. ACM Private CA is a highly-available, secure, managed CA without the upfront and maintenance costs of managing your own CA. If you are using the default Kubernetes certificate authority, there is an opportunity to improve your security and meet compliance requirements with ACM Private CA. ACM Private CA secures private keys in FIPS 140-2 Level 3 hardware security modules (very secure), compared with the default CA storing keys encoded in memory (less secure). A centralized CA also gives you more control and improved auditability for private certificates both inside and outside of a Kubernetes environment. </p>
<h4 id="security-docs-network-short-lived-ca-mode-for-mutual-tls-between-workloads">Short-Lived CA Mode for Mutual TLS Between Workloads<a class="headerlink" href="#security-docs-network-short-lived-ca-mode-for-mutual-tls-between-workloads" title="Permanent link">&para;</a></h4>
<p>When using ACM Private CA for mTLS in EKS, it is recommended that you use short lived certificates with <em>short-lived CA mode</em>. Although it is possible to issue out short-lived certificates in the general-purpose CA mode, using short-lived CA mode works out more cost-effective (~75% cheaper than general mode) for use cases where new certificates need to be issued frequently. In addition to this, you should try to align the validity period of the private certificates with the lifetime of the pods in your EKS cluster. <a href="https://aws.amazon.com/certificate-manager/private-certificate-authority/">Learn more about ACM Private CA and its benefits here</a>.</p>
<h4 id="security-docs-network-setup-instructions">Setup Instructions<a class="headerlink" href="#security-docs-network-setup-instructions" title="Permanent link">&para;</a></h4>
<p>Start by creating a Private CA by following procedures provided in the <a href="https://docs.aws.amazon.com/acm-pca/latest/userguide/create-CA.html">ACM Private CA tech docs</a>. Once you have a Private CA, install cert-manager using <a href="https://cert-manager.io/docs/installation/">regular installation instructions</a>. After installing cert-manager, install the Private CA Kubernetes cert-manager plugin by following the <a href="https://github.com/cert-manager/aws-privateca-issuer#setup">setup instructions in GitHub</a>. The plugin lets cert-manager request private certificates from ACM Private CA.</p>
<p>Now that you have a Private CA and an EKS cluster with cert-manager and the plugin installed, it’s time to set permissions and create the issuer. Update IAM permissions of the EKS node role to allow access to ACM Private CA. Replace the <code>&lt;CA_ARN&gt;</code> with the value from your Private CA:</p>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#security-docs-network-__codelineno-6-1"></a>{
<a id="__codelineno-6-2" name="__codelineno-6-2" href="#security-docs-network-__codelineno-6-2"></a>    &quot;Version&quot;: &quot;2012-10-17&quot;,
<a id="__codelineno-6-3" name="__codelineno-6-3" href="#security-docs-network-__codelineno-6-3"></a>    &quot;Statement&quot;: [
<a id="__codelineno-6-4" name="__codelineno-6-4" href="#security-docs-network-__codelineno-6-4"></a>        {
<a id="__codelineno-6-5" name="__codelineno-6-5" href="#security-docs-network-__codelineno-6-5"></a>            &quot;Sid&quot;: &quot;awspcaissuer&quot;,
<a id="__codelineno-6-6" name="__codelineno-6-6" href="#security-docs-network-__codelineno-6-6"></a>            &quot;Action&quot;: [
<a id="__codelineno-6-7" name="__codelineno-6-7" href="#security-docs-network-__codelineno-6-7"></a>                &quot;acm-pca:DescribeCertificateAuthority&quot;,
<a id="__codelineno-6-8" name="__codelineno-6-8" href="#security-docs-network-__codelineno-6-8"></a>                &quot;acm-pca:GetCertificate&quot;,
<a id="__codelineno-6-9" name="__codelineno-6-9" href="#security-docs-network-__codelineno-6-9"></a>                &quot;acm-pca:IssueCertificate&quot;
<a id="__codelineno-6-10" name="__codelineno-6-10" href="#security-docs-network-__codelineno-6-10"></a>            ],
<a id="__codelineno-6-11" name="__codelineno-6-11" href="#security-docs-network-__codelineno-6-11"></a>            &quot;Effect&quot;: &quot;Allow&quot;,
<a id="__codelineno-6-12" name="__codelineno-6-12" href="#security-docs-network-__codelineno-6-12"></a>            &quot;Resource&quot;: &quot;&lt;CA_ARN&gt;&quot;
<a id="__codelineno-6-13" name="__codelineno-6-13" href="#security-docs-network-__codelineno-6-13"></a>        }
<a id="__codelineno-6-14" name="__codelineno-6-14" href="#security-docs-network-__codelineno-6-14"></a>    ]
<a id="__codelineno-6-15" name="__codelineno-6-15" href="#security-docs-network-__codelineno-6-15"></a>}
</code></pre></div>
<a href="https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html">Service Roles for IAM Accounts, or IRSA</a> can also be used. Please see the Additional Resources section below for complete examples.</p>
<p>Create an Issuer in Amazon EKS by creating a Custom Resource Definition file named cluster-issuer.yaml with the following text in it, replacing <code>&lt;CA_ARN&gt;</code> and <code>&lt;Region&gt;</code> information with your Private CA.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#security-docs-network-__codelineno-7-1"></a>apiVersion: awspca.cert-manager.io/v1beta1
<a id="__codelineno-7-2" name="__codelineno-7-2" href="#security-docs-network-__codelineno-7-2"></a>kind: AWSPCAClusterIssuer
<a id="__codelineno-7-3" name="__codelineno-7-3" href="#security-docs-network-__codelineno-7-3"></a>metadata:
<a id="__codelineno-7-4" name="__codelineno-7-4" href="#security-docs-network-__codelineno-7-4"></a>          name: demo-test-root-ca
<a id="__codelineno-7-5" name="__codelineno-7-5" href="#security-docs-network-__codelineno-7-5"></a>spec:
<a id="__codelineno-7-6" name="__codelineno-7-6" href="#security-docs-network-__codelineno-7-6"></a>          arn: &lt;CA_ARN&gt;
<a id="__codelineno-7-7" name="__codelineno-7-7" href="#security-docs-network-__codelineno-7-7"></a>          region: &lt;Region&gt;
</code></pre></div>
<p>Deploy the Issuer you created.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1" href="#security-docs-network-__codelineno-8-1"></a>kubectl apply -f cluster-issuer.yaml
</code></pre></div>
<p>Your EKS cluster is configured to request certificates from Private CA. You can now use cert-manager's <code>Certificate</code> resource to issue certificates by changing the <code>issuerRef</code> field's values to the Private CA Issuer you created above. For more details on how to specify and request Certificate resources, please check cert-manager's <a href="https://cert-manager.io/docs/usage/certificate/">Certificate Resources guide</a>. <a href="https://github.com/cert-manager/aws-privateca-issuer/tree/main/config/samples/">See examples here</a>.</p>
<h3 id="security-docs-network-acm-private-ca-with-istio-and-cert-manager">ACM Private CA with Istio and cert-manager<a class="headerlink" href="#security-docs-network-acm-private-ca-with-istio-and-cert-manager" title="Permanent link">&para;</a></h3>
<p>If you are running Istio in your EKS cluster, you can disable the Istio control plane (specifically <code>istiod</code>) from functioning as the root Certificate Authority (CA), and configure ACM Private CA as the root CA for mTLS between workloads. If you're going with this approach, consider using the <em>short-lived CA mode</em> in ACM Private CA. Refer to the <a href="#security-docs-network-short-lived-ca-mode-for-mutual-tls-between-workloads">previous section</a> and this <a href="https://aws.amazon.com/blogs/security/how-to-use-aws-private-certificate-authority-short-lived-certificate-mode">blog post</a> for more details.</p>
<h4 id="security-docs-network-how-certificate-signing-works-in-istio-default">How Certificate Signing Works in Istio (Default)<a class="headerlink" href="#security-docs-network-how-certificate-signing-works-in-istio-default" title="Permanent link">&para;</a></h4>
<p>Workloads in Kubernetes are identified using service accounts. If you don't specify a service account, Kubernetes will automatically assign one to your workload. Also, service accounts automatically mount an associated token. This token is used by the service account for workloads to authenticate against the Kubernetes API. The service account may be sufficient as an identity for Kubernetes but Istio has its own identity management system and CA. When a workload starts up with its envoy sidecar proxy, it needs an identity assigned from Istio in order for it to be deemed as trustworthy and allowed to communicate with other services in the mesh.</p>
<p>To get this identity from Istio, the <code>istio-agent</code> sends a request known as a certificate signing request (or CSR) to the Istio control plane. This CSR contains the service account token so that the workload's identity can be verified before being processed. This verification process is handled by <code>istiod</code>, which acts as both the Registration Authority (or RA) and the CA. The RA serves as a gatekeeper that makes sure only verified CSR makes it through to the CA. Once the CSR is verified, it will be forwarded to the CA which will then issue a certificate containing a <a href="https://spiffe.io/">SPIFFE</a> identity with the service account. This certificate is called a SPIFFE verifiable identity document (or SVID). The SVID is assigned to the requesting service for identification purposes and to encrypt the traffic in transit between the communicating services.</p>
<p><img alt="Default flow for Istio Certificate Signing Requests" src="../security/docs/images/default-istio-csr-flow.png" /></p>
<h4 id="security-docs-network-how-certificate-signing-works-in-istio-with-acm-private-ca">How Certificate Signing Works in Istio with ACM Private CA<a class="headerlink" href="#security-docs-network-how-certificate-signing-works-in-istio-with-acm-private-ca" title="Permanent link">&para;</a></h4>
<p>You can use a cert-manager add-on called the Istio Certificate Signing Request agent (<a href="https://cert-manager.io/docs/projects/istio-csr/">istio-csr</a>) to integrate Istio with ACM Private CA. This agent allows Istio workloads and control plane components to be secured with cert manager issuers, in this case ACM Private CA. The <em>istio-csr</em> agent exposes the same service that <em>istiod</em> serves in the default config of validating incoming CSRs. Except, after verification, it will convert the requests into resources that cert manager supports (i.e. integrations with external CA issuers). </p>
<p>Whenever there's a CSR from a workload, it will be forwarded to <em>istio-csr</em>, which will request certificates from ACM Private CA. This communication between <em>istio-csr</em> and ACM Private CA is enabled by the <a href="https://github.com/cert-manager/aws-privateca-issuer">AWS Private CA issuer plugin</a>. Cert manager uses this plugin to request TLS certificates from ACM Private CA. The issuer plugin will communicate with the ACM Private CA service to request a signed certificate for the workload. Once the certificate has been signed, it will be returned to <em>istio-csr</em>, which will read the signed request, and return it to the workload that initiated the CSR.</p>
<p><img alt="Flow for Istio Certificate Signing Requests with istio-csr" src="../security/docs/images/istio-csr-with-acm-private-ca.png" /></p>
<h4 id="security-docs-network-setup-instructions_1">Setup Instructions<a class="headerlink" href="#security-docs-network-setup-instructions_1" title="Permanent link">&para;</a></h4>
<ol>
<li>Start by following the same <a href="#security-docs-network-acm-private-ca-with-cert-manager">setup instructions in this section</a> to complete the following:</li>
<li>Create a Private CA</li>
<li>Install cert-manager</li>
<li>Install the issuer plugin</li>
<li>Set permissions and create an issuer. The issuer represents the CA and is used to sign <code>istiod</code> and mesh workload certificates. It will communicate with ACM Private CA.</li>
<li>Create an <code>istio-system</code> namespace. This is where the <code>istiod certificate</code> and other Istio resources will be deployed.</li>
<li>Install Istio CSR configured with AWS Private CA Issuer Plugin. You can preserve the certificate signing requests for workloads to verify that they get approved and signed (<code>preserveCertificateRequests=true</code>).</li>
</ol>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-9-1" name="__codelineno-9-1" href="#security-docs-network-__codelineno-9-1"></a>helm<span class="w"> </span>install<span class="w"> </span>-n<span class="w"> </span>cert-manager<span class="w"> </span>cert-manager-istio-csr<span class="w"> </span>jetstack/cert-manager-istio-csr<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-9-2" name="__codelineno-9-2" href="#security-docs-network-__codelineno-9-2"></a><span class="w">    </span>--set<span class="w"> </span><span class="s2">&quot;app.certmanager.issuer.group=awspca.cert-manager.io&quot;</span><span class="w"> </span><span class="se">\</span>
<a id="__codelineno-9-3" name="__codelineno-9-3" href="#security-docs-network-__codelineno-9-3"></a><span class="w">    </span>--set<span class="w"> </span><span class="s2">&quot;app.certmanager.issuer.kind=AWSPCAClusterIssuer&quot;</span><span class="w"> </span><span class="se">\</span>
<a id="__codelineno-9-4" name="__codelineno-9-4" href="#security-docs-network-__codelineno-9-4"></a><span class="w">    </span>--set<span class="w"> </span><span class="s2">&quot;app.certmanager.issuer.name=&lt;the-name-of-the-issuer-you-created&gt;&quot;</span><span class="w"> </span><span class="se">\</span>
<a id="__codelineno-9-5" name="__codelineno-9-5" href="#security-docs-network-__codelineno-9-5"></a><span class="w">    </span>--set<span class="w"> </span><span class="s2">&quot;app.certmanager.preserveCertificateRequests=true&quot;</span><span class="w"> </span><span class="se">\</span>
<a id="__codelineno-9-6" name="__codelineno-9-6" href="#security-docs-network-__codelineno-9-6"></a><span class="w">    </span>--set<span class="w"> </span><span class="s2">&quot;app.server.maxCertificateDuration=48h&quot;</span><span class="w"> </span><span class="se">\</span>
<a id="__codelineno-9-7" name="__codelineno-9-7" href="#security-docs-network-__codelineno-9-7"></a><span class="w">    </span>--set<span class="w"> </span><span class="s2">&quot;app.tls.certificateDuration=24h&quot;</span><span class="w"> </span><span class="se">\</span>
<a id="__codelineno-9-8" name="__codelineno-9-8" href="#security-docs-network-__codelineno-9-8"></a><span class="w">    </span>--set<span class="w"> </span><span class="s2">&quot;app.tls.istiodCertificateDuration=24h&quot;</span><span class="w"> </span><span class="se">\</span>
<a id="__codelineno-9-9" name="__codelineno-9-9" href="#security-docs-network-__codelineno-9-9"></a><span class="w">    </span>--set<span class="w"> </span><span class="s2">&quot;app.tls.rootCAFile=/var/run/secrets/istio-csr/ca.pem&quot;</span><span class="w"> </span><span class="se">\</span>
<a id="__codelineno-9-10" name="__codelineno-9-10" href="#security-docs-network-__codelineno-9-10"></a><span class="w">    </span>--set<span class="w"> </span><span class="s2">&quot;volumeMounts[0].name=root-ca&quot;</span><span class="w"> </span><span class="se">\</span>
<a id="__codelineno-9-11" name="__codelineno-9-11" href="#security-docs-network-__codelineno-9-11"></a><span class="w">    </span>--set<span class="w"> </span><span class="s2">&quot;volumeMounts[0].mountPath=/var/run/secrets/istio-csr&quot;</span><span class="w"> </span><span class="se">\</span>
<a id="__codelineno-9-12" name="__codelineno-9-12" href="#security-docs-network-__codelineno-9-12"></a><span class="w">    </span>--set<span class="w"> </span><span class="s2">&quot;volumes[0].name=root-ca&quot;</span><span class="w"> </span><span class="se">\</span>
<a id="__codelineno-9-13" name="__codelineno-9-13" href="#security-docs-network-__codelineno-9-13"></a><span class="w">    </span>--set<span class="w"> </span><span class="s2">&quot;volumes[0].secret.secretName=istio-root-ca&quot;</span>
</code></pre></div>
4. Install Istio with custom configurations to replace <code>istiod</code> with <code>cert-manager istio-csr</code> as the certificate provider for the mesh. This process can be carried out using the <a href="https://tetrate.io/blog/what-is-istio-operator/">Istio Operator</a>. </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-10-1" name="__codelineno-10-1" href="#security-docs-network-__codelineno-10-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">install.istio.io/v1alpha1</span>
<a id="__codelineno-10-2" name="__codelineno-10-2" href="#security-docs-network-__codelineno-10-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">IstioOperator</span>
<a id="__codelineno-10-3" name="__codelineno-10-3" href="#security-docs-network-__codelineno-10-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-10-4" name="__codelineno-10-4" href="#security-docs-network-__codelineno-10-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">istio</span>
<a id="__codelineno-10-5" name="__codelineno-10-5" href="#security-docs-network-__codelineno-10-5"></a><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">istio-system</span>
<a id="__codelineno-10-6" name="__codelineno-10-6" href="#security-docs-network-__codelineno-10-6"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-10-7" name="__codelineno-10-7" href="#security-docs-network-__codelineno-10-7"></a><span class="w">  </span><span class="nt">profile</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;demo&quot;</span>
<a id="__codelineno-10-8" name="__codelineno-10-8" href="#security-docs-network-__codelineno-10-8"></a><span class="w">  </span><span class="nt">hub</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gcr.io/istio-release</span>
<a id="__codelineno-10-9" name="__codelineno-10-9" href="#security-docs-network-__codelineno-10-9"></a><span class="w">  </span><span class="nt">values</span><span class="p">:</span>
<a id="__codelineno-10-10" name="__codelineno-10-10" href="#security-docs-network-__codelineno-10-10"></a><span class="w">   </span><span class="nt">global</span><span class="p">:</span>
<a id="__codelineno-10-11" name="__codelineno-10-11" href="#security-docs-network-__codelineno-10-11"></a><span class="w">     </span><span class="c1"># Change certificate provider to cert-manager istio agent for istio agent</span>
<a id="__codelineno-10-12" name="__codelineno-10-12" href="#security-docs-network-__codelineno-10-12"></a><span class="w">    </span><span class="nt">caAddress</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">cert-manager-istio-csr.cert-manager.svc:443</span>
<a id="__codelineno-10-13" name="__codelineno-10-13" href="#security-docs-network-__codelineno-10-13"></a><span class="w">  </span><span class="nt">components</span><span class="p">:</span>
<a id="__codelineno-10-14" name="__codelineno-10-14" href="#security-docs-network-__codelineno-10-14"></a><span class="w">    </span><span class="nt">pilot</span><span class="p">:</span>
<a id="__codelineno-10-15" name="__codelineno-10-15" href="#security-docs-network-__codelineno-10-15"></a><span class="w">      </span><span class="nt">k8s</span><span class="p">:</span>
<a id="__codelineno-10-16" name="__codelineno-10-16" href="#security-docs-network-__codelineno-10-16"></a><span class="w">        </span><span class="nt">env</span><span class="p">:</span>
<a id="__codelineno-10-17" name="__codelineno-10-17" href="#security-docs-network-__codelineno-10-17"></a><span class="w">          </span><span class="c1"># Disable istiod CA Sever functionality</span>
<a id="__codelineno-10-18" name="__codelineno-10-18" href="#security-docs-network-__codelineno-10-18"></a><span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ENABLE_CA_SERVER</span>
<a id="__codelineno-10-19" name="__codelineno-10-19" href="#security-docs-network-__codelineno-10-19"></a><span class="w">          </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;false&quot;</span>
<a id="__codelineno-10-20" name="__codelineno-10-20" href="#security-docs-network-__codelineno-10-20"></a><span class="w">        </span><span class="nt">overlays</span><span class="p">:</span>
<a id="__codelineno-10-21" name="__codelineno-10-21" href="#security-docs-network-__codelineno-10-21"></a><span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">apps/v1</span>
<a id="__codelineno-10-22" name="__codelineno-10-22" href="#security-docs-network-__codelineno-10-22"></a><span class="w">          </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deployment</span>
<a id="__codelineno-10-23" name="__codelineno-10-23" href="#security-docs-network-__codelineno-10-23"></a><span class="w">          </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">istiod</span>
<a id="__codelineno-10-24" name="__codelineno-10-24" href="#security-docs-network-__codelineno-10-24"></a><span class="w">          </span><span class="nt">patches</span><span class="p">:</span>
<a id="__codelineno-10-25" name="__codelineno-10-25" href="#security-docs-network-__codelineno-10-25"></a>
<a id="__codelineno-10-26" name="__codelineno-10-26" href="#security-docs-network-__codelineno-10-26"></a><span class="w">            </span><span class="c1"># Mount istiod serving and webhook certificate from Secret mount</span>
<a id="__codelineno-10-27" name="__codelineno-10-27" href="#security-docs-network-__codelineno-10-27"></a><span class="w">          </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">spec.template.spec.containers.[name:discovery].args[7]</span>
<a id="__codelineno-10-28" name="__codelineno-10-28" href="#security-docs-network-__codelineno-10-28"></a><span class="w">            </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;--tlsCertFile=/etc/cert-manager/tls/tls.crt&quot;</span>
<a id="__codelineno-10-29" name="__codelineno-10-29" href="#security-docs-network-__codelineno-10-29"></a><span class="w">          </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">spec.template.spec.containers.[name:discovery].args[8]</span>
<a id="__codelineno-10-30" name="__codelineno-10-30" href="#security-docs-network-__codelineno-10-30"></a><span class="w">            </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;--tlsKeyFile=/etc/cert-manager/tls/tls.key&quot;</span>
<a id="__codelineno-10-31" name="__codelineno-10-31" href="#security-docs-network-__codelineno-10-31"></a><span class="w">          </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">spec.template.spec.containers.[name:discovery].args[9]</span>
<a id="__codelineno-10-32" name="__codelineno-10-32" href="#security-docs-network-__codelineno-10-32"></a><span class="w">            </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;--caCertFile=/etc/cert-manager/ca/root-cert.pem&quot;</span>
<a id="__codelineno-10-33" name="__codelineno-10-33" href="#security-docs-network-__codelineno-10-33"></a>
<a id="__codelineno-10-34" name="__codelineno-10-34" href="#security-docs-network-__codelineno-10-34"></a><span class="w">          </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">spec.template.spec.containers.[name:discovery].volumeMounts[6]</span>
<a id="__codelineno-10-35" name="__codelineno-10-35" href="#security-docs-network-__codelineno-10-35"></a><span class="w">            </span><span class="nt">value</span><span class="p">:</span>
<a id="__codelineno-10-36" name="__codelineno-10-36" href="#security-docs-network-__codelineno-10-36"></a><span class="w">              </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">cert-manager</span>
<a id="__codelineno-10-37" name="__codelineno-10-37" href="#security-docs-network-__codelineno-10-37"></a><span class="w">              </span><span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;/etc/cert-manager/tls&quot;</span>
<a id="__codelineno-10-38" name="__codelineno-10-38" href="#security-docs-network-__codelineno-10-38"></a><span class="w">              </span><span class="nt">readOnly</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<a id="__codelineno-10-39" name="__codelineno-10-39" href="#security-docs-network-__codelineno-10-39"></a><span class="w">          </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">spec.template.spec.containers.[name:discovery].volumeMounts[7]</span>
<a id="__codelineno-10-40" name="__codelineno-10-40" href="#security-docs-network-__codelineno-10-40"></a><span class="w">            </span><span class="nt">value</span><span class="p">:</span>
<a id="__codelineno-10-41" name="__codelineno-10-41" href="#security-docs-network-__codelineno-10-41"></a><span class="w">              </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ca-root-cert</span>
<a id="__codelineno-10-42" name="__codelineno-10-42" href="#security-docs-network-__codelineno-10-42"></a><span class="w">              </span><span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;/etc/cert-manager/ca&quot;</span>
<a id="__codelineno-10-43" name="__codelineno-10-43" href="#security-docs-network-__codelineno-10-43"></a><span class="w">              </span><span class="nt">readOnly</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<a id="__codelineno-10-44" name="__codelineno-10-44" href="#security-docs-network-__codelineno-10-44"></a>
<a id="__codelineno-10-45" name="__codelineno-10-45" href="#security-docs-network-__codelineno-10-45"></a><span class="w">          </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">spec.template.spec.volumes[6]</span>
<a id="__codelineno-10-46" name="__codelineno-10-46" href="#security-docs-network-__codelineno-10-46"></a><span class="w">            </span><span class="nt">value</span><span class="p">:</span>
<a id="__codelineno-10-47" name="__codelineno-10-47" href="#security-docs-network-__codelineno-10-47"></a><span class="w">              </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">cert-manager</span>
<a id="__codelineno-10-48" name="__codelineno-10-48" href="#security-docs-network-__codelineno-10-48"></a><span class="w">              </span><span class="nt">secret</span><span class="p">:</span>
<a id="__codelineno-10-49" name="__codelineno-10-49" href="#security-docs-network-__codelineno-10-49"></a><span class="w">                </span><span class="nt">secretName</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">istiod-tls</span>
<a id="__codelineno-10-50" name="__codelineno-10-50" href="#security-docs-network-__codelineno-10-50"></a><span class="w">          </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">spec.template.spec.volumes[7]</span>
<a id="__codelineno-10-51" name="__codelineno-10-51" href="#security-docs-network-__codelineno-10-51"></a><span class="w">            </span><span class="nt">value</span><span class="p">:</span>
<a id="__codelineno-10-52" name="__codelineno-10-52" href="#security-docs-network-__codelineno-10-52"></a><span class="w">              </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ca-root-cert</span>
<a id="__codelineno-10-53" name="__codelineno-10-53" href="#security-docs-network-__codelineno-10-53"></a><span class="w">              </span><span class="nt">configMap</span><span class="p">:</span>
<a id="__codelineno-10-54" name="__codelineno-10-54" href="#security-docs-network-__codelineno-10-54"></a><span class="w">                </span><span class="nt">defaultMode</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">420</span>
<a id="__codelineno-10-55" name="__codelineno-10-55" href="#security-docs-network-__codelineno-10-55"></a><span class="w">                </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">istio-ca-root-cert</span>
</code></pre></div>
<ol>
<li>Deploy the above custom resource you created.</li>
</ol>
<div class="highlight"><pre><span></span><code><a id="__codelineno-11-1" name="__codelineno-11-1" href="#security-docs-network-__codelineno-11-1"></a>istioctl<span class="w"> </span>operator<span class="w"> </span>init
<a id="__codelineno-11-2" name="__codelineno-11-2" href="#security-docs-network-__codelineno-11-2"></a>kubectl<span class="w"> </span>apply<span class="w"> </span>-f<span class="w"> </span>istio-custom-config.yaml
</code></pre></div>
<ol>
<li>Now you can deploy a workload to the mesh in your EKS cluster and <a href="https://istio.io/latest/docs/reference/config/security/peer_authentication/">enforce mTLS</a>. </li>
</ol>
<p><img alt="Istio certificate signing requests" src="../security/docs/images/istio-csr-requests.png" /></p>
<h4 id="security-docs-network-additional-resources_1">Additional Resources<a class="headerlink" href="#security-docs-network-additional-resources_1" title="Permanent link">&para;</a></h4>
<ul>
<li><a href="https://aws.amazon.com/blogs/security/tls-enabled-kubernetes-clusters-with-acm-private-ca-and-amazon-eks-2/">How to implement cert-manager and the ACM Private CA plugin to enable TLS in EKS</a>.</li>
<li><a href="https://aws.amazon.com/blogs/containers/setting-up-end-to-end-tls-encryption-on-amazon-eks-with-the-new-aws-load-balancer-controller/">Setting up end-to-end TLS encryption on Amazon EKS with the new AWS Load Balancer Controller and ACM Private CA</a>.</li>
<li><a href="https://github.com/cert-manager/aws-privateca-issuer">Private CA Kubernetes cert-manager plugin on GitHub</a>.</li>
<li><a href="https://docs.aws.amazon.com/acm-pca/latest/userguide/PcaKubernetes.html">Private CA Kubernetes cert-manager plugin user guide</a>.</li>
<li><a href="https://aws.amazon.com/blogs/security/how-to-use-aws-private-certificate-authority-short-lived-certificate-mode">How to use AWS Private Certificate Authority short-lived certificate mode</a></li>
</ul>
<h2 id="security-docs-network-tooling">Tooling<a class="headerlink" href="#security-docs-network-tooling" title="Permanent link">&para;</a></h2>
<ul>
<li><a href="https://itnext.io/verifying-service-mesh-tls-in-kubernetes-using-ksniff-and-wireshark-2e993b26bf95">Verifying Service Mesh TLS in Kubernetes, Using ksniff and Wireshark</a></li>
<li><a href="https://github.com/eldadru/ksniff">ksniff</a></li>
<li><a href="https://github.com/monzo/egress-operator">egress-operator</a> An operator and DNS plugin to control egress traffic from your cluster without protocol inspection</li>
</ul></section><section class="print-page" id="security-docs-data"><h1 id="security-docs-data-data-encryption-and-secrets-management">Data encryption and secrets management<a class="headerlink" href="#security-docs-data-data-encryption-and-secrets-management" title="Permanent link">&para;</a></h1>
<h2 id="security-docs-data-encryption-at-rest">Encryption at rest<a class="headerlink" href="#security-docs-data-encryption-at-rest" title="Permanent link">&para;</a></h2>
<p>There are three different AWS-native storage options you can use with Kubernetes: <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEBS.html">EBS</a>, <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEFS.html">EFS</a>, and <a href="https://docs.aws.amazon.com/fsx/latest/LustreGuide/what-is.html">FSx for Lustre</a>.  All three offer encryption at rest using a service managed key or a customer master key (CMK). For EBS you can use the in-tree storage driver or the <a href="https://github.com/kubernetes-sigs/aws-ebs-csi-driver">EBS CSI driver</a>.  Both include parameters for encrypting volumes and supplying a CMK.  For EFS, you can use the <a href="https://github.com/kubernetes-sigs/aws-efs-csi-driver">EFS CSI driver</a>, however, unlike EBS, the EFS CSI driver does not support dynamic provisioning.  If you want to use EFS with EKS, you will need to provision and configure at-rest encryption for the file system prior to creating a PV. For further information about EFS file encryption, please refer to <a href="https://docs.aws.amazon.com/efs/latest/ug/encryption-at-rest.html">Encrypting Data at Rest</a>. Besides offering at-rest encryption, EFS and FSx for Lustre include an option for encrypting data in transit.  FSx for Luster does this by default.  For EFS, you can add transport encryption by adding the <code>tls</code> parameter to <code>mountOptions</code> in your PV as in this example: </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#security-docs-data-__codelineno-0-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#security-docs-data-__codelineno-0-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">PersistentVolume</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#security-docs-data-__codelineno-0-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#security-docs-data-__codelineno-0-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">efs-pv</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#security-docs-data-__codelineno-0-5"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#security-docs-data-__codelineno-0-6"></a><span class="w">  </span><span class="nt">capacity</span><span class="p">:</span>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#security-docs-data-__codelineno-0-7"></a><span class="w">    </span><span class="nt">storage</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5Gi</span>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#security-docs-data-__codelineno-0-8"></a><span class="w">  </span><span class="nt">volumeMode</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Filesystem</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#security-docs-data-__codelineno-0-9"></a><span class="w">  </span><span class="nt">accessModes</span><span class="p">:</span>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#security-docs-data-__codelineno-0-10"></a><span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ReadWriteOnce</span>
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#security-docs-data-__codelineno-0-11"></a><span class="w">  </span><span class="nt">persistentVolumeReclaimPolicy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Retain</span>
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#security-docs-data-__codelineno-0-12"></a><span class="w">  </span><span class="nt">storageClassName</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">efs-sc</span>
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#security-docs-data-__codelineno-0-13"></a><span class="w">  </span><span class="nt">mountOptions</span><span class="p">:</span>
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#security-docs-data-__codelineno-0-14"></a><span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">tls</span>
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#security-docs-data-__codelineno-0-15"></a><span class="w">  </span><span class="nt">csi</span><span class="p">:</span>
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#security-docs-data-__codelineno-0-16"></a><span class="w">    </span><span class="nt">driver</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">efs.csi.aws.com</span>
<a id="__codelineno-0-17" name="__codelineno-0-17" href="#security-docs-data-__codelineno-0-17"></a><span class="w">    </span><span class="nt">volumeHandle</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">&lt;file_system_id&gt;</span>
</code></pre></div>
<p>The <a href="https://github.com/kubernetes-sigs/aws-fsx-csi-driver">FSx CSI driver</a> supports dynamic provisioning of Lustre file systems.  It encrypts data with a service managed key by default, although there is an option to provide your own CMK as in this example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#security-docs-data-__codelineno-1-1"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">StorageClass</span>
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#security-docs-data-__codelineno-1-2"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">storage.k8s.io/v1</span>
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#security-docs-data-__codelineno-1-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#security-docs-data-__codelineno-1-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">fsx-sc</span>
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#security-docs-data-__codelineno-1-5"></a><span class="nt">provisioner</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">fsx.csi.aws.com</span>
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#security-docs-data-__codelineno-1-6"></a><span class="nt">parameters</span><span class="p">:</span>
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#security-docs-data-__codelineno-1-7"></a><span class="w">  </span><span class="nt">subnetId</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">subnet-056da83524edbe641</span>
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#security-docs-data-__codelineno-1-8"></a><span class="w">  </span><span class="nt">securityGroupIds</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">sg-086f61ea73388fb6b</span>
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#security-docs-data-__codelineno-1-9"></a><span class="w">  </span><span class="nt">deploymentType</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">PERSISTENT_1</span>
<a id="__codelineno-1-10" name="__codelineno-1-10" href="#security-docs-data-__codelineno-1-10"></a><span class="w">  </span><span class="nt">kmsKeyId</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">&lt;kms_arn&gt;</span>
</code></pre></div>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>As of May 28, 2020 all data written to the ephemeral volume in EKS Fargate pods is encrypted by default using an industry-standard AES-256 cryptographic algorithm. No modifications to your application are necessary as encryption and decryption are handled seamlessly by the service. </p>
</div>
<h2 id="security-docs-data-recommendations">Recommendations<a class="headerlink" href="#security-docs-data-recommendations" title="Permanent link">&para;</a></h2>
<h3 id="security-docs-data-encrypt-data-at-rest">Encrypt data at rest<a class="headerlink" href="#security-docs-data-encrypt-data-at-rest" title="Permanent link">&para;</a></h3>
<p>Encrypting data at rest is considered a best practice.  If you're unsure whether encryption is necessary, encrypt your data. </p>
<h3 id="security-docs-data-rotate-your-cmks-periodically">Rotate your CMKs periodically<a class="headerlink" href="#security-docs-data-rotate-your-cmks-periodically" title="Permanent link">&para;</a></h3>
<p>Configure KMS to automatically rotate your CMKs.  This will rotate your keys once a year while saving old keys indefinitely so that your data can still be decrypted.  For additional information see <a href="https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html">Rotating customer master keys</a></p>
<h3 id="security-docs-data-use-efs-access-points-to-simplify-access-to-shared-datasets">Use EFS access points to simplify access to shared datasets<a class="headerlink" href="#security-docs-data-use-efs-access-points-to-simplify-access-to-shared-datasets" title="Permanent link">&para;</a></h3>
<p>If you have shared datasets with different POSIX file permissions or want to restrict access to part of the shared file system by creating different mount points, consider using EFS access points. To learn more about working with access points, see <a href="https://docs.aws.amazon.com/efs/latest/ug/efs-access-points.html">https://docs.aws.amazon.com/efs/latest/ug/efs-access-points.html</a>. Today, if you want to use an access point (AP) you'll need to reference the AP in the PV's <code>volumeHandle</code> parameter.</p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>As of March 23, 2021 the EFS CSI driver supports dynamic provisioning of EFS Access Points. Access points are application-specific entry points into an EFS file system that make it easier to share a file system between multiple pods. Each EFS file system can have up to 120 PVs. See <a href="https://aws.amazon.com/blogs/containers/introducing-efs-csi-dynamic-provisioning/">Introducing Amazon EFS CSI dynamic provisioning</a> for additional information. </p>
</div>
<h2 id="security-docs-data-secrets-management">Secrets management<a class="headerlink" href="#security-docs-data-secrets-management" title="Permanent link">&para;</a></h2>
<p>Kubernetes secrets are used to store sensitive information, such as user certificates, passwords, or API keys. They are persisted in etcd as base64 encoded strings.  On EKS, the EBS volumes for etcd nodes are encrypted with <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html">EBS encryption</a>.  A pod can retrieve a Kubernetes secrets objects by referencing the secret in the <code>podSpec</code>.  These secrets can either be mapped to an environment variable or mounted as volume. For additional information on creating secrets, see <a href="https://kubernetes.io/docs/concepts/configuration/secret/">https://kubernetes.io/docs/concepts/configuration/secret/</a>. </p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>Secrets in a particular namespace can be referenced by all pods in the secret's namespace.</p>
</div>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>The node authorizer allows the Kubelet to read all of the secrets mounted to the node. </p>
</div>
<h2 id="security-docs-data-recommendations_1">Recommendations<a class="headerlink" href="#security-docs-data-recommendations_1" title="Permanent link">&para;</a></h2>
<h3 id="security-docs-data-use-aws-kms-for-envelope-encryption-of-kubernetes-secrets">Use AWS KMS for envelope encryption of Kubernetes secrets<a class="headerlink" href="#security-docs-data-use-aws-kms-for-envelope-encryption-of-kubernetes-secrets" title="Permanent link">&para;</a></h3>
<p>This allows you to encrypt your secrets with a unique data encryption key (DEK). The DEK is then encrypted using a key encryption key (KEK) from AWS KMS which can be automatically rotated on a recurring schedule. With the KMS plugin for Kubernetes, all Kubernetes secrets are stored in etcd in ciphertext instead of plain text and can only be decrypted by the Kubernetes API server. 
For additional details, see <a href="https://aws.amazon.com/blogs/containers/using-eks-encryption-provider-support-for-defense-in-depth/">using EKS encryption provider support for defense in depth</a></p>
<h3 id="security-docs-data-audit-the-use-of-kubernetes-secrets">Audit the use of Kubernetes Secrets<a class="headerlink" href="#security-docs-data-audit-the-use-of-kubernetes-secrets" title="Permanent link">&para;</a></h3>
<p>On EKS, turn on audit logging and create a CloudWatch metrics filter and alarm to alert you when a secret is used (optional). The following is an example of a metrics filter for the Kubernetes audit log, <code>{($.verb="get") &amp;&amp; ($.objectRef.resource="secret")}</code>.  You can also use the following queries with CloudWatch Log Insights: 
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#security-docs-data-__codelineno-2-1"></a>fields @timestamp, @message
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#security-docs-data-__codelineno-2-2"></a>| sort @timestamp desc
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#security-docs-data-__codelineno-2-3"></a>| limit 100
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#security-docs-data-__codelineno-2-4"></a>| stats count(*) by objectRef.name as secret
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#security-docs-data-__codelineno-2-5"></a>| filter verb=&quot;get&quot; and objectRef.resource=&quot;secrets&quot;
</code></pre></div>
The above query will display the number of times a secret has been accessed within a specific timeframe. 
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#security-docs-data-__codelineno-3-1"></a>fields @timestamp, @message
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#security-docs-data-__codelineno-3-2"></a>| sort @timestamp desc
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#security-docs-data-__codelineno-3-3"></a>| limit 100
<a id="__codelineno-3-4" name="__codelineno-3-4" href="#security-docs-data-__codelineno-3-4"></a>| filter verb=&quot;get&quot; and objectRef.resource=&quot;secrets&quot;
<a id="__codelineno-3-5" name="__codelineno-3-5" href="#security-docs-data-__codelineno-3-5"></a>| display objectRef.namespace, objectRef.name, user.username, responseStatus.code
</code></pre></div>
This query will display the secret, along with the namespace and username of the user who attempted to access the secret and the response code. </p>
<h3 id="security-docs-data-rotate-your-secrets-periodically">Rotate your secrets periodically<a class="headerlink" href="#security-docs-data-rotate-your-secrets-periodically" title="Permanent link">&para;</a></h3>
<p>Kubernetes doesn't automatically rotate secrets.  If you have to rotate secrets, consider using an external secret store, e.g. Vault or AWS Secrets Manager. </p>
<h3 id="security-docs-data-use-separate-namespaces-as-a-way-to-isolate-secrets-from-different-applications">Use separate namespaces as a way to isolate secrets from different applications<a class="headerlink" href="#security-docs-data-use-separate-namespaces-as-a-way-to-isolate-secrets-from-different-applications" title="Permanent link">&para;</a></h3>
<p>If you have secrets that cannot be shared between applications in a namespace, create a separate namespace for those applications.</p>
<h3 id="security-docs-data-use-volume-mounts-instead-of-environment-variables">Use volume mounts instead of environment variables<a class="headerlink" href="#security-docs-data-use-volume-mounts-instead-of-environment-variables" title="Permanent link">&para;</a></h3>
<p>The values of environment variables can unintentionally appear in logs. Secrets mounted as volumes are instantiated as tmpfs volumes (a RAM backed file system) that are automatically removed from the node when the pod is deleted. </p>
<h3 id="security-docs-data-use-an-external-secrets-provider">Use an external secrets provider<a class="headerlink" href="#security-docs-data-use-an-external-secrets-provider" title="Permanent link">&para;</a></h3>
<p>There are several viable alternatives to using Kubernetes secrets, including <a href="https://aws.amazon.com/secrets-manager/">AWS Secrets Manager</a> and Hashicorp's <a href="https://www.hashicorp.com/blog/injecting-vault-secrets-into-kubernetes-pods-via-a-sidecar/">Vault</a>. These services offer features such as fine grained access controls, strong encryption, and automatic rotation of secrets that are not available with Kubernetes Secrets. Bitnami's <a href="https://github.com/bitnami-labs/sealed-secrets">Sealed Secrets</a> is another approach that uses asymmetric encryption to create "sealed secrets". A public key is used to encrypt the secret while the private key used to decrypt the secret is kept within the cluster, allowing you to safely store sealed secrets in source control systems like Git. See <a href="https://aws.amazon.com/blogs/opensource/managing-secrets-deployment-in-kubernetes-using-sealed-secrets/">Managing secrets deployment in Kubernetes using Sealed Secrets</a> for further information. </p>
<p>As the use of external secrets stores has grown, so has need for integrating them with Kubernetes. The <a href="https://github.com/kubernetes-sigs/secrets-store-csi-driver">Secret Store CSI Driver</a> is a community project that uses the CSI driver model to fetch secrets from external secret stores. Currently, the Driver has support for <a href="https://github.com/aws/secrets-store-csi-driver-provider-aws">AWS Secrets Manager</a>, Azure, Vault, and GCP. The AWS provider supports both AWS Secrets Manager <strong>and</strong> AWS Parameter Store. It can also be configured to rotate secrets when they expire and can synchronize AWS Secrets Manager secrets to Kubernetes Secrets. Synchronization of secrets can be useful when you need to reference a secret as an environment variable instead of reading them from a volume. </p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When the the secret store CSI driver has to fetch a secret, it assumes the IRSA role assigned to the pod that references a secret. The code for this operation can be found <a href="https://github.com/aws/secrets-store-csi-driver-provider-aws/blob/main/auth/auth.go">here</a>.</p>
</div>
<p>For additional information about the AWS Secrets &amp; Configuration Provider (ASCP) refer to the following resources:</p>
<ul>
<li><a href="https://aws.amazon.com/blogs/security/how-to-use-aws-secrets-configuration-provider-with-kubernetes-secrets-store-csi-driver/">How to use AWS Secrets Configuration Provider with Kubernetes Secret Store CSI Driver</a></li>
<li><a href="https://docs.aws.amazon.com/secretsmanager/latest/userguide/integrating_csi_driver.html">Integrating Secrets Manager secrets with Kubernetes Secrets Store CSI Driver</a></li>
</ul>
<p><a href="https://github.com/external-secrets/external-secrets">external-secrets</a> is yet another way to use an external secret store with Kubernetes. Like the CSI Driver, external-secrets works against a variety of different backends, including AWS Secrets Manager. The difference is, rather than retrieving secrets from the external secret store, external-secrets copies secrets from these backends to Kubernetes as Secrets.  This lets you manage secrets using your preferred secret store and interact with secrets in a Kubernetes-native way. </p></section><section class="print-page" id="security-docs-runtime"><h1 id="security-docs-runtime-runtime-security">Runtime security<a class="headerlink" href="#security-docs-runtime-runtime-security" title="Permanent link">&para;</a></h1>
<p>Runtime security provides active protection for your containers while they're running.  The idea is to detect and/or prevent malicious activity from occurring inside the container. This can be achieved with a number of mechanisms in the Linux kernel or kernel extensions that are integrated with Kubernetes, such as Linux capabilities, secure computing (seccomp), AppArmor, or SELinux. There are also options like Amazon GuardDuty and third party tools that can assist with establishing baselines and detecting anomalous activity with less manual configuration of Linux kernel mechanisms.</p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Kubernetes does not currently provide any native mechanisms for loading seccomp, AppArmor, or SELinux profiles onto Nodes.  They either have to be loaded manually or installed onto Nodes when they are bootstrapped. This has to be done prior to referencing them in your Pods because the scheduler is unaware of which nodes have profiles. See below how tools like Security Profiles Operator can help automate provisioning of profiles onto nodes.</p>
</div>
<h2 id="security-docs-runtime-security-contexts-and-built-in-kubernetes-controls">Security contexts and built-in Kubernetes controls<a class="headerlink" href="#security-docs-runtime-security-contexts-and-built-in-kubernetes-controls" title="Permanent link">&para;</a></h2>
<p>Many Linux runtime security mechanisms are tightly integrated with Kubernetes and can be configured through Kubernetes <a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/">security contexts</a>. One such option is the <code>privileged</code> flag, which is <code>false</code> by default and if enabled is essentially equivalent to root on the host. It is nearly always inappropriate to enable privileged mode in production workloads, but there are many more controls that can provide more granular privileges to containers as appropriate.</p>
<h3 id="security-docs-runtime-linux-capabilities">Linux capabilities<a class="headerlink" href="#security-docs-runtime-linux-capabilities" title="Permanent link">&para;</a></h3>
<p>Linux capabilities allow you to grant certain capabilities to a Pod or container without providing all the abilities of the root user. Examples include <code>CAP_NET_ADMIN</code>, which allows configuring network interfaces or firewalls, or <code>CAP_SYS_TIME</code>, which allows manipulation of the system clock.</p>
<h3 id="security-docs-runtime-seccomp">Seccomp<a class="headerlink" href="#security-docs-runtime-seccomp" title="Permanent link">&para;</a></h3>
<p>With secure computing (seccomp) you can prevent a containerized application from making certain syscalls to the underlying host operating system's kernel. While the Linux operating system has a few hundred system calls, the lion's share of them are not necessary for running containers. By restricting what syscalls can be made by a container, you can effectively decrease your application's attack surface.</p>
<p>Seccomp works by intercepting syscalls and only allowing those that have been allowlisted to pass through. Docker has a <a href="https://github.com/moby/moby/blob/master/profiles/seccomp/default.json">default</a> seccomp profile which is suitable for a majority of general purpose workloads, and other container runtimes like containerd provide comparable defaults. You can configure your container or Pod to use the container runtime's default seccomp profile by adding the following to the <code>securityContext</code> section of the Pod spec: </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#security-docs-runtime-__codelineno-0-1"></a>securityContext:
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#security-docs-runtime-__codelineno-0-2"></a>  seccompProfile:
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#security-docs-runtime-__codelineno-0-3"></a>    type: RuntimeDefault
</code></pre></div>
<p>As of 1.22 (in alpha, stable as of 1.27), the above <code>RuntimeDefault</code> can be used for all Pods on a Node using a <a href="https://kubernetes.io/docs/tutorials/security/seccomp/#enable-the-use-of-runtimedefault-as-the-default-seccomp-profile-for-all-workloads">single kubelet flag</a>, <code>--seccomp-default</code>. Then the profile specified in <code>securityContext</code> is only needed for other profiles.</p>
<p>It's also possible to create your own profiles for things that require additional privileges. This can be very tedious to do manually, but there are tools like <a href="https://github.com/inspektor-gadget/inspektor-gadget">Inspektor Gadget</a> (also recommended in the <a href="#security-docs-network">network security section</a> for generating network policies) and <a href="https://github.com/inspektor-gadget/inspektor-gadget">Security Profiles Operator</a> that support using tools like eBPF or logs to record baseline privilege requirements as seccomp profiles. Security Profiles Operator further allows automating the deployment of recorded profiles to nodes for use by Pods and containers.</p>
<h3 id="security-docs-runtime-apparmor-and-selinux">AppArmor and SELinux<a class="headerlink" href="#security-docs-runtime-apparmor-and-selinux" title="Permanent link">&para;</a></h3>
<p>AppArmor and SELinux are known as <a href="https://en.wikipedia.org/wiki/Mandatory_access_control">mandatory access control or MAC systems</a>. They are similar in concept to seccomp but with different APIs and abilities, allowing access control for e.g. specific filesystem paths or network ports. Support for these tools depends on the Linux distribution, with Debian/Ubuntu supporting AppArmor and RHEL/CentOS/Bottlerocket/Amazon Linux 2023 supporting SELinux. Also see the <a href="#security-docs-hosts-run-selinux">infrastructure security section</a> for further discussion of SELinux.</p>
<p>Both AppArmor and SELinux are integrated with Kubernetes, but as of Kubernetes 1.28 AppArmor profiles must be specified via <a href="https://kubernetes.io/docs/tutorials/security/apparmor/#securing-a-pod">annotations</a> while SELinux labels can be set through the <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/#selinuxoptions-v1-core">SELinuxOptions</a> field on the security context directly.</p>
<p>As with seccomp profiles, the Security Profiles Operator mentioned above can assist with deploying profiles onto nodes in the cluster. (In the future, the project also aims to generate profiles for AppArmor and SELinux as it does for seccomp.)</p>
<h2 id="security-docs-runtime-recommendations">Recommendations<a class="headerlink" href="#security-docs-runtime-recommendations" title="Permanent link">&para;</a></h2>
<h3 id="security-docs-runtime-use-amazon-guardduty-for-runtime-monitoring-and-detecting-threats-to-your-eks-environments">Use Amazon GuardDuty for runtime monitoring and detecting threats to your EKS environments<a class="headerlink" href="#security-docs-runtime-use-amazon-guardduty-for-runtime-monitoring-and-detecting-threats-to-your-eks-environments" title="Permanent link">&para;</a></h3>
<p>If you do not currently have a solution for continuously monitoring EKS runtimes and analyzing EKS audit logs, and scanning for malware and other suspicious activity, Amazon strongly recommends the use of <a href="https://aws.amazon.com/guardduty/">Amazon GuardDuty</a> for customers who want a simple, fast, secure, scalable, and cost-effective one-click way to protect their AWS environments. Amazon GuardDuty is a security monitoring service that analyzes and processes foundational data sources, such as AWS CloudTrail management events, AWS CloudTrail event logs, VPC flow logs (from Amazon EC2 instances), Kubernetes audit logs, and DNS logs. It also includes EKS runtime monitoring. It uses continuously updated threat intelligence feeds, such as lists of malicious IP addresses and domains, and machine learning to identify unexpected, potentially unauthorized, and malicious activity within your AWS environment. This can include issues like escalation of privileges, use of exposed credentials, or communication with malicious IP addresses, domains, presence of malware on your Amazon EC2 instances and EKS container workloads, or discovery of suspicious API activity. GuardDuty informs you of the status of your AWS environment by producing security findings that you can view in the GuardDuty console or through Amazon EventBridge. GuardDuty also provides support for you to export your findings to an Amazon Simple Storage Service (S3) bucket, and integrate with other services such as AWS Security Hub and Detective.</p>
<p>Watch this AWS Online Tech Talk <a href="https://www.youtube.com/watch?v=oNHGRRroJuE">"Enhanced threat detection for Amazon EKS with Amazon GuardDuty - AWS Online Tech Talks"</a> to see how to enable these additional EKS security features step-by-step in minutes. </p>
<h3 id="security-docs-runtime-optionally-use-a-3rd-party-solution-for-runtime-monitoring">Optionally: Use a 3rd party solution for runtime monitoring<a class="headerlink" href="#security-docs-runtime-optionally-use-a-3rd-party-solution-for-runtime-monitoring" title="Permanent link">&para;</a></h3>
<p>Creating and managing seccomp and Apparmor profiles can be difficult if you're not familiar with Linux security.  If you don't have the time to become proficient, consider using a 3rd party commercial solution.  A lot of them have moved beyond static profiles like Apparmor and seccomp and have begun using machine learning to block or alert on suspicious activity. A handful of these solutions can be found below in the <a href="#security-docs-runtime-#Tools">tools</a> section. Additional options can be found on the <a href="https://aws.amazon.com/marketplace/features/containers">AWS Marketplace for Containers</a>.</p>
<h3 id="security-docs-runtime-consider-adddropping-linux-capabilities-before-writing-seccomp-policies">Consider add/dropping Linux capabilities before writing seccomp policies<a class="headerlink" href="#security-docs-runtime-consider-adddropping-linux-capabilities-before-writing-seccomp-policies" title="Permanent link">&para;</a></h3>
<p>Capabilities involve various checks in kernel functions reachable by syscalls. If the check fails, the syscall typically returns an error. The check can be done either right at the beginning of a specific syscall, or deeper in the kernel in areas that might be reachable through multiple different syscalls (such as writing to a specific privileged file).  Seccomp, on the other hand, is a syscall filter which is applied to all syscalls before they are run. A process can set up a filter which allows them to revoke their right to run certain syscalls, or specific arguments for certain syscalls. </p>
<p>Before using seccomp, consider whether adding/removing Linux capabilities gives you the control you need. See <a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container">https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container</a> for further information. </p>
<h3 id="security-docs-runtime-see-whether-you-can-accomplish-your-aims-by-using-pod-security-policies-psps">See whether you can accomplish your aims by using Pod Security Policies (PSPs)<a class="headerlink" href="#security-docs-runtime-see-whether-you-can-accomplish-your-aims-by-using-pod-security-policies-psps" title="Permanent link">&para;</a></h3>
<p>Pod Security Policies offer a lot of different ways to improve your security posture without introducing undue complexity. Explore the options available in PSPs before venturing into building seccomp and Apparmor profiles.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>As of Kubernetes 1.25, PSPs have been removed and replaced with the <a href="https://kubernetes.io/docs/concepts/security/pod-security-admission/">Pod Security Admission</a> controller. Third-party alternatives which exist include OPA/Gatekeeper and Kyverno. A collection of Gatekeeper constraints and constraint templates for implementing policies commonly found in PSPs can be pulled from the <a href="https://github.com/open-policy-agent/gatekeeper-library/tree/master/library/pod-security-policy">Gatekeeper library</a> repository on GitHub. And many replacements for PSPs can be found in the <a href="https://main.kyverno.io/policies/">Kyverno policy library</a> including the full collection of <a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/">Pod Security Standards</a>.</p>
</div>
<h2 id="security-docs-runtime-additional-resources">Additional Resources<a class="headerlink" href="#security-docs-runtime-additional-resources" title="Permanent link">&para;</a></h2>
<ul>
<li><a href="https://itnext.io/seccomp-in-kubernetes-part-i-7-things-you-should-know-before-you-even-start-97502ad6b6d6">7 things you should know before you start</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/tree/master/test/images/apparmor-loader">AppArmor Loader</a></li>
<li><a href="https://kubernetes.io/docs/tutorials/clusters/apparmor/#setting-up-nodes-with-profiles">Setting up nodes with profiles</a></li>
<li><a href="https://github.com/kubernetes-sigs/security-profiles-operator">Security Profiles Operator</a> is a Kubernetes enhancement which aims to make it easier for users to use SELinux, seccomp and AppArmor in Kubernetes clusters. It provides capabilities for both generating profiles from running workloads and loading profiles onto Kubernetes nodes for use in Pods.
<a href="https://github.com/inspektor-gadget/inspektor-gadget">Inspektor Gadget</a> allows inspecting, tracing, and profiling many aspects of runtime behavior on Kubernetes, including assisting in the generation of seccomp profiles.</li>
</ul>
<h2 id="security-docs-runtime-tools">Tools<a class="headerlink" href="#security-docs-runtime-tools" title="Permanent link">&para;</a></h2>
<ul>
<li><a href="https://www.aquasec.com/products/aqua-cloud-native-security-platform/">Aqua</a></li>
<li><a href="https://www.qualys.com/apps/container-security/">Qualys</a></li>
<li><a href="https://www.stackrox.com/use-cases/threat-detection/">Stackrox</a></li>
<li><a href="https://sysdig.com/products/kubernetes-security/">Sysdig Secure</a></li>
<li><a href="https://docs.paloaltonetworks.com/cn-series">Prisma</a></li>
</ul></section><section class="print-page" id="security-docs-hosts"><h1 id="security-docs-hosts-protecting-the-infrastructure-hosts">Protecting the infrastructure (hosts)<a class="headerlink" href="#security-docs-hosts-protecting-the-infrastructure-hosts" title="Permanent link">&para;</a></h1>
<p>Inasmuch as it's important to secure your container images, it's equally important to safeguard the infrastructure that runs them. This section explores different ways to mitigate risks from attacks launched directly against the host.  These guidelines should be used in conjunction with those outlined in the <a href="#security-docs-runtime">Runtime Security</a> section.</p>
<h2 id="security-docs-hosts-recommendations">Recommendations<a class="headerlink" href="#security-docs-hosts-recommendations" title="Permanent link">&para;</a></h2>
<h3 id="security-docs-hosts-use-an-os-optimized-for-running-containers">Use an OS optimized for running containers<a class="headerlink" href="#security-docs-hosts-use-an-os-optimized-for-running-containers" title="Permanent link">&para;</a></h3>
<p>Consider using Flatcar Linux, Project Atomic, RancherOS, and <a href="https://github.com/bottlerocket-os/bottlerocket/">Bottlerocket</a>, a special purpose OS from AWS designed for running Linux containers.  It includes a reduced attack surface, a disk image that is verified on boot, and enforced permission boundaries using SELinux.</p>
<p>Alternately, use the <a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-amis.html">EKS optimized AMI</a> for your Kubernetes worker nodes. The EKS optimized AMI is released regularly and contains a minimal set of OS packages and binaries necessary to run your containerized workloads.</p>
<p>Please refer <a href="https://github.com/aws-samples/amazon-eks-ami-rhel">Amazon EKS AMI RHEL Build Specification</a> for a sample configuration script which can be used for building a custom Amazon EKS AMI running on Red Hat Enterprise Linux using Hashicorp Packer. This script can be further leveraged to build STIG compliant EKS custom AMIs.</p>
<h3 id="security-docs-hosts-keep-your-worker-node-os-updated">Keep your worker node OS updated<a class="headerlink" href="#security-docs-hosts-keep-your-worker-node-os-updated" title="Permanent link">&para;</a></h3>
<p>Regardless of whether you use a container-optimized host OS like Bottlerocket or a larger, but still minimalist, Amazon Machine Image like the EKS optimized AMIs, it is best practice to keep these host OS images up to date with the latest security patches.</p>
<p>For the EKS optimized AMIs, regularly check the <a href="https://github.com/awslabs/amazon-eks-ami/blob/master/CHANGELOG.md">CHANGELOG</a> and/or <a href="https://github.com/awslabs/amazon-eks-ami/releases">release notes channel</a> and automate the rollout of updated worker node images into your cluster.</p>
<h3 id="security-docs-hosts-treat-your-infrastructure-as-immutable-and-automate-the-replacement-of-your-worker-nodes">Treat your infrastructure as immutable and automate the replacement of your worker nodes<a class="headerlink" href="#security-docs-hosts-treat-your-infrastructure-as-immutable-and-automate-the-replacement-of-your-worker-nodes" title="Permanent link">&para;</a></h3>
<p>Rather than performing in-place upgrades, replace your workers when a new patch or update becomes available. This can be approached a couple of ways. You can either add instances to an existing autoscaling group using the latest AMI as you sequentially cordon and drain nodes until all of the nodes in the group have been replaced with the latest AMI.  Alternatively, you can add instances to a new node group while you sequentially cordon and drain nodes from the old node group until all of the nodes have been replaced.  EKS <a href="https://docs.aws.amazon.com/eks/latest/userguide/managed-node-groups.html">managed node groups</a> uses the first approach and will display a message in the console to upgrade your workers when a new AMI becomes available. <code>eksctl</code> also has a mechanism for creating node groups with the latest AMI and for gracefully cordoning and draining pods from nodes groups before the instances are terminated. If you decide to use a different method for replacing your worker nodes, it is strongly recommended that you automate the process to minimize human oversight as you will likely need to replace workers regularly as new updates/patches are released and when the control plane is upgraded. </p>
<p>With EKS Fargate, AWS will automatically update the underlying infrastructure as updates become available.  Oftentimes this can be done seamlessly, but there may be times when an update will cause your pod to be rescheduled.  Hence, we recommend that you create deployments with multiple replicas when running your application as a Fargate pod.</p>
<h3 id="security-docs-hosts-periodically-run-kube-bench-to-verify-compliance-with-cis-benchmarks-for-kubernetes">Periodically run kube-bench to verify compliance with <a href="https://www.cisecurity.org/benchmark/kubernetes/">CIS benchmarks for Kubernetes</a><a class="headerlink" href="#security-docs-hosts-periodically-run-kube-bench-to-verify-compliance-with-cis-benchmarks-for-kubernetes" title="Permanent link">&para;</a></h3>
<p>kube-bench is an open source project from Aqua that evaluates your cluster against the CIS benchmarks for Kubernetes. The benchmark describes the best practices for securing unmanaged Kubernetes clusters. The CIS Kubernetes Benchmark encompasses the control plane and the data plane. Since Amazon EKS provides a fully managed control plane, not all of the recommendations from the CIS Kubernetes Benchmark are applicable. To ensure this scope reflects how Amazon EKS is implemented, AWS created the <em>CIS Amazon EKS Benchmark</em>. The EKS benchmark inherits from CIS Kubernetes Benchmark with additional inputs from the community with specific configuration considerations for EKS clusters.</p>
<p>When running <a href="https://github.com/aquasecurity/kube-bench">kube-bench</a> against an EKS cluster, follow <a href="https://github.com/aquasecurity/kube-bench/blob/main/docs/running.md#running-cis-benchmark-in-an-eks-cluster">these instructions</a> from Aqua Security. For further information see <a href="https://aws.amazon.com/blogs/containers/introducing-cis-amazon-eks-benchmark/">Introducing The CIS Amazon EKS Benchmark</a>. </p>
<h3 id="security-docs-hosts-minimize-access-to-worker-nodes">Minimize access to worker nodes<a class="headerlink" href="#security-docs-hosts-minimize-access-to-worker-nodes" title="Permanent link">&para;</a></h3>
<p>Instead of enabling SSH access, use <a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html">SSM Session Manager</a> when you need to remote into a host.  Unlike SSH keys which can be lost, copied, or shared, Session Manager allows you to control access to EC2 instances using IAM.  Moreover, it provides an audit trail and log of the commands that were run on the instance.</p>
<p>As of August 19th, 2020 Managed Node Groups support custom AMIs and EC2 Launch Templates.  This allows you to embed the SSM agent into the AMI or install it as the worker node is being bootstrapped. If you rather not modify the Optimized AMI or the ASG's launch template, you can install the SSM agent with a DaemonSet as in this example, https://github.com/aws-samples/ssm-agent-daemonset-installer. </p>
<h4 id="security-docs-hosts-minimal-iam-policy-for-ssm-based-ssh-access">Minimal IAM policy for SSM based SSH Access<a class="headerlink" href="#security-docs-hosts-minimal-iam-policy-for-ssm-based-ssh-access" title="Permanent link">&para;</a></h4>
<p>The <code>AmazonSSMManagedInstanceCore</code> AWS managed policy contains a number of permissions that are not required for SSM Session Manager / SSM RunCommand if you're just looking to avoid SSH access. Of concern specifically is the
<code>*</code> permissions for <code>ssm:GetParameter(s)</code> which would allow for the role to access all parameters in Parameter Store (including SecureStrings with the AWS managed KMS key configured).</p>
<p>The following IAM policy contains the minimal set of permissions to enable node access via SSM Systems Manager.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#security-docs-hosts-__codelineno-0-1"></a><span class="p">{</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#security-docs-hosts-__codelineno-0-2"></a><span class="w">  </span><span class="nt">&quot;Version&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;2012-10-17&quot;</span><span class="p">,</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#security-docs-hosts-__codelineno-0-3"></a><span class="w">  </span><span class="nt">&quot;Statement&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#security-docs-hosts-__codelineno-0-4"></a><span class="w">    </span><span class="p">{</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#security-docs-hosts-__codelineno-0-5"></a><span class="w">      </span><span class="nt">&quot;Sid&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;EnableAccessViaSSMSessionManager&quot;</span><span class="p">,</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#security-docs-hosts-__codelineno-0-6"></a><span class="w">      </span><span class="nt">&quot;Effect&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Allow&quot;</span><span class="p">,</span>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#security-docs-hosts-__codelineno-0-7"></a><span class="w">      </span><span class="nt">&quot;Action&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#security-docs-hosts-__codelineno-0-8"></a><span class="w">        </span><span class="s2">&quot;ssmmessages:OpenDataChannel&quot;</span><span class="p">,</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#security-docs-hosts-__codelineno-0-9"></a><span class="w">        </span><span class="s2">&quot;ssmmessages:OpenControlChannel&quot;</span><span class="p">,</span>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#security-docs-hosts-__codelineno-0-10"></a><span class="w">        </span><span class="s2">&quot;ssmmessages:CreateDataChannel&quot;</span><span class="p">,</span>
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#security-docs-hosts-__codelineno-0-11"></a><span class="w">        </span><span class="s2">&quot;ssmmessages:CreateControlChannel&quot;</span><span class="p">,</span>
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#security-docs-hosts-__codelineno-0-12"></a><span class="w">        </span><span class="s2">&quot;ssm:UpdateInstanceInformation&quot;</span>
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#security-docs-hosts-__codelineno-0-13"></a><span class="w">      </span><span class="p">],</span>
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#security-docs-hosts-__codelineno-0-14"></a><span class="w">      </span><span class="nt">&quot;Resource&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;*&quot;</span>
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#security-docs-hosts-__codelineno-0-15"></a><span class="w">    </span><span class="p">},</span>
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#security-docs-hosts-__codelineno-0-16"></a><span class="w">    </span><span class="p">{</span>
<a id="__codelineno-0-17" name="__codelineno-0-17" href="#security-docs-hosts-__codelineno-0-17"></a><span class="w">      </span><span class="nt">&quot;Sid&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;EnableSSMRunCommand&quot;</span><span class="p">,</span>
<a id="__codelineno-0-18" name="__codelineno-0-18" href="#security-docs-hosts-__codelineno-0-18"></a><span class="w">      </span><span class="nt">&quot;Effect&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Allow&quot;</span><span class="p">,</span>
<a id="__codelineno-0-19" name="__codelineno-0-19" href="#security-docs-hosts-__codelineno-0-19"></a><span class="w">      </span><span class="nt">&quot;Action&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<a id="__codelineno-0-20" name="__codelineno-0-20" href="#security-docs-hosts-__codelineno-0-20"></a><span class="w">        </span><span class="s2">&quot;ssm:UpdateInstanceInformation&quot;</span><span class="p">,</span>
<a id="__codelineno-0-21" name="__codelineno-0-21" href="#security-docs-hosts-__codelineno-0-21"></a><span class="w">        </span><span class="s2">&quot;ec2messages:SendReply&quot;</span><span class="p">,</span>
<a id="__codelineno-0-22" name="__codelineno-0-22" href="#security-docs-hosts-__codelineno-0-22"></a><span class="w">        </span><span class="s2">&quot;ec2messages:GetMessages&quot;</span><span class="p">,</span>
<a id="__codelineno-0-23" name="__codelineno-0-23" href="#security-docs-hosts-__codelineno-0-23"></a><span class="w">        </span><span class="s2">&quot;ec2messages:GetEndpoint&quot;</span><span class="p">,</span>
<a id="__codelineno-0-24" name="__codelineno-0-24" href="#security-docs-hosts-__codelineno-0-24"></a><span class="w">        </span><span class="s2">&quot;ec2messages:FailMessage&quot;</span><span class="p">,</span>
<a id="__codelineno-0-25" name="__codelineno-0-25" href="#security-docs-hosts-__codelineno-0-25"></a><span class="w">        </span><span class="s2">&quot;ec2messages:DeleteMessage&quot;</span><span class="p">,</span>
<a id="__codelineno-0-26" name="__codelineno-0-26" href="#security-docs-hosts-__codelineno-0-26"></a><span class="w">        </span><span class="s2">&quot;ec2messages:AcknowledgeMessage&quot;</span>
<a id="__codelineno-0-27" name="__codelineno-0-27" href="#security-docs-hosts-__codelineno-0-27"></a><span class="w">      </span><span class="p">],</span>
<a id="__codelineno-0-28" name="__codelineno-0-28" href="#security-docs-hosts-__codelineno-0-28"></a><span class="w">      </span><span class="nt">&quot;Resource&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;*&quot;</span>
<a id="__codelineno-0-29" name="__codelineno-0-29" href="#security-docs-hosts-__codelineno-0-29"></a><span class="w">    </span><span class="p">}</span>
<a id="__codelineno-0-30" name="__codelineno-0-30" href="#security-docs-hosts-__codelineno-0-30"></a><span class="w">  </span><span class="p">]</span>
<a id="__codelineno-0-31" name="__codelineno-0-31" href="#security-docs-hosts-__codelineno-0-31"></a><span class="p">}</span>
</code></pre></div>
<p>With this policy in place and the <a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-working-with-install-plugin.html">Session Manager plugin</a> installed, you can then run 
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#security-docs-hosts-__codelineno-1-1"></a>aws<span class="w"> </span>ssm<span class="w"> </span>start-session<span class="w"> </span>--target<span class="w"> </span><span class="o">[</span>INSTANCE_ID_OF_EKS_NODE<span class="o">]</span>
</code></pre></div>
to access the node.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You may also want to consider adding permissions to <a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/getting-started-create-iam-instance-profile.html#create-iam-instance-profile-ssn-logging">enable Session Manager logging</a>.</p>
</div>
<h3 id="security-docs-hosts-deploy-workers-onto-private-subnets">Deploy workers onto private subnets<a class="headerlink" href="#security-docs-hosts-deploy-workers-onto-private-subnets" title="Permanent link">&para;</a></h3>
<p>By deploying workers onto private subnets, you minimize their exposure to the Internet where attacks often originate.  Beginning April 22, 2020, the assignment of public IP addresses to nodes in a managed node groups will be controlled by the subnet they are deployed onto.  Prior to this, nodes in a Managed Node Group were automatically assigned a public IP. If you choose to deploy your worker nodes on to public subnets, implement restrictive AWS security group rules to limit their exposure.</p>
<h3 id="security-docs-hosts-run-amazon-inspector-to-assess-hosts-for-exposure-vulnerabilities-and-deviations-from-best-practices">Run Amazon Inspector to assess hosts for exposure, vulnerabilities, and deviations from best practices<a class="headerlink" href="#security-docs-hosts-run-amazon-inspector-to-assess-hosts-for-exposure-vulnerabilities-and-deviations-from-best-practices" title="Permanent link">&para;</a></h3>
<p>You can use <a href="https://docs.aws.amazon.com/inspector/latest/user/what-is-inspector.html">Amazon Inspector</a> to check for unintended network access to your nodes and for vulnerabilities on the underlying Amazon EC2 instances.</p>
<p>Amazon Inspector can provide common vulnerabilities and exposures (CVE) data for your Amazon EC2 instances only if the Amazon EC2 Systems Manager (SSM) agent is installed and enabled. This agent is preinstalled on several <a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/ami-preinstalled-agent.html">Amazon Machine Images (AMIs)</a> including <a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html">EKS optimized Amazon Linux AMIs</a>. Regardless of SSM agent status, all of your Amazon EC2 instances are scanned for network reachability issues. For more information about configuring scans for Amazon EC2, see <a href="https://docs.aws.amazon.com/inspector/latest/user/enable-disable-scanning-ec2.html">Scanning Amazon EC2 instances</a>.</p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Inspector cannot be run on the infrastructure used to run Fargate pods.</p>
</div>
<h2 id="security-docs-hosts-alternatives">Alternatives<a class="headerlink" href="#security-docs-hosts-alternatives" title="Permanent link">&para;</a></h2>
<h3 id="security-docs-hosts-run-selinux">Run SELinux<a class="headerlink" href="#security-docs-hosts-run-selinux" title="Permanent link">&para;</a></h3>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>Available on Red Hat Enterprise Linux (RHEL), CentOS, Bottlerocket, and Amazon Linux 2023</p>
</div>
<p>SELinux provides an additional layer of security to keep containers isolated from each other and from the host. SELinux allows administrators to enforce mandatory access controls (MAC) for every user, application, process, and file.  Think of it as a backstop that restricts the operations that can be performed against to specific resources based on a set of labels.  On EKS, SELinux can be used to prevent containers from accessing each other's resources.</p>
<p>Container SELinux policies are defined in the <a href="https://github.com/containers/container-selinux">container-selinux</a> package.  Docker CE requires this package (along with its dependencies) so that the processes and files created by Docker (or other container runtimes) run with limited system access. Containers leverage the <code>container_t</code> label which is an alias to <code>svirt_lxc_net_t</code>. These policies effectively prevent containers from accessing certain features of the host.</p>
<p>When you configure SELinux for Docker, Docker automatically labels workloads <code>container_t</code> as a type and gives each container a unique MCS level. This will isolate containers from one another. If you need looser restrictions, you can create your own profile in SElinux which grants a container permissions to specific areas of the file system.  This is similar to PSPs in that you can create different profiles for different containers/pods.  For example, you can have a profile for general workloads with a set of restrictive controls and another for things that require privileged access.</p>
<p>SELinux for Containers has a set of options that can be configured to modify the default restrictions. The following SELinux Booleans can be enabled or disabled based on your needs:</p>
<table>
<thead>
<tr>
<th>Boolean</th>
<th style="text-align: center;">Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>container_connect_any</code></td>
<td style="text-align: center;"><code>off</code></td>
<td>Allow containers to access privileged ports on the host. For example, if you have a container that needs to map ports to 443 or 80 on the host.</td>
</tr>
<tr>
<td><code>container_manage_cgroup</code></td>
<td style="text-align: center;"><code>off</code></td>
<td>Allow containers to manage cgroup configuration. For example, a container running systemd will need this to be enabled.</td>
</tr>
<tr>
<td><code>container_use_cephfs</code></td>
<td style="text-align: center;"><code>off</code></td>
<td>Allow containers to use a ceph file system.</td>
</tr>
</tbody>
</table>
<p>By default, containers are allowed to read/execute under <code>/usr</code> and read most content from <code>/etc</code>. The files under <code>/var/lib/docker</code> and <code>/var/lib/containers</code> have the label <code>container_var_lib_t</code>. To view a full list of default, labels see the <a href="https://github.com/containers/container-selinux/blob/master/container.fc">container.fc</a> file.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#security-docs-hosts-__codelineno-2-1"></a>docker<span class="w"> </span>container<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#security-docs-hosts-__codelineno-2-2"></a><span class="w">  </span>-v<span class="w"> </span>/var/lib/docker/image/overlay2/repositories.json:/host/repositories.json<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#security-docs-hosts-__codelineno-2-3"></a><span class="w">  </span>centos:7<span class="w"> </span>cat<span class="w"> </span>/host/repositories.json
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#security-docs-hosts-__codelineno-2-4"></a><span class="c1"># cat: /host/repositories.json: Permission denied</span>
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#security-docs-hosts-__codelineno-2-5"></a>
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#security-docs-hosts-__codelineno-2-6"></a>docker<span class="w"> </span>container<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#security-docs-hosts-__codelineno-2-7"></a><span class="w">  </span>-v<span class="w"> </span>/etc/passwd:/host/etc/passwd<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-2-8" name="__codelineno-2-8" href="#security-docs-hosts-__codelineno-2-8"></a><span class="w">  </span>centos:7<span class="w"> </span>cat<span class="w"> </span>/host/etc/passwd
<a id="__codelineno-2-9" name="__codelineno-2-9" href="#security-docs-hosts-__codelineno-2-9"></a><span class="c1"># cat: /host/etc/passwd: Permission denied</span>
</code></pre></div>
<p>Files labeled with <code>container_file_t</code> are the only files that are writable by containers. If you want a volume mount to be writeable, you will needed to specify <code>:z</code> or <code>:Z</code> at the end.</p>
<ul>
<li><code>:z</code> will re-label the files so that the container can read/write</li>
<li><code>:Z</code> will re-label the files so that <strong>only</strong> the container can read/write</li>
</ul>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#security-docs-hosts-__codelineno-3-1"></a>ls<span class="w"> </span>-Z<span class="w"> </span>/var/lib/misc
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#security-docs-hosts-__codelineno-3-2"></a><span class="c1"># -rw-r--r--. root root system_u:object_r:var_lib_t:s0   postfix.aliasesdb-stamp</span>
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#security-docs-hosts-__codelineno-3-3"></a>
<a id="__codelineno-3-4" name="__codelineno-3-4" href="#security-docs-hosts-__codelineno-3-4"></a>docker<span class="w"> </span>container<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-3-5" name="__codelineno-3-5" href="#security-docs-hosts-__codelineno-3-5"></a><span class="w">  </span>-v<span class="w"> </span>/var/lib/misc:/host/var/lib/misc:z<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-3-6" name="__codelineno-3-6" href="#security-docs-hosts-__codelineno-3-6"></a><span class="w">  </span>centos:7<span class="w"> </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Relabeled!&quot;</span>
<a id="__codelineno-3-7" name="__codelineno-3-7" href="#security-docs-hosts-__codelineno-3-7"></a>
<a id="__codelineno-3-8" name="__codelineno-3-8" href="#security-docs-hosts-__codelineno-3-8"></a>ls<span class="w"> </span>-Z<span class="w"> </span>/var/lib/misc
<a id="__codelineno-3-9" name="__codelineno-3-9" href="#security-docs-hosts-__codelineno-3-9"></a><span class="c1">#-rw-r--r--. root root system_u:object_r:container_file_t:s0 postfix.aliasesdb-stamp</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#security-docs-hosts-__codelineno-4-1"></a>docker<span class="w"> </span>container<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-4-2" name="__codelineno-4-2" href="#security-docs-hosts-__codelineno-4-2"></a><span class="w">  </span>-v<span class="w"> </span>/var/log:/host/var/log:Z<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-4-3" name="__codelineno-4-3" href="#security-docs-hosts-__codelineno-4-3"></a><span class="w">  </span>fluentbit:latest
</code></pre></div>
<p>In Kubernetes, relabeling is slightly different. Rather than having Docker automatically relabel the files, you can specify a custom MCS label to run the pod. Volumes that support relabeling will automatically be relabeled so that they are accessible. Pods with a matching MCS label will be able to access the volume. If you need strict isolation, set a different MCS label for each pod.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#security-docs-hosts-__codelineno-5-1"></a><span class="nt">securityContext</span><span class="p">:</span>
<a id="__codelineno-5-2" name="__codelineno-5-2" href="#security-docs-hosts-__codelineno-5-2"></a><span class="w">  </span><span class="nt">seLinuxOptions</span><span class="p">:</span>
<a id="__codelineno-5-3" name="__codelineno-5-3" href="#security-docs-hosts-__codelineno-5-3"></a><span class="w">    </span><span class="c1"># Provide a unique MCS label per container</span>
<a id="__codelineno-5-4" name="__codelineno-5-4" href="#security-docs-hosts-__codelineno-5-4"></a><span class="w">    </span><span class="c1"># You can specify user, role, and type also</span>
<a id="__codelineno-5-5" name="__codelineno-5-5" href="#security-docs-hosts-__codelineno-5-5"></a><span class="w">    </span><span class="c1"># enforcement based on type and level (svert)</span>
<a id="__codelineno-5-6" name="__codelineno-5-6" href="#security-docs-hosts-__codelineno-5-6"></a><span class="w">    </span><span class="nt">level</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">s0:c144:c154</span>
</code></pre></div>
<p>In this example <code>s0:c144:c154</code> corresponds to an MCS label assigned to a file that the container is allowed to access.</p>
<p>On EKS you could create policies that allow for privileged containers to run, like FluentD and create an SELinux policy to allow it to read from /var/log on the host without needing to relabel the host directory. Pods with the same label will be able to access the same host volumes.</p>
<p>We have implemented <a href="https://github.com/aws-samples/amazon-eks-custom-amis">sample AMIs for Amazon EKS</a> that have SELinux configured on CentOS 7 and RHEL 7. These AMIs were developed to demonstrate sample implementations that meet requirements of highly regulated customers, such as STIG, CJIS, and C2S.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>SELinux will ignore containers where the type is unconfined.</p>
</div>
<h4 id="security-docs-hosts-additional-resources">Additional resources<a class="headerlink" href="#security-docs-hosts-additional-resources" title="Permanent link">&para;</a></h4>
<ul>
<li><a href="https://platform9.com/blog/selinux-kubernetes-rbac-and-shipping-security-policies-for-on-prem-applications/">SELinux Kubernetes RBAC and Shipping Security Policies for On-prem Applications</a></li>
<li><a href="https://jayunit100.blogspot.com/2019/07/iterative-hardening-of-kubernetes-and.html">Iterative Hardening of Kubernetes</a></li>
<li><a href="https://linux.die.net/man/1/audit2allow">Audit2Allow</a></li>
<li><a href="https://linux.die.net/man/8/sealert">SEAlert</a></li>
<li><a href="https://www.redhat.com/en/blog/generate-selinux-policies-containers-with-udica">Generate SELinux policies for containers with Udica</a> describes a tool that looks at container spec files for Linux capabilities, ports, and mount points, and generates a set of SELinux rules that allow the container to run properly</li>
<li><a href="https://github.com/aws-samples/amazon-eks-custom-amis#hardening">AMI Hardening</a> playbooks for hardening the OS to meet different regulatory requirements</li>
</ul>
<h2 id="security-docs-hosts-tools">Tools<a class="headerlink" href="#security-docs-hosts-tools" title="Permanent link">&para;</a></h2>
<ul>
<li><a href="https://github.com/keikoproj/upgrade-manager">Keiko Upgrade Manager</a> an open source project from Intuit that orchestrates the rotation of worker nodes.</li>
<li><a href="https://sysdig.com/products/kubernetes-security/">Sysdig Secure</a></li>
<li><a href="https://eksctl.io/">eksctl</a></li>
</ul></section><section class="print-page" id="security-docs-compliance"><h1 id="security-docs-compliance-compliance">Compliance<a class="headerlink" href="#security-docs-compliance-compliance" title="Permanent link">&para;</a></h1>
<p>Compliance is a shared responsibility between AWS and the consumers of its services. Generally speaking, AWS is responsible for “security of the cloud” whereas its users are responsible for “security in the cloud.” The line that delineates what AWS and its users are responsible for will vary depending on the service. For example, with Fargate, AWS is responsible for managing the physical security of its data centers, the hardware, the virtual infrastructure (Amazon EC2), and the container runtime (Docker). Users of Fargate are responsible for securing the container image and their application. Knowing who is responsible for what is an important consideration when running workloads that must adhere to compliance standards.</p>
<p>The following table shows the compliance programs with which the different container services conform.</p>
<table>
<thead>
<tr>
<th>Compliance Program</th>
<th style="text-align: center;">Amazon ECS Orchestrator</th>
<th style="text-align: center;">Amazon EKS Orchestrator</th>
<th style="text-align: center;">ECS Fargate</th>
<th style="text-align: center;">Amazon ECR</th>
</tr>
</thead>
<tbody>
<tr>
<td>PCI DSS Level 1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td>HIPAA Eligible</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td>SOC I</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td>SOC II</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td>SOC III</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td>ISO 27001:2013</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td>ISO 9001:2015</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td>ISO 27017:2015</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td>ISO 27018:2019</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td>IRAP</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td>FedRAMP Moderate (East/West)</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td>FedRAMP High (GovCloud)</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td>DOD CC SRG</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">DISA Review (IL5)</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td>HIPAA BAA</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td>MTCS</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td>C5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td>K-ISMS</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td>ENS High</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td>OSPAR</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td>HITRUST CSF</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
<p>Compliance status changes over time. For the latest status, always refer to <a href="https://aws.amazon.com/compliance/services-in-scope/">https://aws.amazon.com/compliance/services-in-scope/</a>.</p>
<p>For further information about cloud accreditation models and best practices, see the AWS whitepaper, <a href="https://d1.awsstatic.com/whitepapers/accreditation-models-for-secure-cloud-adoption.pdf">Accreditation Models for Secure Cloud Adoption</a></p>
<h2 id="security-docs-compliance-shifting-left">Shifting Left<a class="headerlink" href="#security-docs-compliance-shifting-left" title="Permanent link">&para;</a></h2>
<p>The concept of shifting left involves catching policy violations and errors earlier in the software development lifecycle. From a security perspective, this can be very beneficial. A developer, for example, can fix issues with their configuration before their application is deployed to the cluster. Catching mistakes like this earlier will help prevent configurations that violate your policies from being deployed.</p>
<h3 id="security-docs-compliance-policy-as-code">Policy as Code<a class="headerlink" href="#security-docs-compliance-policy-as-code" title="Permanent link">&para;</a></h3>
<p>Policy can be thought of as a set of rules for governing behaviors, i.e. behaviors that are allowed or those that are prohibited. For example, you may have a policy that says that all Dockerfiles should include a USER directive that causes the container to run as a non-root user. As a document, a policy like this can be hard to discover and enforce. It may also become outdated as your requirements change. With Policy as Code (PaC) solutions, you can automate security, compliance, and privacy controls that detect, prevent, reduce, and counteract known and persistent threats. Furthermore, they give you mechanism to codify your policies and manage them as you do other code artifacts. The benefit of this approach is that you can reuse your DevOps and GitOps strategies to manage and consistently apply policies across fleets of Kubernetes clusters. Please refer to <a href="https://aws.github.io/aws-eks-best-practices/security/docs/pods/#pod-security">Pod Security</a> for information about PaC options and the future of PSPs.</p>
<h2 id="security-docs-compliance-recommendations">Recommendations<a class="headerlink" href="#security-docs-compliance-recommendations" title="Permanent link">&para;</a></h2>
<h3 id="security-docs-compliance-use-policy-as-code-tools-in-pipelines-to-detect-violations-before-deployment">Use policy-as-code tools in pipelines to detect violations before deployment<a class="headerlink" href="#security-docs-compliance-use-policy-as-code-tools-in-pipelines-to-detect-violations-before-deployment" title="Permanent link">&para;</a></h3>
<ul>
<li><a href="https://www.openpolicyagent.org/">OPA</a> is an open source policy engine that's part of the CNCF. It's used for making policy decisions and can be run a variety of different ways, e.g. as a language library or a service. OPA policies are written in a Domain Specific Language (DSL) called Rego. While it is often run as part of a Kubernetes Dynamic Admission Controller as the <a href="https://github.com/open-policy-agent/gatekeeper">Gatekeeper</a> project, OPA can also be incorporated into your CI/CD pipeline. This allows developers to get feedback about their configuration earlier in the release cycle which can subsequently help them resolve issues before they get to production. A collection of common OPA policies can be found in the GitHub <a href="https://github.com/aws/aws-eks-best-practices/tree/master/policies/opa">repository</a> for this project.</li>
<li><a href="https://github.com/open-policy-agent/conftest">Conftest</a> is built on top of OPA and it provides a developer focused experience for testing Kubernetes configuration.</li>
<li><a href="https://kyverno.io/">Kyverno</a> is a policy engine designed for Kubernetes. With Kyverno, policies are managed as Kubernetes resources and no new language is required to write policies. This allows using familiar tools such as kubectl, git, and kustomize to manage policies. Kyverno policies can validate, mutate, and generate Kubernetes resources plus ensure OCI image supply chain security. The <a href="https://kyverno.io/docs/kyverno-cli/">Kyverno CLI</a> can be used to test policies and validate resources as part of a CI/CD pipeline. All the Kyverno community policies can be found on the <a href="https://kyverno.io/policies/">Kyverno website</a>, and for examples using the Kyverno CLI to write tests in pipelines, see the <a href="https://github.com/kyverno/policies">policies repository</a>.</li>
</ul>
<h2 id="security-docs-compliance-tools-and-resources">Tools and resources<a class="headerlink" href="#security-docs-compliance-tools-and-resources" title="Permanent link">&para;</a></h2>
<ul>
<li><a href="https://github.com/aquasecurity/kube-bench">kube-bench</a></li>
<li><a href="https://github.com/docker/docker-bench-security">docker-bench-security</a></li>
<li><a href="https://aws.amazon.com/inspector/">AWS Inspector</a></li>
<li><a href="https://github.com/kubernetes/community/blob/master/sig-security/security-audit-2019/findings/Kubernetes%20Final%20Report.pdf">Kubernetes Security Review</a> A 3rd party security assessment of Kubernetes 1.13.4 (2019)</li>
</ul></section><section class="print-page" id="security-docs-incidents"><h1 id="security-docs-incidents-incident-response-and-forensics">Incident response and forensics<a class="headerlink" href="#security-docs-incidents-incident-response-and-forensics" title="Permanent link">&para;</a></h1>
<p>Your ability to react quickly to an incident can help minimize damage caused from a breach. Having a reliable alerting system that can warn you of suspicious behavior is the first step in a good incident response plan. When an incident does arise, you have to quickly decide whether to destroy and replace the effected container, or isolate and inspect the container. If you choose to isolate the container as part of a forensic investigation and root cause analysis, then the following set of activities should be followed:</p>
<h2 id="security-docs-incidents-sample-incident-response-plan">Sample incident response plan<a class="headerlink" href="#security-docs-incidents-sample-incident-response-plan" title="Permanent link">&para;</a></h2>
<h3 id="security-docs-incidents-identify-the-offending-pod-and-worker-node">Identify the offending Pod and worker node<a class="headerlink" href="#security-docs-incidents-identify-the-offending-pod-and-worker-node" title="Permanent link">&para;</a></h3>
<p>Your first course of action should be to isolate the damage.  Start by identifying where the breach occurred and isolate that Pod and its node from the rest of the infrastructure.</p>
<h3 id="security-docs-incidents-identify-the-offending-pods-and-worker-nodes-using-workload-name">Identify the offending Pods and worker nodes using workload name<a class="headerlink" href="#security-docs-incidents-identify-the-offending-pods-and-worker-nodes-using-workload-name" title="Permanent link">&para;</a></h3>
<p>If you know the name and namespace of the offending pod, you can identify the the worker node running the pod as follows:
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#security-docs-incidents-__codelineno-0-1"></a>kubectl get pods &lt;name&gt; --namespace &lt;namespace&gt; -o=jsonpath=&#39;{.spec.nodeName}{&quot;\n&quot;}&#39;   
</code></pre></div>
If a Workload Resource (https://kubernetes.io/docs/concepts/workloads/controllers/) such as a Deployment has been compromised, it is likely that all the pods that are part of the workload resource are compromised. Use the following command to list all the pods of the Workload Resource and the nodes they are running on:
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#security-docs-incidents-__codelineno-1-1"></a>selector=$(kubectl get deployments &lt;name&gt; \
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#security-docs-incidents-__codelineno-1-2"></a> --namespace &lt;namespace&gt; -o json | jq -j \
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#security-docs-incidents-__codelineno-1-3"></a>&#39;.spec.selector.matchLabels | to_entries | .[] | &quot;\(.key)=\(.value)&quot;&#39;)
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#security-docs-incidents-__codelineno-1-4"></a>
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#security-docs-incidents-__codelineno-1-5"></a>kubectl get pods --namespace &lt;namespace&gt; --selector=$selector \
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#security-docs-incidents-__codelineno-1-6"></a>-o json | jq -r &#39;.items[] | &quot;\(.metadata.name) \(.spec.nodeName)&quot;&#39;
</code></pre></div>
The above command is for deployments. You can run the same command for other workload resources such as replicasets,, statefulsets, etc. </p>
<h3 id="security-docs-incidents-identify-the-offending-pods-and-worker-nodes-using-service-account-name">Identify the offending Pods and worker nodes using service account name<a class="headerlink" href="#security-docs-incidents-identify-the-offending-pods-and-worker-nodes-using-service-account-name" title="Permanent link">&para;</a></h3>
<p>In some cases, you may identify that a service account is compromised.  It is likely that pods using the identified service account are compromised. You can identify all the pods using the service account and nodes they are running on with the following command:
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#security-docs-incidents-__codelineno-2-1"></a>kubectl get pods -o json --namespace &lt;namespace&gt; | \
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#security-docs-incidents-__codelineno-2-2"></a>    jq -r &#39;.items[] |
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#security-docs-incidents-__codelineno-2-3"></a>    select(.spec.serviceAccount == &quot;&lt;service account name&gt;&quot;) |
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#security-docs-incidents-__codelineno-2-4"></a>    &quot;\(.metadata.name) \(.spec.nodeName)&quot;&#39;
</code></pre></div></p>
<h3 id="security-docs-incidents-identify-pods-with-vulnerable-or-compromised-images-and-worker-nodes">Identify Pods with vulnerable or compromised images and worker nodes<a class="headerlink" href="#security-docs-incidents-identify-pods-with-vulnerable-or-compromised-images-and-worker-nodes" title="Permanent link">&para;</a></h3>
<p>In some cases, you may discover that a container image being used in pods on your cluster is malicious or compromised. A container image is malicious or compromised, if it was found to contain malware, is a known bad image or has a CVE that has been exploited. You should consider all the pods using the container image compromised. You can identify the pods using the image and nodes they are running on with the following command:
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#security-docs-incidents-__codelineno-3-1"></a>IMAGE=&lt;Name of the malicious/compromised image&gt;
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#security-docs-incidents-__codelineno-3-2"></a>
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#security-docs-incidents-__codelineno-3-3"></a>kubectl get pods -o json --all-namespaces | \
<a id="__codelineno-3-4" name="__codelineno-3-4" href="#security-docs-incidents-__codelineno-3-4"></a>    jq -r --arg image &quot;$IMAGE&quot; &#39;.items[] | 
<a id="__codelineno-3-5" name="__codelineno-3-5" href="#security-docs-incidents-__codelineno-3-5"></a>    select(.spec.containers[] | .image == $image) | 
<a id="__codelineno-3-6" name="__codelineno-3-6" href="#security-docs-incidents-__codelineno-3-6"></a>    &quot;\(.metadata.name) \(.metadata.namespace) \(.spec.nodeName)&quot;&#39;
</code></pre></div></p>
<h3 id="security-docs-incidents-isolate-the-pod-by-creating-a-network-policy-that-denies-all-ingress-and-egress-traffic-to-the-pod">Isolate the Pod by creating a Network Policy that denies all ingress and egress traffic to the pod<a class="headerlink" href="#security-docs-incidents-isolate-the-pod-by-creating-a-network-policy-that-denies-all-ingress-and-egress-traffic-to-the-pod" title="Permanent link">&para;</a></h3>
<p>A deny all traffic rule may help stop an attack that is already underway by severing all connections to the pod. The following Network Policy will apply to a pod with the label <code>app=web</code>. 
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#security-docs-incidents-__codelineno-4-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">networking.k8s.io/v1</span>
<a id="__codelineno-4-2" name="__codelineno-4-2" href="#security-docs-incidents-__codelineno-4-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">NetworkPolicy</span>
<a id="__codelineno-4-3" name="__codelineno-4-3" href="#security-docs-incidents-__codelineno-4-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-4-4" name="__codelineno-4-4" href="#security-docs-incidents-__codelineno-4-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">default-deny</span>
<a id="__codelineno-4-5" name="__codelineno-4-5" href="#security-docs-incidents-__codelineno-4-5"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-4-6" name="__codelineno-4-6" href="#security-docs-incidents-__codelineno-4-6"></a><span class="w">  </span><span class="nt">podSelector</span><span class="p">:</span>
<a id="__codelineno-4-7" name="__codelineno-4-7" href="#security-docs-incidents-__codelineno-4-7"></a><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w"> </span>
<a id="__codelineno-4-8" name="__codelineno-4-8" href="#security-docs-incidents-__codelineno-4-8"></a><span class="w">      </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">web</span>
<a id="__codelineno-4-9" name="__codelineno-4-9" href="#security-docs-incidents-__codelineno-4-9"></a><span class="w">  </span><span class="nt">policyTypes</span><span class="p">:</span>
<a id="__codelineno-4-10" name="__codelineno-4-10" href="#security-docs-incidents-__codelineno-4-10"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Ingress</span>
<a id="__codelineno-4-11" name="__codelineno-4-11" href="#security-docs-incidents-__codelineno-4-11"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Egress</span>
</code></pre></div></p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>A Network Policy may prove ineffective if an attacker has gained access to underlying host. If you suspect that has happened, you can use <a href="https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html">AWS Security Groups</a> to isolate a compromised host from other hosts. When changing a host's security group, be aware that it will impact all containers running on that host.  </p>
</div>
<h3 id="security-docs-incidents-revoke-temporary-security-credentials-assigned-to-the-pod-or-worker-node-if-necessary">Revoke temporary security credentials assigned to the pod or worker node if necessary<a class="headerlink" href="#security-docs-incidents-revoke-temporary-security-credentials-assigned-to-the-pod-or-worker-node-if-necessary" title="Permanent link">&para;</a></h3>
<p>If the worker node has been assigned an IAM role that allows Pods to gain access to other AWS resources, remove those roles from the instance to prevent further damage from the attack. Similarly, if the Pod has been assigned an IAM role, evaluate whether you can safely remove the IAM policies from the role without impacting other workloads.</p>
<h3 id="security-docs-incidents-cordon-the-worker-node">Cordon the worker node<a class="headerlink" href="#security-docs-incidents-cordon-the-worker-node" title="Permanent link">&para;</a></h3>
<p>By cordoning the impacted worker node, you're informing the scheduler to avoid scheduling pods onto the affected node. This will allow you to remove the node for forensic study without disrupting other workloads.</p>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>This guidance is not applicable to Fargate where each Fargate pod run in its own sandboxed environment.  Instead of cordoning, sequester the affected Fargate pods by applying a network policy that denies all ingress and egress traffic. </p>
</div>
<h3 id="security-docs-incidents-enable-termination-protection-on-impacted-worker-node">Enable termination protection on impacted worker node<a class="headerlink" href="#security-docs-incidents-enable-termination-protection-on-impacted-worker-node" title="Permanent link">&para;</a></h3>
<p>An attacker may attempt to erase their misdeeds by terminating an affected node.  Enabling <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/terminating-instances.html#Using_ChangingDisableAPITermination">termination protection</a> can prevent this from happening.  <a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html#instance-protection">Instance scale-in protection</a> will protect the node from a scale-in event. </p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>You cannot enable termination protection on a Spot instance. </p>
</div>
<h3 id="security-docs-incidents-label-the-offending-podnode-with-a-label-indicating-that-it-is-part-of-an-active-investigation">Label the offending Pod/Node with a label indicating that it is part of an active investigation<a class="headerlink" href="#security-docs-incidents-label-the-offending-podnode-with-a-label-indicating-that-it-is-part-of-an-active-investigation" title="Permanent link">&para;</a></h3>
<p>This will serve as a warning to cluster administrators not to tamper with the affected Pods/Nodes until the investigation is complete. </p>
<h3 id="security-docs-incidents-capture-volatile-artifacts-on-the-worker-node">Capture volatile artifacts on the worker node<a class="headerlink" href="#security-docs-incidents-capture-volatile-artifacts-on-the-worker-node" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Capture the operating system memory</strong>. This will capture the Docker daemon (or other container runtime) and its subprocesses per container. This can be accomplished using tools like <a href="https://github.com/504ensicsLabs/LiME">LiME</a> and <a href="https://www.volatilityfoundation.org/">Volatility</a>, or through higher-level tools such as <a href="https://aws.amazon.com/solutions/implementations/automated-forensics-orchestrator-for-amazon-ec2/">Automated Forensics Orchestrator for Amazon EC2</a> that build on top of them.</li>
<li><strong>Perform a netstat tree dump of the processes running and the open ports</strong>. This will capture the docker daemon and its subprocess per container. </li>
<li>
<p><strong>Run commands to save container-level state before evidence is altered</strong>. You can use capabilities of the container runtime to capture information about currently running containers. For example, with Docker, you could do the following:</p>
<ul>
<li><code>docker top CONTAINER</code> for processes running.</li>
<li><code>docker logs CONTAINER</code> for daemon level held logs.</li>
<li><code>docker inspect CONTAINER</code> for various information about the container.</li>
</ul>
<p>The same could be achieved with containerd using the <a href="https://github.com/containerd/nerdctl">nerdctl</a> CLI, in place of <code>docker</code> (e.g. <code>nerdctl inspect</code>). Some additional commands are available depending on the container runtime. For example, Docker has <code>docker diff</code> to see changes to the container filesystem or <code>docker checkpoint</code> to save all container state including volatile memory (RAM). See <a href="https://kubernetes.io/blog/2022/12/05/forensic-container-checkpointing-alpha/">this Kubernetes blog post</a> for discussion of similar capabilities with containerd or CRI-O runtimes.</p>
</li>
<li>
<p><strong>Pause the container for forensic capture</strong>.</p>
</li>
<li><strong>Snapshot the instance's EBS volumes</strong>.</li>
</ul>
<h3 id="security-docs-incidents-redeploy-compromised-pod-or-workload-resource">Redeploy compromised Pod or Workload Resource<a class="headerlink" href="#security-docs-incidents-redeploy-compromised-pod-or-workload-resource" title="Permanent link">&para;</a></h3>
<p>Once you have gathered data for forensic analysis, you can redeploy the compromised pod or workload resource.</p>
<p>First roll out the fix for the vulnerability that was compromised and start new replacement pods. Then delete the vulnerable pods. </p>
<p>If the vulnerable pods are managed by a higher-level Kubernetes workload resource (for example, a Deployment or DaemonSet), deleting them will schedule new ones. So vulnerable pods will be launched again. In that case you should deploy a new replacement workload resource after fixing the vulnerability. Then you should delete the vulnerable workload.</p>
<h2 id="security-docs-incidents-recommendations">Recommendations<a class="headerlink" href="#security-docs-incidents-recommendations" title="Permanent link">&para;</a></h2>
<h3 id="security-docs-incidents-review-the-aws-security-incident-response-whitepaper">Review the AWS Security Incident Response Whitepaper<a class="headerlink" href="#security-docs-incidents-review-the-aws-security-incident-response-whitepaper" title="Permanent link">&para;</a></h3>
<p>While this section gives a brief overview along with a few  recommendations for handling suspected security breaches, the topic is exhaustively covered in the white paper, <a href="https://docs.aws.amazon.com/whitepapers/latest/aws-security-incident-response-guide/welcome.html">AWS Security Incident Response</a>.</p>
<h3 id="security-docs-incidents-practice-security-game-days">Practice security game days<a class="headerlink" href="#security-docs-incidents-practice-security-game-days" title="Permanent link">&para;</a></h3>
<p>Divide your security practitioners into 2 teams: red and blue.  The red team will be focused on probing different systems for vulnerabilities while the blue team will be responsible for defending against them.  If you don't have enough security practitioners to create separate teams, consider hiring an outside entity that has knowledge of Kubernetes exploits. </p>
<p><a href="https://github.com/cyberark/kubesploit">Kubesploit</a> is a penetration testing framework from CyberArk that you can use to conduct game days. Unlike other tools which scan your cluster for vulnerabilities, kubesploit simulates a real-world attack. This gives your blue team an opportunity to practice its response to an attack and gauge its effectiveness.</p>
<h3 id="security-docs-incidents-run-penetration-tests-against-your-cluster">Run penetration tests against your cluster<a class="headerlink" href="#security-docs-incidents-run-penetration-tests-against-your-cluster" title="Permanent link">&para;</a></h3>
<p>Periodically attacking your own cluster can help you discover vulnerabilities and misconfigurations.  Before getting started, follow the <a href="https://aws.amazon.com/security/penetration-testing/">penetration test guidelines</a> before conducting a test against your cluster. </p>
<h2 id="security-docs-incidents-tools">Tools<a class="headerlink" href="#security-docs-incidents-tools" title="Permanent link">&para;</a></h2>
<ul>
<li><a href="https://github.com/aquasecurity/kube-hunter">kube-hunter</a>, a penetration testing tool for Kubernetes. </li>
<li><a href="https://www.gremlin.com/product/#kubernetes">Gremlin</a>, a chaos engineering toolkit that you can use to simulate attacks against your applications and infrastructure. </li>
<li><a href="https://github.com/kubernetes/sig-security/blob/main/sig-security-external-audit/security-audit-2019/findings/AtredisPartners_Attacking_Kubernetes-v1.0.pdf">Attacking and Defending Kubernetes Installations</a></li>
<li><a href="https://www.cyberark.com/resources/threat-research-blog/kubesploit-a-new-offensive-tool-for-testing-containerized-environments">kubesploit</a></li>
</ul>
<h2 id="security-docs-incidents-videos">Videos<a class="headerlink" href="#security-docs-incidents-videos" title="Permanent link">&para;</a></h2>
<ul>
<li><a href="https://www.youtube.com/watch?v=CH7S5rE3j8w">Advanced Persistent Threats</a></li>
<li><a href="https://www.youtube.com/watch?v=LtCx3zZpOfs">Kubernetes Practical Attack and Defense</a></li>
<li><a href="https://www.youtube.com/watch?v=1LMo0CftVC4">Compromising Kubernetes Cluster by Exploiting RBAC Permissions</a></li>
</ul></section><section class="print-page" id="security-docs-image"><h1 id="security-docs-image-image-security">Image security<a class="headerlink" href="#security-docs-image-image-security" title="Permanent link">&para;</a></h1>
<p>You should consider the container image as your first line of defense against an attack. An insecure, poorly constructed image can allow an attacker to escape the bounds of the container and gain access to the host. Once on the host, an attacker can gain access to sensitive information or move laterally within the cluster or with your AWS account. The following best practices will help mitigate risk of this happening.</p>
<h2 id="security-docs-image-recommendations">Recommendations<a class="headerlink" href="#security-docs-image-recommendations" title="Permanent link">&para;</a></h2>
<h3 id="security-docs-image-create-minimal-images">Create minimal images<a class="headerlink" href="#security-docs-image-create-minimal-images" title="Permanent link">&para;</a></h3>
<p>Start by removing all extraneous binaries from the container image. If you’re using an unfamiliar image from Dockerhub, inspect the image using an application like <a href="https://github.com/wagoodman/dive">Dive</a> which can show you the contents of each of the container’s layers. Remove all binaries with the SETUID and SETGID bits as they can be used to escalate privilege and consider removing all shells and utilities like nc and curl that can be used for nefarious purposes. You can find the files with SETUID and SETGID bits with the following command:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#security-docs-image-__codelineno-0-1"></a>find / -perm /6000 -type f -exec ls -ld {} \;
</code></pre></div>
<p>To remove the special permissions from these files, add the following directive to your container image:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#security-docs-image-__codelineno-1-1"></a><span class="k">RUN</span><span class="w"> </span>find<span class="w"> </span>/<span class="w"> </span>-xdev<span class="w"> </span>-perm<span class="w"> </span>/6000<span class="w"> </span>-type<span class="w"> </span>f<span class="w"> </span>-exec<span class="w"> </span>chmod<span class="w"> </span>a-s<span class="w"> </span><span class="o">{}</span><span class="w"> </span><span class="se">\;</span><span class="w"> </span><span class="o">||</span><span class="w"> </span>true
</code></pre></div>
<p>Colloquially, this is known as de-fanging your image.</p>
<h3 id="security-docs-image-use-multi-stage-builds">Use multi-stage builds<a class="headerlink" href="#security-docs-image-use-multi-stage-builds" title="Permanent link">&para;</a></h3>
<p>Using multi-stage builds is a way to create minimal images. Oftentimes, multi-stage builds are used to automate parts of the Continuous Integration cycle. For example, multi-stage builds can be used to lint your source code or perform static code analysis. This affords developers an opportunity to get near immediate feedback instead of waiting for a pipeline to execute. Multi-stage builds are attractive from a security standpoint because they allow you to minimize the size of the final image pushed to your container registry. Container images devoid of build tools and other extraneous binaries improves your security posture by reducing the attack surface of the image. For additional information about multi-stage builds, see https://docs.docker.com/develop/develop-images/multistage-build/.</p>
<h3 id="security-docs-image-create-software-bill-of-materials-sboms-for-your-container-image">Create Software Bill of Materials (SBOMs) for your container image<a class="headerlink" href="#security-docs-image-create-software-bill-of-materials-sboms-for-your-container-image" title="Permanent link">&para;</a></h3>
<p>A “software bill of materials” (SBOM) is a nested inventory of the software artifacts that make up your container image.
SBOM is a key building block in software security and software supply chain risk management. <a href="https://anchore.com/sbom/">Generating, storing SBOMS in a central repository and scanning SBOMs for vulnerabilities</a> helps address the following concerns:</p>
<ul>
<li><strong>Visibility</strong>: understand what components make up your container image. Storing in a central repository allows SBOMs to be audited and scanned anytime, even post deployment to detect and respond to new vulnerabilities such as zero day vulnerabilities.</li>
<li><strong>Provenance Verification</strong>: assurance that existing assumptions of where and how an artifact originates from are true and that the artifact or its accompanying metadata have not been tampered with during the build or delivery processes.</li>
<li><strong>Trustworthiness</strong>: assurance that a given artifact and its contents can be trusted to do what it is purported to do, i.e. is suitable for a purpose. This involves judgement on whether the code is safe to execute and making informed decisions about the risks associated with executing the code. Trustworthiness is assured by creating an attested pipeline execution report along with attested SBOM and attested CVE scan report to assure the consumers of the image that this image is in-fact created through secure means (pipeline) with secure components.</li>
<li><strong>Dependency Trust Verification</strong>: recursive checking of an artifact’s dependency tree for trustworthiness and provenance of the artifacts it uses. Drift in SBOMs can help detect malicious activity including unauthorized, untrusted dependencies, infiltration attempts.</li>
</ul>
<p>The following tools can be used to generate SBOM:</p>
<ul>
<li><a href="https://docs.aws.amazon.com/inspector">Amazon Inspector</a> can be used to <a href="https://docs.aws.amazon.com/inspector/latest/user/sbom-export.html">create and export SBOMs</a>.</li>
<li><a href="https://github.com/anchore/syft">Syft from Anchore</a> can also be used for SBOM generation. For quicker vulnerability scans, the SBOM generated for a container image can be used as an input to scan. The SBOM and scan report are then <a href="https://github.com/sigstore/cosign/blob/main/doc/cosign_attach_attestation.md">attested and attached</a> to the image before pushing the image to a central OCI repository such as Amazon ECR for review and audit purposes.</li>
</ul>
<p>Learn more about securing your software supply chain by reviewing <a href="https://project.linuxfoundation.org/hubfs/CNCF_SSCP_v1.pdf">CNCF Software Supply Chain Best Practices guide</a>.</p>
<h3 id="security-docs-image-scan-images-for-vulnerabilities-regularly">Scan images for vulnerabilities regularly<a class="headerlink" href="#security-docs-image-scan-images-for-vulnerabilities-regularly" title="Permanent link">&para;</a></h3>
<p>Like their virtual machine counterparts, container images can contain binaries and application libraries with vulnerabilities or develop vulnerabilities over time. The best way to safeguard against exploits is by regularly scanning your images with an image scanner. Images that are stored in Amazon ECR can be scanned on push or on-demand (once during a 24 hour period). ECR currently supports <a href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/image-scanning.html">two types of scanning - Basic and Enhanced</a>. Basic scanning leverages <a href="https://github.com/quay/clair">Clair</a> an open source image scanning solution for no cost. <a href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/image-scanning-enhanced.html">Enhanced scanning</a> uses Amazon Inspector to provide automatic continuous scans for <a href="https://aws.amazon.com/inspector/pricing/">additional cost</a>. After an image is scanned, the results are logged to the event stream for ECR in EventBridge. You can also see the results of a scan from within the ECR console. Images with a HIGH or CRITICAL vulnerability should be deleted or rebuilt. If an image that has been deployed develops a vulnerability, it should be replaced as soon as possible.</p>
<p>Knowing where images with vulnerabilities have been deployed is essential to keeping your environment secure. While you could conceivably build an image tracking solution yourself, there are already several commercial offerings that provide this and other advanced capabilities out of the box, including:</p>
<ul>
<li><a href="https://github.com/anchore/grype">Grype</a></li>
<li><a href="https://docs.paloaltonetworks.com/prisma/prisma-cloud/prisma-cloud-admin-compute/tools/twistcli_scan_images">Palo Alto - Prisma Cloud (twistcli)</a></li>
<li><a href="https://www.aquasec.com/">Aqua</a></li>
<li><a href="https://github.com/Portshift/kubei">Kubei</a></li>
<li><a href="https://github.com/aquasecurity/trivy">Trivy</a></li>
<li><a href="https://support.snyk.io/hc/en-us/articles/360003946917-Test-images-with-the-Snyk-Container-CLI">Snyk</a></li>
</ul>
<p>A Kubernetes validation webhook could also be used to validate that images are free of critical vulnerabilities. Validation webhooks are invoked prior to the Kubernetes API. They are typically used to reject requests that don't comply with the validation criteria defined in the webhook. <a href="https://aws.amazon.com/blogs/containers/building-serverless-admission-webhooks-for-kubernetes-with-aws-sam/">This</a> is an example of a serverless webhook that calls the ECR describeImageScanFindings API to determine whether a pod is pulling an image with critical vulnerabilities. If vulnerabilities are found, the pod is rejected and a message with list of CVEs is returned as an Event.</p>
<h3 id="security-docs-image-use-attestations-to-validate-artifact-integrity">Use attestations to validate artifact integrity<a class="headerlink" href="#security-docs-image-use-attestations-to-validate-artifact-integrity" title="Permanent link">&para;</a></h3>
<p>An attestation is a cryptographically signed “statement” that claims something - a “predicate” e.g. a pipeline run or the SBOM or the vulnerability scan report is true about another thing - a “subject” i.e. the container image.</p>
<p>Attestations help users to validate that an artifact comes from a trusted source in the software supply chain. As an example, we may use a container image without knowing all the software components or dependencies that are included in that image. However, if we trust whatever the producer of the container image says about what software is present, we can use the producer’s attestation to rely on that artifact. This means that we can proceed to use the artifact safely in our workflow in place of having done the analysis ourself.</p>
<ul>
<li>Attestations can be created using <a href="https://docs.aws.amazon.com/signer/latest/developerguide/Welcome.html">AWS Signer</a> or <a href="https://github.com/sigstore/cosign/blob/main/doc/cosign_attest.md">Sigstore cosign</a>.</li>
<li>Kubernetes admission controllers such as <a href="https://kyverno.io/">Kyverno</a> can be used to <a href="https://kyverno.io/docs/writing-policies/verify-images/sigstore/">verify attestations</a>.</li>
<li>Refer to this <a href="https://catalog.us-east-1.prod.workshops.aws/workshops/49343bb7-2cc5-4001-9d3b-f6a33b3c4442/en-US/0-introduction">workshop</a> to learn more about software supply chain management best practices on AWS using open source tools with topics including creating and attaching attestations to a container image.</li>
</ul>
<h3 id="security-docs-image-create-iam-policies-for-ecr-repositories">Create IAM policies for ECR repositories<a class="headerlink" href="#security-docs-image-create-iam-policies-for-ecr-repositories" title="Permanent link">&para;</a></h3>
<p>Nowadays, it is not uncommon for an organization to have multiple development teams operating independently within a shared AWS account. If these teams don't need to share assets, you may want to create a set of IAM policies that restrict access to the repositories each team can interact with. A good way to implement this is by using ECR <a href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/Repositories.html#repository-concepts">namespaces</a>. Namespaces are a way to group similar repositories together. For example, all of the registries for team A can be prefaced with the team-a/ while those for team B can use the team-b/ prefix. The policy to restrict access might look like the following:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#security-docs-image-__codelineno-2-1"></a><span class="p">{</span>
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#security-docs-image-__codelineno-2-2"></a><span class="w">  </span><span class="nt">&quot;Version&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;2012-10-17&quot;</span><span class="p">,</span>
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#security-docs-image-__codelineno-2-3"></a><span class="w">  </span><span class="nt">&quot;Statement&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#security-docs-image-__codelineno-2-4"></a><span class="w">    </span><span class="p">{</span>
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#security-docs-image-__codelineno-2-5"></a><span class="w">      </span><span class="nt">&quot;Sid&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;AllowPushPull&quot;</span><span class="p">,</span>
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#security-docs-image-__codelineno-2-6"></a><span class="w">      </span><span class="nt">&quot;Effect&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Allow&quot;</span><span class="p">,</span>
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#security-docs-image-__codelineno-2-7"></a><span class="w">      </span><span class="nt">&quot;Action&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<a id="__codelineno-2-8" name="__codelineno-2-8" href="#security-docs-image-__codelineno-2-8"></a><span class="w">        </span><span class="s2">&quot;ecr:GetDownloadUrlForLayer&quot;</span><span class="p">,</span>
<a id="__codelineno-2-9" name="__codelineno-2-9" href="#security-docs-image-__codelineno-2-9"></a><span class="w">        </span><span class="s2">&quot;ecr:BatchGetImage&quot;</span><span class="p">,</span>
<a id="__codelineno-2-10" name="__codelineno-2-10" href="#security-docs-image-__codelineno-2-10"></a><span class="w">        </span><span class="s2">&quot;ecr:BatchCheckLayerAvailability&quot;</span><span class="p">,</span>
<a id="__codelineno-2-11" name="__codelineno-2-11" href="#security-docs-image-__codelineno-2-11"></a><span class="w">        </span><span class="s2">&quot;ecr:PutImage&quot;</span><span class="p">,</span>
<a id="__codelineno-2-12" name="__codelineno-2-12" href="#security-docs-image-__codelineno-2-12"></a><span class="w">        </span><span class="s2">&quot;ecr:InitiateLayerUpload&quot;</span><span class="p">,</span>
<a id="__codelineno-2-13" name="__codelineno-2-13" href="#security-docs-image-__codelineno-2-13"></a><span class="w">        </span><span class="s2">&quot;ecr:UploadLayerPart&quot;</span><span class="p">,</span>
<a id="__codelineno-2-14" name="__codelineno-2-14" href="#security-docs-image-__codelineno-2-14"></a><span class="w">        </span><span class="s2">&quot;ecr:CompleteLayerUpload&quot;</span>
<a id="__codelineno-2-15" name="__codelineno-2-15" href="#security-docs-image-__codelineno-2-15"></a><span class="w">      </span><span class="p">],</span>
<a id="__codelineno-2-16" name="__codelineno-2-16" href="#security-docs-image-__codelineno-2-16"></a><span class="w">      </span><span class="nt">&quot;Resource&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<a id="__codelineno-2-17" name="__codelineno-2-17" href="#security-docs-image-__codelineno-2-17"></a><span class="w">        </span><span class="s2">&quot;arn:aws:ecr:&lt;region&gt;:&lt;account_id&gt;:repository/team-a/*&quot;</span>
<a id="__codelineno-2-18" name="__codelineno-2-18" href="#security-docs-image-__codelineno-2-18"></a><span class="w">      </span><span class="p">]</span>
<a id="__codelineno-2-19" name="__codelineno-2-19" href="#security-docs-image-__codelineno-2-19"></a><span class="w">    </span><span class="p">}</span>
<a id="__codelineno-2-20" name="__codelineno-2-20" href="#security-docs-image-__codelineno-2-20"></a><span class="w">  </span><span class="p">]</span>
<a id="__codelineno-2-21" name="__codelineno-2-21" href="#security-docs-image-__codelineno-2-21"></a><span class="p">}</span>
</code></pre></div>
<h3 id="security-docs-image-consider-using-ecr-private-endpoints">Consider using ECR private endpoints<a class="headerlink" href="#security-docs-image-consider-using-ecr-private-endpoints" title="Permanent link">&para;</a></h3>
<p>The ECR API has a public endpoint. Consequently, ECR registries can be accessed from the Internet so long as the request has been authenticated and authorized by IAM. For those who need to operate in a sandboxed environment where the cluster VPC lacks an Internet Gateway (IGW), you can configure a private endpoint for ECR. Creating a private endpoint enables you to privately access the ECR API through a private IP address instead of routing traffic across the Internet. For additional information on this topic, see <a href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/vpc-endpoints.html">Amazon ECR interface VPC endpoints</a>.</p>
<h3 id="security-docs-image-implement-endpoint-policies-for-ecr">Implement endpoint policies for ECR<a class="headerlink" href="#security-docs-image-implement-endpoint-policies-for-ecr" title="Permanent link">&para;</a></h3>
<p>The default endpoint policy for allows access to all ECR repositories within a region. This might allow an attacker/insider to exfiltrate data by packaging it as a container image and pushing it to a registry in another AWS account. Mitigating this risk involves creating an endpoint policy that limits API access to ECR repositories. For example, the following policy allows all AWS principles in your account to perform all actions against your and only your ECR repositories:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#security-docs-image-__codelineno-3-1"></a><span class="p">{</span>
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#security-docs-image-__codelineno-3-2"></a><span class="w">  </span><span class="nt">&quot;Statement&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#security-docs-image-__codelineno-3-3"></a><span class="w">    </span><span class="p">{</span>
<a id="__codelineno-3-4" name="__codelineno-3-4" href="#security-docs-image-__codelineno-3-4"></a><span class="w">      </span><span class="nt">&quot;Sid&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;LimitECRAccess&quot;</span><span class="p">,</span>
<a id="__codelineno-3-5" name="__codelineno-3-5" href="#security-docs-image-__codelineno-3-5"></a><span class="w">      </span><span class="nt">&quot;Principal&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;*&quot;</span><span class="p">,</span>
<a id="__codelineno-3-6" name="__codelineno-3-6" href="#security-docs-image-__codelineno-3-6"></a><span class="w">      </span><span class="nt">&quot;Action&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;*&quot;</span><span class="p">,</span>
<a id="__codelineno-3-7" name="__codelineno-3-7" href="#security-docs-image-__codelineno-3-7"></a><span class="w">      </span><span class="nt">&quot;Effect&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Allow&quot;</span><span class="p">,</span>
<a id="__codelineno-3-8" name="__codelineno-3-8" href="#security-docs-image-__codelineno-3-8"></a><span class="w">      </span><span class="nt">&quot;Resource&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;arn:aws:ecr:&lt;region&gt;:&lt;account_id&gt;:repository/*&quot;</span>
<a id="__codelineno-3-9" name="__codelineno-3-9" href="#security-docs-image-__codelineno-3-9"></a><span class="w">    </span><span class="p">}</span>
<a id="__codelineno-3-10" name="__codelineno-3-10" href="#security-docs-image-__codelineno-3-10"></a><span class="w">  </span><span class="p">]</span>
<a id="__codelineno-3-11" name="__codelineno-3-11" href="#security-docs-image-__codelineno-3-11"></a><span class="p">}</span>
</code></pre></div>
<p>You can enhance this further by setting a condition that uses the new <code>PrincipalOrgID</code> attribute which will prevent pushing/pulling of images by an IAM principle that is not part of your AWS Organization. See, <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html#condition-keys-principalorgid">aws:PrincipalOrgID</a> for additional details.
We recommended applying the same policy to both the <code>com.amazonaws.&lt;region&gt;.ecr.dkr</code> and the <code>com.amazonaws.&lt;region&gt;.ecr.api</code> endpoints.
Since EKS pulls images for kube-proxy, coredns, and aws-node from ECR, you will need to add the account ID of the registry, e.g. <code>602401143452.dkr.ecr.us-west-2.amazonaws.com/*</code> to the list of resources in the endpoint policy or alter the policy to allow pulls from "*" and restrict pushes to your account ID. The table below reveals the mapping between the AWS accounts where EKS images are vended from and cluster region.</p>
<table>
<thead>
<tr>
<th>Account Number</th>
<th>Region</th>
</tr>
</thead>
<tbody>
<tr>
<td>602401143452</td>
<td>All commercial regions except for those listed below</td>
</tr>
<tr>
<td>---</td>
<td>---</td>
</tr>
<tr>
<td>800184023465</td>
<td>ap-east-1 - Asia Pacific (Hong Kong)</td>
</tr>
<tr>
<td>558608220178</td>
<td>me-south-1 - Middle East (Bahrain)</td>
</tr>
<tr>
<td>918309763551</td>
<td>cn-north-1 - China (Beijing)</td>
</tr>
<tr>
<td>961992271922</td>
<td>cn-northwest-1 - China (Ningxia)</td>
</tr>
</tbody>
</table>
<p>For further information about using endpoint policies, see <a href="https://aws.amazon.com/blogs/containers/using-vpc-endpoint-policies-to-control-amazon-ecr-access/">Using VPC endpoint policies to control Amazon ECR access</a>.</p>
<h3 id="security-docs-image-implement-lifecycle-policies-for-ecr">Implement lifecycle policies for ECR<a class="headerlink" href="#security-docs-image-implement-lifecycle-policies-for-ecr" title="Permanent link">&para;</a></h3>
<p>The <a href="https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf">NIST Application Container Security Guide</a> warns about the risk of "stale images in registries", noting that over time old images with vulnerable, out-of-date software packages should be removed to prevent accidental deployment and exposure.
Each ECR repository can have a lifecycle policy that sets rules for when images expire. The <a href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/LifecyclePolicies.html">AWS official documentation</a> describes how to set up test rules, evaluate them and then apply them. There are several <a href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/lifecycle_policy_examples.html">lifecycle policy examples</a> in the official docs that show different ways of filtering the images in a repository:</p>
<ul>
<li>Filtering by image age or count</li>
<li>Filtering by tagged or untagged images</li>
<li>Filtering by image tags, either in multiple rules or a single rule</li>
</ul>
<details class="warning" open="open">
<summary>Warning</summary>
<p>If the image for long running application is purged from ECR, it can cause an image pull errors when the application is redeployed or scaled horizontally. When using image lifecycle policies, be sure you have good CI/CD practices in place to keep deployments and the images that they reference up to date and always create [image] expiry rules that account for how often you do releases/deployments.</p>
</details>
<h3 id="security-docs-image-create-a-set-of-curated-images">Create a set of curated images<a class="headerlink" href="#security-docs-image-create-a-set-of-curated-images" title="Permanent link">&para;</a></h3>
<p>Rather than allowing developers to create their own images, consider creating a set of vetted images for the different application stacks in your organization. By doing so, developers can forego learning how to compose Dockerfiles and concentrate on writing code. As changes are merged into Master, a CI/CD pipeline can automatically compile the asset, store it in an artifact repository and copy the artifact into the appropriate image before pushing it to a Docker registry like ECR. At the very least you should create a set of base images from which developers to create their own Dockerfiles. Ideally, you want to avoid pulling images from Dockerhub because a) you don't always know what is in the image and b) about <a href="https://www.kennasecurity.com/blog/one-fifth-of-the-most-used-docker-containers-have-at-least-one-critical-vulnerability/">a fifth</a> of the top 1000 images have vulnerabilities. A list of those images and their vulnerabilities can be found at https://vulnerablecontainers.org/.</p>
<h3 id="security-docs-image-add-the-user-directive-to-your-dockerfiles-to-run-as-a-non-root-user">Add the USER directive to your Dockerfiles to run as a non-root user<a class="headerlink" href="#security-docs-image-add-the-user-directive-to-your-dockerfiles-to-run-as-a-non-root-user" title="Permanent link">&para;</a></h3>
<p>As was mentioned in the pod security section, you should avoid running container as root. While you can configure this as part of the podSpec, it is a good habit to use the <code>USER</code> directive to your Dockerfiles. The <code>USER</code> directive sets the UID to use when running <code>RUN</code>, <code>ENTRYPOINT</code>, or <code>CMD</code> instruction that appears after the USER directive.</p>
<h3 id="security-docs-image-lint-your-dockerfiles">Lint your Dockerfiles<a class="headerlink" href="#security-docs-image-lint-your-dockerfiles" title="Permanent link">&para;</a></h3>
<p>Linting can be used to verify that your Dockerfiles are adhering to a set of predefined guidelines, e.g. the inclusion of the <code>USER</code> directive or the requirement that all images be tagged. <a href="https://github.com/projectatomic/dockerfile_lint">dockerfile_lint</a> is an open source project from RedHat that verifies common best practices and includes a rule engine that you can use to build your own rules for linting Dockerfiles. It can be incorporated into a CI pipeline, in that builds with Dockerfiles that violate a rule will automatically fail.</p>
<h3 id="security-docs-image-build-images-from-scratch">Build images from Scratch<a class="headerlink" href="#security-docs-image-build-images-from-scratch" title="Permanent link">&para;</a></h3>
<p>Reducing the attack surface of your container images should be primary aim when building images. The ideal way to do this is by creating minimal images that are devoid of binaries that can be used to exploit vulnerabilities. Fortunately, Docker has a mechanism to create images from <a href="https://docs.docker.com/develop/develop-images/baseimages/#create-a-simple-parent-image-using-scratch"><code>scratch</code></a>. With languages like Go, you can create a static linked binary and reference it in your Dockerfile as in this example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#security-docs-image-__codelineno-4-1"></a><span class="c">############################</span>
<a id="__codelineno-4-2" name="__codelineno-4-2" href="#security-docs-image-__codelineno-4-2"></a><span class="c"># STEP 1 build executable binary</span>
<a id="__codelineno-4-3" name="__codelineno-4-3" href="#security-docs-image-__codelineno-4-3"></a><span class="c">############################</span>
<a id="__codelineno-4-4" name="__codelineno-4-4" href="#security-docs-image-__codelineno-4-4"></a><span class="k">FROM</span><span class="w"> </span><span class="s">golang:alpine</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="s">builder#</span><span class="w"> </span>Install<span class="w"> </span>git.
<a id="__codelineno-4-5" name="__codelineno-4-5" href="#security-docs-image-__codelineno-4-5"></a><span class="c"># Git is required for fetching the dependencies.</span>
<a id="__codelineno-4-6" name="__codelineno-4-6" href="#security-docs-image-__codelineno-4-6"></a><span class="k">RUN</span><span class="w"> </span>apk<span class="w"> </span>update<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>apk<span class="w"> </span>add<span class="w"> </span>--no-cache<span class="w"> </span>gitWORKDIR<span class="w"> </span><span class="nv">$GOPATH</span>/src/mypackage/myapp/COPY<span class="w"> </span>.<span class="w"> </span>.<span class="w"> </span>#<span class="w"> </span>Fetch<span class="w"> </span>dependencies.
<a id="__codelineno-4-7" name="__codelineno-4-7" href="#security-docs-image-__codelineno-4-7"></a><span class="c"># Using go get.</span>
<a id="__codelineno-4-8" name="__codelineno-4-8" href="#security-docs-image-__codelineno-4-8"></a><span class="k">RUN</span><span class="w"> </span>go<span class="w"> </span>get<span class="w"> </span>-d<span class="w"> </span>-v#<span class="w"> </span>Build<span class="w"> </span>the<span class="w"> </span>binary.
<a id="__codelineno-4-9" name="__codelineno-4-9" href="#security-docs-image-__codelineno-4-9"></a><span class="k">RUN</span><span class="w"> </span>go<span class="w"> </span>build<span class="w"> </span>-o<span class="w"> </span>/go/bin/hello
<a id="__codelineno-4-10" name="__codelineno-4-10" href="#security-docs-image-__codelineno-4-10"></a>
<a id="__codelineno-4-11" name="__codelineno-4-11" href="#security-docs-image-__codelineno-4-11"></a><span class="c">############################</span>
<a id="__codelineno-4-12" name="__codelineno-4-12" href="#security-docs-image-__codelineno-4-12"></a><span class="c"># STEP 2 build a small image</span>
<a id="__codelineno-4-13" name="__codelineno-4-13" href="#security-docs-image-__codelineno-4-13"></a><span class="c">############################</span>
<a id="__codelineno-4-14" name="__codelineno-4-14" href="#security-docs-image-__codelineno-4-14"></a><span class="k">FROM</span><span class="w"> </span><span class="s">scratch#</span><span class="w"> </span><span class="k">Copy</span><span class="w"> </span>our<span class="w"> </span>static<span class="w"> </span>executable.
<a id="__codelineno-4-15" name="__codelineno-4-15" href="#security-docs-image-__codelineno-4-15"></a><span class="k">COPY</span><span class="w"> </span>--from<span class="o">=</span>builder<span class="w"> </span>/go/bin/hello<span class="w"> </span>/go/bin/hello#<span class="w"> </span>Run<span class="w"> </span>the<span class="w"> </span>hello<span class="w"> </span>binary.
<a id="__codelineno-4-16" name="__codelineno-4-16" href="#security-docs-image-__codelineno-4-16"></a><span class="k">ENTRYPOINT</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;/go/bin/hello&quot;</span><span class="p">]</span>
</code></pre></div>
<p>This creates a container image that consists of your application and nothing else, making it extremely secure.</p>
<h3 id="security-docs-image-use-immutable-tags-with-ecr">Use immutable tags with ECR<a class="headerlink" href="#security-docs-image-use-immutable-tags-with-ecr" title="Permanent link">&para;</a></h3>
<p><a href="https://aws.amazon.com/about-aws/whats-new/2019/07/amazon-ecr-now-supports-immutable-image-tags/">Immutable tags</a> force you to update the image tag on each push to the image repository. This can thwart an attacker from overwriting an image with a malicious version without changing the image's tags. Additionally, it gives you a way to easily and uniquely identify an image.</p>
<h3 id="security-docs-image-sign-your-images-sboms-pipeline-runs-and-vulnerability-reports">Sign your images, SBOMs, pipeline runs and vulnerability reports<a class="headerlink" href="#security-docs-image-sign-your-images-sboms-pipeline-runs-and-vulnerability-reports" title="Permanent link">&para;</a></h3>
<p>When Docker was first introduced, there was no cryptographic model for verifying container images. With v2, Docker added digests to the image manifest. This allowed an image’s configuration to be hashed and for the hash to be used to generate an ID for the image. When image signing is enabled, the Docker engine verifies the manifest’s signature, ensuring that the content was produced from a trusted source and no tampering has occurred. After each layer is downloaded, the engine verifies the digest of the layer, ensuring that the content matches the content specified in the manifest. Image signing effectively allows you to create a secure supply chain, through the verification of digital signatures associated with the image.</p>
<p>We can use <a href="https://docs.aws.amazon.com/signer/latest/developerguide/Welcome.html">AWS Signer</a> or <a href="https://github.com/sigstore/cosign">Sigstore Cosign</a>, to sign container images, create attestations for SBOMs, vulnerability scan reports and pipeline run reports. These attestations assure the trustworthiness and integrity of the image, that it is in fact created by the trusted pipeline without any interference or tampering, and that it contains only the software components that are documented (in the SBOM) that is verified and trusted by the image publisher. These attestations can be attached to the container image and pushed to the repository.</p>
<p>In the next section we will see how to use the attested artifacts for audits and admissions controller verification.</p>
<h3 id="security-docs-image-image-integrity-verification-using-kubernetes-admission-controller">Image integrity verification using Kubernetes admission controller<a class="headerlink" href="#security-docs-image-image-integrity-verification-using-kubernetes-admission-controller" title="Permanent link">&para;</a></h3>
<p>We can verify image signatures, attested artifacts in an automated way before deploying the image to target Kubernetes cluster using <a href="https://kubernetes.io/blog/2019/03/21/a-guide-to-kubernetes-admission-controllers/">dynamic admission controller</a> and admit deployments only when the security metadata of the artifacts comply with the admission controller policies.</p>
<p>For example we can write a policy that cryptographically verifies the signature of an image, an attested SBOM, attested pipeline run report, or attested CVE scan report. We can write conditions in the policy to check data in the report, e.g. a CVE scan should not have any critical CVEs. Deployment is allowed only for images that satisfy these conditions and all other deployments will be rejected by the admissions controller.</p>
<p>Examples of admission controller include:</p>
<ul>
<li><a href="https://kyverno.io/">Kyverno</a></li>
<li><a href="https://github.com/open-policy-agent/gatekeeper">OPA Gatekeeper</a></li>
<li><a href="https://github.com/IBM/portieris">Portieris</a></li>
<li><a href="https://github.com/deislabs/ratify">Ratify</a></li>
<li><a href="https://github.com/grafeas/kritis">Kritis</a></li>
<li><a href="https://github.com/kelseyhightower/grafeas-tutorial">Grafeas tutorial</a></li>
<li><a href="https://github.com/Shopify/voucher">Voucher</a></li>
</ul>
<h3 id="security-docs-image-update-the-packages-in-your-container-images">Update the packages in your container images<a class="headerlink" href="#security-docs-image-update-the-packages-in-your-container-images" title="Permanent link">&para;</a></h3>
<p>You should include RUN <code>apt-get update &amp;&amp; apt-get upgrade</code> in your Dockerfiles to upgrade the packages in your images. Although upgrading requires you to run as root, this occurs during image build phase. The application doesn't need to run as root. You can install the updates and then switch to a different user with the USER directive. If your base image runs as a non-root user, switch to root and back; don't solely rely on the maintainers of the base image to install the latest security updates.</p>
<p>Run <code>apt-get clean</code> to delete the installer files from <code>/var/cache/apt/archives/</code>. You can also run <code>rm -rf /var/lib/apt/lists/*</code> after installing packages. This removes the index files or the lists of packages that are available to install. Be aware that these commands may be different for each package manager. For example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#security-docs-image-__codelineno-5-1"></a><span class="k">RUN</span><span class="w"> </span>apt-get<span class="w"> </span>update<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>apt-get<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-5-2" name="__codelineno-5-2" href="#security-docs-image-__codelineno-5-2"></a><span class="w">    </span>curl<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-5-3" name="__codelineno-5-3" href="#security-docs-image-__codelineno-5-3"></a><span class="w">    </span>git<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-5-4" name="__codelineno-5-4" href="#security-docs-image-__codelineno-5-4"></a><span class="w">    </span>libsqlite3-dev<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-5-5" name="__codelineno-5-5" href="#security-docs-image-__codelineno-5-5"></a><span class="w">    </span><span class="o">&amp;&amp;</span><span class="w"> </span>apt-get<span class="w"> </span>clean<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>rm<span class="w"> </span>-rf<span class="w"> </span>/var/lib/apt/lists/*
</code></pre></div>
<h2 id="security-docs-image-tools">Tools<a class="headerlink" href="#security-docs-image-tools" title="Permanent link">&para;</a></h2>
<ul>
<li><a href="https://github.com/docker-slim/docker-slim">docker-slim</a> Build secure minimal images</li>
<li><a href="https://github.com/goodwithtech/dockle">dockle</a> Verifies that your Dockerfile aligns with best practices for creating secure images</li>
<li><a href="https://github.com/projectatomic/dockerfile_lint">dockerfile-lint</a> Rule based linter for Dockerfiles</li>
<li><a href="https://github.com/hadolint/hadolint">hadolint</a> A smart dockerfile linter</li>
<li><a href="https://github.com/open-policy-agent/gatekeeper">Gatekeeper and OPA</a> A policy based admission controller</li>
<li><a href="https://kyverno.io/">Kyverno</a> A Kubernetes-native policy engine</li>
<li><a href="https://in-toto.io/">in-toto</a> Allows the user to verify if a step in the supply chain was intended to be performed, and if the step was performed by the right actor</li>
<li><a href="https://github.com/theupdateframework/notary">Notary</a> A project for signing container images</li>
<li><a href="https://github.com/notaryproject/nv2">Notary v2</a></li>
<li><a href="https://grafeas.io/">Grafeas</a> An open artifact metadata API to audit and govern your software supply chain</li>
</ul></section><h1 class='nav-section-title-end'>Ended: Security</h1>
                        <h1 class='nav-section-title' id='section-cluster-autoscaling'>
                            Cluster Autoscaling <a class='headerlink' href='#section-cluster-autoscaling' title='Permanent link'>↵</a>
                        </h1>
                        <section class="print-page" id="karpenter"><h1 id="karpenter-karpenter-best-practices">Karpenter Best Practices<a class="headerlink" href="#karpenter-karpenter-best-practices" title="Permanent link">&para;</a></h1>
<h2 id="karpenter-karpenter">Karpenter<a class="headerlink" href="#karpenter-karpenter" title="Permanent link">&para;</a></h2>
<p>Karpenter is an open-source cluster autoscaler that automatically provisions new nodes in response to unschedulable pods. Karpenter evaluates the aggregate resource requirements of the pending pods and chooses the optimal instance type to run them. It will automatically scale-in or terminate instances that don’t have any non-daemonset pods to reduce waste. It also supports a consolidation feature which will actively move pods around and either delete or replace nodes with cheaper versions to reduce cluster cost.</p>
<p><strong>Reasons to use Karpenter</strong></p>
<p>Before the launch of Karpenter, Kubernetes users relied primarily on <a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html">Amazon EC2 Auto Scaling groups</a> and the <a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler">Kubernetes Cluster Autoscaler</a> (CAS) to dynamically adjust the compute capacity of their clusters. With Karpenter, you don’t need to create dozens of node groups to achieve the flexibility and diversity you get with Karpenter. Moreover, Karpenter is not as tightly coupled to Kubernetes versions (as CAS is) and doesn’t require you to jump between AWS and Kubernetes APIs.</p>
<p>Karpenter consolidates instance orchestration responsibilities within a single system, which is simpler, more stable and cluster-aware. Karpenter was designed to overcome some of the challenges presented by Cluster Autoscaler by providing simplified ways to:</p>
<ul>
<li>Provision nodes based on workload requirements.</li>
<li>Create diverse node configurations by instance type, using flexible workload provisioner options. Instead of managing many specific custom node groups, Karpenter could let you manage diverse workload capacity with a single, flexible provisioner.</li>
<li>Achieve improved pod scheduling at scale by quickly launching nodes and scheduling pods.</li>
</ul>
<p>For information and documentation on using Karpenter, visit the <a href="https://karpenter.sh/">karpenter.sh</a> site.</p>
<h2 id="karpenter-recommendations">Recommendations<a class="headerlink" href="#karpenter-recommendations" title="Permanent link">&para;</a></h2>
<p>Best practices are divided into sections on Karpenter itself, provisioners, and pod scheduling.</p>
<h2 id="karpenter-karpenter-best-practices_1">Karpenter best practices<a class="headerlink" href="#karpenter-karpenter-best-practices_1" title="Permanent link">&para;</a></h2>
<p>The following best practices cover topics related to Karpenter itself.</p>
<h3 id="karpenter-use-karpenter-for-workloads-with-changing-capacity-needs">Use Karpenter for workloads with changing capacity needs<a class="headerlink" href="#karpenter-use-karpenter-for-workloads-with-changing-capacity-needs" title="Permanent link">&para;</a></h3>
<p>Karpenter brings scaling management closer to Kubernetes native APIs than do <a href="https://aws.amazon.com/blogs/containers/amazon-eks-cluster-multi-zone-auto-scaling-groups/">Autoscaling Groups</a> (ASGs) and <a href="https://docs.aws.amazon.com/eks/latest/userguide/managed-node-groups.html">Managed Node Groups</a> (MNGs). ASGs and MNGs are AWS-native abstractions where scaling is triggered based on AWS level metrics, such as EC2 CPU load. <a href="https://docs.aws.amazon.com/eks/latest/userguide/autoscaling.html#cluster-autoscaler">Cluster Autoscaler</a> bridges the Kubernetes abstractions into AWS abstractions, but loses some flexibility because of that, such as scheduling for a specific availability zone.</p>
<p>Karpenter removes a layer of AWS abstraction to bring some of the flexibility directly into Kubernetes. Karpenter is best used for clusters with workloads that encounter periods of high, spiky demand or have diverse compute requirements. MNGs and ASGs are good for clusters running workloads that tend to be more static and consistent.  You can use a mix of dynamically and statically managed nodes, depending on your requirements.</p>
<h3 id="karpenter-consider-other-autoscaling-projects-when">Consider other autoscaling projects when...<a class="headerlink" href="#karpenter-consider-other-autoscaling-projects-when" title="Permanent link">&para;</a></h3>
<p>You need features that are still being developed in Karpenter. Because Karpenter is a relatively new project, consider other autoscaling projects for the time being if you have a need for features that are not yet part of Karpenter.</p>
<h3 id="karpenter-run-the-karpenter-controller-on-eks-fargate-or-on-a-worker-node-that-belongs-to-a-node-group">Run the Karpenter controller on EKS Fargate or on a worker node that belongs to a node group<a class="headerlink" href="#karpenter-run-the-karpenter-controller-on-eks-fargate-or-on-a-worker-node-that-belongs-to-a-node-group" title="Permanent link">&para;</a></h3>
<p>Karpenter is installed using a <a href="https://karpenter.sh/docs/getting-started/">Helm chart</a>. The Helm chart installs the Karpenter controller and a webhook pod as a Deployment that needs to run before the controller can be used for scaling your cluster. We recommend a minimum of one small node group with at least one worker node. As an alternative, you can run these pods on EKS Fargate by creating a Fargate profile for the <code>karpenter</code> namespace. Doing so will cause all pods deployed into this namespace to run on EKS Fargate. Do not run Karpenter on a node that is managed by Karpenter.</p>
<h3 id="karpenter-avoid-using-custom-launch-templates-with-karpenter">Avoid using custom launch templates with Karpenter<a class="headerlink" href="#karpenter-avoid-using-custom-launch-templates-with-karpenter" title="Permanent link">&para;</a></h3>
<p>Karpenter strongly recommends against using custom launch templates. Using custom launch templates prevents multi-architecture support, the ability to automatically upgrade nodes, and securityGroup discovery. Using launch templates may also cause confusion because certain fields are duplicated within Karpenter’s provisioners while others are ignored by Karpenter, e.g. subnets and instance types.</p>
<p>You can often avoid using launch templates by using custom user data and/or directly specifying custom AMIs in the AWS node template.  More information on how to do this is available at <a href="https://karpenter.sh/docs/concepts/node-templates">Node Templates</a>.</p>
<h3 id="karpenter-exclude-instance-types-that-do-not-fit-your-workload">Exclude instance types that do not fit your workload<a class="headerlink" href="#karpenter-exclude-instance-types-that-do-not-fit-your-workload" title="Permanent link">&para;</a></h3>
<p>Consider excluding specific instances types with the <a href="http://node.kubernetes.io/instance-type">node.kubernetes.io/instance-type</a> key if they are not required by workloads running in your cluster.</p>
<p>The following example shows how to avoid provisioning large Graviton instances.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#karpenter-__codelineno-0-1"></a><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">node.kubernetes.io/instance-type</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#karpenter-__codelineno-0-2"></a><span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">operator</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">NotIn</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#karpenter-__codelineno-0-3"></a><span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">values</span><span class="p p-Indicator">:</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#karpenter-__codelineno-0-4"></a><span class="w">      </span><span class="s">&#39;m6g.16xlarge&#39;</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#karpenter-__codelineno-0-5"></a><span class="w">      </span><span class="s">&#39;m6gd.16xlarge&#39;</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#karpenter-__codelineno-0-6"></a><span class="w">      </span><span class="s">&#39;r6g.16xlarge&#39;</span>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#karpenter-__codelineno-0-7"></a><span class="w">      </span><span class="s">&#39;r6gd.16xlarge&#39;</span>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#karpenter-__codelineno-0-8"></a><span class="w">      </span><span class="s">&#39;c6g.16xlarge&#39;</span>
</code></pre></div>
<h3 id="karpenter-enable-interruption-handling-when-using-spot">Enable Interruption Handling when using Spot<a class="headerlink" href="#karpenter-enable-interruption-handling-when-using-spot" title="Permanent link">&para;</a></h3>
<p>Karpenter supports <a href="https://karpenter.sh/docs/concepts/deprovisioning/#interruption">native interruption handling</a>, enabled through the <code>aws.interruptionQueue</code> value in <a href="https://karpenter.sh/docs/concepts/settings/#configmap">Karpenter settings</a>. Interruption handling watches for upcoming involuntary interruption events that would cause disruption to your workloads such as:</p>
<ul>
<li>Spot Interruption Warnings</li>
<li>Scheduled Change Health Events (Maintenance Events)</li>
<li>Instance Terminating Events</li>
<li>Instance Stopping Events</li>
</ul>
<p>When Karpenter detects one of these events will occur to your nodes, it automatically cordons, drains, and terminates the node(s) ahead of the interruption event to give the maximum amount of time for workload cleanup prior to interruption. It is not advised to use AWS Node Termination Handler alongside Karpenter as explained <a href="https://karpenter.sh/docs/faq/#interruption-handling">here</a>.</p>
<p>Pods that require checkpointing or other forms of graceful draining, requiring the 2-mins before shutdown should enable Karpenter interruption handling in their clusters.</p>
<h3 id="karpenter-amazon-eks-private-cluster-without-outbound-internet-access"><strong>Amazon EKS private cluster without outbound internet access</strong><a class="headerlink" href="#karpenter-amazon-eks-private-cluster-without-outbound-internet-access" title="Permanent link">&para;</a></h3>
<p>When provisioning an EKS Cluster into a VPC with no route to the internet, you have to make sure you’ve configured your environment in accordance with the private cluster <a href="https://docs.aws.amazon.com/eks/latest/userguide/private-clusters.html#private-cluster-requirements">requirements</a> that appear in EKS documentation. In addition, you need to make sure you’ve created an STS VPC regional endpoint in your VPC. If not, you will see errors similar to those that appear below.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#karpenter-__codelineno-1-1"></a><span class="go">ERROR controller.controller.metrics Reconciler error {&quot;commit&quot;: &quot;5047f3c&quot;, &quot;reconciler group&quot;: &quot;karpenter.sh&quot;, &quot;reconciler kind&quot;: &quot;Provisioner&quot;, &quot;name&quot;: &quot;default&quot;, &quot;namespace&quot;: &quot;&quot;, &quot;error&quot;: &quot;fetching instance types using ec2.DescribeInstanceTypes, WebIdentityErr: failed to retrieve credentials\ncaused by: RequestError: send request failed\ncaused by: Post \&quot;https://sts.&lt;region&gt;.amazonaws.com/\&quot;: dial tcp x.x.x.x:443: i/o timeout&quot;}</span>
</code></pre></div>
<p>These changes are necessary in a private cluster because the Karpenter Controller uses IAM Roles for Service Accounts (IRSA). Pods configured with IRSA acquire credentials by calling the AWS Security Token Service (AWS STS) API. If there is no outbound internet access, you must create and use an <strong><em>AWS STS VPC endpoint in your VPC</em></strong>.</p>
<p>Private clusters also require you to create a <strong><em>VPC endpoint for SSM</em></strong>. When Karpenter tries to provision a new node, it queries the Launch template configs and an SSM parameter. If you do not have a SSM VPC endpoint in your VPC, it will cause the following error:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#karpenter-__codelineno-2-1"></a><span class="go">INFO    controller.provisioning Waiting for unschedulable pods  {&quot;commit&quot;: &quot;5047f3c&quot;, &quot;provisioner&quot;: &quot;default&quot;}</span>
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#karpenter-__codelineno-2-2"></a><span class="go">INFO    controller.provisioning Batched 3 pods in 1.000572709s  {&quot;commit&quot;: &quot;5047f3c&quot;, &quot;provisioner&quot;: &quot;default&quot;}</span>
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#karpenter-__codelineno-2-3"></a><span class="go">INFO    controller.provisioning Computed packing of 1 node(s) for 3 pod(s) with instance type option(s) [c4.xlarge c6i.xlarge c5.xlarge c5d.xlarge c5a.xlarge c5n.xlarge m6i.xlarge m4.xlarge m6a.xlarge m5ad.xlarge m5d.xlarge t3.xlarge m5a.xlarge t3a.xlarge m5.xlarge r4.xlarge r3.xlarge r5ad.xlarge r6i.xlarge r5a.xlarge]        {&quot;commit&quot;: &quot;5047f3c&quot;, &quot;provisioner&quot;: &quot;default&quot;}</span>
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#karpenter-__codelineno-2-4"></a><span class="go">ERROR   controller.provisioning Could not launch node, launching instances, getting launch template configs, getting launch templates, getting ssm parameter, RequestError: send request failed</span>
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#karpenter-__codelineno-2-5"></a><span class="go">caused by: Post &quot;https://ssm.&lt;region&gt;.amazonaws.com/&quot;: dial tcp x.x.x.x:443: i/o timeout  {&quot;commit&quot;: &quot;5047f3c&quot;, &quot;provisioner&quot;: &quot;default&quot;}</span>
</code></pre></div>
<p>There is no <strong><em>VPC endpoint for the <a href="https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/using-pelong.html">Price List Query API</a></em></strong>.
As a result, pricing data will go stale over time.
Karpenter gets around this by including on-demand pricing data in its binary, but only updates that data when Karpenter is upgraded.
Failed requests for pricing data will result in the following error messages:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#karpenter-__codelineno-3-1"></a><span class="go">ERROR   controller.aws.pricing  updating on-demand pricing, RequestError: send request failed</span>
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#karpenter-__codelineno-3-2"></a><span class="go">caused by: Post &quot;https://api.pricing.us-east-1.amazonaws.com/&quot;: dial tcp 52.94.231.236:443: i/o timeout; RequestError: send request failed</span>
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#karpenter-__codelineno-3-3"></a><span class="go">caused by: Post &quot;https://api.pricing.us-east-1.amazonaws.com/&quot;: dial tcp 52.94.231.236:443: i/o timeout, using existing pricing data from 2022-08-17T00:19:52Z  {&quot;commit&quot;: &quot;4b5f953&quot;}</span>
</code></pre></div>
<p>In summary, to use Karpenter in a completely Private EKS Clusters, you need to create the following VPC endpoints:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#karpenter-__codelineno-4-1"></a><span class="go">com.amazonaws.&lt;region&gt;.ec2</span>
<a id="__codelineno-4-2" name="__codelineno-4-2" href="#karpenter-__codelineno-4-2"></a><span class="go">com.amazonaws.&lt;region&gt;.ecr.api</span>
<a id="__codelineno-4-3" name="__codelineno-4-3" href="#karpenter-__codelineno-4-3"></a><span class="go">com.amazonaws.&lt;region&gt;.ecr.dkr</span>
<a id="__codelineno-4-4" name="__codelineno-4-4" href="#karpenter-__codelineno-4-4"></a><span class="go">com.amazonaws.&lt;region&gt;.s3 – For pulling container images</span>
<a id="__codelineno-4-5" name="__codelineno-4-5" href="#karpenter-__codelineno-4-5"></a><span class="go">com.amazonaws.&lt;region&gt;.sts – For IAM roles for service accounts</span>
<a id="__codelineno-4-6" name="__codelineno-4-6" href="#karpenter-__codelineno-4-6"></a><span class="go">com.amazonaws.&lt;region&gt;.ssm - For resolving default AMIs</span>
<a id="__codelineno-4-7" name="__codelineno-4-7" href="#karpenter-__codelineno-4-7"></a><span class="go">com.amazonaws.&lt;region&gt;.sqs - For accessing SQS if using interruption handling</span>
</code></pre></div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Karpenter (controller and webhook deployment) container images must be in or copied to Amazon ECR private or to a another private registry accessible from inside the VPC. The reason for this is that the Karpenter controller and webhook pods currently use Public ECR images. If these are not available from within the VPC, or from networks peered with the VPC, you will get Image pull errors when Kubernetes tries to pull these images from ECR public.</p>
</div>
<p>For further information, see <a href="https://github.com/aws/karpenter/issues/988">Issue 988</a> and <a href="https://github.com/aws/karpenter/issues/1157">Issue 1157</a>.</p>
<h2 id="karpenter-creating-provisioners">Creating provisioners<a class="headerlink" href="#karpenter-creating-provisioners" title="Permanent link">&para;</a></h2>
<p>The following best practices cover topics related to creating provisioners.</p>
<h3 id="karpenter-create-multiple-provisioners-when">Create multiple provisioners when...<a class="headerlink" href="#karpenter-create-multiple-provisioners-when" title="Permanent link">&para;</a></h3>
<p>When different teams are sharing a cluster and need to run their workloads on different worker nodes, or have different OS or instance type requirements, create multiple provisioners. For example, one team may want to use Bottlerocket, while another may want to use Amazon Linux. Likewise, one team might have access to expensive GPU hardware that wouldn’t be needed by another team. Using multiple provisioners makes sure that the most appropriate assets are available to each team.</p>
<h3 id="karpenter-create-provisioners-that-are-mutually-exclusive-or-weighted">Create provisioners that are mutually exclusive or weighted<a class="headerlink" href="#karpenter-create-provisioners-that-are-mutually-exclusive-or-weighted" title="Permanent link">&para;</a></h3>
<p>It is recommended to create Provisioners that are either mutually exclusive or weighted to provide consistent scheduling behavior. If they are not and multiple Provisioners are matched, Karpenter will randomly choose which to use, causing unexpected results. Useful examples for creating multiple provisioners include the following:</p>
<p>Creating a Provisioner with GPU and only allowing special workloads to run on these (expensive) nodes:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#karpenter-__codelineno-5-1"></a><span class="c1"># Provisioner for GPU Instances with Taints</span>
<a id="__codelineno-5-2" name="__codelineno-5-2" href="#karpenter-__codelineno-5-2"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">karpenter.sh/v1alpha5</span>
<a id="__codelineno-5-3" name="__codelineno-5-3" href="#karpenter-__codelineno-5-3"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Provisioner</span>
<a id="__codelineno-5-4" name="__codelineno-5-4" href="#karpenter-__codelineno-5-4"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-5-5" name="__codelineno-5-5" href="#karpenter-__codelineno-5-5"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gpu</span>
<a id="__codelineno-5-6" name="__codelineno-5-6" href="#karpenter-__codelineno-5-6"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-5-7" name="__codelineno-5-7" href="#karpenter-__codelineno-5-7"></a><span class="w">  </span><span class="nt">requirements</span><span class="p">:</span>
<a id="__codelineno-5-8" name="__codelineno-5-8" href="#karpenter-__codelineno-5-8"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">node.kubernetes.io/instance-type</span>
<a id="__codelineno-5-9" name="__codelineno-5-9" href="#karpenter-__codelineno-5-9"></a><span class="w">    </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">In</span>
<a id="__codelineno-5-10" name="__codelineno-5-10" href="#karpenter-__codelineno-5-10"></a><span class="w">    </span><span class="nt">values</span><span class="p">:</span>
<a id="__codelineno-5-11" name="__codelineno-5-11" href="#karpenter-__codelineno-5-11"></a><span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">p3.8xlarge</span>
<a id="__codelineno-5-12" name="__codelineno-5-12" href="#karpenter-__codelineno-5-12"></a><span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">p3.16xlarge</span>
<a id="__codelineno-5-13" name="__codelineno-5-13" href="#karpenter-__codelineno-5-13"></a><span class="w">  </span><span class="nt">taints</span><span class="p">:</span>
<a id="__codelineno-5-14" name="__codelineno-5-14" href="#karpenter-__codelineno-5-14"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">effect</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">NoSchedule</span>
<a id="__codelineno-5-15" name="__codelineno-5-15" href="#karpenter-__codelineno-5-15"></a><span class="w">    </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvidia.com/gpu</span>
<a id="__codelineno-5-16" name="__codelineno-5-16" href="#karpenter-__codelineno-5-16"></a><span class="w">    </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;true&quot;</span>
<a id="__codelineno-5-17" name="__codelineno-5-17" href="#karpenter-__codelineno-5-17"></a><span class="w">  </span><span class="nt">ttlSecondsAfterEmpty</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">60</span>
</code></pre></div>
<p>Deployment with toleration for the taint:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#karpenter-__codelineno-6-1"></a><span class="c1"># Deployment of GPU Workload will have tolerations defined</span>
<a id="__codelineno-6-2" name="__codelineno-6-2" href="#karpenter-__codelineno-6-2"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">apps/v1</span>
<a id="__codelineno-6-3" name="__codelineno-6-3" href="#karpenter-__codelineno-6-3"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deployment</span>
<a id="__codelineno-6-4" name="__codelineno-6-4" href="#karpenter-__codelineno-6-4"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-6-5" name="__codelineno-6-5" href="#karpenter-__codelineno-6-5"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">inflate-gpu</span>
<a id="__codelineno-6-6" name="__codelineno-6-6" href="#karpenter-__codelineno-6-6"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-6-7" name="__codelineno-6-7" href="#karpenter-__codelineno-6-7"></a><span class="w">  </span><span class="l l-Scalar l-Scalar-Plain">...</span>
<a id="__codelineno-6-8" name="__codelineno-6-8" href="#karpenter-__codelineno-6-8"></a><span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">spec</span><span class="p p-Indicator">:</span>
<a id="__codelineno-6-9" name="__codelineno-6-9" href="#karpenter-__codelineno-6-9"></a><span class="w">      </span><span class="nt">tolerations</span><span class="p">:</span>
<a id="__codelineno-6-10" name="__codelineno-6-10" href="#karpenter-__codelineno-6-10"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;nvidia.com/gpu&quot;</span>
<a id="__codelineno-6-11" name="__codelineno-6-11" href="#karpenter-__codelineno-6-11"></a><span class="w">        </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Exists&quot;</span>
<a id="__codelineno-6-12" name="__codelineno-6-12" href="#karpenter-__codelineno-6-12"></a><span class="w">        </span><span class="nt">effect</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;NoSchedule&quot;</span>
</code></pre></div>
<p>For a general deployment for another team, the provisioner spec could include nodeAffinify. A Deployment could then use nodeSelectorTerms to match <code>billing-team</code>.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#karpenter-__codelineno-7-1"></a><span class="c1"># Provisioner for regular EC2 instances</span>
<a id="__codelineno-7-2" name="__codelineno-7-2" href="#karpenter-__codelineno-7-2"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">karpenter.sh/v1alpha5</span>
<a id="__codelineno-7-3" name="__codelineno-7-3" href="#karpenter-__codelineno-7-3"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Provisioner</span>
<a id="__codelineno-7-4" name="__codelineno-7-4" href="#karpenter-__codelineno-7-4"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-7-5" name="__codelineno-7-5" href="#karpenter-__codelineno-7-5"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">generalcompute</span>
<a id="__codelineno-7-6" name="__codelineno-7-6" href="#karpenter-__codelineno-7-6"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-7-7" name="__codelineno-7-7" href="#karpenter-__codelineno-7-7"></a><span class="w">  </span><span class="nt">labels</span><span class="p">:</span>
<a id="__codelineno-7-8" name="__codelineno-7-8" href="#karpenter-__codelineno-7-8"></a><span class="w">    </span><span class="nt">billing-team</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">my-team</span>
<a id="__codelineno-7-9" name="__codelineno-7-9" href="#karpenter-__codelineno-7-9"></a><span class="w">  </span><span class="nt">requirements</span><span class="p">:</span>
<a id="__codelineno-7-10" name="__codelineno-7-10" href="#karpenter-__codelineno-7-10"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">node.kubernetes.io/instance-type</span>
<a id="__codelineno-7-11" name="__codelineno-7-11" href="#karpenter-__codelineno-7-11"></a><span class="w">    </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">In</span>
<a id="__codelineno-7-12" name="__codelineno-7-12" href="#karpenter-__codelineno-7-12"></a><span class="w">    </span><span class="nt">values</span><span class="p">:</span>
<a id="__codelineno-7-13" name="__codelineno-7-13" href="#karpenter-__codelineno-7-13"></a><span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">m5.large</span>
<a id="__codelineno-7-14" name="__codelineno-7-14" href="#karpenter-__codelineno-7-14"></a><span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">m5.xlarge</span>
<a id="__codelineno-7-15" name="__codelineno-7-15" href="#karpenter-__codelineno-7-15"></a><span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">m5.2xlarge</span>
<a id="__codelineno-7-16" name="__codelineno-7-16" href="#karpenter-__codelineno-7-16"></a><span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">c5.large</span>
<a id="__codelineno-7-17" name="__codelineno-7-17" href="#karpenter-__codelineno-7-17"></a><span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">c5.xlarge</span>
<a id="__codelineno-7-18" name="__codelineno-7-18" href="#karpenter-__codelineno-7-18"></a><span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">c5a.large</span>
<a id="__codelineno-7-19" name="__codelineno-7-19" href="#karpenter-__codelineno-7-19"></a><span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">c5a.xlarge</span>
<a id="__codelineno-7-20" name="__codelineno-7-20" href="#karpenter-__codelineno-7-20"></a><span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">r5.large</span>
<a id="__codelineno-7-21" name="__codelineno-7-21" href="#karpenter-__codelineno-7-21"></a><span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">r5.xlarge</span>
</code></pre></div>
<p>Deployment using nodeAffinity:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1" href="#karpenter-__codelineno-8-1"></a><span class="c1"># Deployment will have spec.affinity.nodeAffinity defined</span>
<a id="__codelineno-8-2" name="__codelineno-8-2" href="#karpenter-__codelineno-8-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deployment</span>
<a id="__codelineno-8-3" name="__codelineno-8-3" href="#karpenter-__codelineno-8-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-8-4" name="__codelineno-8-4" href="#karpenter-__codelineno-8-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">workload-my-team</span>
<a id="__codelineno-8-5" name="__codelineno-8-5" href="#karpenter-__codelineno-8-5"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-8-6" name="__codelineno-8-6" href="#karpenter-__codelineno-8-6"></a><span class="w">  </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">200</span>
<a id="__codelineno-8-7" name="__codelineno-8-7" href="#karpenter-__codelineno-8-7"></a><span class="w">  </span><span class="l l-Scalar l-Scalar-Plain">...</span>
<a id="__codelineno-8-8" name="__codelineno-8-8" href="#karpenter-__codelineno-8-8"></a><span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">spec</span><span class="p p-Indicator">:</span>
<a id="__codelineno-8-9" name="__codelineno-8-9" href="#karpenter-__codelineno-8-9"></a><span class="w">      </span><span class="nt">affinity</span><span class="p">:</span>
<a id="__codelineno-8-10" name="__codelineno-8-10" href="#karpenter-__codelineno-8-10"></a><span class="w">        </span><span class="nt">nodeAffinity</span><span class="p">:</span>
<a id="__codelineno-8-11" name="__codelineno-8-11" href="#karpenter-__codelineno-8-11"></a><span class="w">          </span><span class="nt">requiredDuringSchedulingIgnoredDuringExecution</span><span class="p">:</span>
<a id="__codelineno-8-12" name="__codelineno-8-12" href="#karpenter-__codelineno-8-12"></a><span class="w">            </span><span class="nt">nodeSelectorTerms</span><span class="p">:</span>
<a id="__codelineno-8-13" name="__codelineno-8-13" href="#karpenter-__codelineno-8-13"></a><span class="w">              </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">matchExpressions</span><span class="p">:</span>
<a id="__codelineno-8-14" name="__codelineno-8-14" href="#karpenter-__codelineno-8-14"></a><span class="w">                </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;billing-team&quot;</span>
<a id="__codelineno-8-15" name="__codelineno-8-15" href="#karpenter-__codelineno-8-15"></a><span class="w">                  </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;In&quot;</span>
<a id="__codelineno-8-16" name="__codelineno-8-16" href="#karpenter-__codelineno-8-16"></a><span class="w">                  </span><span class="nt">values</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;my-team&quot;</span><span class="p p-Indicator">]</span>
</code></pre></div>
<h3 id="karpenter-use-timers-ttl-to-automatically-delete-nodes-from-the-cluster">Use timers (TTL) to automatically delete nodes from the cluster<a class="headerlink" href="#karpenter-use-timers-ttl-to-automatically-delete-nodes-from-the-cluster" title="Permanent link">&para;</a></h3>
<p>You can use timers on provisioned nodes to set when to delete nodes that are devoid of workload pods or have reached an expiration time. Node expiry can be used as a means of upgrading, so that nodes are retired and replaced with updated versions. See <a href="https://karpenter.sh/docs/concepts/deprovisioning">How Karpenter nodes are deprovisioned</a> in the Karpenter documentation for information on using  <strong><code>ttlSecondsUntilExpired</code></strong> and <strong><code>ttlSecondsAfterEmpty</code></strong> to deprovision nodes.</p>
<h3 id="karpenter-avoid-overly-constraining-the-instance-types-that-karpenter-can-provision-especially-when-utilizing-spot">Avoid overly constraining the Instance Types that Karpenter can provision, especially when utilizing Spot<a class="headerlink" href="#karpenter-avoid-overly-constraining-the-instance-types-that-karpenter-can-provision-especially-when-utilizing-spot" title="Permanent link">&para;</a></h3>
<p>When using Spot, Karpenter uses the <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-fleet-allocation-strategy.html">Price Capacity Optimized</a> allocation strategy to provision EC2 instances. This strategy instructs EC2 to provision instances from the deepest pools for the number of instances that you are launching and have the lowest risk of interruption. EC2 Fleet then requests Spot instances from the lowest priced of these pools. The more instance types you allow Karpenter to utilize, the better EC2 can optimize your spot instance’s runtime. By default, Karpenter will use all Instance Types EC2 offers in the region and availability zones your cluster is deployed in. Karpenter intelligently chooses from the set of all instance types based on pending pods to make sure your pods are scheduled onto appropriately sized and equipped instances. For example, if your pod does not require a GPU, Karpenter will not schedule your pod to an EC2 instance type supporting a GPU. When you're unsure about which instance types to use, you can run the Amazon <a href="https://github.com/aws/amazon-ec2-instance-selector">ec2-instance-selector</a> to generate a list of instance types that match your compute requirements. For example, the CLI takes memory vCPU, architecture, and region as input parameters and provides you with a list of EC2 instances that satisfy those constraints.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-9-1" name="__codelineno-9-1" href="#karpenter-__codelineno-9-1"></a><span class="gp">$ </span>ec2-instance-selector<span class="w"> </span>--memory<span class="w"> </span><span class="m">4</span><span class="w"> </span>--vcpus<span class="w"> </span><span class="m">2</span><span class="w"> </span>--cpu-architecture<span class="w"> </span>x86_64<span class="w"> </span>-r<span class="w"> </span>ap-southeast-1
<a id="__codelineno-9-2" name="__codelineno-9-2" href="#karpenter-__codelineno-9-2"></a><span class="go">c5.large</span>
<a id="__codelineno-9-3" name="__codelineno-9-3" href="#karpenter-__codelineno-9-3"></a><span class="go">c5a.large</span>
<a id="__codelineno-9-4" name="__codelineno-9-4" href="#karpenter-__codelineno-9-4"></a><span class="go">c5ad.large</span>
<a id="__codelineno-9-5" name="__codelineno-9-5" href="#karpenter-__codelineno-9-5"></a><span class="go">c5d.large</span>
<a id="__codelineno-9-6" name="__codelineno-9-6" href="#karpenter-__codelineno-9-6"></a><span class="go">c6i.large</span>
<a id="__codelineno-9-7" name="__codelineno-9-7" href="#karpenter-__codelineno-9-7"></a><span class="go">t2.medium</span>
<a id="__codelineno-9-8" name="__codelineno-9-8" href="#karpenter-__codelineno-9-8"></a><span class="go">t3.medium</span>
<a id="__codelineno-9-9" name="__codelineno-9-9" href="#karpenter-__codelineno-9-9"></a><span class="go">t3a.medium</span>
</code></pre></div>
<p>You shouldn’t place too many constraints on Karpenter when using Spot instances because doing so can affect the availability of your applications. Say, for example, all of the instances of a particular type are reclaimed and there are no suitable alternatives available to replace them. Your pods will remain in a pending state until the spot capacity for the configured instance types is replenished. You can reduce the risk of insufficient capacity errors by spreading your instances across different availability zones, because spot pools are different across AZs. That said, the general best practice is to allow Karpenter to use a diverse set of instance types when using Spot.</p>
<h2 id="karpenter-scheduling-pods">Scheduling Pods<a class="headerlink" href="#karpenter-scheduling-pods" title="Permanent link">&para;</a></h2>
<p>The following best practices relate to deploying pods In a cluster using Karpenter for node provisioning.</p>
<h3 id="karpenter-follow-eks-best-practices-for-high-availability">Follow EKS best practices for high availability<a class="headerlink" href="#karpenter-follow-eks-best-practices-for-high-availability" title="Permanent link">&para;</a></h3>
<p>If you need to run highly available applications, follow general EKS best practice <a href="https://aws.github.io/aws-eks-best-practices/reliability/docs/application/#recommendations">recommendations</a>. See <a href="https://karpenter.sh/docs/concepts/scheduling/#topology-spread">Topology Spread</a> in Karpenter documentation for details on how to spread pods across nodes and zones. Use <a href="https://karpenter.sh/docs/troubleshooting/#disruption-budgets">Disruption Budgets</a> to set the minimum available pods that need to be maintained, in case there are attempts to evict or delete pods.</p>
<h3 id="karpenter-use-layered-constraints-to-constrain-the-compute-features-available-from-your-cloud-provider">Use layered Constraints to constrain the compute features available from your cloud provider<a class="headerlink" href="#karpenter-use-layered-constraints-to-constrain-the-compute-features-available-from-your-cloud-provider" title="Permanent link">&para;</a></h3>
<p>Karpenter’s model of layered constraints allows you to create a complex set of provisioner and pod deployment constraints to get the best possible matches for pod scheduling. Examples of constraints that a pod spec can request include the following:</p>
<ul>
<li>Needing to run in availability zones where only particular applications are available. Say, for example, you have pod that has to communicate with another application that runs on an EC2 instance residing in a particular availability zone. If your aim is to reduce cross-AZ traffic in your VPC, you may want to co-locate the pods in the AZ where the EC2 instance is located. This sort of targeting is often accomplished using node selectors. For additional information on <a href="https://karpenter.sh/docs/concepts/scheduling/#selecting-nodes">Node selectors</a>, please refer to the Kubernetes documentation.</li>
<li>Requiring certain kinds of processors or other hardware. See the <a href="https://karpenter.sh/docs/concepts/scheduling/#acceleratorsgpu-resources">Accelerators</a> section of the Karpenter docs for a podspec example that requires the pod to run on a GPU.</li>
</ul>
<h3 id="karpenter-create-billing-alarms-to-monitor-your-compute-spend">Create billing alarms to monitor your compute spend<a class="headerlink" href="#karpenter-create-billing-alarms-to-monitor-your-compute-spend" title="Permanent link">&para;</a></h3>
<p>When you configure your cluster to automatically scale, you should create billing alarms to warn you when your spend has exceeded a threshold and add resource limits to your Karpenter configuration. Setting resource limits with Karpenter is similar to setting an AWS autoscaling group’s maximum capacity in that it represents the maximum amount of compute resources that can be instantiated by a Karpenter provisioner.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is not possible to set a global limit for the whole cluster. Limits apply to specific provisioners.</p>
</div>
<p>The snippet below tells Karpenter to only provision a maximum of 1000 CPU cores and 1000Gi of memory. Karpenter will stop adding capacity only when the limit is met or exceeded. When a limit is exceeded the Karpenter controller will write <code>memory resource usage of 1001 exceeds limit of 1000</code> or a similar looking message to the controller’s logs. If you are routing your container logs to CloudWatch logs, you can create a <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html">metrics filter</a> to look for specific patterns or terms in your logs and then create a <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html">CloudWatch alarm</a> to alert you when your configured metrics threshold is breached.</p>
<p>For further information using limits with Karpenter, see <a href="https://karpenter.sh/docs/concepts/provisioners/#speclimitsresources">Setting Resource Limits</a> in the Karpenter documentation.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-10-1" name="__codelineno-10-1" href="#karpenter-__codelineno-10-1"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-10-2" name="__codelineno-10-2" href="#karpenter-__codelineno-10-2"></a><span class="w">  </span><span class="nt">limits</span><span class="p">:</span>
<a id="__codelineno-10-3" name="__codelineno-10-3" href="#karpenter-__codelineno-10-3"></a><span class="w">    </span><span class="nt">resources</span><span class="p">:</span>
<a id="__codelineno-10-4" name="__codelineno-10-4" href="#karpenter-__codelineno-10-4"></a><span class="w">      </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1000</span>
<a id="__codelineno-10-5" name="__codelineno-10-5" href="#karpenter-__codelineno-10-5"></a><span class="w">      </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1000Gi</span>
</code></pre></div>
<p>If you don’t use limits or constrain the instance types that Karpenter can provision, Karpenter will continue adding compute capacity to your cluster as needed. While configuring Karpenter in this way allows your cluster to scale freely, it can also have significant cost implications. It is for this reason that we recommend that configuring billing alarms. Billing alarms allow you to be alerted and proactively notified when the calculated estimated charges in your account(s) exceed a defined threshold. See <a href="https://aws.amazon.com/blogs/mt/setting-up-an-amazon-cloudwatch-billing-alarm-to-proactively-monitor-estimated-charges/">Setting up an Amazon CloudWatch Billing Alarm to Proactively Monitor Estimated Charges</a> for additional information.</p>
<p>You may also want to enable Cost Anomaly Detection which is an AWS Cost Management feature that uses machine learning to continuously monitor your cost and usage to detect unusual spends. Further information can be found in the <a href="https://docs.aws.amazon.com/cost-management/latest/userguide/getting-started-ad.html">AWS Cost Anomaly Detection Getting Started</a> guide. If you’ve gone so far as to create a budget in AWS Budgets, you can also configure an action to notify you when a specific threshold has been breached. With budget actions you can send an email, post a message to an SNS topic, or send a message to a chatbot like Slack. For further information see <a href="https://docs.aws.amazon.com/cost-management/latest/userguide/budgets-controls.html">Configuring AWS Budgets actions</a>.</p>
<h3 id="karpenter-use-the-do-not-evict-annotation-to-prevent-karpenter-from-deprovisioning-a-node">Use the do-not-evict annotation to prevent Karpenter from deprovisioning a node<a class="headerlink" href="#karpenter-use-the-do-not-evict-annotation-to-prevent-karpenter-from-deprovisioning-a-node" title="Permanent link">&para;</a></h3>
<p>If you are running a critical application on a Karpenter-provisioned node, such as a <em>long running</em> batch job or stateful application, <em>and</em> the node’s TTL has expired, the application will be interrupted when the instance is terminated. By adding a <code>karpenter.sh/do-not-evict</code> annotation to the pod, you are instructing Karpenter to preserve the node until the Pod is terminated or the <code>do-not-evict</code> annotation is removed. See <a href="https://karpenter.sh/docs/concepts/deprovisioning/#disabling-deprovisioning">Deprovisioning</a> documentation for further information.</p>
<p>If the only non-daemonset pods left on a node are those associated with jobs, Karpenter is able to target and terminate those nodes so long as the job status is succeed or failed.</p>
<h3 id="karpenter-configure-requestslimits-for-all-non-cpu-resources-when-using-consolidation">Configure requests=limits for all non-CPU resources when using consolidation<a class="headerlink" href="#karpenter-configure-requestslimits-for-all-non-cpu-resources-when-using-consolidation" title="Permanent link">&para;</a></h3>
<p>Consolidation and scheduling in general work by comparing the pods resource requests vs the amount of allocatable resources on a node.  The resource limits are not considered.  As an example, pods that have a memory limit that is larger than the memory request can burst above the request.  If several pods on the same node burst at the same time, this can cause some of the pods to be terminated due to an out of memory (OOM) condition.  Consolidation can make this more likely to occur as it works to pack pods onto nodes only considering their requests.</p>
<h3 id="karpenter-use-limitranges-to-configure-defaults-for-resource-requests-and-limits">Use LimitRanges to configure defaults for resource requests and limits<a class="headerlink" href="#karpenter-use-limitranges-to-configure-defaults-for-resource-requests-and-limits" title="Permanent link">&para;</a></h3>
<p>Because Kubernetes doesn’t set default requests or limits, a container’s consumption of resources from the underlying host, CPU, and memory is unbound. The Kubernetes scheduler looks at a pod’s total requests (the higher of the total requests from the pod’s containers or the total resources from the pod’s Init containers) to determine which worker node to schedule the pod onto. Similarly, Karpenter considers a pod’s requests to determine which type of instance it provisions. You can use a limit range to apply a sensible default for a namespace, in case resource requests are not specified by some pods.</p>
<p>See <a href="https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">Configure Default Memory Requests and Limits for a Namespace</a></p>
<h3 id="karpenter-apply-accurate-resource-requests-to-all-workloads">Apply accurate resource requests to all workloads<a class="headerlink" href="#karpenter-apply-accurate-resource-requests-to-all-workloads" title="Permanent link">&para;</a></h3>
<p>Karpenter is able to launch nodes that best fit your workloads when its information about your workloads requirements is accurate.  This is particularly important if using Karpenter's consolidation feature.</p>
<p>See <a href="https://aws.github.io/aws-eks-best-practices/reliability/docs/dataplane/#configure-and-size-resource-requestslimits-for-all-workloads">Configure and Size Resource Requests/Limits for all Workloads</a></p>
<h2 id="karpenter-coredns-recommendations">CoreDNS recommendations<a class="headerlink" href="#karpenter-coredns-recommendations" title="Permanent link">&para;</a></h2>
<h3 id="karpenter-update-the-configuration-of-coredns-to-maintain-reliability">Update the configuration of CoreDNS to maintain reliability<a class="headerlink" href="#karpenter-update-the-configuration-of-coredns-to-maintain-reliability" title="Permanent link">&para;</a></h3>
<p>When deploying CoreDNS pods on nodes managed by Karpenter, given Karpenter's dynamic nature in rapidly terminating/creating new nodes to align with demand, it is advisable to adhere to the following best practices:</p>
<p><a href="https://aws.github.io/aws-eks-best-practices/scalability/docs/cluster-services/#coredns-lameduck-duration">CoreDNS lameduck duration</a></p>
<p><a href="https://aws.github.io/aws-eks-best-practices/scalability/docs/cluster-services/#coredns-readiness-probe">CoreDNS readiness probe</a></p>
<p>This will ensure that DNS queries are not directed to a CoreDNS Pod that is not yet ready or has been terminated.</p>
<h2 id="karpenter-karpenter-blueprints">Karpenter Blueprints<a class="headerlink" href="#karpenter-karpenter-blueprints" title="Permanent link">&para;</a></h2>
<p>As Karpenter takes an application-first approach to provision compute capacity for to the Kubernetes data plane, there are common workload scenarios that you might be wondering how to configure them properly. <a href="https://github.com/aws-samples/karpenter-blueprints">Karpenter Blueprints</a> is a repository that includes a list of common workload scenarios following the best practices described here. You'll have all the resources you need to even create an EKS cluster with Karpenter configured, and test each of the blueprints included in the repository. You can combine different blueprints to finally create the one you need for your workload(s).</p>
<h2 id="karpenter-additional-resources">Additional Resources<a class="headerlink" href="#karpenter-additional-resources" title="Permanent link">&para;</a></h2>
<ul>
<li><a href="https://ec2spotworkshops.com/karpenter.html">Karpenter/Spot Workshop</a></li>
<li><a href="https://youtu.be/_FXRIKWJWUk">Karpenter Node Provisioner</a></li>
<li><a href="https://youtu.be/zXqrNJaTCrU">TGIK Karpenter</a></li>
<li><a href="https://youtu.be/3QsVRHVdOnM">Karpenter vs. Cluster Autoscaler</a></li>
<li><a href="https://www.youtube.com/watch?v=43g8uPohTgc">Groupless Autoscaling with Karpenter</a></li>
<li><a href="https://community.aws/tutorials/run-kubernetes-clusters-for-less-with-amazon-ec2-spot-and-karpenter#step-6-optional-simulate-spot-interruption">Tutorial: Run Kubernetes Clusters for Less with Amazon EC2 Spot and Karpenter</a></li>
</ul></section><section class="print-page" id="cluster-autoscaling"><h1 id="cluster-autoscaling-kubernetes-cluster-autoscaler">Kubernetes Cluster Autoscaler<a class="headerlink" href="#cluster-autoscaling-kubernetes-cluster-autoscaler" title="Permanent link">&para;</a></h1>
<iframe width="560" height="315" src="https://www.youtube.com/embed/FIBc8GkjFU0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

<h2 id="cluster-autoscaling-overview">Overview<a class="headerlink" href="#cluster-autoscaling-overview" title="Permanent link">&para;</a></h2>
<p><a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler">The Kubernetes Cluster Autoscaler</a> is a popular Cluster Autoscaling solution maintained by <a href="https://github.com/kubernetes/community/tree/master/sig-autoscaling">SIG Autoscaling</a>. It is responsible for ensuring that your cluster has enough nodes to schedule your pods without wasting resources. It watches for pods that fail to schedule and for nodes that are underutilized. It then simulates the addition or removal of nodes before applying the change to your cluster. The AWS Cloud Provider implementation within Cluster Autoscaler controls the <code>.DesiredReplicas</code> field of your EC2 Auto Scaling Groups.</p>
<p><img alt="" src="../cluster-autoscaling/architecture.png" /></p>
<p>This guide will provide a mental model for configuring the Cluster Autoscaler and choosing the best set of tradeoffs to meet your organization’s requirements. While there is no single best configuration, there are a set of configuration options that enable you to trade off performance, scalability, cost, and availability. Additionally, this guide will provide tips and best practices for optimizing your configuration for AWS.</p>
<h3 id="cluster-autoscaling-glossary">Glossary<a class="headerlink" href="#cluster-autoscaling-glossary" title="Permanent link">&para;</a></h3>
<p>The following terminology will be used frequently throughout this document. These terms can have broad meaning, but are limited to the definitions below for the purposes of this document.</p>
<p><strong>Scalability</strong> refers to how well the Cluster Autoscaler performs as your Kubernetes Cluster increases in number of pods and nodes. As scalability limits are reached, the Cluster Autoscaler’s performance and functionality degrades. As the Cluster Autoscaler exceeds its scalability limits, it may no longer add or remove nodes in your cluster.</p>
<p><strong>Performance</strong> refers to how quickly the Cluster Autoscaler is able to make and execute scaling decisions. A perfectly performing Cluster Autoscaler would instantly make a decision and trigger a scaling action in response to stimuli, such as a pod becoming unschedulable.</p>
<p><strong>Availability</strong> means that pods can be scheduled quickly and without disruption. This includes when newly created pods need to be scheduled and when a scaled down node terminates any remaining pods scheduled to it.</p>
<p><strong>Cost</strong> is determined by the decision behind scale out and scale in events. Resources are wasted if an existing node is underutilized or a new node is added that is too large for incoming pods. Depending on the use case, there can be costs associated with prematurely terminating pods due to an aggressive scale down decision.</p>
<p><strong>Node Groups</strong> are an abstract Kubernetes concept for a group of nodes within a cluster.  It is not a true Kubernetes resource, but exists as an abstraction in the Cluster Autoscaler, Cluster API, and other components. Nodes within a Node Group share properties like labels and taints, but may consist of multiple Availability Zones or Instance Types.</p>
<p><strong>EC2 Auto Scaling Groups</strong> can be used as an implementation of Node Groups on EC2. EC2 Auto Scaling Groups are configured to launch instances that automatically join their Kubernetes Clusters and apply labels and taints to their corresponding Node resource in the Kubernetes API.</p>
<p><strong>EC2 Managed Node Groups</strong> are another implementation of Node Groups on EC2. They abstract away the complexity manually configuring EC2 Autoscaling Scaling Groups and provide additional management features like node version upgrade and graceful node termination.</p>
<h3 id="cluster-autoscaling-operating-the-cluster-autoscaler">Operating the Cluster Autoscaler<a class="headerlink" href="#cluster-autoscaling-operating-the-cluster-autoscaler" title="Permanent link">&para;</a></h3>
<p>The Cluster Autoscaler is typically installed as a <a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler/cloudprovider/aws/examples">Deployment</a> in your cluster. It uses <a href="https://en.wikipedia.org/wiki/Leader_election">leader election</a> to ensure high availability, but work is done by a single replica at a time. It is not horizontally scalable. For basic setups, the default it should work out of the box using the provided <a href="https://docs.aws.amazon.com/eks/latest/userguide/cluster-autoscaler.html">installation instructions</a>, but there are a few things to keep in mind.</p>
<p>Ensure that:</p>
<ul>
<li>The Cluster Autoscaler’s version matches the Cluster’s Version. Cross version compatibility  is <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/README.md#releases">not tested or supported</a>.</li>
<li><a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler/cloudprovider/aws#auto-discovery-setup">Auto Discovery</a> is enabled, unless you have specific advanced use cases that prevent use of this mode.</li>
</ul>
<h3 id="cluster-autoscaling-employ-least-privileged-access-to-the-iam-role">Employ least privileged access to the IAM role<a class="headerlink" href="#cluster-autoscaling-employ-least-privileged-access-to-the-iam-role" title="Permanent link">&para;</a></h3>
<p>When the <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md#Auto-discovery-setup">Auto Discovery</a> is used, we strongly recommend that you employ least privilege access by limiting Actions <code>autoscaling:SetDesiredCapacity</code> and <code>autoscaling:TerminateInstanceInAutoScalingGroup</code> to the Auto Scaling groups that are scoped to the current cluster.</p>
<p>This will prevents a Cluster Autoscaler running in one cluster from modifying nodegroups in a different cluster even if the <code>--node-group-auto-discovery</code> argument wasn't scoped down to the nodegroups of the cluster using tags (for example <code>k8s.io/cluster-autoscaler/&lt;cluster-name&gt;</code>).</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#cluster-autoscaling-__codelineno-0-1"></a><span class="p">{</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#cluster-autoscaling-__codelineno-0-2"></a><span class="w">    </span><span class="nt">&quot;Version&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;2012-10-17&quot;</span><span class="p">,</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#cluster-autoscaling-__codelineno-0-3"></a><span class="w">    </span><span class="nt">&quot;Statement&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#cluster-autoscaling-__codelineno-0-4"></a><span class="w">        </span><span class="p">{</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#cluster-autoscaling-__codelineno-0-5"></a><span class="w">            </span><span class="nt">&quot;Effect&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Allow&quot;</span><span class="p">,</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#cluster-autoscaling-__codelineno-0-6"></a><span class="w">            </span><span class="nt">&quot;Action&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#cluster-autoscaling-__codelineno-0-7"></a><span class="w">                </span><span class="s2">&quot;autoscaling:SetDesiredCapacity&quot;</span><span class="p">,</span>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#cluster-autoscaling-__codelineno-0-8"></a><span class="w">                </span><span class="s2">&quot;autoscaling:TerminateInstanceInAutoScalingGroup&quot;</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#cluster-autoscaling-__codelineno-0-9"></a><span class="w">            </span><span class="p">],</span>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#cluster-autoscaling-__codelineno-0-10"></a><span class="w">            </span><span class="nt">&quot;Resource&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;*&quot;</span><span class="p">,</span>
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#cluster-autoscaling-__codelineno-0-11"></a><span class="w">            </span><span class="nt">&quot;Condition&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#cluster-autoscaling-__codelineno-0-12"></a><span class="w">                </span><span class="nt">&quot;StringEquals&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#cluster-autoscaling-__codelineno-0-13"></a><span class="w">                    </span><span class="nt">&quot;aws:ResourceTag/k8s.io/cluster-autoscaler/enabled&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;true&quot;</span><span class="p">,</span>
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#cluster-autoscaling-__codelineno-0-14"></a><span class="w">                    </span><span class="nt">&quot;aws:ResourceTag/k8s.io/cluster-autoscaler/&lt;my-cluster&gt;&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;owned&quot;</span>
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#cluster-autoscaling-__codelineno-0-15"></a><span class="w">                </span><span class="p">}</span>
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#cluster-autoscaling-__codelineno-0-16"></a><span class="w">            </span><span class="p">}</span>
<a id="__codelineno-0-17" name="__codelineno-0-17" href="#cluster-autoscaling-__codelineno-0-17"></a><span class="w">        </span><span class="p">},</span>
<a id="__codelineno-0-18" name="__codelineno-0-18" href="#cluster-autoscaling-__codelineno-0-18"></a><span class="w">        </span><span class="p">{</span>
<a id="__codelineno-0-19" name="__codelineno-0-19" href="#cluster-autoscaling-__codelineno-0-19"></a><span class="w">            </span><span class="nt">&quot;Effect&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Allow&quot;</span><span class="p">,</span>
<a id="__codelineno-0-20" name="__codelineno-0-20" href="#cluster-autoscaling-__codelineno-0-20"></a><span class="w">            </span><span class="nt">&quot;Action&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<a id="__codelineno-0-21" name="__codelineno-0-21" href="#cluster-autoscaling-__codelineno-0-21"></a><span class="w">                </span><span class="s2">&quot;autoscaling:DescribeAutoScalingInstances&quot;</span><span class="p">,</span>
<a id="__codelineno-0-22" name="__codelineno-0-22" href="#cluster-autoscaling-__codelineno-0-22"></a><span class="w">                </span><span class="s2">&quot;autoscaling:DescribeAutoScalingGroups&quot;</span><span class="p">,</span>
<a id="__codelineno-0-23" name="__codelineno-0-23" href="#cluster-autoscaling-__codelineno-0-23"></a><span class="w">                </span><span class="s2">&quot;autoscaling:DescribeScalingActivities&quot;</span><span class="p">,</span>
<a id="__codelineno-0-24" name="__codelineno-0-24" href="#cluster-autoscaling-__codelineno-0-24"></a><span class="w">                </span><span class="s2">&quot;ec2:DescribeLaunchTemplateVersions&quot;</span><span class="p">,</span>
<a id="__codelineno-0-25" name="__codelineno-0-25" href="#cluster-autoscaling-__codelineno-0-25"></a><span class="w">                </span><span class="s2">&quot;autoscaling:DescribeTags&quot;</span><span class="p">,</span>
<a id="__codelineno-0-26" name="__codelineno-0-26" href="#cluster-autoscaling-__codelineno-0-26"></a><span class="w">                </span><span class="s2">&quot;autoscaling:DescribeLaunchConfigurations&quot;</span><span class="p">,</span>
<a id="__codelineno-0-27" name="__codelineno-0-27" href="#cluster-autoscaling-__codelineno-0-27"></a><span class="w">                </span><span class="s2">&quot;ec2:DescribeInstanceTypes&quot;</span>
<a id="__codelineno-0-28" name="__codelineno-0-28" href="#cluster-autoscaling-__codelineno-0-28"></a><span class="w">            </span><span class="p">],</span>
<a id="__codelineno-0-29" name="__codelineno-0-29" href="#cluster-autoscaling-__codelineno-0-29"></a><span class="w">            </span><span class="nt">&quot;Resource&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;*&quot;</span>
<a id="__codelineno-0-30" name="__codelineno-0-30" href="#cluster-autoscaling-__codelineno-0-30"></a><span class="w">        </span><span class="p">}</span>
<a id="__codelineno-0-31" name="__codelineno-0-31" href="#cluster-autoscaling-__codelineno-0-31"></a><span class="w">    </span><span class="p">]</span>
<a id="__codelineno-0-32" name="__codelineno-0-32" href="#cluster-autoscaling-__codelineno-0-32"></a><span class="p">}</span>
</code></pre></div>
<h3 id="cluster-autoscaling-configuring-your-node-groups">Configuring your Node Groups<a class="headerlink" href="#cluster-autoscaling-configuring-your-node-groups" title="Permanent link">&para;</a></h3>
<p>Effective autoscaling starts with correctly configuring a set of Node Groups for your cluster. Selecting the right set of Node Groups is key to maximizing availability and reducing cost across your workloads. AWS implements Node Groups using EC2 Auto Scaling Groups, which are flexible to a large number of use cases. However, the Cluster Autoscaler makes some assumptions about your Node Groups. Keeping your EC2 Auto Scaling Group configurations consistent with these assumptions will minimize undesired behavior.</p>
<p>Ensure that:</p>
<ul>
<li>Each Node in a Node Group has identical scheduling properties, such as Labels, Taints, and Resources.</li>
<li>For MixedInstancePolicies, the Instance Types must be of the same shape for CPU, Memory, and GPU</li>
<li>The first Instance Type specified in the policy will be used to simulate scheduling.</li>
<li>If your policy has additional Instance Types with more resources, resources may be wasted after scale out.</li>
<li>If your policy has additional Instance Types with less resources, pods may fail to schedule on the instances.</li>
<li>Node Groups with many nodes are preferred over many Node Groups with fewer nodes. This will have the biggest impact on scalability.</li>
<li>Wherever possible, prefer EC2 features when both systems provide support (e.g. Regions, MixedInstancePolicy)</li>
</ul>
<p><em>Note: We recommend using <a href="https://docs.aws.amazon.com/eks/latest/userguide/managed-node-groups.html">EKS Managed Node Groups</a>. Managed Node Groups come with powerful management features, including features for Cluster Autoscaler like automatic EC2 Auto Scaling Group discovery and graceful node termination.</em></p>
<h2 id="cluster-autoscaling-optimizing-for-performance-and-scalability">Optimizing for Performance and Scalability<a class="headerlink" href="#cluster-autoscaling-optimizing-for-performance-and-scalability" title="Permanent link">&para;</a></h2>
<p>Understanding the autoscaling algorithm’s runtime complexity will help you tune the Cluster Autoscaler to continue operating smoothly in large clusters with greater than <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/proposals/scalability_tests.md">1,000 nodes</a>.</p>
<p>The primary knobs for tuning scalability of the Cluster Autoscaler are the resources provided to the process, the scan interval of the algorithm, and the number of Node Groups in the cluster. There are other factors involved in the true runtime complexity of this algorithm, such as scheduling plugin complexity and number of pods. These are considered to be unconfigurable parameters as they are natural to the cluster’s workload and cannot easily be tuned.</p>
<p>The Cluster Autoscaler loads the entire cluster’s state into memory, including Pods, Nodes, and Node Groups. On each scan interval, the algorithm identifies unschedulable pods and simulates scheduling for each Node Group. Tuning these factors come with different tradeoffs which should be carefully considered for your use case.</p>
<h3 id="cluster-autoscaling-vertically-autoscaling-the-cluster-autoscaler">Vertically Autoscaling the Cluster Autoscaler<a class="headerlink" href="#cluster-autoscaling-vertically-autoscaling-the-cluster-autoscaler" title="Permanent link">&para;</a></h3>
<p>The simplest way to scale the Cluster Autoscaler to larger clusters is to increase the resource requests for its deployment. Both memory and CPU should be increased for large clusters, though this varies significantly with cluster size. The autoscaling algorithm stores all pods and nodes in memory, which can result in a memory footprint larger than a gigabyte in some cases. Increasing resources is typically done manually. If you find that constant resource tuning is creating an operational burden, consider using the <a href="https://github.com/kubernetes/autoscaler/tree/master/addon-resizer">Addon Resizer</a> or <a href="https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler">Vertical Pod Autoscaler</a>.</p>
<h3 id="cluster-autoscaling-reducing-the-number-of-node-groups">Reducing the number of Node Groups<a class="headerlink" href="#cluster-autoscaling-reducing-the-number-of-node-groups" title="Permanent link">&para;</a></h3>
<p>Minimizing the number of node groups is one way to ensure that the Cluster Autoscaler will continue to perform well on large clusters. This may be challenging for some organizations who structure their node groups per team or per application. While this is fully supported by the Kubernetes API, this is considered to be a Cluster Autoscaler anti-pattern with repercussions for scalability. There are many reasons to use multiple node groups (e.g. Spot or GPUs), but in many cases there are alternative designs that achieve the same effect while using a small number of groups.</p>
<p>Ensure that:</p>
<ul>
<li>Pod isolation is done using Namespaces rather than Node Groups.</li>
<li>This may not be possible in low-trust multi-tenant clusters.</li>
<li>Pod ResourceRequests and ResourceLimits are properly set to avoid resource contention.</li>
<li>Larger instance types will result in more optimal bin packing and reduced system pod overhead.</li>
<li>NodeTaints or NodeSelectors are used to schedule pods as the exception, not as the rule.</li>
<li>Regional resources are defined as a single EC2 Auto Scaling Group with multiple Availability Zones.</li>
</ul>
<h3 id="cluster-autoscaling-reducing-the-scan-interval">Reducing the Scan Interval<a class="headerlink" href="#cluster-autoscaling-reducing-the-scan-interval" title="Permanent link">&para;</a></h3>
<p>A low scan interval (e.g. 10 seconds) will ensure that the Cluster Autoscaler responds as quickly as possible when pods become unschedulable. However, each scan results in many API calls to the Kubernetes API and EC2 Auto Scaling Group or EKS Managed Node Group APIs. These API calls can result in rate limiting or even service unavailability for your Kubernetes Control Plane.</p>
<p>The default scan interval is 10 seconds, but on AWS, launching a node takes significantly longer to launch a new instance. This means that it’s possible to increase the interval without significantly increasing overall scale up time. For example, if it takes 2 minutes to launch a node, changing the interval to 1 minute will result a tradeoff of 6x reduced API calls for 38% slower scale ups.</p>
<h3 id="cluster-autoscaling-sharding-across-node-groups">Sharding Across Node Groups<a class="headerlink" href="#cluster-autoscaling-sharding-across-node-groups" title="Permanent link">&para;</a></h3>
<p>The Cluster Autoscaler can be configured to operate on a specific set of Node Groups. Using this functionality, it’s possible to deploy multiple instances of the Cluster Autoscaler, each configured to operate on a different set of Node Groups. This strategy enables you use arbitrarily large numbers of Node Groups, trading cost for scalability. We only recommend using this as a last resort for improving performance.</p>
<p>The Cluster Autoscaler was not originally designed for this configuration, so there are some side effects. Since the shards do not communicate, it’s possible for multiple autoscalers to attempt to schedule an unschedulable pod. This can result in unnecessary scale out of multiple Node Groups. These extra nodes will scale back in after the <code>scale-down-delay</code>.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#cluster-autoscaling-__codelineno-1-1"></a>metadata:
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#cluster-autoscaling-__codelineno-1-2"></a>  name: cluster-autoscaler
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#cluster-autoscaling-__codelineno-1-3"></a>  namespace: cluster-autoscaler-1
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#cluster-autoscaling-__codelineno-1-4"></a>
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#cluster-autoscaling-__codelineno-1-5"></a>...
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#cluster-autoscaling-__codelineno-1-6"></a>
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#cluster-autoscaling-__codelineno-1-7"></a>--nodes=1:10:k8s-worker-asg-1
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#cluster-autoscaling-__codelineno-1-8"></a>--nodes=1:10:k8s-worker-asg-2
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#cluster-autoscaling-__codelineno-1-9"></a>
<a id="__codelineno-1-10" name="__codelineno-1-10" href="#cluster-autoscaling-__codelineno-1-10"></a>---
<a id="__codelineno-1-11" name="__codelineno-1-11" href="#cluster-autoscaling-__codelineno-1-11"></a>
<a id="__codelineno-1-12" name="__codelineno-1-12" href="#cluster-autoscaling-__codelineno-1-12"></a>metadata:
<a id="__codelineno-1-13" name="__codelineno-1-13" href="#cluster-autoscaling-__codelineno-1-13"></a>  name: cluster-autoscaler
<a id="__codelineno-1-14" name="__codelineno-1-14" href="#cluster-autoscaling-__codelineno-1-14"></a>  namespace: cluster-autoscaler-2
<a id="__codelineno-1-15" name="__codelineno-1-15" href="#cluster-autoscaling-__codelineno-1-15"></a>
<a id="__codelineno-1-16" name="__codelineno-1-16" href="#cluster-autoscaling-__codelineno-1-16"></a>...
<a id="__codelineno-1-17" name="__codelineno-1-17" href="#cluster-autoscaling-__codelineno-1-17"></a>
<a id="__codelineno-1-18" name="__codelineno-1-18" href="#cluster-autoscaling-__codelineno-1-18"></a>--nodes=1:10:k8s-worker-asg-3
<a id="__codelineno-1-19" name="__codelineno-1-19" href="#cluster-autoscaling-__codelineno-1-19"></a>--nodes=1:10:k8s-worker-asg-4
</code></pre></div>
<p>Ensure that:</p>
<ul>
<li>Each shard is configured to point to a unique set of EC2 Auto Scaling Groups</li>
<li>Each shard is deployed to a separate namespace to avoid leader election conflicts</li>
</ul>
<h2 id="cluster-autoscaling-optimizing-for-cost-and-availability">Optimizing for Cost and Availability<a class="headerlink" href="#cluster-autoscaling-optimizing-for-cost-and-availability" title="Permanent link">&para;</a></h2>
<h3 id="cluster-autoscaling-spot-instances">Spot Instances<a class="headerlink" href="#cluster-autoscaling-spot-instances" title="Permanent link">&para;</a></h3>
<p>You can use Spot Instances in your node groups and save up to 90% off the on-demand price, with the trade-off the Spot Instances can be interrupted at any time when EC2 needs the capacity back. Insufficient Capacity Errors will occur when your EC2 Auto Scaling group cannot scale up due to lack of available capacity. Maximizing diversity by selecting many instance families can increase your chance of achieving your desired scale by tapping into many Spot capacity pools, and decrease the impact of Spot Instance interruptions on your cluster availability. Mixed Instance Policies with Spot Instances are a great way to increase diversity without increasing the number of node groups. Keep in mind, if you need guaranteed resources, use On-Demand Instances instead of Spot Instances.</p>
<p>It’s critical that all Instance Types have similar resource capacity when configuring Mixed Instance Policies. The autoscaler’s scheduling simulator uses the first InstanceType in the MixedInstancePolicy. If subsequent Instance Types are larger, resources may be wasted after a scale up. If smaller, your pods may fail to schedule on the new instances due to insufficient capacity. For example, M4, M5, M5a, and M5n instances all have similar amounts of CPU and Memory and are great candidates for a MixedInstancePolicy. The <a href="https://github.com/aws/amazon-ec2-instance-selector">EC2 Instance Selector</a> tool can help you identify similar instance types.</p>
<p><img alt="" src="../cluster-autoscaling/spot_mix_instance_policy.jpg" /></p>
<p>It's recommended to isolate On-Demand and Spot capacity into separate EC2 Auto Scaling groups. This is preferred over using a <a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-purchase-options.html#asg-instances-distribution">base capacity strategy</a> because the scheduling properties are fundamentally different. Since Spot Instances be interrupted at any time (when EC2 needs the capacity back), users will often taint their preemptable nodes, requiring an explicit pod toleration to the preemption behavior. These taints result in different scheduling properties for the nodes, so they should be separated into multiple EC2 Auto Scaling Groups.</p>
<p>The Cluster Autoscaler has a concept of <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-are-expanders">Expanders</a>, which provide different strategies for selecting which Node Group to scale. The strategy <code>--expander=least-waste</code> is a good general purpose default, and if you're going to use multiple node groups for Spot Instance diversification (as described in the image above), it could help further cost-optimize the node groups by scaling the group which would be best utilized after the scaling activity.</p>
<h3 id="cluster-autoscaling-prioritizing-a-node-group-asg">Prioritizing a node group / ASG<a class="headerlink" href="#cluster-autoscaling-prioritizing-a-node-group-asg" title="Permanent link">&para;</a></h3>
<p>You may also configure priority based autoscaling by using the Priority expander. <code>--expander=priority</code> enables your cluster to prioritize a node group / ASG, and if it is unable to scale for any reason, it will choose the next node group in the prioritized list. This is useful in situations where, for example, you want to use P3 instance types because their GPU provides optimal performance for your workload, but as a second option you can also use P2 instance types.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#cluster-autoscaling-__codelineno-2-1"></a>apiVersion: v1
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#cluster-autoscaling-__codelineno-2-2"></a>kind: ConfigMap
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#cluster-autoscaling-__codelineno-2-3"></a>metadata:
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#cluster-autoscaling-__codelineno-2-4"></a>  name: cluster-autoscaler-priority-expander
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#cluster-autoscaling-__codelineno-2-5"></a>  namespace: kube-system
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#cluster-autoscaling-__codelineno-2-6"></a>data:
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#cluster-autoscaling-__codelineno-2-7"></a>  priorities: |-
<a id="__codelineno-2-8" name="__codelineno-2-8" href="#cluster-autoscaling-__codelineno-2-8"></a>    10:
<a id="__codelineno-2-9" name="__codelineno-2-9" href="#cluster-autoscaling-__codelineno-2-9"></a>      - .*p2-node-group.*
<a id="__codelineno-2-10" name="__codelineno-2-10" href="#cluster-autoscaling-__codelineno-2-10"></a>    50:
<a id="__codelineno-2-11" name="__codelineno-2-11" href="#cluster-autoscaling-__codelineno-2-11"></a>      - .*p3-node-group.*
</code></pre></div>
<p>Cluster Autoscaler will try to scale up the EC2 Auto Scaling group matching the name <em>p3-node-group</em>. If this operation does not succeed within <code>--max-node-provision-time</code>, it will attempt to scale an EC2 Auto Scaling group matching the name <em>p2-node-group</em>.
This value defaults to 15 minutes and can be reduced for more responsive node group selection, though if the value is too low, it can cause unnecessary scale outs.</p>
<h3 id="cluster-autoscaling-overprovisioning">Overprovisioning<a class="headerlink" href="#cluster-autoscaling-overprovisioning" title="Permanent link">&para;</a></h3>
<p>The Cluster Autoscaler minimizes costs by ensuring that nodes are only added to the cluster when needed and are removed when unused. This significantly impacts deployment latency because many pods will be forced to wait for a node scale up before they can be scheduled. Nodes can take multiple minutes to become available, which can increase pod scheduling latency by an order of magnitude.</p>
<p>This can be mitigated using <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-can-i-configure-overprovisioning-with-cluster-autoscaler">overprovisioning</a>, which trades cost for scheduling latency. Overprovisioning is implemented using temporary pods with negative priority, which occupy space in the cluster. When newly created pods are unschedulable and have higher priority, the temporary pods will be preempted to make room. The temporary pods then become unschedulable, triggering the Cluster Autoscaler to scale out new overprovisioned nodes.</p>
<p>There are other less obvious benefits to overprovisioning. Without overprovisioning, one of the side effects of a highly utilized cluster is that pods will make less optimal scheduling decisions using the <code>preferredDuringSchedulingIgnoredDuringExecution</code> rule of Pod or Node Affinity. A common use case for this is to separate pods for a highly available application across availability zones using AntiAffinity. Overprovisioning can significantly increase the chance that a node of the correct zone is available.</p>
<p>The amount of overprovisioned capacity is a careful business decision for your organization. At its core, it’s a tradeoff between performance and cost. One way to make this decision is to determine your average scale up frequency and divide it by the amount of time it takes to scale up a new node. For example, if on average you require a new node every 30 seconds and EC2 takes 30 seconds to provision a new node, a single node of overprovisioning will ensure that there’s always an extra node available, reducing scheduling latency by 30 seconds at the cost of a single additional EC2 Instance. To improve zonal scheduling decisions, overprovision a number of nodes equal to the number of availability zones in your EC2 Auto Scaling Group to ensure that the scheduler can select the best zone for incoming pods.</p>
<h3 id="cluster-autoscaling-prevent-scale-down-eviction">Prevent Scale Down Eviction<a class="headerlink" href="#cluster-autoscaling-prevent-scale-down-eviction" title="Permanent link">&para;</a></h3>
<p>Some workloads are expensive to evict. Big data analysis, machine learning tasks, and test runners will eventually complete, but must be restarted if interrupted. The Cluster Autoscaler will attempt to scale down any node under the scale-down-utilization-threshold, which will interrupt any remaining pods on the node. This can be prevented by ensuring that pods that are expensive to evict are protected by a label recognized by the Cluster Autoscaler.</p>
<p>Ensure that:</p>
<ul>
<li>Expensive to evict pods have the annotation <code>cluster-autoscaler.kubernetes.io/safe-to-evict=false</code></li>
</ul>
<h2 id="cluster-autoscaling-advanced-use-cases">Advanced Use Cases<a class="headerlink" href="#cluster-autoscaling-advanced-use-cases" title="Permanent link">&para;</a></h2>
<h3 id="cluster-autoscaling-ebs-volumes">EBS Volumes<a class="headerlink" href="#cluster-autoscaling-ebs-volumes" title="Permanent link">&para;</a></h3>
<p>Persistent storage is critical for building stateful applications, such as database or distributed caches. <a href="https://aws.amazon.com/premiumsupport/knowledge-center/eks-persistent-storage/">EBS Volumes</a> enable this use case on Kubernetes, but are limited to a specific zone. These applications can be highly available if sharded across multiple AZs using a separate EBS Volume for each AZ. The Cluster Autoscaler can then balance the scaling of the EC2 Autoscaling Groups.</p>
<p>Ensure that:</p>
<ul>
<li>Node group balancing is enabled by setting <code>balance-similar-node-groups=true</code>.</li>
<li>Node Groups are configured with identical settings except for different availability zones and EBS Volumes.</li>
</ul>
<h3 id="cluster-autoscaling-co-scheduling">Co-Scheduling<a class="headerlink" href="#cluster-autoscaling-co-scheduling" title="Permanent link">&para;</a></h3>
<p>Machine learning distributed training jobs benefit significantly from the minimized latency of same-zone node configurations. These workloads deploy multiple pods to a specific zone. This can be achieved by setting Pod Affinity for all co-scheduled pods or Node Affinity using <code>topologyKey: failure-domain.beta.kubernetes.io/zone</code>. The Cluster Autoscaler will then scale out a specific zone to match demands. You may wish to allocate multiple EC2 Auto Scaling Groups, one per availability zone to enable failover for the entire co-scheduled workload.</p>
<p>Ensure that:</p>
<ul>
<li>Node group balancing is enabled by setting <code>balance-similar-node-groups=false</code></li>
<li><a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity">Node Affinity</a> and/or <a href="https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/">Pod Preemption</a> is used when clusters include both Regional and Zonal Node Groups.</li>
<li>Use <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity">Node Affinity</a> to force or encourage regional pods to avoid zonal Node Groups, and vice versa.</li>
<li>If zonal pods schedule onto regional node groups, this will result in imbalanced capacity for your regional pods.</li>
<li>If your zonal workloads can tolerate disruption and relocation, configure <a href="https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/">Pod Preemption</a> to enable regionally scaled pods to force preemption and rescheduling on a less contested zone.</li>
</ul>
<h3 id="cluster-autoscaling-accelerators">Accelerators<a class="headerlink" href="#cluster-autoscaling-accelerators" title="Permanent link">&para;</a></h3>
<p>Some clusters take advantage of specialized hardware accelerators such as GPU. When scaling out, the accelerator device plugin can take several minutes to advertise the resource to the cluster. The Cluster Autoscaler has simulated that this node will have the accelerator, but until the accelerator becomes ready and updates the node’s available resources, pending pods can not be scheduled on the node. This can result in <a href="https://github.com/kubernetes/kubernetes/issues/54959">repeated unnecessary scale out</a>.</p>
<p>Additionally, nodes with accelerators and high CPU or Memory utilization will not be considered for scale down, even if the accelerator is unused. This behavior can be expensive due to the relative cost of accelerators. Instead, the Cluster Autoscaler can apply special rules to consider nodes for scale down if they have unoccupied accelerators.</p>
<p>To ensure the correct behavior for these cases, you can configure the kubelet on your accelerator nodes to label the node before it joins the cluster. The Cluster Autoscaler will use this label selector to trigger the accelerator optimized behavior.</p>
<p>Ensure that:</p>
<ul>
<li>The Kubelet for GPU nodes is configured with <code>--node-labels k8s.amazonaws.com/accelerator=$ACCELERATOR_TYPE</code></li>
<li>Nodes with Accelerators adhere to the identical scheduling properties rule noted above.</li>
</ul>
<h3 id="cluster-autoscaling-scaling-from-0">Scaling from 0<a class="headerlink" href="#cluster-autoscaling-scaling-from-0" title="Permanent link">&para;</a></h3>
<p>Cluster Autoscaler is capable of scaling Node Groups to and from zero, which can yield significant cost savings. It detects the CPU, memory, and GPU resources of an Auto Scaling Group by inspecting the InstanceType specified in its LaunchConfiguration or LaunchTemplate. Some pods require additional resources like <code>WindowsENI</code> or <code>PrivateIPv4Address</code> or specific NodeSelectors or Taints which cannot be discovered from the LaunchConfiguration. The Cluster Autoscaler can account for these factors by discovering them from tags on the EC2 Auto Scaling Group. For example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#cluster-autoscaling-__codelineno-3-1"></a>Key: k8s.io/cluster-autoscaler/node-template/resources/$RESOURCE_NAME
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#cluster-autoscaling-__codelineno-3-2"></a>Value: 5
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#cluster-autoscaling-__codelineno-3-3"></a>Key: k8s.io/cluster-autoscaler/node-template/label/$LABEL_KEY
<a id="__codelineno-3-4" name="__codelineno-3-4" href="#cluster-autoscaling-__codelineno-3-4"></a>Value: $LABEL_VALUE
<a id="__codelineno-3-5" name="__codelineno-3-5" href="#cluster-autoscaling-__codelineno-3-5"></a>Key: k8s.io/cluster-autoscaler/node-template/taint/$TAINT_KEY
<a id="__codelineno-3-6" name="__codelineno-3-6" href="#cluster-autoscaling-__codelineno-3-6"></a>Value: NoSchedule
</code></pre></div>
<p><em>Note: Keep in mind, when scaling to zero your capacity is returned to EC2 and may be unavailable in the future.</em></p>
<h2 id="cluster-autoscaling-additional-parameters">Additional Parameters<a class="headerlink" href="#cluster-autoscaling-additional-parameters" title="Permanent link">&para;</a></h2>
<p>There are many configuration options that can be used to tune the behavior and performance of the Cluster Autoscaler.
A complete list of parameters is available on <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-are-the-parameters-to-ca">GitHub</a>.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Parameter</td>
<td>Description</td>
<td>Default</td>
</tr>
<tr>
<td>scan-interval</td>
<td>How often cluster is reevaluated for scale up or down</td>
<td>10 seconds</td>
</tr>
<tr>
<td>max-empty-bulk-delete</td>
<td>Maximum number of empty nodes that can be deleted at the same time.</td>
<td>10</td>
</tr>
<tr>
<td>scale-down-delay-after-add</td>
<td>How long after scale up that scale down evaluation resumes</td>
<td>10 minutes</td>
</tr>
<tr>
<td>scale-down-delay-after-delete</td>
<td>How long after node deletion that scale down evaluation resumes, defaults to scan-interval</td>
<td>scan-interval</td>
</tr>
<tr>
<td>scale-down-delay-after-failure</td>
<td>How long after scale down failure that scale down evaluation resumes</td>
<td>3 minutes</td>
</tr>
<tr>
<td>scale-down-unneeded-time</td>
<td>How long a node should be unneeded before it is eligible for scale down</td>
<td>10 minutes</td>
</tr>
<tr>
<td>scale-down-unready-time</td>
<td>How long an unready node should be unneeded before it is eligible for scale down</td>
<td>20 minutes</td>
</tr>
<tr>
<td>scale-down-utilization-threshold</td>
<td>Node utilization level, defined as sum of requested resources divided by capacity, below which a node can be considered for scale down</td>
<td>0.5</td>
</tr>
<tr>
<td>scale-down-non-empty-candidates-count</td>
<td>Maximum number of non empty nodes considered in one iteration as candidates for scale down with drain. Lower value means better CA responsiveness but possible slower scale down latency. Higher value can affect CA performance with big clusters (hundreds of nodes). Set to non positive value to turn this heuristic off - CA will not limit the number of nodes it considers.“</td>
<td>30</td>
</tr>
<tr>
<td>scale-down-candidates-pool-ratio</td>
<td>A ratio of nodes that are considered as additional non empty candidates for scale down when some candidates from previous iteration are no longer valid. Lower value means better CA responsiveness but possible slower scale down latency. Higher value can affect CA performance with big clusters (hundreds of nodes). Set to 1.0 to turn this heuristics off - CA will take all nodes as additional candidates.</td>
<td>0.1</td>
</tr>
<tr>
<td>scale-down-candidates-pool-min-count</td>
<td>Minimum number of nodes that are considered as additional non empty candidates for scale down when some candidates from previous iteration are no longer valid. When calculating the pool size for additional candidates we take <code>max(#nodes * scale-down-candidates-pool-ratio, scale-down-candidates-pool-min-count)</code></td>
<td>50</td>
</tr>
</tbody>
</table>
<h2 id="cluster-autoscaling-additional-resources">Additional Resources<a class="headerlink" href="#cluster-autoscaling-additional-resources" title="Permanent link">&para;</a></h2>
<p>This page contains a list of Cluster Autoscaler presentations and demos. If you'd like to add a presentation or demo here, please send a pull request.</p>
<table>
<thead>
<tr>
<th>Presentation/Demo</th>
<th>Presenters</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://sched.co/Zemi">Autoscaling and Cost Optimization on Kubernetes: From 0 to 100</a></td>
<td>Guy Templeton, Skyscanner &amp; Jiaxin Shan, Amazon</td>
</tr>
<tr>
<td><a href="https://youtu.be/odxPyW_rZNQ">SIG-Autoscaling Deep Dive</a></td>
<td>Maciek Pytel &amp; Marcin Wielgus</td>
</tr>
</tbody>
</table>
<h2 id="cluster-autoscaling-references">References<a class="headerlink" href="#cluster-autoscaling-references" title="Permanent link">&para;</a></h2>
<ul>
<li><a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md">https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md</a></li>
<li><a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md">https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md</a></li>
<li><a href="https://github.com/aws/amazon-ec2-instance-selector">https://github.com/aws/amazon-ec2-instance-selector</a></li>
<li><a href="https://github.com/aws/aws-node-termination-handler">https://github.com/aws/aws-node-termination-handler</a></li>
</ul></section><h1 class='nav-section-title-end'>Ended: Cluster Autoscaling</h1>
                        <h1 class='nav-section-title' id='section-reliability'>
                            Reliability <a class='headerlink' href='#section-reliability' title='Permanent link'>↵</a>
                        </h1>
                        <section class="print-page" id="reliability-docs"><h1 id="reliability-docs-amazon-eks-best-practices-guide-for-reliability">Amazon EKS Best Practices Guide for Reliability<a class="headerlink" href="#reliability-docs-amazon-eks-best-practices-guide-for-reliability" title="Permanent link">&para;</a></h1>
<p>This section provides guidance about making workloads running on EKS resilient and highly-available  </p>
<h2 id="reliability-docs-how-to-use-this-guide">How to use this guide<a class="headerlink" href="#reliability-docs-how-to-use-this-guide" title="Permanent link">&para;</a></h2>
<p>This guide is meant for developers and architects who want to develop and operate highly-available and fault-tolerant services in EKS. The guide is organized into different topic areas for easier consumption. Each topic starts with a brief overview, followed by a list of recommendations and best practices for the reliability of your EKS clusters.</p>
<h2 id="reliability-docs-introduction">Introduction<a class="headerlink" href="#reliability-docs-introduction" title="Permanent link">&para;</a></h2>
<p>The reliability best practices for EKS have been grouped under the following topics:</p>
<ul>
<li>Applications</li>
<li>Control Plane </li>
<li>Data Plane</li>
</ul>
<hr />
<p>What makes a system reliable? If a system can function consistently and meet demands in spite of changes in its environment over a period of time, it can be called reliable. To achieve this, the system has to detect failures, automatically heal itself, and have the ability to scale based on demand. </p>
<p>Customers can use Kubernetes as a foundation to operate mission-critical applications and services reliably. But aside from incorporating container-based application design principles, running workloads reliably also requires a reliable infrastructure. In Kubernetes, infrastructure comprises the control plane and data plane. </p>
<p>EKS provides a production-grade Kubernetes control plane that is designed to be highly-available and fault-tolerant. </p>
<p>In EKS, AWS is responsible for the reliability of the Kubernetes control plane. EKS runs Kubernetes control plane across three availability zones in an AWS Region. It automatically manages the availability and scalability of the Kubernetes API servers and the etcd cluster. </p>
<p>The responsibility for the data plane’s reliability is shared between you, the customer, and AWS. EKS offers three options for Kubernetes data plane. Fargate, which is the most managed option, handles provisioning and scaling of the data plane. The second option, managed nodes groups, handles provisioning, and updates of the data plane. And finally, self-managed nodes is the least managed option for the data plane. The more AWS-managed data plane you use, the less responsibility you have</p>
<p><a href="https://docs.aws.amazon.com/eks/latest/userguide/managed-node-groups.html">Managed node groups</a> automate the provisioning and lifecycle management of EC2 nodes. You can use the EKS API (using EKS console, AWS API, AWS CLI, CloudFormation, Terraform, or <code>eksctl</code>),   to create, scale, and upgrade managed nodes. Managed nodes run EKS-optimized Amazon Linux 2 EC2 instances in your account, and you can install custom software packages by enabling SSH access. When you provision managed nodes, they run as part of an EKS-managed Auto Scaling Group that can span multiple Availability Zones; you control this through the subnets you provide when creating managed nodes. EKS also automatically tags managed nodes so they can be used with Cluster Autoscaler. </p>
<blockquote>
<p>Amazon EKS follows the shared responsibility model for CVEs and security patches on managed node groups. Because managed nodes run the Amazon EKS-optimized AMIs, Amazon EKS is responsible for building patched versions of these AMIs when bug fixes. However, you are responsible for deploying these patched AMI versions to your managed node groups. </p>
</blockquote>
<p>EKS also <a href="https://docs.aws.amazon.com/eks/latest/userguide/update-managed-node-group.html">manages updating the nodes</a> although you have to initiate the update process. The process of <a href="https://docs.aws.amazon.com/eks/latest/userguide/managed-node-update-behavior.html">updating managed node</a> is explained in the EKS documentation. </p>
<p>If you run self-managed nodes, you can use <a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html">Amazon EKS-optimized Linux AMI</a> to create worker nodes. You are responsible for patching and upgrading the AMI and the nodes. It is a best practice to use <code>eksctl</code>, CloudFormation, or infrastructure as code tools to provision self-managed nodes because this will make it easy for you to <a href="https://docs.aws.amazon.com/eks/latest/userguide/update-workers.html">upgrade self-managed nodes</a>. Consider <a href="https://docs.aws.amazon.com/eks/latest/userguide/migrate-stack.html">migrating to new nodes</a> when updating worker nodes because the migration process <strong>taints</strong> the old node group as <code>NoSchedule</code> and <strong>drains</strong> the nodes after a new stack is ready to accept the existing pod workload. However, you can also perform an <a href="https://docs.aws.amazon.com/eks/latest/userguide/update-stack.html">in-place upgrade of self-managed nodes</a>.</p>
<p><img alt="Shared Responsibility Model - Fargate" src="../reliability/docs/images/SRM-Fargate.jpeg" /></p>
<p><img alt="Shared Responsibility Model - MNG" src="../reliability/docs/images/SRM-MNG.jpeg" /></p>
<p>This guide includes a set of recommendations that you can use to improve the reliability of your EKS data plane, Kubernetes core components, and your applications.</p>
<h2 id="reliability-docs-feedback">Feedback<a class="headerlink" href="#reliability-docs-feedback" title="Permanent link">&para;</a></h2>
<p>This guide is being released on GitHub to collect direct feedback and suggestions from the broader EKS/Kubernetes community. If you have a best practice that you feel we ought to include in the guide, please file an issue or submit a PR in the GitHub repository. We intend to update the guide periodically as new features are added to the service or when a new best practice evolves.</p></section><section class="print-page" id="reliability-docs-application"><h1 id="reliability-docs-application-running-highly-available-applications">Running highly-available applications<a class="headerlink" href="#reliability-docs-application-running-highly-available-applications" title="Permanent link">&para;</a></h1>
<p>Your customers expect your application to be always available, including when you're making changes and especially during spikes in traffic. A scalable and resilient architecture keeps your applications and services running without disruptions, which keeps your users happy. A scalable infrastructure grows and shrinks based on the needs of the business. Eliminating single points of failure is a critical step towards improving an application’s availability and making it resilient.</p>
<p>With Kubernetes, you can operate your applications and run them in a highly-available and resilient fashion. Its declarative management ensures that once you’ve set up the application, Kubernetes will continuously try to <a href="https://kubernetes.io/docs/concepts/architecture/controller/#desired-vs-current">match the current state with the desired state</a>.</p>
<h2 id="reliability-docs-application-recommendations">Recommendations<a class="headerlink" href="#reliability-docs-application-recommendations" title="Permanent link">&para;</a></h2>
<h3 id="reliability-docs-application-avoid-running-singleton-pods">Avoid running singleton Pods<a class="headerlink" href="#reliability-docs-application-avoid-running-singleton-pods" title="Permanent link">&para;</a></h3>
<p>If your entire application runs in a single Pod, then your application will be unavailable if that Pod gets terminated. Instead of deploying applications using individual pods, create <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployments</a>. If a Pod that is created by a Deployment fails or gets terminated, the Deployment <a href="https://kubernetes.io/docs/concepts/architecture/controller/">controller</a> will start a new pod to ensure the specified number of replica Pods are always running. </p>
<h3 id="reliability-docs-application-run-multiple-replicas">Run multiple replicas<a class="headerlink" href="#reliability-docs-application-run-multiple-replicas" title="Permanent link">&para;</a></h3>
<p>Running multiple replicas Pods of an app using a Deployment helps it run in a highly-available manner. If one replica fails, the remaining replicas will still function, albeit at reduced capacity until Kubernetes creates another Pod to make up for the loss. Furthermore, you can use the <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">Horizontal Pod Autoscaler</a> to scale replicas automatically based on workload demand. </p>
<h3 id="reliability-docs-application-schedule-replicas-across-nodes">Schedule replicas across nodes<a class="headerlink" href="#reliability-docs-application-schedule-replicas-across-nodes" title="Permanent link">&para;</a></h3>
<p>Running multiple replicas won’t be very useful if all the replicas are running on the same node, and the node becomes unavailable. Consider using pod anti-affinity or pod topology spread constraints to spread replicas of a Deployment across multiple worker nodes. </p>
<p>You can further improve a typical application’s reliability by running it across multiple AZs. </p>
<h4 id="reliability-docs-application-using-pod-anti-affinity-rules">Using Pod anti-affinity rules<a class="headerlink" href="#reliability-docs-application-using-pod-anti-affinity-rules" title="Permanent link">&para;</a></h4>
<p>The manifest below tells Kubernetes scheduler to <em>prefer</em> to place pods on separate nodes and AZs. It doesn’t require distinct nodes or AZ because if it did, then Kubernetes will not be able to schedule any pods once there is a pod running in each AZ. If your application requires just three replicas, you can use <code>requiredDuringSchedulingIgnoredDuringExecution</code> for <code>topologyKey: topology.kubernetes.io/zone</code>, and Kubernetes scheduler will not schedule two pods in the same AZ.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#reliability-docs-application-__codelineno-0-1"></a>apiVersion: apps/v1
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#reliability-docs-application-__codelineno-0-2"></a>kind: Deployment
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#reliability-docs-application-__codelineno-0-3"></a>metadata:
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#reliability-docs-application-__codelineno-0-4"></a>  name: spread-host-az
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#reliability-docs-application-__codelineno-0-5"></a>  labels:
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#reliability-docs-application-__codelineno-0-6"></a>    app: web-server
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#reliability-docs-application-__codelineno-0-7"></a>spec:
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#reliability-docs-application-__codelineno-0-8"></a>  replicas: 4
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#reliability-docs-application-__codelineno-0-9"></a>  selector:
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#reliability-docs-application-__codelineno-0-10"></a>    matchLabels:
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#reliability-docs-application-__codelineno-0-11"></a>      app: web-server
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#reliability-docs-application-__codelineno-0-12"></a>  template:
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#reliability-docs-application-__codelineno-0-13"></a>    metadata:
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#reliability-docs-application-__codelineno-0-14"></a>      labels:
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#reliability-docs-application-__codelineno-0-15"></a>        app: web-server
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#reliability-docs-application-__codelineno-0-16"></a>    spec:
<a id="__codelineno-0-17" name="__codelineno-0-17" href="#reliability-docs-application-__codelineno-0-17"></a>      affinity:
<a id="__codelineno-0-18" name="__codelineno-0-18" href="#reliability-docs-application-__codelineno-0-18"></a>        podAntiAffinity:
<a id="__codelineno-0-19" name="__codelineno-0-19" href="#reliability-docs-application-__codelineno-0-19"></a>          preferredDuringSchedulingIgnoredDuringExecution:
<a id="__codelineno-0-20" name="__codelineno-0-20" href="#reliability-docs-application-__codelineno-0-20"></a>          - podAffinityTerm:
<a id="__codelineno-0-21" name="__codelineno-0-21" href="#reliability-docs-application-__codelineno-0-21"></a>              labelSelector:
<a id="__codelineno-0-22" name="__codelineno-0-22" href="#reliability-docs-application-__codelineno-0-22"></a>                matchExpressions:
<a id="__codelineno-0-23" name="__codelineno-0-23" href="#reliability-docs-application-__codelineno-0-23"></a>                - key: app
<a id="__codelineno-0-24" name="__codelineno-0-24" href="#reliability-docs-application-__codelineno-0-24"></a>                  operator: In
<a id="__codelineno-0-25" name="__codelineno-0-25" href="#reliability-docs-application-__codelineno-0-25"></a>                  values:
<a id="__codelineno-0-26" name="__codelineno-0-26" href="#reliability-docs-application-__codelineno-0-26"></a>                  - web-server
<a id="__codelineno-0-27" name="__codelineno-0-27" href="#reliability-docs-application-__codelineno-0-27"></a>              topologyKey: topology.kubernetes.io/zone
<a id="__codelineno-0-28" name="__codelineno-0-28" href="#reliability-docs-application-__codelineno-0-28"></a>            weight: 100
<a id="__codelineno-0-29" name="__codelineno-0-29" href="#reliability-docs-application-__codelineno-0-29"></a>          - podAffinityTerm:
<a id="__codelineno-0-30" name="__codelineno-0-30" href="#reliability-docs-application-__codelineno-0-30"></a>              labelSelector:
<a id="__codelineno-0-31" name="__codelineno-0-31" href="#reliability-docs-application-__codelineno-0-31"></a>                matchExpressions:
<a id="__codelineno-0-32" name="__codelineno-0-32" href="#reliability-docs-application-__codelineno-0-32"></a>                - key: app
<a id="__codelineno-0-33" name="__codelineno-0-33" href="#reliability-docs-application-__codelineno-0-33"></a>                  operator: In
<a id="__codelineno-0-34" name="__codelineno-0-34" href="#reliability-docs-application-__codelineno-0-34"></a>                  values:
<a id="__codelineno-0-35" name="__codelineno-0-35" href="#reliability-docs-application-__codelineno-0-35"></a>                  - web-server
<a id="__codelineno-0-36" name="__codelineno-0-36" href="#reliability-docs-application-__codelineno-0-36"></a>              topologyKey: kubernetes.io/hostname 
<a id="__codelineno-0-37" name="__codelineno-0-37" href="#reliability-docs-application-__codelineno-0-37"></a>            weight: 99
<a id="__codelineno-0-38" name="__codelineno-0-38" href="#reliability-docs-application-__codelineno-0-38"></a>      containers:
<a id="__codelineno-0-39" name="__codelineno-0-39" href="#reliability-docs-application-__codelineno-0-39"></a>      - name: web-app
<a id="__codelineno-0-40" name="__codelineno-0-40" href="#reliability-docs-application-__codelineno-0-40"></a>        image: nginx:1.16-alpine
</code></pre></div>
<h4 id="reliability-docs-application-using-pod-topology-spread-constraints">Using Pod topology spread constraints<a class="headerlink" href="#reliability-docs-application-using-pod-topology-spread-constraints" title="Permanent link">&para;</a></h4>
<p>Similar to pod anti-affinity rules, pod topology spread constraints allow you to make your application available across different failure (or topology) domains like hosts or AZs. This approach works very well when you're trying to ensure fault tolerance as well as availability by having multiple replicas in each of the different topology domains. Pod anti-affinity rules, on the other hand, can easily produce a result where you have a single replica in a topology domain because the pods with an anti-affinity toward each other have a repelling effect. In such cases, a single replica on a dedicated node isn't ideal for fault tolerance nor is it a good use of resources. With topology spread constraints, you have more control over the spread or distribution that the scheduler should try to apply across the topology domains. Here are some important properties to use in this approach:
1. The <code>maxSkew</code> is used to control or determine the maximum point to which things can be uneven across the topology domains. For example, if an application has 10 replicas and is deployed across 3 AZs, you can't get an even spread, but you can influence how uneven the distribution will be. In this case, the <code>maxSkew</code> can be anything between 1 and 10. A value of 1 means you can potentially end up with a spread like <code>4,3,3</code>, <code>3,4,3</code> or <code>3,3,4</code> across the 3 AZs. In contrast, a value of 10 means you can potentially end up with a spread like <code>10,0,0</code>, <code>0,10,0</code> or <code>0,0,10</code> across 3 AZs.
2. The <code>topologyKey</code> is a key for one of the node labels and defines the type of topology domain that should be used for the pod distribution. For example, a zonal spread would have the following key-value pair:
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#reliability-docs-application-__codelineno-1-1"></a>topologyKey: &quot;topology.kubernetes.io/zone&quot;
</code></pre></div>
3. The <code>whenUnsatisfiable</code> property is used to determine how you want the scheduler to respond if the desired constraints can't be satisfied.
4. The <code>labelSelector</code> is used to find matching pods so that the scheduler can be aware of them when deciding where to place pods in accordance with the constraints that you specify.</p>
<p>In addition to these above, there are other fields that you can read about further in the <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/">Kubernetes documentation</a>.</p>
<p><img alt="Pod topology spread constraints across 3 AZs" src="../reliability/docs/images/pod-topology-spread-constraints.jpg" /></p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#reliability-docs-application-__codelineno-2-1"></a>apiVersion: apps/v1
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#reliability-docs-application-__codelineno-2-2"></a>kind: Deployment
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#reliability-docs-application-__codelineno-2-3"></a>metadata:
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#reliability-docs-application-__codelineno-2-4"></a>  name: spread-host-az
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#reliability-docs-application-__codelineno-2-5"></a>  labels:
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#reliability-docs-application-__codelineno-2-6"></a>    app: web-server
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#reliability-docs-application-__codelineno-2-7"></a>spec:
<a id="__codelineno-2-8" name="__codelineno-2-8" href="#reliability-docs-application-__codelineno-2-8"></a>  replicas: 10
<a id="__codelineno-2-9" name="__codelineno-2-9" href="#reliability-docs-application-__codelineno-2-9"></a>  selector:
<a id="__codelineno-2-10" name="__codelineno-2-10" href="#reliability-docs-application-__codelineno-2-10"></a>    matchLabels:
<a id="__codelineno-2-11" name="__codelineno-2-11" href="#reliability-docs-application-__codelineno-2-11"></a>      app: web-server
<a id="__codelineno-2-12" name="__codelineno-2-12" href="#reliability-docs-application-__codelineno-2-12"></a>  template:
<a id="__codelineno-2-13" name="__codelineno-2-13" href="#reliability-docs-application-__codelineno-2-13"></a>    metadata:
<a id="__codelineno-2-14" name="__codelineno-2-14" href="#reliability-docs-application-__codelineno-2-14"></a>      labels:
<a id="__codelineno-2-15" name="__codelineno-2-15" href="#reliability-docs-application-__codelineno-2-15"></a>        app: web-server
<a id="__codelineno-2-16" name="__codelineno-2-16" href="#reliability-docs-application-__codelineno-2-16"></a>    spec:
<a id="__codelineno-2-17" name="__codelineno-2-17" href="#reliability-docs-application-__codelineno-2-17"></a>      topologySpreadConstraints:
<a id="__codelineno-2-18" name="__codelineno-2-18" href="#reliability-docs-application-__codelineno-2-18"></a>      - maxSkew: 1
<a id="__codelineno-2-19" name="__codelineno-2-19" href="#reliability-docs-application-__codelineno-2-19"></a>        topologyKey: &quot;topology.kubernetes.io/zone&quot;
<a id="__codelineno-2-20" name="__codelineno-2-20" href="#reliability-docs-application-__codelineno-2-20"></a>        whenUnsatisfiable: ScheduleAnyway
<a id="__codelineno-2-21" name="__codelineno-2-21" href="#reliability-docs-application-__codelineno-2-21"></a>        labelSelector:
<a id="__codelineno-2-22" name="__codelineno-2-22" href="#reliability-docs-application-__codelineno-2-22"></a>          matchLabels:
<a id="__codelineno-2-23" name="__codelineno-2-23" href="#reliability-docs-application-__codelineno-2-23"></a>            app: express-test
<a id="__codelineno-2-24" name="__codelineno-2-24" href="#reliability-docs-application-__codelineno-2-24"></a>      containers:
<a id="__codelineno-2-25" name="__codelineno-2-25" href="#reliability-docs-application-__codelineno-2-25"></a>      - name: web-app
<a id="__codelineno-2-26" name="__codelineno-2-26" href="#reliability-docs-application-__codelineno-2-26"></a>        image: nginx:1.16-alpine
</code></pre></div>
<h3 id="reliability-docs-application-run-kubernetes-metrics-server">Run Kubernetes Metrics Server<a class="headerlink" href="#reliability-docs-application-run-kubernetes-metrics-server" title="Permanent link">&para;</a></h3>
<p>Install the Kubernetes <a href="https://github.com/kubernetes-sigs/metrics-server">metrics server</a> to help scale your applications. Kubernetes autoscaler add-ons like <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">HPA</a> and <a href="https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler">VPA</a> need to track metrics of applications to scale them. The metrics-server collects resource metrics that can be used to make scaling decisions. The metrics are collected from kubelets and served in <a href="https://github.com/kubernetes/metrics">Metrics API format</a>.</p>
<p>The metrics server doesn’t retain any data, and it’s not a monitoring solution. Its purpose is to expose CPU and memory usage metrics to other systems. If you want to track your application's state over time, you need a monitoring tool like Prometheus or Amazon CloudWatch. </p>
<p>Follow the <a href="https://docs.aws.amazon.com/eks/latest/userguide/metrics-server.html">EKS documentation</a> to install metrics-server in your EKS cluster. </p>
<h2 id="reliability-docs-application-horizontal-pod-autoscaler-hpa">Horizontal Pod Autoscaler (HPA)<a class="headerlink" href="#reliability-docs-application-horizontal-pod-autoscaler-hpa" title="Permanent link">&para;</a></h2>
<p>HPA can automatically scale your application in response to demand and help you avoid impacting your customers during peak traffic. It is implemented as a control loop in Kubernetes that periodically queries metrics from APIs that provide resource metrics.</p>
<p>HPA can retrieve metrics from the following APIs:
1. <code>metrics.k8s.io</code> also known as Resource Metrics API — Provides CPU and memory usage for pods
2. <code>custom.metrics.k8s.io</code> — Provides metrics from other metric collectors like Prometheus; these metrics are <strong>internal</strong> to your Kubernetes cluster. 
3. <code>external.metrics.k8s.io</code> — Provides metrics that are <strong>external</strong> to your Kubernetes cluster (E.g., SQS Queue Depth, ELB latency).</p>
<p>You must use one of these three APIs to provide the metric to scale your application. </p>
<h3 id="reliability-docs-application-scaling-applications-based-on-custom-or-external-metrics">Scaling applications based on custom or external metrics<a class="headerlink" href="#reliability-docs-application-scaling-applications-based-on-custom-or-external-metrics" title="Permanent link">&para;</a></h3>
<p>You can use custom or external metrics to scale your application on metrics other than CPU or memory utilization. <a href="https://github.com/kubernetes-sigs/custom-metrics-apiserver">Custom Metrics</a> API servers provide the <code>custom-metrics.k8s.io</code> API that HPA can use to autoscale applications. </p>
<p>You can use the <a href="https://github.com/directxman12/k8s-prometheus-adapter">Prometheus Adapter for Kubernetes Metrics APIs</a> to collect metrics from Prometheus and use with the HPA. In this case, Prometheus adapter will expose Prometheus metrics in <a href="https://github.com/kubernetes/metrics/blob/master/pkg/apis/metrics/types.go">Metrics API format</a>. </p>
<p>Once you deploy the Prometheus Adapter, you can query custom metrics using kubectl.
<code>kubectl get —raw /apis/custom.metrics.k8s.io/v1beta1/</code></p>
<p>External metrics, as the name suggests, provide the Horizontal Pod Autoscaler the ability to scale deployments using metrics that are external to the Kubernetes cluster. For example, in batch processing workloads, it is common to scale the number of replicas based on the number of jobs in flight in an SQS queue.</p>
<p>To autoscale a Deployment using a CloudWatch metric, for example, <a href="https://github.com/awslabs/k8s-cloudwatch-adapter/blob/master/samples/sqs/README.md">scaling a batch-processor application based on SQS queue depth</a>, you can use <a href="https://github.com/awslabs/k8s-cloudwatch-adapter"><code>k8s-cloudwatch-adapter</code></a>. <code>k8s-cloudwatch-adapter</code> is a community project and not maintained by AWS. </p>
<h2 id="reliability-docs-application-vertical-pod-autoscaler-vpa">Vertical Pod Autoscaler (VPA)<a class="headerlink" href="#reliability-docs-application-vertical-pod-autoscaler-vpa" title="Permanent link">&para;</a></h2>
<p>VPA automatically adjusts the CPU and memory reservation for your Pods to help you “right-size” your applications. For applications that need to be scaled vertically - which is done by increasing resource allocation - you can use <a href="https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler">VPA</a> to automatically scale Pod replicas or provide scaling recommendations.</p>
<p>Your application may become temporarily unavailable if VPA needs to scale it because VPA’s current implementation does not perform in-place adjustments to Pods; instead, it will recreate the Pod that needs to be scaled. </p>
<p><a href="https://docs.aws.amazon.com/eks/latest/userguide/vertical-pod-autoscaler.html">EKS Documentation</a> includes a walkthrough for setting up VPA. </p>
<p><a href="https://github.com/FairwindsOps/goldilocks/">Fairwinds Goldilocks</a> project provides a dashboard to visualize VPA recommendations for CPU and memory requests and limits. Its VPA update mode allows you to auto-scale Pods based on VPA recommendations. </p>
<h2 id="reliability-docs-application-updating-applications">Updating applications<a class="headerlink" href="#reliability-docs-application-updating-applications" title="Permanent link">&para;</a></h2>
<p>Modern applications require rapid innovation with a high degree of stability and availability. Kubernetes gives you the tools to update your applications continuously without disrupting your customers. </p>
<p>Let’s look at some of the best practices that make it possible to quickly deploy changes without sacrificing availability.</p>
<h3 id="reliability-docs-application-have-a-mechanism-to-perform-rollbacks">Have a mechanism to perform rollbacks<a class="headerlink" href="#reliability-docs-application-have-a-mechanism-to-perform-rollbacks" title="Permanent link">&para;</a></h3>
<p>Having an undo button can evade disasters. It is a best practice to test deployments in a separate lower environment (test or development environment) before updating the production cluster. Using a CI/CD pipeline can help you automate and test deployments. With a continuous deployment pipeline, you can quickly revert to the older version if the upgrade happens to be defective.  </p>
<p>You can use Deployments to update a running application. This is typically done by updating the container image. You can use <code>kubectl</code> to update a Deployment like this:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#reliability-docs-application-__codelineno-3-1"></a>kubectl<span class="w"> </span>--record<span class="w"> </span>deployment.apps/nginx-deployment<span class="w"> </span><span class="nb">set</span><span class="w"> </span>image<span class="w"> </span>nginx-deployment<span class="w"> </span><span class="nv">nginx</span><span class="o">=</span>nginx:1.16.1
</code></pre></div>
<p>The <code>--record</code> argument record the changes to the Deployment and helps you if you need to perform a rollback. <code>kubectl rollout history deployment</code> shows you the recorded changes to Deployments in your cluster. You can rollback a change using <code>kubectl rollout undo deployment &lt;DEPLOYMENT_NAME&gt;</code>.</p>
<p>By default, when you update a Deployment that requires a recreation of pods, Deployment will perform a <a href="https://kubernetes.io/docs/tutorials/kubernetes-basics/update/update-intro/">rolling update</a>. In other words, Kubernetes will only update a portion of the running pods in a Deployment and not all the Pods at once. You can control how Kubernetes performs rolling updates through <code>RollingUpdateStrategy</code> property. </p>
<p>When performing a <em>rolling update</em> of a Deployment, you can use the <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#max-unavailable"><code>Max Unavailable</code></a> property to specify the maximum number of Pods that can be unavailable during the update. The <code>Max Surge</code> property of Deployment allows you to set the maximum number of Pods that can be created over the desired number of Pods.</p>
<p>Consider adjusting <code>max unavailable</code> to ensure that a rollout doesn’t disrupt your customers. For example, Kubernetes sets 25% <code>max unavailable</code> by default, which means if you have 100 Pods, you may have only 75 Pods actively working during a rollout. If your application needs a minimum of 80 Pods, this rollout can be disruptive. Instead, you can set <code>max unavailable</code> to 20% to ensure that there are at least 80 functional Pods throughout the rollout. </p>
<h3 id="reliability-docs-application-use-bluegreen-deployments">Use blue/green deployments<a class="headerlink" href="#reliability-docs-application-use-bluegreen-deployments" title="Permanent link">&para;</a></h3>
<p>Changes are inherently risky, but changes that cannot be undone can be potentially catastrophic. Change procedures that allow you to effectively turn back time through a <em>rollback</em> make enhancements and experimentation safer. Blue/green deployments give you a method to quickly retract the changes if things go wrong. In this deployment strategy, you create an environment for the new version. This environment is identical to the current version of the application being updated. Once the new environment is provisioned, traffic is routed to the new environment. If the new version produces the desired results without generating errors, the old environment is terminated. Otherwise, traffic is restored to the old version. </p>
<p>You can perform blue/green deployments in Kubernetes by creating a new Deployment that is identical to the existing version’s Deployment. Once you verify that the Pods in the new Deployment are running without errors, you can start sending traffic to the new Deployment by changing the <code>selector</code> spec in the Service that routes traffic to your application’s Pods.</p>
<p>Many continuous integration tools such as <a href="https://fluxcd.io">Flux</a>, <a href="https://www.jenkins.io">Jenkins</a>, and <a href="https://spinnaker.io">Spinnaker</a> let you automate blue/green deployments. AWS Containers Blog includes a walkthrough using AWS Load Balancer Controller: <a href="https://aws.amazon.com/blogs/containers/using-aws-load-balancer-controller-for-blue-green-deployment-canary-deployment-and-a-b-testing/">Using AWS Load Balancer Controller for blue/green deployment, canary deployment and A/B testing</a></p>
<h3 id="reliability-docs-application-use-canary-deployments">Use Canary deployments<a class="headerlink" href="#reliability-docs-application-use-canary-deployments" title="Permanent link">&para;</a></h3>
<p>Canary deployments are a variant of blue/green deployments that can significantly remove risk from changes. In this deployment strategy, you create a new Deployment with fewer Pods alongside your old Deployment, and divert a small percentage of traffic to the new Deployment. If metrics indicate that the new version is performing as well or better than the existing version, you progressively increase traffic to the new Deployment while scaling it up until all traffic is diverted to the new Deployment. If there's an issue, you can route all traffic to the old Deployment and stop sending traffic to the new Deployment.</p>
<p>Although Kubernetes offers no native way to perform canary deployments, you can use tools such as <a href="https://github.com/weaveworks/flagger">Flagger</a> with <a href="https://docs.flagger.app/tutorials/istio-progressive-delivery">Istio</a> or <a href="https://docs.flagger.app/install/flagger-install-on-eks-appmesh">App Mesh</a>.</p>
<h2 id="reliability-docs-application-health-checks-and-self-healing">Health checks and self-healing<a class="headerlink" href="#reliability-docs-application-health-checks-and-self-healing" title="Permanent link">&para;</a></h2>
<p>No software is bug-free, but Kubernetes can help you to minimize the impact of software failures. In the past, if an application crashed, someone had to remediate the situation by restarting the application manually. Kubernetes gives you the ability to detect software failures in your Pods and automatically replace them with new replicas. With Kubernetes you can monitor the health of your applications and automatically replace unhealthy instances.  </p>
<p>Kubernetes supports three types of <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/">health-checks</a>:</p>
<ol>
<li>Liveness probe</li>
<li>Startup probe (supported in Kubernetes version 1.16+)</li>
<li>Readiness probe</li>
</ol>
<p><a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/">Kubelet</a>, the Kubernetes agent, is responsible for running all the above-mentioned checks. Kubelet can check a Pods' health in three ways: kubelet can either run a shell command inside a Pod's container, send an HTTP GET request to its container, or open a TCP socket on a specified port.  </p>
<p>If you choose an <code>exec</code>-based probe, which runs a shell script inside a container, ensure that the shell command exits <em>before</em> the <code>timeoutSeconds</code> value expires. Otherwise, your node will have <code>&lt;defunct&gt;</code> processes, leading to node failure. </p>
<h2 id="reliability-docs-application-recommendations_1">Recommendations<a class="headerlink" href="#reliability-docs-application-recommendations_1" title="Permanent link">&para;</a></h2>
<h3 id="reliability-docs-application-use-liveness-probe-to-remove-unhealthy-pods">Use Liveness Probe to remove unhealthy pods<a class="headerlink" href="#reliability-docs-application-use-liveness-probe-to-remove-unhealthy-pods" title="Permanent link">&para;</a></h3>
<p>The Liveness probe can detect <em>deadlock</em> conditions where the process continues to run, but the application becomes unresponsive. For example, if you are running a web service that listens on port 80, you can configure a Liveness probe to send an HTTP GET request on Pod’s port 80. Kubelet will periodically send a GET request to the Pod and expect a response; if the Pod responds between 200-399 then the kubelet considers that Pod is healthy; otherwise, the Pod will be marked as unhealthy. If a Pod fails health-checks continuously, the kubelet will terminate it. </p>
<p>You can use <code>initialDelaySeconds</code> to delay the first probe.</p>
<p>When using the Liveness Probe, ensure that your application doesn’t run into a situation in which all Pods simultaneously fail the Liveness Probe because Kubernetes will try to replace all your Pods, which will render your application offline. Furthermore, Kubernetes will continue to create new Pods that will also fail Liveness Probes, putting unnecessary strain on the control plane. Avoid configuring the Liveness Probe to depend on an a factor that is external to your Pod, for example, a external database. In other words, a non-responsive external-to-your-Pod database shouldn’t make your Pods fail their Liveness Probes.</p>
<p>Sandor Szücs’s post <a href="https://srcco.de/posts/kubernetes-liveness-probes-are-dangerous.html">LIVENESS PROBES ARE DANGEROUS</a> describes problems that can be caused by misconfigured probes.</p>
<h3 id="reliability-docs-application-use-startup-probe-for-applications-that-take-longer-to-start">Use Startup Probe for applications that take longer to start<a class="headerlink" href="#reliability-docs-application-use-startup-probe-for-applications-that-take-longer-to-start" title="Permanent link">&para;</a></h3>
<p>When your app needs additional time to startup, you can use the Startup Probe to delay the Liveness and Readiness Probe. For example, a Java app that needs to hydrate cache from a database may need up to two minutes before it is fully functional. Any Liveness or Readiness Probe until it becomes fully functional might fail. Configuring a Startup Probe will allow the Java app to become <em>healthy</em> before Liveness or Readiness Probe are executed. </p>
<p>Until the Startup Probe succeeds, all the other Probes are disabled. You can define the maximum time Kubernetes should wait for application startup. If, after the maximum configured time, the Pod still fails Startup Probes, it will be terminated, and a new Pod will be created. </p>
<p>The Startup Probe is similar to the Liveness Probe -- if they fail, the Pod is recreated. As Ricardo A. explains in his post <a href="https://medium.com/swlh/fantastic-probes-and-how-to-configure-them-fef7e030bd2f">Fantastic Probes And How To Configure Them</a>, Startup Probes should be used when the startup time of an application is unpredictable. If you know your application needs ten seconds to start, you should use Liveness/Readiness Probe with <code>initialDelaySeconds</code> instead.</p>
<h3 id="reliability-docs-application-use-readiness-probe-to-detect-partial-unavailability">Use Readiness Probe to detect partial unavailability<a class="headerlink" href="#reliability-docs-application-use-readiness-probe-to-detect-partial-unavailability" title="Permanent link">&para;</a></h3>
<p>While the Liveness probe detects failures in an app that are resolved by terminating the Pod (hence, restarting the app), Readiness Probe detects conditions where the app may be <em>temporarily</em> unavailable. In these situations, the app may become temporarily unresponsive; however, it is expected to be healthy again once this operation completes.  </p>
<p>For example, during intense disk I/O operations, applications may be temporarily unavailable to handle requests. Here, terminating the application’s Pod is not a remedy; at the same time, additional requests sent to the Pod can fail. </p>
<p>You can use the Readiness Probe to detect temporary unavailability in your app and stop sending requests to its Pod until it becomes functional again. <em>Unlike Liveness Probe, where a failure would result in a recreation of Pod, a failed Readiness Probe would mean that Pod will not receive any traffic from Kubernetes Service</em>. When the Readiness Probe succeeds, Pod will resume receiving traffic from Service. </p>
<p>Just like the Liveness Probe, avoid configuring Readiness Probes that depend on a resource that’s external to the Pod (such as a database). Here’s a scenario where a poorly configured Readiness can render the application nonfunctional - if a Pod’s Readiness Probe fails when the app’s database is unreachable, other Pod replicas will also fail simultaneously since they share the same health-check criteria. Setting the probe in this way will ensure that whenever the database is unavailable, the Pod’s Readiness Probes will fail, and Kubernetes will stop sending traffic <em>all</em> Pods.  </p>
<p>A side-effect of using Readiness Probes is that they can increase the time it takes to update Deployments. New replicas will not receive traffic unless Readiness Probes are successful; until then, old replicas will continue to receive traffic. </p>
<hr />
<h2 id="reliability-docs-application-dealing-with-disruptions">Dealing with disruptions<a class="headerlink" href="#reliability-docs-application-dealing-with-disruptions" title="Permanent link">&para;</a></h2>
<p>Pods have a finite lifetime - even if you have long-running Pods, it’s prudent to ensure Pods terminate correctly when the time comes. Depending on your upgrade strategy, Kubernetes cluster upgrades may require you to create new worker nodes, which requires all Pods to be recreated on newer nodes. Proper termination handling and Pod Disruption Budgets can help you avoid service disruptions as Pods are removed from older nodes and recreated on newer nodes.  </p>
<p>The preferred way to upgrade worker nodes is by creating new worker nodes and terminating old ones. Before terminating worker nodes, you should <code>drain</code> it. When a worker node is drained, all its pods are <em>safely</em> evicted. Safely is a key word here; when pods on a worker are evicted, they are not simply sent a <code>SIGKILL</code> signal. Instead, a <code>SIGTERM</code> signal is sent to the main process (PID 1) of each container in the Pods being evicted. After the <code>SIGTERM</code> signal is sent, Kubernetes will give the process some time (grace period) before a <code>SIGKILL</code> signal is sent. This grace period is 30 seconds by default; you can override the default by using <code>grace-period</code> flag in kubectl or declare <code>terminationGracePeriodSeconds</code> in your Podspec.</p>
<p><code>kubectl delete pod &lt;pod name&gt; —grace-period=&lt;seconds&gt;</code></p>
<p>It is common to have containers in which the main process doesn’t have PID 1. Consider this Python-based sample container:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#reliability-docs-application-__codelineno-4-1"></a>$ kubectl exec python-app -it ps
<a id="__codelineno-4-2" name="__codelineno-4-2" href="#reliability-docs-application-__codelineno-4-2"></a> PID USER TIME COMMAND
<a id="__codelineno-4-3" name="__codelineno-4-3" href="#reliability-docs-application-__codelineno-4-3"></a> 1   root 0:00 {script.sh} /bin/sh ./script.sh
<a id="__codelineno-4-4" name="__codelineno-4-4" href="#reliability-docs-application-__codelineno-4-4"></a> 5   root 0:00 python app.py
</code></pre></div>
<p>In this example, the shell script receives <code>SIGTERM</code>, the main process, which happens to be a Python application in this example, doesn’t get a <code>SIGTERM</code> signal. When the Pod is terminated, the Python application will be killed abruptly. This can be remediated by changing the <a href="https://docs.docker.com/engine/reference/builder/#entrypoint"><code>ENTRYPOINT</code></a> of the container to launch the Python application. Alternatively, you can use a tool like <a href="https://github.com/Yelp/dumb-init">dumb-init</a> to ensure that your application can handle signals.  </p>
<p>You can also use <a href="https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks">Container hooks</a> to execute a script or an HTTP request at container start or stop. The <code>PreStop</code> hook action runs before the container receives a <code>SIGTERM</code> signal and must complete before this signal is sent. The <code>terminationGracePeriodSeconds</code> value applies from when the <code>PreStop</code> hook action begins executing, not when the <code>SIGTERM</code> signal is sent.</p>
<h2 id="reliability-docs-application-recommendations_2">Recommendations<a class="headerlink" href="#reliability-docs-application-recommendations_2" title="Permanent link">&para;</a></h2>
<h3 id="reliability-docs-application-protect-critical-workload-with-pod-disruption-budgets">Protect critical workload with Pod Disruption Budgets<a class="headerlink" href="#reliability-docs-application-protect-critical-workload-with-pod-disruption-budgets" title="Permanent link">&para;</a></h3>
<p>Pod Disruption Budget or PDB can temporarily halt the eviction process if the number of replicas of an application falls below the declared threshold. The eviction process will continue once the number of available replicas is over the threshold. You can use PDB to declare the <code>minAvailable</code> and <code>maxUnavailable</code> number of replicas. For example, if you want at least three copies of your app to be available, you can create a PDB. </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#reliability-docs-application-__codelineno-5-1"></a>apiVersion: policy/v1beta1
<a id="__codelineno-5-2" name="__codelineno-5-2" href="#reliability-docs-application-__codelineno-5-2"></a>kind: PodDisruptionBudget
<a id="__codelineno-5-3" name="__codelineno-5-3" href="#reliability-docs-application-__codelineno-5-3"></a>metadata:
<a id="__codelineno-5-4" name="__codelineno-5-4" href="#reliability-docs-application-__codelineno-5-4"></a>  name: my-svc-pdb
<a id="__codelineno-5-5" name="__codelineno-5-5" href="#reliability-docs-application-__codelineno-5-5"></a>spec:
<a id="__codelineno-5-6" name="__codelineno-5-6" href="#reliability-docs-application-__codelineno-5-6"></a>  minAvailable: 3
<a id="__codelineno-5-7" name="__codelineno-5-7" href="#reliability-docs-application-__codelineno-5-7"></a>  selector:
<a id="__codelineno-5-8" name="__codelineno-5-8" href="#reliability-docs-application-__codelineno-5-8"></a>    matchLabels:
<a id="__codelineno-5-9" name="__codelineno-5-9" href="#reliability-docs-application-__codelineno-5-9"></a>      app: my-svc
</code></pre></div>
<p>The above PDB policy tells Kubernetes to halt the eviction process until three or more replicas are available. Node draining respects <code>PodDisruptionBudgets</code>. During an EKS managed node group upgrade, <a href="https://docs.aws.amazon.com/eks/latest/userguide/managed-node-update-behavior.html">nodes are drained with a fifteen-minute timeout</a>. After fifteen minutes, if the update is not forced (the option is called Rolling update in the EKS console), the update fails. If the update is forced, the pods are deleted.</p>
<p>For self-managed nodes, you can also use tools like <a href="https://github.com/aws/aws-node-termination-handler">AWS Node Termination Handler</a>, which ensures that the Kubernetes control plane responds appropriately to events that can cause your EC2 instance to become unavailable, such as <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-instances-status-check_sched.html">EC2 maintenance</a> events and <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-interruptions.html">EC2 Spot interruptions</a>. It uses the Kubernetes API to cordon the node to ensure no new Pods are scheduled, then drains it, terminating any running Pods.</p>
<p>You can use Pod anti-affinity to schedule a Deployment‘s Pods on different nodes and avoid PDB related delays during node upgrades. </p>
<h3 id="reliability-docs-application-practice-chaos-engineering">Practice chaos engineering<a class="headerlink" href="#reliability-docs-application-practice-chaos-engineering" title="Permanent link">&para;</a></h3>
<blockquote>
<p>Chaos Engineering is the discipline of experimenting on a distributed system in order to build confidence in the system’s capability to withstand turbulent conditions in production.</p>
</blockquote>
<p>In his blog, Dominik Tornow explains that <a href="https://medium.com/@dominik.tornow/the-mechanics-of-kubernetes-ac8112eaa302">Kubernetes is a declarative system</a> where “<em>the user supplies a representation of the desired state of the system to the system. The system then considers the current state and the desired state to determine the sequence of commands to transition from the current state to the desired state.</em>” This means Kubernetes always stores the <em>desired state</em> and if the system deviates, Kubernetes will take action to restore the state. For example, if a worker node becomes unavailable, Kubernetes will reschedule the Pods onto another worker node. Similarly, if a <code>replica</code> crashes, the <a href="https://kubernetes.io/docs/concepts/architecture/controller/#design">Deployment Contoller</a> will create a new <code>replica</code>. In this way, Kubernetes controllers automatically fix failures. </p>
<p>Chaos engineering tools like <a href="https://www.gremlin.com">Gremlin</a> help you test the resiliency of your Kubernetes cluster and identify single points of failure. Tools that introduce artificial chaos in your cluster (and beyond) can uncover systemic weaknesses, present an opportunity to identify  bottlenecks and misconfigurations, and rectify problems in a controlled environment. The Chaos Engineering philosophy advocates breaking things on purpose and stress testing infrastructure to minimize unanticipated downtime. </p>
<h3 id="reliability-docs-application-use-a-service-mesh">Use a Service Mesh<a class="headerlink" href="#reliability-docs-application-use-a-service-mesh" title="Permanent link">&para;</a></h3>
<p>You can use a service mesh to improve your application’s resiliency. Service meshes enable service-to-service communication and increase the observability of your microservices network. Most service mesh products work by having a small network proxy run alongside each service that intercepts and inspects the application’s network traffic. You can place your application in a mesh without modifying your application. Using service proxy’s built-in features, you can have it generate network statistics, create access logs, and add HTTP headers to outbound requests for distributed tracing.</p>
<p>A service mesh can help you make your microservices more resilient with features like automatic request retries, timeouts, circuit-breaking, and rate-limiting.</p>
<p>If you operate multiple clusters, you can use a service mesh to enable cross-cluster service-to-service communication.</p>
<h3 id="reliability-docs-application-service-meshes">Service Meshes<a class="headerlink" href="#reliability-docs-application-service-meshes" title="Permanent link">&para;</a></h3>
<ul>
<li><a href="https://aws.amazon.com/app-mesh/">AWS App Mesh</a></li>
<li><a href="https://istio.io">Istio</a></li>
<li><a href="http://linkerd.io">LinkerD</a></li>
<li><a href="https://www.consul.io">Consul</a></li>
</ul>
<hr />
<h2 id="reliability-docs-application-observability">Observability<a class="headerlink" href="#reliability-docs-application-observability" title="Permanent link">&para;</a></h2>
<p>Observability is an umbrella term that includes monitoring, logging, and tracing. Microservices based applications are distributed by nature. Unlike monolithic applications where monitoring a single system is sufficient, in a distributed application architecture, you need to monitor each component’s performance. You can use cluster-level monitoring, logging, and distributed tracing systems to identify issues in your cluster before they disrupt your customers. </p>
<p>Kubernetes built-in tools for troubleshooting and monitoring are limited. The metrics-server collects resource metrics and stores them in memory but doesn’t persist them. You can view the logs of a Pod using kubectl, but Kubernetes doesn't automatically retain logs. And the implementation of distributed tracing is done either at the application code level or using services meshes. </p>
<p>Kubernetes' extensibility shines here. Kubernetes allows you to bring your preferred centralized monitoring, logging, and tracing solution. </p>
<h2 id="reliability-docs-application-recommendations_3">Recommendations<a class="headerlink" href="#reliability-docs-application-recommendations_3" title="Permanent link">&para;</a></h2>
<h3 id="reliability-docs-application-monitor-your-applications">Monitor your applications<a class="headerlink" href="#reliability-docs-application-monitor-your-applications" title="Permanent link">&para;</a></h3>
<p>The number of metrics you need to monitor in modern applications is growing continuously. It helps if you have an automated way to track your applications so you can focus on solving your customer’s challenges. Cluster-wide monitoring tools like <a href="https://prometheus.io">Prometheus</a> or <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/ContainerInsights.html">CloudWatch Container Insights</a> can monitor your cluster and workload and provide you signals when, or preferably, before things go wrong. </p>
<p>Monitoring tools allow you to create alerts that your operations team can subscribe to. Consider rules to activate alarms for events that can, when exacerbated, lead to an outage or impact application performance. </p>
<p>If you’re unclear on which metrics you should monitor, you can take inspiration from these methods:</p>
<ul>
<li><a href="https://www.weave.works/blog/a-practical-guide-from-instrumenting-code-to-specifying-alerts-with-the-red-method">RED method</a>. Stands for requests, errors, and duration. </li>
<li><a href="http://www.brendangregg.com/usemethod.html">USE method</a>. Stands for utilization, saturation, and errors.  </li>
</ul>
<p>Sysdig’s post <a href="https://sysdig.com/blog/alerting-kubernetes/">Best practices for alerting on Kubernetes</a> includes a comprehensive list of components that can impact the availability of your applications.</p>
<h3 id="reliability-docs-application-use-prometheus-client-library-to-expose-application-metrics">Use Prometheus client library to expose application metrics<a class="headerlink" href="#reliability-docs-application-use-prometheus-client-library-to-expose-application-metrics" title="Permanent link">&para;</a></h3>
<p>In addition to monitoring the state of the application and aggregating standard metrics, you can also use the <a href="https://prometheus.io/docs/instrumenting/clientlibs/">Prometheus client library</a> to expose application-specific custom metrics to improve the application's observability.</p>
<h3 id="reliability-docs-application-use-centralized-logging-tools-to-collect-and-persist-logs">Use centralized logging tools to collect and persist logs<a class="headerlink" href="#reliability-docs-application-use-centralized-logging-tools-to-collect-and-persist-logs" title="Permanent link">&para;</a></h3>
<p>Logging in EKS falls under two categories: control plane logs and application logs. EKS control plane logging provides audit and diagnostic logs directly from the control plane to CloudWatch Logs in your account. Application logs are logs produced by Pods running inside your cluster. Application logs include logs produced by Pods that run the business logic applications and Kubernetes system components such as CoreDNS, Cluster Autoscaler, Prometheus, etc. </p>
<p><a href="https://docs.aws.amazon.com/eks/latest/userguide/control-plane-logs.html">EKS provide five types of control plane logs</a>:</p>
<ol>
<li>Kubernetes API server component logs</li>
<li>Audit</li>
<li>Authenticator</li>
<li>Controller manager </li>
<li>Scheduler</li>
</ol>
<p>The controller manager and scheduler logs can help diagnose control plane problems such as bottlenecks and errors. By default, EKS control plane logs aren’t sent to CloudWatch Logs. You can enable control plane logging and select the types of EKS control plane logs you’d like to capture for each cluster in your account</p>
<p>Collecting application logs requires installing a log aggregator tool like <a href="http://fluentbit.io">Fluent Bit</a>, <a href="https://www.fluentd.org">Fluentd</a>, or <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/deploy-container-insights-EKS.html">CloudWatch Container Insights</a> in your cluster.  </p>
<p>Kubernetes log aggregator tools run as DaemonSets and scrape container logs from nodes. Application logs are then sent to a centralized destination for storage. For example, CloudWatch Container Insights can use either Fluent Bit or Fluentd to collect logs and ship them to CloudWatch Logs for storage. Fluent Bit and Fluentd support many popular log analytics systems such as Elasticsearch and InfluxDB giving you the ability to change the storage backend for your logs by modifying Fluent bit or Fluentd’s log configuration. </p>
<h3 id="reliability-docs-application-use-a-distributed-tracing-system-to-identify-bottlenecks">Use a distributed tracing system to identify bottlenecks<a class="headerlink" href="#reliability-docs-application-use-a-distributed-tracing-system-to-identify-bottlenecks" title="Permanent link">&para;</a></h3>
<p>A typical modern application has components distributed over the network, and its reliability depends on the proper functioning of each of the components that make up the application. You can use a distributed tracing solution to understand how requests flow and how systems communicate. Traces can show you where bottlenecks exist in your application network and prevent problems that can cause cascading failures. </p>
<p>You have two options to implement tracing in your applications: you can either implement distributed tracing at the code level using shared libraries or use a service mesh. </p>
<p>Implementing tracing at the code level can be disadvantageous. In this method, you have to make changes to your code. This is further complicated if you have polyglot applications. You’re also responsible for maintaining yet another library, across your services. </p>
<p>Service Meshes like <a href="http://linkerd.io">LinkerD</a>, <a href="http://istio.io">Istio</a>, and <a href="https://aws.amazon.com/app-mesh/">AWS App Mesh</a> can be used to implement distributed tracing in your application with minimal changes to the application code. You can use service mesh to standardize metrics generation, logging, and tracing. </p>
<p>Tracing tools like <a href="https://aws.amazon.com/xray/">AWS X-Ray</a>, <a href="https://www.jaegertracing.io">Jaeger</a> support both shared library and service mesh implementations. </p>
<p>Consider using a tracing tool like <a href="https://aws.amazon.com/xray/">AWS X-Ray</a> or <a href="https://www.jaegertracing.io">Jaeger</a> that supports both (shared library and service mesh) implementations so you will not have to switch tools if you later adopt service mesh. </p></section><section class="print-page" id="reliability-docs-controlplane"><h1 id="reliability-docs-controlplane-eks-control-plane">EKS Control Plane<a class="headerlink" href="#reliability-docs-controlplane-eks-control-plane" title="Permanent link">&para;</a></h1>
<p>Amazon Elastic Kubernetes Service (EKS) is a managed Kubernetes service that makes it easy for you to run Kubernetes on AWS without needing to install, operate, and maintain your own Kubernetes control plane or worker nodes. It runs upstream Kubernetes and is certified Kubernetes conformant. This conformance ensures that EKS supports the Kubernetes APIs, just like the open-source community version that you can install on EC2 or on-premises. Existing applications running on upstream Kubernetes are compatible with Amazon EKS.</p>
<p>EKS automatically manages the availability and scalability of the Kubernetes control plane nodes, and it automatically replaces unhealthy control plane nodes.</p>
<h2 id="reliability-docs-controlplane-eks-architecture">EKS Architecture<a class="headerlink" href="#reliability-docs-controlplane-eks-architecture" title="Permanent link">&para;</a></h2>
<p>EKS architecture is designed to eliminate any single points of failure that may compromise the availability and durability of the Kubernetes control plane.</p>
<p>The Kubernetes control plane managed by EKS runs inside an EKS managed VPC. The EKS control plane comprises the Kubernetes API server nodes, etcd cluster. Kubernetes API server nodes that run components like the API server, scheduler, and <code>kube-controller-manager</code> run in an auto-scaling group. EKS runs a minimum of two API server nodes in distinct Availability Zones (AZs) within in AWS region. Likewise, for durability, the etcd server nodes also run in an auto-scaling group that spans three AZs. EKS runs a NAT Gateway in each AZ, and API servers and etcd servers run in a private subnet. This architecture ensures that an event in a single AZ doesn’t affect the EKS cluster's availability.</p>
<p>When you create a new cluster, Amazon EKS creates a highly-available endpoint for the managed Kubernetes API server that you use to communicate with your cluster (using tools like <code>kubectl</code>). The managed endpoint uses NLB to load balance Kubernetes API servers. EKS also provisions two <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html">ENI</a>s in different AZs to facilitate communication to your worker nodes.</p>
<p><img alt="EKS Data plane network connectivity" src="../reliability/docs/images/eks-data-plane-connectivity.jpeg" /></p>
<p>You can <a href="https://docs.aws.amazon.com/eks/latest/userguide/cluster-endpoint.html">configure whether your Kubernetes cluster’s API server</a> is reachable from the public internet (using the public endpoint) or through your VPC (using the EKS-managed ENIs) or both.</p>
<p>Whether users and worker nodes connect to the API server using the public endpoint or the EKS-managed ENI, there are redundant paths for connection.</p>
<h2 id="reliability-docs-controlplane-recommendations">Recommendations<a class="headerlink" href="#reliability-docs-controlplane-recommendations" title="Permanent link">&para;</a></h2>
<h2 id="reliability-docs-controlplane-monitor-control-plane-metrics">Monitor Control Plane Metrics<a class="headerlink" href="#reliability-docs-controlplane-monitor-control-plane-metrics" title="Permanent link">&para;</a></h2>
<p>Monitoring Kubernetes API metrics can give you insights into control plane performance and identify issues. An unhealthy control plane can compromise the availability of the workloads running inside the cluster. For example, poorly written controllers can overload the API servers, affecting your application's availability.</p>
<p>Kubernetes exposes control plane metrics at the  <code>/metrics</code> endpoint.</p>
<p>You can view the metrics exposed using <code>kubectl</code>:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#reliability-docs-controlplane-__codelineno-0-1"></a>kubectl<span class="w"> </span>get<span class="w"> </span>--raw<span class="w"> </span>/metrics
</code></pre></div>
<p>These metrics are represented in a <a href="https://github.com/prometheus/docs/blob/master/content/docs/instrumenting/exposition_formats.md">Prometheus text format</a>.</p>
<p>You can use Prometheus to collect and store these metrics. In May 2020, CloudWatch added support for monitoring Prometheus metrics in CloudWatch Container Insights. So you can also use Amazon CloudWatch to monitor the EKS control plane. You can use <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/ContainerInsights-Prometheus-Setup-configure.html#ContainerInsights-Prometheus-Setup-new-exporters">Tutorial for Adding a New Prometheus Scrape Target: Prometheus KPI Server Metrics</a> to collect metrics and create CloudWatch dashboard to monitor your cluster’s control plane.</p>
<p>You can find Kubernetes API server metrics  <a href="https://github.com/kubernetes/apiserver/blob/master/pkg/endpoints/metrics/metrics.go">here</a>. For example, <code>apiserver_request_duration_seconds</code> can indicate how long API requests are taking to run.</p>
<p>Consider monitoring these control plane metrics:</p>
<h3 id="reliability-docs-controlplane-api-server">API Server<a class="headerlink" href="#reliability-docs-controlplane-api-server" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th style="text-align: left;">Metric</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><code>apiserver_request_total</code></td>
<td style="text-align: left;">Counter of apiserver requests broken out for each verb, dry run value, group, version, resource, scope, component, and HTTP response code.</td>
</tr>
<tr>
<td style="text-align: left;"><code>apiserver_request_duration_seconds*</code></td>
<td style="text-align: left;">Response latency distribution in seconds for each verb, dry run value, group, version, resource, subresource, scope, and component.</td>
</tr>
<tr>
<td style="text-align: left;"><code>apiserver_admission_controller_admission_duration_seconds</code></td>
<td style="text-align: left;">Admission controller latency histogram in seconds, identified by name and broken out for each operation and API resource and type (validate or admit).</td>
</tr>
<tr>
<td style="text-align: left;"><code>apiserver_admission_webhook_rejection_count</code></td>
<td style="text-align: left;">Count of admission webhook rejections. Identified by name, operation, rejection_code, type (validating or admit), error_type (calling_webhook_error, apiserver_internal_error, no_error)</td>
</tr>
<tr>
<td style="text-align: left;"><code>rest_client_request_duration_seconds</code></td>
<td style="text-align: left;">Request latency in seconds. Broken down by verb and URL.</td>
</tr>
<tr>
<td style="text-align: left;"><code>rest_client_requests_total</code></td>
<td style="text-align: left;">Number of HTTP requests, partitioned by status code, method, and host.</td>
</tr>
</tbody>
</table>
<h3 id="reliability-docs-controlplane-etcd">etcd<a class="headerlink" href="#reliability-docs-controlplane-etcd" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th style="text-align: left;">Metric</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><code>etcd_request_duration_seconds</code></td>
<td style="text-align: left;">Etcd request latency in seconds for each operation and object type.</td>
</tr>
<tr>
<td style="text-align: left;"><code>etcd_db_total_size_in_bytes</code> or <br /><code>apiserver_storage_db_total_size_in_bytes</code> (starting with EKS v1.26) or <br /><code>apiserver_storage_size_bytes</code> (starting with EKS v1.28)</td>
<td style="text-align: left;">Etcd database size.</td>
</tr>
</tbody>
</table>
<p>Consider using the <a href="https://grafana.com/grafana/dashboards/14623">Kubernetes Monitoring Overview Dashboard</a> to visualize and monitor Kubernetes API server requests and latency and etcd latency metrics.</p>
<p>The following Prometheus query can be used to monitor the current size of etcd. The query assumes there is job called <code>kube-apiserver</code> for scraping metrics from API metrics endpoint and the EKS version is below v1.26. </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#reliability-docs-controlplane-__codelineno-1-1"></a>max(etcd_db_total_size_in_bytes{job=&quot;kube-apiserver&quot;} / (8 * 1024 * 1024 * 1024))
</code></pre></div>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>When the database size limit is exceeded, etcd emits a no space alarm and stops taking further write requests. In other words, the cluster becomes read-only, and all requests to mutate objects such as creating new pods, scaling deployments, etc., will be rejected by the cluster’s API server.</p>
</div>
<h2 id="reliability-docs-controlplane-cluster-authentication">Cluster Authentication<a class="headerlink" href="#reliability-docs-controlplane-cluster-authentication" title="Permanent link">&para;</a></h2>
<p>EKS currently supports two types of authentication: <a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#service-account-tokens">bearer/service account tokens</a> and IAM authentication which uses <a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#webhook-token-authentication">webhook token authentication</a>. When users call the Kubernetes API, a webhook passes an authentication token included in the request to IAM. The token, a base 64 signed URL, is generated by the AWS Command Line Interface (<a href="https://aws.amazon.com/cli/">AWS CLI</a>).</p>
<p>The IAM user or role that creates the EKS Cluster automatically gets full access to the cluster. You can manage access to the EKS cluster by editing the <a href="https://docs.aws.amazon.com/eks/latest/userguide/add-user-role.html"><code>aws-auth</code> configmap</a>.</p>
<p>If you misconfigure the <code>aws-auth</code> configmap and lose access to the cluster, you can still use the cluster creator’s user or role to access your EKS cluster.</p>
<p>In the unlikely event that you cannot use the IAM service in the AWS region, you can also use the Kubernetes service account’s bearer token to manage the cluster.</p>
<p>Create a “super-admin” account that is permitted to perform all actions in the cluster:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#reliability-docs-controlplane-__codelineno-2-1"></a>kubectl -n kube-system create serviceaccount super-admin
</code></pre></div>
<p>Create a role binding that gives super-admin cluster-admin role:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#reliability-docs-controlplane-__codelineno-3-1"></a>kubectl create clusterrolebinding super-admin-rb --clusterrole=cluster-admin --serviceaccount=kube-system:super-admin
</code></pre></div>
<p>Get service account’s secret:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#reliability-docs-controlplane-__codelineno-4-1"></a>SECRET_NAME=`kubectl -n kube-system get serviceaccount/super-admin -o jsonpath=&#39;{.secrets[0].name}&#39;`
</code></pre></div>
<p>Get token associated with the secret:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#reliability-docs-controlplane-__codelineno-5-1"></a>TOKEN=`kubectl -n kube-system get secret $SECRET_NAME -o jsonpath=&#39;{.data.token}&#39;| base64 --decode`
</code></pre></div>
<p>Add service account and token to <code>kubeconfig</code>:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#reliability-docs-controlplane-__codelineno-6-1"></a>kubectl config set-credentials super-admin --token=$TOKEN
</code></pre></div>
<p>Set the current-context in <code>kubeconfig</code> to use super-admin account:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#reliability-docs-controlplane-__codelineno-7-1"></a>kubectl config set-context --current --user=super-admin
</code></pre></div>
<p>Final <code>kubeconfig</code> should look like this:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1" href="#reliability-docs-controlplane-__codelineno-8-1"></a>apiVersion: v1
<a id="__codelineno-8-2" name="__codelineno-8-2" href="#reliability-docs-controlplane-__codelineno-8-2"></a>clusters:
<a id="__codelineno-8-3" name="__codelineno-8-3" href="#reliability-docs-controlplane-__codelineno-8-3"></a>- cluster:
<a id="__codelineno-8-4" name="__codelineno-8-4" href="#reliability-docs-controlplane-__codelineno-8-4"></a>    certificate-authority-data:&lt;REDACTED&gt;
<a id="__codelineno-8-5" name="__codelineno-8-5" href="#reliability-docs-controlplane-__codelineno-8-5"></a>    server: https://&lt;CLUSTER&gt;.gr7.us-west-2.eks.amazonaws.com
<a id="__codelineno-8-6" name="__codelineno-8-6" href="#reliability-docs-controlplane-__codelineno-8-6"></a>  name: arn:aws:eks:us-west-2:&lt;account number&gt;:cluster/&lt;cluster name&gt;
<a id="__codelineno-8-7" name="__codelineno-8-7" href="#reliability-docs-controlplane-__codelineno-8-7"></a>contexts:
<a id="__codelineno-8-8" name="__codelineno-8-8" href="#reliability-docs-controlplane-__codelineno-8-8"></a>- context:
<a id="__codelineno-8-9" name="__codelineno-8-9" href="#reliability-docs-controlplane-__codelineno-8-9"></a>    cluster: arn:aws:eks:us-west-2:&lt;account number&gt;:cluster/&lt;cluster name&gt;
<a id="__codelineno-8-10" name="__codelineno-8-10" href="#reliability-docs-controlplane-__codelineno-8-10"></a>    user: super-admin
<a id="__codelineno-8-11" name="__codelineno-8-11" href="#reliability-docs-controlplane-__codelineno-8-11"></a>  name: arn:aws:eks:us-west-2:&lt;account number&gt;:cluster/&lt;cluster name&gt;
<a id="__codelineno-8-12" name="__codelineno-8-12" href="#reliability-docs-controlplane-__codelineno-8-12"></a>current-context: arn:aws:eks:us-west-2:&lt;account number&gt;:cluster/&lt;cluster name&gt;
<a id="__codelineno-8-13" name="__codelineno-8-13" href="#reliability-docs-controlplane-__codelineno-8-13"></a>kind: Config
<a id="__codelineno-8-14" name="__codelineno-8-14" href="#reliability-docs-controlplane-__codelineno-8-14"></a>preferences: {}
<a id="__codelineno-8-15" name="__codelineno-8-15" href="#reliability-docs-controlplane-__codelineno-8-15"></a>users:
<a id="__codelineno-8-16" name="__codelineno-8-16" href="#reliability-docs-controlplane-__codelineno-8-16"></a>#- name: arn:aws:eks:us-west-2:&lt;account number&gt;:cluster/&lt;cluster name&gt;
<a id="__codelineno-8-17" name="__codelineno-8-17" href="#reliability-docs-controlplane-__codelineno-8-17"></a>#  user:
<a id="__codelineno-8-18" name="__codelineno-8-18" href="#reliability-docs-controlplane-__codelineno-8-18"></a>#    exec:
<a id="__codelineno-8-19" name="__codelineno-8-19" href="#reliability-docs-controlplane-__codelineno-8-19"></a>#      apiVersion: client.authentication.k8s.io/v1alpha1
<a id="__codelineno-8-20" name="__codelineno-8-20" href="#reliability-docs-controlplane-__codelineno-8-20"></a>#      args:
<a id="__codelineno-8-21" name="__codelineno-8-21" href="#reliability-docs-controlplane-__codelineno-8-21"></a>#      - --region
<a id="__codelineno-8-22" name="__codelineno-8-22" href="#reliability-docs-controlplane-__codelineno-8-22"></a>#      - us-west-2
<a id="__codelineno-8-23" name="__codelineno-8-23" href="#reliability-docs-controlplane-__codelineno-8-23"></a>#      - eks
<a id="__codelineno-8-24" name="__codelineno-8-24" href="#reliability-docs-controlplane-__codelineno-8-24"></a>#      - get-token
<a id="__codelineno-8-25" name="__codelineno-8-25" href="#reliability-docs-controlplane-__codelineno-8-25"></a>#      - --cluster-name
<a id="__codelineno-8-26" name="__codelineno-8-26" href="#reliability-docs-controlplane-__codelineno-8-26"></a>#      - &lt;&lt;cluster name&gt;&gt;
<a id="__codelineno-8-27" name="__codelineno-8-27" href="#reliability-docs-controlplane-__codelineno-8-27"></a>#      command: aws
<a id="__codelineno-8-28" name="__codelineno-8-28" href="#reliability-docs-controlplane-__codelineno-8-28"></a>#      env: null
<a id="__codelineno-8-29" name="__codelineno-8-29" href="#reliability-docs-controlplane-__codelineno-8-29"></a>- name: super-admin
<a id="__codelineno-8-30" name="__codelineno-8-30" href="#reliability-docs-controlplane-__codelineno-8-30"></a>  user:
<a id="__codelineno-8-31" name="__codelineno-8-31" href="#reliability-docs-controlplane-__codelineno-8-31"></a>    token: &lt;&lt;super-admin sa’s secret&gt;&gt;
</code></pre></div>
<h2 id="reliability-docs-controlplane-admission-webhooks">Admission Webhooks<a class="headerlink" href="#reliability-docs-controlplane-admission-webhooks" title="Permanent link">&para;</a></h2>
<p>Kubernetes has two types of admission webhooks: <a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers">validating admission webhooks and mutating admission webhooks</a>. These allow a user to extend the kubernetes API and validate or mutate objects before they are accepted by the API. Poor configurations of these webhooks can distabilize the EKS control plane by blocking cluster critical operations.</p>
<p>In order to avoid impacting cluster critical operations either avoid setting "catch-all" webhooks like the following:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-9-1" name="__codelineno-9-1" href="#reliability-docs-controlplane-__codelineno-9-1"></a>- name: &quot;pod-policy.example.com&quot;
<a id="__codelineno-9-2" name="__codelineno-9-2" href="#reliability-docs-controlplane-__codelineno-9-2"></a>  rules:
<a id="__codelineno-9-3" name="__codelineno-9-3" href="#reliability-docs-controlplane-__codelineno-9-3"></a>  - apiGroups:   [&quot;*&quot;]
<a id="__codelineno-9-4" name="__codelineno-9-4" href="#reliability-docs-controlplane-__codelineno-9-4"></a>    apiVersions: [&quot;*&quot;]
<a id="__codelineno-9-5" name="__codelineno-9-5" href="#reliability-docs-controlplane-__codelineno-9-5"></a>    operations:  [&quot;*&quot;]
<a id="__codelineno-9-6" name="__codelineno-9-6" href="#reliability-docs-controlplane-__codelineno-9-6"></a>    resources:   [&quot;*&quot;]
<a id="__codelineno-9-7" name="__codelineno-9-7" href="#reliability-docs-controlplane-__codelineno-9-7"></a>    scope: &quot;*&quot;
</code></pre></div>
<p>Or make sure the webhook has a fail open policy with a timeout shorter than 30 seconds to ensure that if your webhook is unavailable it will not impair cluster critical workloads.</p>
<h3 id="reliability-docs-controlplane-block-pods-with-unsafe-sysctls">Block Pods with unsafe <code>sysctls</code><a class="headerlink" href="#reliability-docs-controlplane-block-pods-with-unsafe-sysctls" title="Permanent link">&para;</a></h3>
<p><code>Sysctl</code> is a Linux utility that allows users to modify kernel parameters during runtime. These kernel parameters control various aspects of the operating system's behavior, such as network, file system, virtual memory, and process management.</p>
<p>Kubernetes allows assigning <code>sysctl</code> profiles for Pods. Kubernetes categorizes <code>systcls</code> as safe and unsafe. Safe <code>sysctls</code> are namespaced in the container or Pod, and setting them doesn’t impact other Pods on the node or the node itself. In contrast, unsafe sysctls are disabled by default since they can potentially disrupt other Pods or make the node unstable.</p>
<p>As unsafe <code>sysctls</code> are disabled by default, the kubelet will not create a Pod with unsafe <code>sysctl</code> profile. If you create such a Pod, the scheduler will repeatedly assign such Pods to nodes, while the node fails to launch it. This infinite loop ultimately strains the cluster control plane, making the cluster unstable.</p>
<p>Consider using <a href="https://github.com/open-policy-agent/gatekeeper-library/blob/377cb915dba2db10702c25ef1ee374b4aa8d347a/src/pod-security-policy/forbidden-sysctls/constraint.tmpl">OPA Gatekeeper</a> or <a href="https://kyverno.io/policies/pod-security/baseline/restrict-sysctls/restrict-sysctls/">Kyverno</a> to reject  Pods with unsafe <code>sysctls</code>.</p>
<h2 id="reliability-docs-controlplane-handling-cluster-upgrades">Handling Cluster Upgrades<a class="headerlink" href="#reliability-docs-controlplane-handling-cluster-upgrades" title="Permanent link">&para;</a></h2>
<p>Since April 2021, Kubernetes release cycle has been changed from four releases a year (once a quarter) to three releases a year. A new minor version (like 1.<strong>21</strong> or 1.<strong>22</strong>) is released approximately <a href="https://kubernetes.io/blog/2021/07/20/new-kubernetes-release-cadence/#what-s-changing-and-when">every fifteen weeks</a>. Starting with Kubernetes 1.19, each minor version is supported for approximately twelve months after it's first released. With the advent of Kubernetes v1.28, the compatibility skew between the control plane and worker nodes has expanded from n-2 to n-3 minor versions. To learn more, see <a href="#upgrades">Best Practices for Cluster Upgrades</a>.</p>
<h2 id="reliability-docs-controlplane-running-large-clusters">Running large clusters<a class="headerlink" href="#reliability-docs-controlplane-running-large-clusters" title="Permanent link">&para;</a></h2>
<p>EKS actively monitors the load on control plane instances and automatically scales them to ensure high performance. However, you should account for potential performance issues and limits within Kubernetes and quotas in AWS services when running large clusters.</p>
<ul>
<li>Clusters with more than 1000 services may experience network latency with using <code>kube-proxy</code> in <code>iptables</code> mode according to the <a href="https://www.projectcalico.org/comparing-kube-proxy-modes-iptables-or-ipvs/">tests performed by the ProjectCalico team</a>. The solution is to switch to <a href="https://medium.com/@jeremy.i.cowan/the-problem-with-kube-proxy-enabling-ipvs-on-eks-169ac22e237e">running <code>kube-proxy</code> in <code>ipvs</code> mode</a>.</li>
<li>You may also experience <a href="https://docs.aws.amazon.com/AWSEC2/latest/APIReference/throttling.html">EC2 API request throttling</a> if the CNI needs to request IP addresses for Pods or if you need to create new EC2 instances frequently. You can reduce calls EC2 API by configuring the CNI to cache IP addresses. You can use larger EC2 instance types to reduce EC2 scaling events.</li>
</ul>
<h2 id="reliability-docs-controlplane-know-limits-and-service-quotas">Know limits and service quotas<a class="headerlink" href="#reliability-docs-controlplane-know-limits-and-service-quotas" title="Permanent link">&para;</a></h2>
<p>AWS sets service limits (an upper limit on the number of each resource your team can request) to protect you from accidentally over-provisioning resources. <a href="https://docs.aws.amazon.com/eks/latest/userguide/service-quotas.html">Amazon EKS Service Quotas</a> lists the service limits. There are two types of limits, soft limits, that can be changed using <a href="https://docs.aws.amazon.com/eks/latest/userguide/service-quotas.html">AWS Service Quotas</a>. Hard limits cannot be changed. You should consider these values when architecting your applications. Consider reviewing these service limits periodically and incorporate them during in your application design.</p>
<ul>
<li>Besides the limits from orchestration engines, there are limits in other AWS services, such as Elastic Load Balancing (ELB) and Amazon VPC, that may affect your application performance.</li>
<li>More about EC2 limits here: <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-resource-limits.html">EC2 service limits</a>.</li>
<li>Each EC2 instance limits the number of packets that can be sent to the <a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-dns.html#vpc-dns-limits">Amazon-provided DNS server</a> to a maximum of 1024 packets per second per network interface.</li>
<li>In EKS environment, etcd storage limit is <strong>8 GiB</strong> as per <a href="https://etcd.io/docs/v3.5/dev-guide/limit/#storage-size-limit">upstream guidance</a>. Please monitor metric <code>etcd_db_total_size_in_bytes</code> to track etcd db size. You can refer to <a href="https://github.com/etcd-io/etcd/blob/main/contrib/mixin/mixin.libsonnet#L213-L240">alert rules</a> <code>etcdBackendQuotaLowSpace</code> and <code>etcdExcessiveDatabaseGrowth</code> to setup this monitoring.</li>
</ul>
<h2 id="reliability-docs-controlplane-additional-resources">Additional Resources:<a class="headerlink" href="#reliability-docs-controlplane-additional-resources" title="Permanent link">&para;</a></h2>
<ul>
<li><a href="https://aws.amazon.com/blogs/containers/de-mystifying-cluster-networking-for-amazon-eks-worker-nodes/">De-mystifying cluster networking for Amazon EKS worker nodes</a></li>
<li><a href="https://docs.aws.amazon.com/eks/latest/userguide/cluster-endpoint.html">Amazon EKS cluster endpoint access control</a></li>
<li><a href="https://www.youtube.com/watch?v=7vxDWDD2YnM">AWS re:Invent 2019: Amazon EKS under the hood (CON421-R1)</a></li>
</ul></section><section class="print-page" id="reliability-docs-dataplane"><h1 id="reliability-docs-dataplane-eks-data-plane">EKS Data Plane<a class="headerlink" href="#reliability-docs-dataplane-eks-data-plane" title="Permanent link">&para;</a></h1>
<p>To operate high-available and resilient applications, you need a highly-available and resilient data plane. An elastic data plane ensures that Kubernetes can scale and heal your applications automatically. A resilient data plane consists of two or more worker nodes, can grow and shrink with the workload, and automatically recover from failures.</p>
<p>You have two choices for worker nodes with EKS: <a href="https://docs.aws.amazon.com/eks/latest/userguide/worker.html">EC2 instances</a> and <a href="https://docs.aws.amazon.com/eks/latest/userguide/fargate.html">Fargate</a>. If you choose EC2 instances, you can manage the worker nodes yourself or use <a href="https://docs.aws.amazon.com/eks/latest/userguide/managed-node-groups.html">EKS managed node groups</a>. You can have a cluster with a mix of managed, self-managed worker nodes, and Fargate. </p>
<p>EKS on Fargate offers the easiest path to a resilient data plane. Fargate runs each Pod in an isolated compute environment. Each Pod running on Fargate gets its own worker node. Fargate automatically scales the data plane as Kubernetes scales pods. You can scale both the data plane and your workload by using the <a href="https://docs.aws.amazon.com/eks/latest/userguide/horizontal-pod-autoscaler.html">horizontal pod autoscaler</a>.</p>
<p>The preferred way to scale EC2 worker nodes is by using <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md">Kubernetes Cluster Autoscaler</a>, <a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html">EC2 Auto Scaling groups</a> or community projects like <a href="https://github.com/atlassian/escalator">Atlassian’s Escalator</a>.</p>
<h2 id="reliability-docs-dataplane-recommendations">Recommendations<a class="headerlink" href="#reliability-docs-dataplane-recommendations" title="Permanent link">&para;</a></h2>
<h3 id="reliability-docs-dataplane-use-ec2-auto-scaling-groups-to-create-worker-nodes">Use EC2 Auto Scaling Groups to create worker nodes<a class="headerlink" href="#reliability-docs-dataplane-use-ec2-auto-scaling-groups-to-create-worker-nodes" title="Permanent link">&para;</a></h3>
<p>It is a best practice to create worker nodes using EC2 Auto Scaling groups instead of creating individual EC2 instances and joining them to the cluster. Auto Scaling Groups will automatically replace any terminated or failed nodes ensuring that the cluster always has the capacity to run your workload. </p>
<h3 id="reliability-docs-dataplane-use-kubernetes-cluster-autoscaler-to-scale-nodes">Use Kubernetes Cluster Autoscaler to scale nodes<a class="headerlink" href="#reliability-docs-dataplane-use-kubernetes-cluster-autoscaler-to-scale-nodes" title="Permanent link">&para;</a></h3>
<p>Cluster Autoscaler adjusts the size of the data plane when there are pods that cannot be run because the cluster has insufficient resources, and adding another worker node would help. Although Cluster Autoscaler is a reactive process, it waits until pods are in <em>Pending</em> state due to insufficient capacity in the cluster. When such an event occurs, it adds EC2 instances to the cluster. Whenever the cluster runs out of capacity, new replicas - or new pods - will be unavailable (<em>in Pending state</em>) until worker nodes are added. This delay may impact your applications' reliability if the data plane cannot scale fast enough to meet the demands of the workload. If a worker node is consistently underutilized and all of its pods can be scheduled on other worker nodes, Cluster Autoscaler terminates it.</p>
<h3 id="reliability-docs-dataplane-configure-over-provisioning-with-cluster-autoscaler">Configure over-provisioning with Cluster Autoscaler<a class="headerlink" href="#reliability-docs-dataplane-configure-over-provisioning-with-cluster-autoscaler" title="Permanent link">&para;</a></h3>
<p>Cluster Autoscaler triggers a scale-up of the data-plane when Pods in the cluster are already <em>Pending</em>. Hence, there may be a delay between the time your application needs more replicas, and when it, in fact, gets more replicas. An option to account for this possible delay is through adding more than required replicas, inflating the number of replicas for the application. </p>
<p>Another pattern that Cluster Autoscaler recommends uses <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-can-i-configure-overprovisioning-with-cluster-autoscaler"><em>pause</em> Pods and the Priority Preemption feature</a>. The <em>pause Pod</em> runs a <a href="https://github.com/kubernetes/kubernetes/tree/master/build/pause">pause container</a>, which as the name suggests, does nothing but acts as a placeholder for compute capacity that can be used by other Pods in your cluster. Because it runs with a <em>very low assigned priority</em>, the pause Pod gets evicted from the node when another Pod needs to be created, and the cluster doesn’t have available capacity. The Kubernetes Scheduler notices the eviction of the pause Pod and tries to reschedule it. But since the cluster is running at capacity, the pause Pod remains <em>Pending</em>, to which the Cluster Autoscaler reacts by adding nodes. </p>
<p>A Helm chart is available to install <a href="https://github.com/helm/charts/tree/master/stable/cluster-overprovisioner">cluster overprovisioner</a>.</p>
<h3 id="reliability-docs-dataplane-using-cluster-autoscaler-with-multiple-auto-scaling-groups">Using Cluster Autoscaler with multiple Auto Scaling Groups<a class="headerlink" href="#reliability-docs-dataplane-using-cluster-autoscaler-with-multiple-auto-scaling-groups" title="Permanent link">&para;</a></h3>
<p>Run the Cluster Autoscaler with the <code>--node-group-auto-discovery</code> flag enabled. Doing so will allow the Cluster Autoscaler to find all autoscaling groups that include a particular defined tag and prevents the need to define and maintain each autoscaling group in the manifest.</p>
<h3 id="reliability-docs-dataplane-using-cluster-autoscaler-with-local-storage">Using Cluster Autoscaler with local storage<a class="headerlink" href="#reliability-docs-dataplane-using-cluster-autoscaler-with-local-storage" title="Permanent link">&para;</a></h3>
<p>By default, the Cluster Autoscaler does not scale-down nodes that have pods deployed with local storage attached. Set the <code>--skip-nodes-with-local-storage</code> flag to false to allow Cluster Autoscaler to scale-down these nodes.</p>
<h3 id="reliability-docs-dataplane-spread-worker-nodes-and-workload-across-multiple-azs">Spread worker nodes and workload across multiple AZs<a class="headerlink" href="#reliability-docs-dataplane-spread-worker-nodes-and-workload-across-multiple-azs" title="Permanent link">&para;</a></h3>
<p>You can protect your workloads from failures in an individual AZ by running worker nodes and pods in multiple AZs. You can control the AZ the worker nodes are created in using the subnets you create the nodes in.</p>
<p>If you are using Kubernetes 1.18+, the recommended method for spreading pods across AZs is to use <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods">Topology Spread Constraints for Pods</a>.</p>
<p>The deployment below spreads pods across AZs if possible, letting those pods run anyway if not:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#reliability-docs-dataplane-__codelineno-0-1"></a>apiVersion: apps/v1
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#reliability-docs-dataplane-__codelineno-0-2"></a>kind: Deployment
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#reliability-docs-dataplane-__codelineno-0-3"></a>metadata:
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#reliability-docs-dataplane-__codelineno-0-4"></a>  name: web-server
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#reliability-docs-dataplane-__codelineno-0-5"></a>spec:
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#reliability-docs-dataplane-__codelineno-0-6"></a>  replicas: 3
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#reliability-docs-dataplane-__codelineno-0-7"></a>  selector:
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#reliability-docs-dataplane-__codelineno-0-8"></a>    matchLabels:
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#reliability-docs-dataplane-__codelineno-0-9"></a>      app: web-server
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#reliability-docs-dataplane-__codelineno-0-10"></a>  template:
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#reliability-docs-dataplane-__codelineno-0-11"></a>    metadata:
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#reliability-docs-dataplane-__codelineno-0-12"></a>      labels:
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#reliability-docs-dataplane-__codelineno-0-13"></a>        app: web-server
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#reliability-docs-dataplane-__codelineno-0-14"></a>    spec:
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#reliability-docs-dataplane-__codelineno-0-15"></a>      topologySpreadConstraints:
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#reliability-docs-dataplane-__codelineno-0-16"></a>        - maxSkew: 1
<a id="__codelineno-0-17" name="__codelineno-0-17" href="#reliability-docs-dataplane-__codelineno-0-17"></a>          whenUnsatisfiable: ScheduleAnyway
<a id="__codelineno-0-18" name="__codelineno-0-18" href="#reliability-docs-dataplane-__codelineno-0-18"></a>          topologyKey: topology.kubernetes.io/zone
<a id="__codelineno-0-19" name="__codelineno-0-19" href="#reliability-docs-dataplane-__codelineno-0-19"></a>          labelSelector:
<a id="__codelineno-0-20" name="__codelineno-0-20" href="#reliability-docs-dataplane-__codelineno-0-20"></a>            matchLabels:
<a id="__codelineno-0-21" name="__codelineno-0-21" href="#reliability-docs-dataplane-__codelineno-0-21"></a>              app: web-server
<a id="__codelineno-0-22" name="__codelineno-0-22" href="#reliability-docs-dataplane-__codelineno-0-22"></a>      containers:
<a id="__codelineno-0-23" name="__codelineno-0-23" href="#reliability-docs-dataplane-__codelineno-0-23"></a>      - name: web-app
<a id="__codelineno-0-24" name="__codelineno-0-24" href="#reliability-docs-dataplane-__codelineno-0-24"></a>        image: nginx
<a id="__codelineno-0-25" name="__codelineno-0-25" href="#reliability-docs-dataplane-__codelineno-0-25"></a>        resources:
<a id="__codelineno-0-26" name="__codelineno-0-26" href="#reliability-docs-dataplane-__codelineno-0-26"></a>          requests:
<a id="__codelineno-0-27" name="__codelineno-0-27" href="#reliability-docs-dataplane-__codelineno-0-27"></a>            cpu: 1
</code></pre></div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code>kube-scheduler</code> is only aware of topology domains via nodes that exist with those labels.  If the above deployment is deployed to a cluster with nodes only in a single zone, all of the pods will schedule on those nodes as <code>kube-scheduler</code> isn't aware of the other zones.  For this topology spread to work as expected with the scheduler, nodes must already exist in all zones.  This issue will be resolved in Kubernetes 1.24 with the addition of the <code>MinDomainsInPodToplogySpread</code> <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#api">feature gate</a> which allows specifying a <code>minDomains</code> property to inform the scheduler of the number of eligible domains.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Setting <code>whenUnsatisfiable</code> to <code>DoNotSchedule</code> will cause pods to be unschedulable if the topology spread constraint can't be fulfilled.  It should only be set if its preferable for pods to not run instead of violating the topology spread constraint.</p>
</div>
<p>On older versions of Kubernetes, you can use pod anti-affinity rules to schedule pods across multiple AZs. The manifest below informs Kubernetes scheduler to <em>prefer</em> scheduling pods in distinct AZs. </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#reliability-docs-dataplane-__codelineno-1-1"></a>apiVersion: apps/v1
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#reliability-docs-dataplane-__codelineno-1-2"></a>kind: Deployment
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#reliability-docs-dataplane-__codelineno-1-3"></a>metadata:
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#reliability-docs-dataplane-__codelineno-1-4"></a>  name: web-server
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#reliability-docs-dataplane-__codelineno-1-5"></a>  labels:
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#reliability-docs-dataplane-__codelineno-1-6"></a>    app: web-server
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#reliability-docs-dataplane-__codelineno-1-7"></a>spec:
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#reliability-docs-dataplane-__codelineno-1-8"></a>  replicas: 4
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#reliability-docs-dataplane-__codelineno-1-9"></a>  selector:
<a id="__codelineno-1-10" name="__codelineno-1-10" href="#reliability-docs-dataplane-__codelineno-1-10"></a>    matchLabels:
<a id="__codelineno-1-11" name="__codelineno-1-11" href="#reliability-docs-dataplane-__codelineno-1-11"></a>      app: web-server
<a id="__codelineno-1-12" name="__codelineno-1-12" href="#reliability-docs-dataplane-__codelineno-1-12"></a>  template:
<a id="__codelineno-1-13" name="__codelineno-1-13" href="#reliability-docs-dataplane-__codelineno-1-13"></a>    metadata:
<a id="__codelineno-1-14" name="__codelineno-1-14" href="#reliability-docs-dataplane-__codelineno-1-14"></a>      labels:
<a id="__codelineno-1-15" name="__codelineno-1-15" href="#reliability-docs-dataplane-__codelineno-1-15"></a>        app: web-server
<a id="__codelineno-1-16" name="__codelineno-1-16" href="#reliability-docs-dataplane-__codelineno-1-16"></a>    spec:
<a id="__codelineno-1-17" name="__codelineno-1-17" href="#reliability-docs-dataplane-__codelineno-1-17"></a>      affinity:
<a id="__codelineno-1-18" name="__codelineno-1-18" href="#reliability-docs-dataplane-__codelineno-1-18"></a>        podAntiAffinity:
<a id="__codelineno-1-19" name="__codelineno-1-19" href="#reliability-docs-dataplane-__codelineno-1-19"></a>          preferredDuringSchedulingIgnoredDuringExecution:
<a id="__codelineno-1-20" name="__codelineno-1-20" href="#reliability-docs-dataplane-__codelineno-1-20"></a>          - podAffinityTerm:
<a id="__codelineno-1-21" name="__codelineno-1-21" href="#reliability-docs-dataplane-__codelineno-1-21"></a>              labelSelector:
<a id="__codelineno-1-22" name="__codelineno-1-22" href="#reliability-docs-dataplane-__codelineno-1-22"></a>                matchExpressions:
<a id="__codelineno-1-23" name="__codelineno-1-23" href="#reliability-docs-dataplane-__codelineno-1-23"></a>                - key: app
<a id="__codelineno-1-24" name="__codelineno-1-24" href="#reliability-docs-dataplane-__codelineno-1-24"></a>                  operator: In
<a id="__codelineno-1-25" name="__codelineno-1-25" href="#reliability-docs-dataplane-__codelineno-1-25"></a>                  values:
<a id="__codelineno-1-26" name="__codelineno-1-26" href="#reliability-docs-dataplane-__codelineno-1-26"></a>                  - web-server
<a id="__codelineno-1-27" name="__codelineno-1-27" href="#reliability-docs-dataplane-__codelineno-1-27"></a>              topologyKey: failure-domain.beta.kubernetes.io/zone
<a id="__codelineno-1-28" name="__codelineno-1-28" href="#reliability-docs-dataplane-__codelineno-1-28"></a>            weight: 100
<a id="__codelineno-1-29" name="__codelineno-1-29" href="#reliability-docs-dataplane-__codelineno-1-29"></a>      containers:
<a id="__codelineno-1-30" name="__codelineno-1-30" href="#reliability-docs-dataplane-__codelineno-1-30"></a>      - name: web-app
<a id="__codelineno-1-31" name="__codelineno-1-31" href="#reliability-docs-dataplane-__codelineno-1-31"></a>        image: nginx
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Do not require that pods be scheduled across distinct AZs otherwise, the number of pods in a deployment will never exceed the number of AZs. </p>
</div>
<h3 id="reliability-docs-dataplane-ensure-capacity-in-each-az-when-using-ebs-volumes">Ensure capacity in each AZ when using EBS volumes<a class="headerlink" href="#reliability-docs-dataplane-ensure-capacity-in-each-az-when-using-ebs-volumes" title="Permanent link">&para;</a></h3>
<p>If you use <a href="https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html">Amazon EBS to provide Persistent Volumes</a>, then you need to ensure that the pods and associated EBS volume are located in the same AZ. At the time of writing, EBS volumes are only available within a single AZ. A Pod cannot access EBS-backed persistent volumes located in a different AZ. Kubernetes <a href="https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#topologykubernetesiozone">scheduler knows which AZ a worker node</a> is located in. Kubernetes will always schedule a Pod that requires an EBS volume in the same AZ as the volume. However, if there are no worker nodes available in the AZ where the volume is located, then the Pod cannot be scheduled. </p>
<p>Create Auto Scaling Group for each AZ with enough capacity to ensure that the cluster always has capacity to schedule pods in the same AZ as the EBS volumes they need. In addition, you should enable the <code>--balance-similar-node-groups</code> feature in Cluster Autoscaler.</p>
<p>If you are running an application that uses EBS volume but has no requirements to be highly available, then you can restrict the deployment of the application to a single AZ. In EKS, worker nodes are automatically added <code>failure-domain.beta.kubernetes.io/zone</code> label, which contains the name of the AZ. You can see the labels attached to your nodes by running <code>kubectl get nodes --show-labels</code>. More information about built-in node labels is available <a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#built-in-node-labels">here</a>. You can use node selectors to schedule a pod in a particular AZ. </p>
<p>In the example below, the pod will only be scheduled in <code>us-west-2c</code> AZ:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#reliability-docs-dataplane-__codelineno-2-1"></a>apiVersion: v1
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#reliability-docs-dataplane-__codelineno-2-2"></a>kind: Pod
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#reliability-docs-dataplane-__codelineno-2-3"></a>metadata:
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#reliability-docs-dataplane-__codelineno-2-4"></a>  name: single-az-pod
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#reliability-docs-dataplane-__codelineno-2-5"></a>spec:
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#reliability-docs-dataplane-__codelineno-2-6"></a>  affinity:
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#reliability-docs-dataplane-__codelineno-2-7"></a>    nodeAffinity:
<a id="__codelineno-2-8" name="__codelineno-2-8" href="#reliability-docs-dataplane-__codelineno-2-8"></a>      requiredDuringSchedulingIgnoredDuringExecution:
<a id="__codelineno-2-9" name="__codelineno-2-9" href="#reliability-docs-dataplane-__codelineno-2-9"></a>        nodeSelectorTerms:
<a id="__codelineno-2-10" name="__codelineno-2-10" href="#reliability-docs-dataplane-__codelineno-2-10"></a>        - matchExpressions:
<a id="__codelineno-2-11" name="__codelineno-2-11" href="#reliability-docs-dataplane-__codelineno-2-11"></a>          - key: failure-domain.beta.kubernetes.io/zone
<a id="__codelineno-2-12" name="__codelineno-2-12" href="#reliability-docs-dataplane-__codelineno-2-12"></a>            operator: In
<a id="__codelineno-2-13" name="__codelineno-2-13" href="#reliability-docs-dataplane-__codelineno-2-13"></a>            values:
<a id="__codelineno-2-14" name="__codelineno-2-14" href="#reliability-docs-dataplane-__codelineno-2-14"></a>            - us-west-2c
<a id="__codelineno-2-15" name="__codelineno-2-15" href="#reliability-docs-dataplane-__codelineno-2-15"></a>  containers:
<a id="__codelineno-2-16" name="__codelineno-2-16" href="#reliability-docs-dataplane-__codelineno-2-16"></a>  - name: single-az-container
<a id="__codelineno-2-17" name="__codelineno-2-17" href="#reliability-docs-dataplane-__codelineno-2-17"></a>    image: kubernetes/pause
</code></pre></div>
<p>Persistent volumes (backed by EBS) are also automatically labeled with the name of AZ; you can see which AZ your persistent volume belongs to by running <code>kubectl get pv -L topology.ebs.csi.aws.com/zone</code>. When a pod is created and claims a volume, Kubernetes will schedule the Pod on a node in the same AZ as the volume. </p>
<p>Consider this scenario; you have an EKS cluster with one node group. This node group has three worker nodes spread across three AZs. You have an application that uses an EBS-backed Persistent Volume. When you create this application and the corresponding volume, its Pod gets created in the first of the three AZs. Then, the worker node that runs this Pod becomes unhealthy and subsequently unavailable for use. Cluster Autoscaler will replace the unhealthy node with a new worker node; however, because the autoscaling group spans across three AZs, the new worker node may get launched in the second or the third AZ, but not in the first AZ as the situation demands. As the AZ-constrained EBS volume only exists in the first AZ, but there are no worker nodes available in that AZ, the Pod cannot be scheduled. Therefore, you should create one node group in each AZ, so there is always enough capacity available to run pods that cannot be scheduled in other AZs. </p>
<p>Alternatively, <a href="https://github.com/kubernetes-sigs/aws-efs-csi-driver">EFS</a> can simplify cluster autoscaling when running applications that need persistent storage. Clients can access EFS file systems concurrently from all the AZs in the region. Even if a Pod using EFS-backed Persistent Volume gets terminated and gets scheduled in different AZ, it will be able to mount the volume.</p>
<h3 id="reliability-docs-dataplane-run-node-problem-detector">Run node-problem-detector<a class="headerlink" href="#reliability-docs-dataplane-run-node-problem-detector" title="Permanent link">&para;</a></h3>
<p>Failures in worker nodes can impact the availability of your applications. <a href="https://github.com/kubernetes/node-problem-detector">node-problem-detector</a> is a Kubernetes add-on that you can install in your cluster to detect worker node issues. You can use a <a href="https://github.com/kubernetes/node-problem-detector#remedy-systems">npd’s remedy system</a> to drain and terminate the node automatically.</p>
<h3 id="reliability-docs-dataplane-reserving-resources-for-system-and-kubernetes-daemons">Reserving resources for system and Kubernetes daemons<a class="headerlink" href="#reliability-docs-dataplane-reserving-resources-for-system-and-kubernetes-daemons" title="Permanent link">&para;</a></h3>
<p>You can improve worker nodes' stability by <a href="https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/">reserving compute capacity for the operating system and Kubernetes daemons</a>. Pods  - especially ones without <code>limits</code> declared - can saturate system resources putting nodes in a situation where operating system processes and Kubernetes daemons (<code>kubelet</code>, container runtime, etc.) compete with pods for system resources. You can use <code>kubelet</code> flags <code>--system-reserved</code> and <code>--kube-reserved</code> to reserve resources for system process (<code>udev</code>, <code>sshd</code>, etc.) and Kubernetes daemons respectively. </p>
<p>If you use the <a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html">EKS-optimized Linux AMI</a>, the CPU, memory, and storage are reserved for the system and Kubernetes daemons by default. When worker nodes based on this AMI launch, EC2 user-data is configured to trigger the <a href="https://github.com/awslabs/amazon-eks-ami/blob/master/files/bootstrap.sh"><code>bootstrap.sh</code> script</a>. This script calculates CPU and memory reservations based on the number of CPU cores and total memory available on the EC2 instance. The calculated values are written to the <code>KubeletConfiguration</code> file located at <code>/etc/kubernetes/kubelet/kubelet-config.json</code>. </p>
<p>You may need to increase the system resource reservation if you run custom daemons on the node and the amount of CPU and memory reserved by default is insufficient. </p>
<p><code>eksctl</code> offers the easiest way to customize <a href="https://eksctl.io/usage/customizing-the-kubelet/">resource reservation for system and Kubernetes daemons</a>. </p>
<h3 id="reliability-docs-dataplane-implement-qos">Implement QoS<a class="headerlink" href="#reliability-docs-dataplane-implement-qos" title="Permanent link">&para;</a></h3>
<p>For critical applications, consider defining <code>requests</code>=<code>limits</code> for the container in the Pod. This will ensure that the container will not be killed if another Pod requests resources.</p>
<p>It is a best practice to implement CPU and memory limits for all containers as it prevents a container inadvertently consuming system resources impacting the availability of other co-located processes.</p>
<h3 id="reliability-docs-dataplane-configure-and-size-resource-requestslimits-for-all-workloads">Configure and Size Resource Requests/Limits for all Workloads<a class="headerlink" href="#reliability-docs-dataplane-configure-and-size-resource-requestslimits-for-all-workloads" title="Permanent link">&para;</a></h3>
<p>Some general guidance can be applied to sizing resource requests and limits for workloads:</p>
<ul>
<li>
<p>Do not specify resource limits on CPU.  In the absence of limits, the request acts as a weight on <a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#how-pods-with-resource-limits-are-run">how much relative CPU time containers get</a>. This allows your workloads to use the full CPU without an artificial limit or starvation.</p>
</li>
<li>
<p>For non-CPU resources, configuring <code>requests</code>=<code>limits</code> provides the most predictable behavior. If <code>requests</code>!=<code>limits</code>, the container also has its <a href="https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/#qos-classes">QOS</a> reduced from Guaranteed to Burstable making it more likely to be evicted in the event of <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/">node pressure</a>.</p>
</li>
<li>
<p>For non-CPU resources, do not specify a limit that is much larger than the request.  The larger <code>limits</code> are configured relative to <code>requests</code>, the more likely nodes will be overcommitted leading to high chances of workload interruption.</p>
</li>
<li>
<p>Correctly sized requests are particularly important when using a node auto-scaling solution like <a href="https://aws.github.io/aws-eks-best-practices/karpenter/">Karpenter</a> or <a href="https://aws.github.io/aws-eks-best-practices/cluster-autoscaling/">Cluster AutoScaler</a>.  These tools look at your workload requests to determine the number and size of nodes to be provisioned. If your requests are too small with larger limits, you may find your workloads evicted or OOM killed if they have been tightly packed on a node.</p>
</li>
</ul>
<p>Determining resource requests can be difficult, but tools like the <a href="https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler">Vertical Pod Autoscaler</a> can help you 'right-size' the requests by observing container resource usage at runtime.  Other tools that may be useful for determining request sizes include:</p>
<ul>
<li><a href="https://github.com/FairwindsOps/goldilocks">Goldilocks</a></li>
<li><a href="https://www.parca.dev/">Parca</a></li>
<li><a href="https://prodfiler.com/">Prodfiler</a></li>
<li><a href="https://mhausenblas.info/right-size-guide/">rsg</a></li>
</ul>
<h3 id="reliability-docs-dataplane-configure-resource-quotas-for-namespaces">Configure resource quotas for namespaces<a class="headerlink" href="#reliability-docs-dataplane-configure-resource-quotas-for-namespaces" title="Permanent link">&para;</a></h3>
<p>Namespaces are intended for use in environments with many users spread across multiple teams, or projects. They provide a scope for names and are a way to divide cluster resources between multiple teams, projects, workloads. You can limit the aggregate resource consumption in a namespace. The <a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/"><code>ResourceQuota</code></a> object can limit the quantity of objects that can be created in a namespace by type, as well as the total amount of compute resources that may be consumed by resources in that project. You can limit the total sum of storage and/or compute (CPU and memory) resources that can be requested in a given namespace.</p>
<blockquote>
<p>If resource quota is enabled for a namespace for compute resources like CPU and memory, users must specify requests or limits for each container in that namespace.</p>
</blockquote>
<p>Consider configuring quotas for each namespace. Consider using <code>LimitRanges</code> to automatically apply preconfigured limits to containers within a namespaces. </p>
<h3 id="reliability-docs-dataplane-limit-container-resource-usage-within-a-namespace">Limit container resource usage within a namespace<a class="headerlink" href="#reliability-docs-dataplane-limit-container-resource-usage-within-a-namespace" title="Permanent link">&para;</a></h3>
<p>Resource Quotas help limit the amount of resources a namespace can use. The <a href="https://kubernetes.io/docs/concepts/policy/limit-range/"><code>LimitRange</code> object</a> can help you implement minimum and maximum resources a container can request. Using <code>LimitRange</code> you can set a default request and limits for containers, which is helpful if setting compute resource limits is not a standard practice in your organization. As the name suggests, <code>LimitRange</code> can enforce minimum and maximum compute resources usage per Pod or Container in a namespace. As well as, enforce minimum and maximum storage request per PersistentVolumeClaim in a namespace.</p>
<p>Consider using <code>LimitRange</code> in conjunction with <code>ResourceQuota</code> to enforce limits at a container as well as namespace level. Setting these limits will ensure that a container or a namespace does not impinge on resources used by other tenants in the cluster. </p>
<h2 id="reliability-docs-dataplane-coredns">CoreDNS<a class="headerlink" href="#reliability-docs-dataplane-coredns" title="Permanent link">&para;</a></h2>
<p>CoreDNS fulfills name resolution and service discovery functions in Kubernetes. It is installed by default on EKS clusters. For interoperability, the Kubernetes Service for CoreDNS is still named <a href="https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/">kube-dns</a>. CoreDNS Pods run as part of a Deployment in <code>kube-system</code> namespace, and in EKS, by default, it runs two replicas with declared requests and limits. DNS queries are sent to the <code>kube-dns</code> Service that runs in the <code>kube-system</code> Namespace.</p>
<h2 id="reliability-docs-dataplane-recommendations_1">Recommendations<a class="headerlink" href="#reliability-docs-dataplane-recommendations_1" title="Permanent link">&para;</a></h2>
<h3 id="reliability-docs-dataplane-monitor-coredns-metrics">Monitor CoreDNS metrics<a class="headerlink" href="#reliability-docs-dataplane-monitor-coredns-metrics" title="Permanent link">&para;</a></h3>
<p>CoreDNS has built in support for <a href="https://github.com/coredns/coredns/tree/master/plugin/metrics">Prometheus</a>. You should especially consider monitoring CoreDNS latency (<code>coredns_dns_request_duration_seconds_sum</code>, before <a href="https://github.com/coredns/coredns/blob/master/notes/coredns-1.7.0.md">1.7.0</a> version the metric was called <code>core_dns_response_rcode_count_total</code>), errors (<code>coredns_dns_responses_total</code>, NXDOMAIN, SERVFAIL, FormErr) and CoreDNS Pod’s memory consumption.</p>
<p>For troubleshooting purposes, you can use kubectl to view CoreDNS logs:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#reliability-docs-dataplane-__codelineno-3-1"></a><span class="k">for</span><span class="w"> </span>p<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="k">$(</span>kubectl<span class="w"> </span>get<span class="w"> </span>pods<span class="w"> </span>-n<span class="w"> </span>kube-system<span class="w"> </span>-l<span class="w"> </span>k8s-app<span class="o">=</span>kube-dns<span class="w"> </span>-o<span class="w"> </span><span class="nv">jsonpath</span><span class="o">=</span><span class="s1">&#39;{.items[*].metadata.name}&#39;</span><span class="k">)</span><span class="p">;</span><span class="w"> </span><span class="k">do</span><span class="w"> </span>kubectl<span class="w"> </span>logs<span class="w"> </span><span class="nv">$p</span><span class="w"> </span>-n<span class="w"> </span>kube-system<span class="p">;</span><span class="w"> </span><span class="k">done</span>
</code></pre></div>
<h3 id="reliability-docs-dataplane-use-nodelocal-dnscache">Use NodeLocal DNSCache<a class="headerlink" href="#reliability-docs-dataplane-use-nodelocal-dnscache" title="Permanent link">&para;</a></h3>
<p>You can improve the Cluster DNS performance by running <a href="https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/">NodeLocal DNSCache</a>. This feature runs a DNS caching agent on cluster nodes as a DaemonSet. All the pods use the DNS caching agent running on the node for name resolution instead of using <code>kube-dns</code> Service.</p>
<h3 id="reliability-docs-dataplane-configure-cluster-proportional-scaler-for-coredns">Configure cluster-proportional-scaler for CoreDNS<a class="headerlink" href="#reliability-docs-dataplane-configure-cluster-proportional-scaler-for-coredns" title="Permanent link">&para;</a></h3>
<p>Another method of improving Cluster DNS performance is by <a href="https://kubernetes.io/docs/tasks/administer-cluster/dns-horizontal-autoscaling/#enablng-dns-horizontal-autoscaling">automatically horizontally scaling the CoreDNS Deployment</a> based on the number of nodes and CPU cores in the cluster. <a href="https://github.com/kubernetes-sigs/cluster-proportional-autoscaler/blob/master/README.md">Horizontal cluster-proportional-autoscaler</a> is a container that resizes the number of replicas of a Deployment based on the size of the schedulable data-plane.</p>
<p>Nodes and the aggregate of CPU cores in the nodes are the two metrics with which you can scale CoreDNS. You can use both metrics simultaneously. If you use larger nodes, CoreDNS scaling is based on the number of CPU cores. Whereas, if you use smaller nodes, the number of CoreDNS replicas depends on the  CPU cores in your data-plane. Proportional autoscaler configuration looks like this:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#reliability-docs-dataplane-__codelineno-4-1"></a>linear: &#39;{&quot;coresPerReplica&quot;:256,&quot;min&quot;:1,&quot;nodesPerReplica&quot;:16}&#39;
</code></pre></div>
<h3 id="reliability-docs-dataplane-choosing-an-ami-with-node-group">Choosing an AMI with Node Group<a class="headerlink" href="#reliability-docs-dataplane-choosing-an-ami-with-node-group" title="Permanent link">&para;</a></h3>
<p>EKS provides optimized EC2 AMIs that are used by customers to create both self-managed and managed nodegroups. These AMIs are published in every region for every supported Kubernetes version. EKS marks these AMIs as deprecated when any CVEs or bugs are discovered. Hence, the recommendation is not to consume deprecated AMIs while choosing an AMI for the node group.</p>
<p>Deprecated AMIs can be filtered using Ec2 describe-images api using below command:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#reliability-docs-dataplane-__codelineno-5-1"></a>aws ec2 describe-images --image-id ami-0d551c4f633e7679c --no-include-deprecated
</code></pre></div>
<p>You can also recognize a deprecated AMI by verifying if the describe-image output contains a DeprecationTime as a field. For ex:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#reliability-docs-dataplane-__codelineno-6-1"></a>aws ec2 describe-images --image-id ami-xxx --no-include-deprecated
<a id="__codelineno-6-2" name="__codelineno-6-2" href="#reliability-docs-dataplane-__codelineno-6-2"></a>{
<a id="__codelineno-6-3" name="__codelineno-6-3" href="#reliability-docs-dataplane-__codelineno-6-3"></a>    &quot;Images&quot;: [
<a id="__codelineno-6-4" name="__codelineno-6-4" href="#reliability-docs-dataplane-__codelineno-6-4"></a>        {
<a id="__codelineno-6-5" name="__codelineno-6-5" href="#reliability-docs-dataplane-__codelineno-6-5"></a>            &quot;Architecture&quot;: &quot;x86_64&quot;,
<a id="__codelineno-6-6" name="__codelineno-6-6" href="#reliability-docs-dataplane-__codelineno-6-6"></a>            &quot;CreationDate&quot;: &quot;2022-07-13T15:54:06.000Z&quot;,
<a id="__codelineno-6-7" name="__codelineno-6-7" href="#reliability-docs-dataplane-__codelineno-6-7"></a>            &quot;ImageId&quot;: &quot;ami-xxx&quot;,
<a id="__codelineno-6-8" name="__codelineno-6-8" href="#reliability-docs-dataplane-__codelineno-6-8"></a>            &quot;ImageLocation&quot;: &quot;123456789012/eks_xxx&quot;,
<a id="__codelineno-6-9" name="__codelineno-6-9" href="#reliability-docs-dataplane-__codelineno-6-9"></a>            &quot;ImageType&quot;: &quot;machine&quot;,
<a id="__codelineno-6-10" name="__codelineno-6-10" href="#reliability-docs-dataplane-__codelineno-6-10"></a>            &quot;Public&quot;: false,
<a id="__codelineno-6-11" name="__codelineno-6-11" href="#reliability-docs-dataplane-__codelineno-6-11"></a>            &quot;OwnerId&quot;: &quot;123456789012&quot;,
<a id="__codelineno-6-12" name="__codelineno-6-12" href="#reliability-docs-dataplane-__codelineno-6-12"></a>            &quot;PlatformDetails&quot;: &quot;Linux/UNIX&quot;,
<a id="__codelineno-6-13" name="__codelineno-6-13" href="#reliability-docs-dataplane-__codelineno-6-13"></a>            &quot;UsageOperation&quot;: &quot;RunInstances&quot;,
<a id="__codelineno-6-14" name="__codelineno-6-14" href="#reliability-docs-dataplane-__codelineno-6-14"></a>            &quot;State&quot;: &quot;available&quot;,
<a id="__codelineno-6-15" name="__codelineno-6-15" href="#reliability-docs-dataplane-__codelineno-6-15"></a>            &quot;BlockDeviceMappings&quot;: [
<a id="__codelineno-6-16" name="__codelineno-6-16" href="#reliability-docs-dataplane-__codelineno-6-16"></a>                {
<a id="__codelineno-6-17" name="__codelineno-6-17" href="#reliability-docs-dataplane-__codelineno-6-17"></a>                    &quot;DeviceName&quot;: &quot;/dev/xvda&quot;,
<a id="__codelineno-6-18" name="__codelineno-6-18" href="#reliability-docs-dataplane-__codelineno-6-18"></a>                    &quot;Ebs&quot;: {
<a id="__codelineno-6-19" name="__codelineno-6-19" href="#reliability-docs-dataplane-__codelineno-6-19"></a>                        &quot;DeleteOnTermination&quot;: true,
<a id="__codelineno-6-20" name="__codelineno-6-20" href="#reliability-docs-dataplane-__codelineno-6-20"></a>                        &quot;SnapshotId&quot;: &quot;snap-0993a2fc4bbf4f7f4&quot;,
<a id="__codelineno-6-21" name="__codelineno-6-21" href="#reliability-docs-dataplane-__codelineno-6-21"></a>                        &quot;VolumeSize&quot;: 20,
<a id="__codelineno-6-22" name="__codelineno-6-22" href="#reliability-docs-dataplane-__codelineno-6-22"></a>                        &quot;VolumeType&quot;: &quot;gp2&quot;,
<a id="__codelineno-6-23" name="__codelineno-6-23" href="#reliability-docs-dataplane-__codelineno-6-23"></a>                        &quot;Encrypted&quot;: false
<a id="__codelineno-6-24" name="__codelineno-6-24" href="#reliability-docs-dataplane-__codelineno-6-24"></a>                    }
<a id="__codelineno-6-25" name="__codelineno-6-25" href="#reliability-docs-dataplane-__codelineno-6-25"></a>                }
<a id="__codelineno-6-26" name="__codelineno-6-26" href="#reliability-docs-dataplane-__codelineno-6-26"></a>            ],
<a id="__codelineno-6-27" name="__codelineno-6-27" href="#reliability-docs-dataplane-__codelineno-6-27"></a>            &quot;Description&quot;: &quot;EKS Kubernetes Worker AMI with AmazonLinux2 image, (k8s: 1.19.15, docker: 20.10.13-2.amzn2, containerd: 1.4.13-3.amzn2)&quot;,
<a id="__codelineno-6-28" name="__codelineno-6-28" href="#reliability-docs-dataplane-__codelineno-6-28"></a>            &quot;EnaSupport&quot;: true,
<a id="__codelineno-6-29" name="__codelineno-6-29" href="#reliability-docs-dataplane-__codelineno-6-29"></a>            &quot;Hypervisor&quot;: &quot;xen&quot;,
<a id="__codelineno-6-30" name="__codelineno-6-30" href="#reliability-docs-dataplane-__codelineno-6-30"></a>            &quot;Name&quot;: &quot;aws_eks_optimized_xxx&quot;,
<a id="__codelineno-6-31" name="__codelineno-6-31" href="#reliability-docs-dataplane-__codelineno-6-31"></a>            &quot;RootDeviceName&quot;: &quot;/dev/xvda&quot;,
<a id="__codelineno-6-32" name="__codelineno-6-32" href="#reliability-docs-dataplane-__codelineno-6-32"></a>            &quot;RootDeviceType&quot;: &quot;ebs&quot;,
<a id="__codelineno-6-33" name="__codelineno-6-33" href="#reliability-docs-dataplane-__codelineno-6-33"></a>            &quot;SriovNetSupport&quot;: &quot;simple&quot;,
<a id="__codelineno-6-34" name="__codelineno-6-34" href="#reliability-docs-dataplane-__codelineno-6-34"></a>            &quot;VirtualizationType&quot;: &quot;hvm&quot;,
<a id="__codelineno-6-35" name="__codelineno-6-35" href="#reliability-docs-dataplane-__codelineno-6-35"></a>            &quot;DeprecationTime&quot;: &quot;2023-02-09T19:41:00.000Z&quot;
<a id="__codelineno-6-36" name="__codelineno-6-36" href="#reliability-docs-dataplane-__codelineno-6-36"></a>        }
<a id="__codelineno-6-37" name="__codelineno-6-37" href="#reliability-docs-dataplane-__codelineno-6-37"></a>    ]
<a id="__codelineno-6-38" name="__codelineno-6-38" href="#reliability-docs-dataplane-__codelineno-6-38"></a>}
</code></pre></div></section><h1 class='nav-section-title-end'>Ended: Reliability</h1>
                        <h1 class='nav-section-title' id='section-windows-containers'>
                            Windows Containers <a class='headerlink' href='#section-windows-containers' title='Permanent link'>↵</a>
                        </h1>
                        <section class="print-page" id="windows-docs-ami"><h1 id="windows-docs-ami-amazon-eks-optimized-windows-ami-management">Amazon EKS optimized Windows AMI management<a class="headerlink" href="#windows-docs-ami-amazon-eks-optimized-windows-ami-management" title="Permanent link">&para;</a></h1>
<p>Windows Amazon EKS optimized AMIs are built on top of Windows Server 2019 and Windows Server 2022. They are configured to serve as the base image for Amazon EKS nodes. By default, the AMIs include the following components:
- <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/">kubelet</a>
- <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/">kube-proxy</a>
- <a href="https://github.com/kubernetes-sigs/aws-iam-authenticator">AWS IAM Authenticator for Kubernetes</a>
- <a href="https://github.com/kubernetes-csi/csi-proxy">csi-proxy</a>
- <a href="https://containerd.io/">containerd</a></p>
<p>You can programmatically retrieve the Amazon Machine Image (AMI) ID for Amazon EKS optimized AMIs by querying the AWS Systems Manager Parameter Store API. This parameter eliminates the need for you to manually look up Amazon EKS optimized AMI IDs. For more information about the Systems Manager Parameter Store API, see <a href="https://docs.aws.amazon.com/systems-manager/latest/APIReference/API_GetParameter.html">GetParameter</a>. Your user account must have the ssm:GetParameter IAM permission to retrieve the Amazon EKS optimized AMI metadata.</p>
<p>The following example retrieves the AMI ID for the latest Amazon EKS optimized AMI for Windows Server 2019 LTSC Core. The version number listed in the AMI name relates to the corresponding Kubernetes build it is prepared for.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#windows-docs-ami-__codelineno-0-1"></a>aws<span class="w"> </span>ssm<span class="w"> </span>get-parameter<span class="w"> </span>--name<span class="w"> </span>/aws/service/ami-windows-latest/Windows_Server-2019-English-Core-EKS_Optimized-1.21/image_id<span class="w"> </span>--region<span class="w"> </span>us-east-1<span class="w"> </span>--query<span class="w"> </span><span class="s2">&quot;Parameter.Value&quot;</span><span class="w"> </span>--output<span class="w"> </span>text
</code></pre></div>
<p>Example output:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#windows-docs-ami-__codelineno-1-1"></a>ami-09770b3eec4552d4e
</code></pre></div>
<h2 id="windows-docs-ami-managing-your-own-amazon-eks-optimized-windows-ami">Managing your own Amazon EKS optimized Windows AMI<a class="headerlink" href="#windows-docs-ami-managing-your-own-amazon-eks-optimized-windows-ami" title="Permanent link">&para;</a></h2>
<p>An essential step towards production environments is maintaining the same Amazon EKS optimized Windows AMI and kubelet version across the Amazon EKS cluster. </p>
<p>Using the same version across the Amazon EKS cluster reduces the time during troubleshooting and increases cluster consistency. <a href="https://aws.amazon.com/image-builder/">Amazon EC2 Image Builder</a> helps create and maintain custom Amazon EKS optimized Windows AMIs to be used across an Amazon EKS cluster.</p>
<p>Use Amazon EC2 Image Builder to select between Windows Server versions, AWS Windows Server AMI release dates, and/or OS build version. The build components step, allows you to select between existing EKS Optimized Windows Artifacts as well as the kubelet versions. For more information: https://docs.aws.amazon.com/eks/latest/userguide/eks-custom-ami-windows.html</p>
<p><img alt="" src="../windows/docs/images/build-components.png" /></p>
<p><strong>NOTE:</strong> Prior to selecting a base image, consult the <a href="#windows-docs-licensing">Windows Server Version and License</a> section for important details pertaining to release channel updates.</p>
<h2 id="windows-docs-ami-configuring-faster-launching-for-custom-eks-optimized-amis">Configuring faster launching for custom EKS optimized AMIs<a class="headerlink" href="#windows-docs-ami-configuring-faster-launching-for-custom-eks-optimized-amis" title="Permanent link">&para;</a></h2>
<p>When using a custom Windows Amazon EKS optimized AMI, Windows worker nodes can be launched up to 65% faster by enabling the Fast Launch feature. This feature maintains a set of pre-provisioned snapshots which have the <em>Sysprep specialize</em>, <em>Windows Out of Box Experience (OOBE)</em> steps and required reboots already completed. These snapshots are then used on subsequent launches, reducing the time to scale-out or replace nodes. Fast Launch can be only enabled for AMIs <em>you own</em> through the EC2 console or in the AWS CLI and the number of snapshots maintained is configurable. </p>
<p><strong>NOTE:</strong> Fast Launch is not compatible with the default Amazon-provided EKS optimized AMI, create a custom AMI as above before attempting to enable it. </p>
<p>For more information: <a href="https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/windows-ami-version-history.html#win-ami-config-fast-launch">AWS Windows AMIs - Configure your AMI for faster launching</a></p>
<h2 id="windows-docs-ami-caching-windows-base-layers-on-custom-amis">Caching Windows base layers on custom AMIs<a class="headerlink" href="#windows-docs-ami-caching-windows-base-layers-on-custom-amis" title="Permanent link">&para;</a></h2>
<p>Windows container images are larger than their Linux counterparts. If you are running any containerized .NET Framework-based application, the average image size is around 8.24GB. During pod scheduling, the container image must be fully pulled and extracted in the disk before the pod reaches Running status.</p>
<p>During this process, the container runtime (containerd) pulls and extracts the entire container image in the disk. The pull operation is a parallel process, meaning the container runtime pulls the container image layers in parallel. In contrast, the extraction operation occurs in a sequential process, and it is I/O intensive. Due to that, the container image can take more than 8 minutes to be fully extracted and ready to be used by the container runtime (containerd), and as a result, the pod startup time can take several minutes.</p>
<p>As mentioned in the <strong>Patching Windows Server and Container</strong> topic, there is an option to build a custom AMI with EKS. During the AMI preparation, you can add an additional EC2 Image builder component to pull all the necessary Windows container images locally and then generate the AMI. This strategy will drastically reduce the time a pod reaches the status <strong>Running</strong>. </p>
<p>On Amazon EC2 Image Builder, create a <a href="https://docs.aws.amazon.com/imagebuilder/latest/userguide/manage-components.html">component</a> to download the necessary images and attach it to the Image recipe. The following example pulls a specific image from a ECR repository. </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#windows-docs-ami-__codelineno-2-1"></a>name: ContainerdPull
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#windows-docs-ami-__codelineno-2-2"></a>description: This component pulls the necessary containers images for a cache strategy.
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#windows-docs-ami-__codelineno-2-3"></a>schemaVersion: 1.0
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#windows-docs-ami-__codelineno-2-4"></a>
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#windows-docs-ami-__codelineno-2-5"></a>phases:
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#windows-docs-ami-__codelineno-2-6"></a>  - name: build
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#windows-docs-ami-__codelineno-2-7"></a>    steps:
<a id="__codelineno-2-8" name="__codelineno-2-8" href="#windows-docs-ami-__codelineno-2-8"></a>      - name: containerdpull
<a id="__codelineno-2-9" name="__codelineno-2-9" href="#windows-docs-ami-__codelineno-2-9"></a>        action: ExecutePowerShell
<a id="__codelineno-2-10" name="__codelineno-2-10" href="#windows-docs-ami-__codelineno-2-10"></a>        inputs:
<a id="__codelineno-2-11" name="__codelineno-2-11" href="#windows-docs-ami-__codelineno-2-11"></a>          commands:
<a id="__codelineno-2-12" name="__codelineno-2-12" href="#windows-docs-ami-__codelineno-2-12"></a>            - Set-ExecutionPolicy Unrestricted -Force
<a id="__codelineno-2-13" name="__codelineno-2-13" href="#windows-docs-ami-__codelineno-2-13"></a>            - (Get-ECRLoginCommand).Password | docker login --username AWS --password-stdin 111000111000.dkr.ecr.us-east-1.amazonaws.com
<a id="__codelineno-2-14" name="__codelineno-2-14" href="#windows-docs-ami-__codelineno-2-14"></a>            - ctr image pull mcr.microsoft.com/dotnet/framework/aspnet:latest
<a id="__codelineno-2-15" name="__codelineno-2-15" href="#windows-docs-ami-__codelineno-2-15"></a>            - ctr image pull 111000111000.dkr.ecr.us-east-1.amazonaws.com/myappcontainerimage:latest
</code></pre></div>
<p>To make sure the following component works as expected, check if the IAM role used by EC2 Image builder (EC2InstanceProfileForImageBuilder) has the attached policies:</p>
<p><img alt="" src="../windows/docs/images/permissions-policies.png" /></p>
<h2 id="windows-docs-ami-blog-post">Blog post<a class="headerlink" href="#windows-docs-ami-blog-post" title="Permanent link">&para;</a></h2>
<p>In the following blog post, you will find a step by step on how to implement caching strategy for custom Amazon EKS Windows AMIs:</p>
<p><a href="https://aws.amazon.com/blogs/containers/speeding-up-windows-container-launch-times-with-ec2-image-builder-and-image-cache-strategy/">Speeding up Windows container launch times with EC2 Image builder and image cache strategy</a></p></section><section class="print-page" id="windows-docs-gmsa"><h1 id="windows-docs-gmsa-configure-gmsa-for-windows-pods-and-containers">Configure gMSA for Windows Pods and containers<a class="headerlink" href="#windows-docs-gmsa-configure-gmsa-for-windows-pods-and-containers" title="Permanent link">&para;</a></h1>
<h2 id="windows-docs-gmsa-what-is-a-gmsa-account">What is a gMSA account<a class="headerlink" href="#windows-docs-gmsa-what-is-a-gmsa-account" title="Permanent link">&para;</a></h2>
<p>Windows-based applications such as .NET applications often use Active Directory as an identity provider, providing authorization/authentication using NTLM or Kerberos protocol. </p>
<p>An application server to exchange Kerberos tickets with Active Directory requires to be domain-joined. Windows containers don’t support domain joins and would not make much sense as containers are ephemeral resources, creating a burden on the Active Directory RID pool.</p>
<p>However, administrators can leverage <a href="https://docs.microsoft.com/en-us/windows-server/security/group-managed-service-accounts/group-managed-service-accounts-overview">gMSA Active Directory</a> accounts to negotiate a Windows authentication for resources such as Windows containers, NLB, and server farms.</p>
<h2 id="windows-docs-gmsa-windows-container-and-gmsa-use-case">Windows container and gMSA use case<a class="headerlink" href="#windows-docs-gmsa-windows-container-and-gmsa-use-case" title="Permanent link">&para;</a></h2>
<p>Applications that leverage on Windows authentication, and run as Windows containers, benefit from gMSA because the Windows Node is used to exchange the Kerberos ticket on behalf of the container.There are two options available to setup the Windows worker node to support gMSA integration:</p>
<h4 id="windows-docs-gmsa-1-domain-joined-windows-worker-nodes">1 - Domain-joined Windows worker nodes<a class="headerlink" href="#windows-docs-gmsa-1-domain-joined-windows-worker-nodes" title="Permanent link">&para;</a></h4>
<p>In this setup, the Windows worker node is domain-joined in the Active Directory domain, and the AD Computer account of the Windows worker nodes is used to authenticate against Active Directory and retrieve the gMSA identity to be used with the pod. </p>
<p>In the domain-joined approach, you can easily manage and harden your Windows worker nodes using existing Active Directory GPOs; however, it generates additional operational overhead and delays during Windows worker node joining in the Kubernetes cluster, as it requires additional reboots during node startup and Active Directory garage cleaning after the Kubernetes cluster terminates nodes.</p>
<p>In the following blog post, you will find a detailed step-by-step on how to implement the Domain-joined Windows worker node approach:</p>
<p><a href="https://aws.amazon.com/blogs/containers/windows-authentication-on-amazon-eks-windows-pods/">Windows Authentication on Amazon EKS Windows pods</a></p>
<h4 id="windows-docs-gmsa-2-domainless-windows-worker-nodes">2 - Domainless Windows worker nodes<a class="headerlink" href="#windows-docs-gmsa-2-domainless-windows-worker-nodes" title="Permanent link">&para;</a></h4>
<p>In this setup, the Windows worker node isn't joined in the Active Directory domain, and a "portable" identity (user/password) is used to authenticate against Active Directory and retrieve the gMSA identity to be used with the pod.</p>
<p><img alt="" src="../windows/docs/images/domainless_gmsa.png" /></p>
<p>The portable identity is an Active Directory user; the identity (user/password) is stored on AWS Secrets Manager or AWS System Manager Parameter Store, and an AWS-developed plugin called ccg_plugin will be used to retrieve this identity from AWS Secrets Manager or AWS System Manager Parameter Store and pass it to containerd to retrieve the gMSA identity and made it available for the pod.</p>
<p>In this domainless approach, you can benefit from not having any Active Directory interaction during Windows worker node startup when using gMSA and reducing the operational overhead for Active Directory administrators.</p>
<p>In the following blog post, you will find a detailed step-by-step on how to implement the Domainless Windows worker node approach:</p>
<p><a href="https://aws.amazon.com/blogs/containers/domainless-windows-authentication-for-amazon-eks-windows-pods/">Domainless Windows Authentication for Amazon EKS Windows pods</a></p>
<h4 id="windows-docs-gmsa-important-note">Important note<a class="headerlink" href="#windows-docs-gmsa-important-note" title="Permanent link">&para;</a></h4>
<p>Despite the pod being able to use a gMSA account, it is necessary to also setup the application or service accordingly to support Windows authentication, for instance, in order to setup Microsoft IIS to support Windows authentication, you should prepared it via dockerfile:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#windows-docs-gmsa-__codelineno-0-1"></a><span class="k">RUN</span><span class="w"> </span>Install-WindowsFeature<span class="w"> </span>-Name<span class="w"> </span>Web-Windows-Auth<span class="w"> </span>-IncludeAllSubFeature
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#windows-docs-gmsa-__codelineno-0-2"></a><span class="k">RUN</span><span class="w"> </span>Import-Module<span class="w"> </span>WebAdministration<span class="p">;</span><span class="w"> </span>Set-ItemProperty<span class="w"> </span><span class="s1">&#39;IIS:\AppPools\SiteName&#39;</span><span class="w"> </span>-name<span class="w"> </span>processModel.identityType<span class="w"> </span>-value<span class="w"> </span><span class="m">2</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#windows-docs-gmsa-__codelineno-0-3"></a><span class="k">RUN</span><span class="w"> </span>Import-Module<span class="w"> </span>WebAdministration<span class="p">;</span><span class="w"> </span>Set-WebConfigurationProperty<span class="w"> </span>-Filter<span class="w"> </span><span class="s1">&#39;/system.webServer/security/authentication/anonymousAuthentication&#39;</span><span class="w"> </span>-Name<span class="w"> </span>Enabled<span class="w"> </span>-Value<span class="w"> </span>False<span class="w"> </span>-PSPath<span class="w"> </span><span class="s1">&#39;IIS:\&#39;</span><span class="w"> </span>-Location<span class="w"> </span><span class="s1">&#39;SiteName&#39;</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#windows-docs-gmsa-__codelineno-0-4"></a><span class="k">RUN</span><span class="w"> </span>Import-Module<span class="w"> </span>WebAdministration<span class="p">;</span><span class="w"> </span>Set-WebConfigurationProperty<span class="w"> </span>-Filter<span class="w"> </span><span class="s1">&#39;/system.webServer/security/authentication/windowsAuthentication&#39;</span><span class="w"> </span>-Name<span class="w"> </span>Enabled<span class="w"> </span>-Value<span class="w"> </span>True<span class="w"> </span>-PSPath<span class="w"> </span><span class="s1">&#39;IIS:\&#39;</span><span class="w"> </span>-Location<span class="w"> </span><span class="s1">&#39;SiteName&#39;</span>
</code></pre></div></section><section class="print-page" id="windows-docs-hardening"><h1 id="windows-docs-hardening-hardening-the-windows-worker-node">Hardening the Windows worker node<a class="headerlink" href="#windows-docs-hardening-hardening-the-windows-worker-node" title="Permanent link">&para;</a></h1>
<p>Windows Server hardening involves identifying and remediating security vulnerabilities before they are exploited. </p>
<p>Microsoft offers a range of tools like <a href="https://www.microsoft.com/en-us/download/details.aspx?id=55319">Microsoft Security Compliance Toolkit</a> and <a href="https://docs.microsoft.com/en-us/windows/security/threat-protection/windows-security-baselines">Security Baselines</a> that should be applied to an operational system.</p>
<p>This guide focus specifically on Windows nodes running on Amazon Elastic Kubernetes Service (EKS).</p>
<h2 id="windows-docs-hardening-reducing-attack-surface-with-windows-server-core">Reducing attack surface with Windows Server Core<a class="headerlink" href="#windows-docs-hardening-reducing-attack-surface-with-windows-server-core" title="Permanent link">&para;</a></h2>
<p>Windows Server Core is a minimal installation option that is available as part of the <a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-windows-ami.html">EKS Optimized Windows AMI</a>. Deploying Windows Server Core has a couple benefits. First, it has a relatively small disk footprint being 6GB on Server Core against 10GB on Windows Server with Desktop experience. Second, it has smaller attack surface because of its smaller code base.</p>
<p>You can specify the Server Core EKS Optimized AMI for Windows when you deploy your nodes through <code>eksctl</code> or Cloudformation.</p>
<p>The example below is an eksctl manifest for a Windows node group based on Windows Server Core 2004:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#windows-docs-hardening-__codelineno-0-1"></a><span class="nt">nodeGroups</span><span class="p">:</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#windows-docs-hardening-__codelineno-0-2"></a><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">windows-ng</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#windows-docs-hardening-__codelineno-0-3"></a><span class="w">  </span><span class="nt">instanceType</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">c5.xlarge</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#windows-docs-hardening-__codelineno-0-4"></a><span class="w">  </span><span class="nt">minSize</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#windows-docs-hardening-__codelineno-0-5"></a><span class="w">  </span><span class="nt">volumeSize</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">50</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#windows-docs-hardening-__codelineno-0-6"></a><span class="w">  </span><span class="nt">amiFamily</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">WindowsServer2019CoreContainer</span>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#windows-docs-hardening-__codelineno-0-7"></a><span class="w">  </span><span class="nt">ssh</span><span class="p">:</span>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#windows-docs-hardening-__codelineno-0-8"></a><span class="w">    </span><span class="nt">allow</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
</code></pre></div>
<p>The amiFamily name conventions can be found on the <a href="https://eksctl.io/usage/custom-ami-support/">eksctl official documentation.</a></p>
<h2 id="windows-docs-hardening-avoiding-rdp-connections">Avoiding RDP connections<a class="headerlink" href="#windows-docs-hardening-avoiding-rdp-connections" title="Permanent link">&para;</a></h2>
<p>Remote Desktop Protocol (RDP) is a connection protocol developed by Microsoft to provide users with a graphical interface to connect to another Windows computer over a network. </p>
<p>As a best practice, you should treat your Windows worker nodes as if they were immutable. That means no management connections, no updates, and no troubleshooting. Any modification and update should be implemented as a new custom AMI and replaced by updating an Auto Scaling group. See <strong>Patching Windows Servers and Containers</strong> and <strong>Amazon EKS optimized Windows AMI management</strong>.</p>
<p>Disable RDP connections on Windows nodes during the deployment by passing the value <strong>false</strong> on the ssh property, as the example below:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#windows-docs-hardening-__codelineno-1-1"></a><span class="nt">nodeGroups</span><span class="p">:</span>
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#windows-docs-hardening-__codelineno-1-2"></a><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">windows-ng</span>
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#windows-docs-hardening-__codelineno-1-3"></a><span class="w">  </span><span class="nt">instanceType</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">c5.xlarge</span>
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#windows-docs-hardening-__codelineno-1-4"></a><span class="w">  </span><span class="nt">minSize</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#windows-docs-hardening-__codelineno-1-5"></a><span class="w">  </span><span class="nt">volumeSize</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">50</span>
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#windows-docs-hardening-__codelineno-1-6"></a><span class="w">  </span><span class="nt">amiFamily</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">WindowsServer2019CoreContainer</span>
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#windows-docs-hardening-__codelineno-1-7"></a><span class="w">  </span><span class="nt">ssh</span><span class="p">:</span>
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#windows-docs-hardening-__codelineno-1-8"></a><span class="w">    </span><span class="nt">allow</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
</code></pre></div>
<p>If access to the Windows node is needed, use <a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html">AWS System Manager Session Manager</a> to establish a secure PowerShell session through the AWS Console and SSM agent. To see how to implement the solution watch <a href="https://www.youtube.com/watch?v=nt6NTWQ-h6o">Securely Access Windows Instances Using AWS Systems Manager Session Manager</a></p>
<p>In order to use System Manager Session Manager an additional IAM policy must be applied to the Windows nodes. Below is an example where the <strong>AmazonSSMManagedInstanceCore</strong> is specified in the <code>eksctl</code> cluster manifest:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#windows-docs-hardening-__codelineno-2-1"></a><span class="w"> </span><span class="nt">nodeGroups</span><span class="p">:</span>
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#windows-docs-hardening-__codelineno-2-2"></a><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">windows-ng</span>
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#windows-docs-hardening-__codelineno-2-3"></a><span class="w">  </span><span class="nt">instanceType</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">c5.xlarge</span>
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#windows-docs-hardening-__codelineno-2-4"></a><span class="w">  </span><span class="nt">minSize</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#windows-docs-hardening-__codelineno-2-5"></a><span class="w">  </span><span class="nt">volumeSize</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">50</span>
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#windows-docs-hardening-__codelineno-2-6"></a><span class="w">  </span><span class="nt">amiFamily</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">WindowsServer2019CoreContainer</span>
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#windows-docs-hardening-__codelineno-2-7"></a><span class="w">  </span><span class="nt">ssh</span><span class="p">:</span>
<a id="__codelineno-2-8" name="__codelineno-2-8" href="#windows-docs-hardening-__codelineno-2-8"></a><span class="w">    </span><span class="nt">allow</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<a id="__codelineno-2-9" name="__codelineno-2-9" href="#windows-docs-hardening-__codelineno-2-9"></a><span class="w">  </span><span class="nt">iam</span><span class="p">:</span>
<a id="__codelineno-2-10" name="__codelineno-2-10" href="#windows-docs-hardening-__codelineno-2-10"></a><span class="w">    </span><span class="nt">attachPolicyARNs</span><span class="p">:</span>
<a id="__codelineno-2-11" name="__codelineno-2-11" href="#windows-docs-hardening-__codelineno-2-11"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy</span>
<a id="__codelineno-2-12" name="__codelineno-2-12" href="#windows-docs-hardening-__codelineno-2-12"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy</span>
<a id="__codelineno-2-13" name="__codelineno-2-13" href="#windows-docs-hardening-__codelineno-2-13"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">arn:aws:iam::aws:policy/ElasticLoadBalancingFullAccess</span>
<a id="__codelineno-2-14" name="__codelineno-2-14" href="#windows-docs-hardening-__codelineno-2-14"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly</span>
<a id="__codelineno-2-15" name="__codelineno-2-15" href="#windows-docs-hardening-__codelineno-2-15"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore</span>
</code></pre></div>
<h2 id="windows-docs-hardening-amazon-inspector">Amazon Inspector<a class="headerlink" href="#windows-docs-hardening-amazon-inspector" title="Permanent link">&para;</a></h2>
<blockquote>
<p><a href="https://aws.amazon.com/inspector/">Amazon Inspector</a> is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices. After performing an assessment, Amazon Inspector produces a detailed list of security findings prioritized by level of severity. These findings can be reviewed directly or as part of detailed assessment reports which are available via the Amazon Inspector console or API.</p>
</blockquote>
<p>Amazon Inspector can be used to run CIS Benchmark assessment on the Windows worker node and it can be installed on a Windows Server Core by performing the following tasks:</p>
<ol>
<li>Download the following .exe file:
https://inspector-agent.amazonaws.com/windows/installer/latest/AWSAgentInstall.exe</li>
<li>Transfer the agent to the Windows worker node.</li>
<li>Run the following command on PowerShell to install the Amazon Inspector Agent: <code>.\AWSAgentInstall.exe /install</code></li>
</ol>
<p>Below is the ouput after the first run. As you can see, it generated findings based on the <a href="https://cve.mitre.org/">CVE</a> database. You can use this to harden your Worker nodes or create an AMI based on the hardened configurations.</p>
<p><img alt="" src="../windows/docs/images/inspector-agent.png" /></p>
<p>For more information on Amazon Inspector, including how to install Amazon Inspector agents, set up the CIS Benchmark assessment, and generate reports, watch the <a href="https://www.youtube.com/watch?v=nIcwiJ85EKU">Improving the security and compliance of Windows Workloads with Amazon Inspector</a> video.</p>
<h2 id="windows-docs-hardening-amazon-guardduty">Amazon GuardDuty<a class="headerlink" href="#windows-docs-hardening-amazon-guardduty" title="Permanent link">&para;</a></h2>
<blockquote>
<p><a href="https://aws.amazon.com/guardduty/">Amazon GuardDuty</a> is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3. With the cloud, the collection and aggregation of account and network activities is simplified, but it can be time consuming for security teams to continuously analyze event log data for potential threats. </p>
</blockquote>
<p>By using Amazon GuardDuty you have visilitiby on malicious actitivy against Windows worker nodes, like RDP brute force and Port Probe attacks. </p>
<p>Watch the <a href="https://www.youtube.com/watch?v=ozEML585apQ">Threat Detection for Windows Workloads using Amazon GuardDuty</a> video to learn how to implement and run CIS Benchmarks on Optimized EKS Windows AMI</p>
<h2 id="windows-docs-hardening-security-in-amazon-ec2-for-windows">Security in Amazon EC2 for Windows<a class="headerlink" href="#windows-docs-hardening-security-in-amazon-ec2-for-windows" title="Permanent link">&para;</a></h2>
<p>Read up on the <a href="https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/ec2-security.html">Security best practices for Amazon EC2 Windows instances</a> to implement security controls at every layer.</p></section><section class="print-page" id="windows-docs-images"><h1 id="windows-docs-images-container-image-scanning">Container image scanning<a class="headerlink" href="#windows-docs-images-container-image-scanning" title="Permanent link">&para;</a></h1>
<p>Image Scanning is an automated vulnerability assessment feature that helps improve the security of your application’s container images by scanning them for a broad range of operating system vulnerabilities.</p>
<p>Currently, the Amazon Elastic Container Registry (ECR) is only able to scan Linux container image for vulnerabilities. However; there are third-party tools which can be integrated with an existing CI/CD pipeline for Windows container image scanning.</p>
<ul>
<li><a href="https://anchore.com/blog/scanning-windows-container-images/">Anchore</a></li>
<li><a href="https://docs.paloaltonetworks.com/prisma/prisma-cloud/prisma-cloud-admin-compute/vulnerability_management/windows_image_scanning.html">PaloAlto Prisma Cloud </a></li>
<li><a href="https://www.trendmicro.com/en_us/business/products/hybrid-cloud/smart-check-image-scanning.html">Trend Micro - Deep Security Smart Check</a></li>
</ul>
<p>To learn more about how to integrate these solutions with Amazon Elastic Container Repository (ECR), check:</p>
<ul>
<li><a href="https://anchore.com/blog/scanning-images-on-amazon-elastic-container-registry/">Anchore, scanning images on Amazon Elastic Container Registry (ECR)</a></li>
<li><a href="https://docs.paloaltonetworks.com/prisma/prisma-cloud/prisma-cloud-admin-compute/vulnerability_management/registry_scanning0/scan_ecr.html">PaloAlto, scanning images on Amazon Elastic Container Registry (ECR)</a></li>
<li><a href="https://cloudone.trendmicro.com/docs/container-security/sc-about/">TrendMicro, scanning images on Amazon Elastic Container Registry (ECR)</a></li>
</ul></section><section class="print-page" id="windows-docs-licensing"><h1 id="windows-docs-licensing-choosing-a-windows-server-version-and-license">Choosing a Windows Server Version and License<a class="headerlink" href="#windows-docs-licensing-choosing-a-windows-server-version-and-license" title="Permanent link">&para;</a></h1>
<p>There are two primary release channels available to Windows Server customers, the Long-Term Servicing Channel and the Semi-Annual Channel.</p>
<p>You can keep servers on the Long-Term Servicing Channel (LTSC), move them to the Semi-Annual Channel (SAC), or have some servers on either track, depending on what works best for your needs.</p>
<h2 id="windows-docs-licensing-long-term-servicing-channel-ltsc">Long-Term Servicing Channel (LTSC)<a class="headerlink" href="#windows-docs-licensing-long-term-servicing-channel-ltsc" title="Permanent link">&para;</a></h2>
<p>Formerly called the “Long-Term Servicing Branch”, this is the release model you are already familiar with where a new major version of Windows Server is released every 2-3 years. Users are entitled to 5 years of mainstream support and 5 years of extended support. This channel is appropriate for systems that require a longer servicing option and functional stability. Deployments of Windows Server 2019 and earlier versions of Windows Server will not be affected by the new Semi-Annual Channel releases. The Long-Term Servicing Channel will continue to receive security and non-security updates, only receiving select new features and functionality.</p>
<h2 id="windows-docs-licensing-semi-annual-channel-sac">Semi-Annual Channel (SAC)<a class="headerlink" href="#windows-docs-licensing-semi-annual-channel-sac" title="Permanent link">&para;</a></h2>
<p>Windows Server products in the Semi-Annual Channel have new releases available twice a year, in spring and fall. Each release in this channel is supported for 18 months from the initial release.</p>
<p>Most of the features introduced in the Semi-Annual Channel will be rolled up into the next Long-Term Servicing Channel release of Windows Server. The editions, functionality, and supporting content might vary from release to release depending on customer feedback. In this model, Windows Server releases are identified by the year and month or half of release: for example, in 2020, the release in the 4th month (April) is identified as version 2004. This naming changed with the last SAC release which is identified as 20H2.</p>
<h2 id="windows-docs-licensing-which-channel-should-i-use">Which channel should I use?<a class="headerlink" href="#windows-docs-licensing-which-channel-should-i-use" title="Permanent link">&para;</a></h2>
<p>Microsoft is moving to the LTSC as the primary release channel. The two current SAC builds will be supported until the end of their 18-month lifecycles ending 2021-12-14 for version 2004 and 2022-05-10 for version 20H2.</p>
<p>Important features optimized for Container workloads which originated in the SAC have been incorporated into the LTSC build:</p>
<ul>
<li>Direct Server Return (DSR) support. (available in the LTSC <a href="https://support.microsoft.com/en-us/topic/august-20-2020-kb4571748-os-build-17763-1432-preview-fa1db909-8923-e70f-9aef-ba09edaee6f0">August 2020 Cumulative Update</a>)</li>
</ul>
<p><strong>What is Direct Server Return?</strong>
DSR is an implementation of asymmetric network load distribution in load balanced systems, meaning that the request and response traffic use a different network path.</p>
<h2 id="windows-docs-licensing-licensing">Licensing<a class="headerlink" href="#windows-docs-licensing-licensing" title="Permanent link">&para;</a></h2>
<p>At Amazon Web Services (AWS), the EKS Optimized AMIs for Windows are based on the Datacenter version, which doesn't have a limitation on the numbers of containers running on a worker node. For more information: https://docs.microsoft.com/en-us/virtualization/windowscontainers/about/faq</p></section><section class="print-page" id="windows-docs-logging"><h1 id="windows-docs-logging-logging">Logging<a class="headerlink" href="#windows-docs-logging-logging" title="Permanent link">&para;</a></h1>
<p>Containerized applications typically direct application logs to STDOUT. The container runtime traps these logs and does something with them - typically writes to a file. Where these files are stored depends on the container runtime and configuration. </p>
<p>One fundamental difference with Windows pods is they do not generate STDOUT. You can run <a href="https://github.com/microsoft/windows-container-tools/tree/master/LogMonitor">LogMonitor</a> to retrieve the ETW (Event Tracing for Windows), Windows Event Logs and other application specific logs from running Windows containers and pipes formatted log output to STDOUT. These logs can then be streamed using fluent-bit or fluentd to your desired destination such as Amazon CloudWatch.</p>
<p>The Log collection mechanism retrieves STDOUT/STDERR logs from Kubernetes pods. A <a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/">DaemonSet</a> is a common way to collect logs from containers. It gives you the ability to manage log routing/filtering/enrichment independently of the application. A fluentd DaemonSet can be used to stream these logs and any other application generated logs to a desired log aggregator.</p>
<p>More detailed information about log streaming from Windows workloads to CloudWatch is explained <a href="https://aws.amazon.com/blogs/containers/streaming-logs-from-amazon-eks-windows-pods-to-amazon-cloudwatch-logs-using-fluentd/">here</a> </p>
<h2 id="windows-docs-logging-logging-recomendations">Logging Recomendations<a class="headerlink" href="#windows-docs-logging-logging-recomendations" title="Permanent link">&para;</a></h2>
<p>The general logging best practices are no different when operating Windows workloads in Kubernetes. </p>
<ul>
<li>Always log <strong>structured log entries</strong> (JSON/SYSLOG) which makes handling log entries easier as there are many pre-written parsers for such structured formats.</li>
<li><strong>Centralize</strong> logs - dedicated logging containers can be used specifically to gather and forward log messages from all containers to a destination</li>
<li>Keep <strong>log verbosity</strong> down except when debugging. Verbosity places a lot of stress on the logging infrastructure and significant events can be lost in the noise.</li>
<li>
<p>Always log the <strong>application information</strong> along with <strong>transaction/request id</strong> for traceability. Kubernetes objects do-not carry the application name, so for example a pod name <code>windows-twryrqyw</code> may not carry any meaning when debugging logs. This helps with traceability and troubleshooting applications with your aggregated logs.</p>
<p>How you generate these transaction/correlation id's depends on the programming construct. But a very common pattern is to use a logging Aspect/Interceptor, which can use <a href="https://logging.apache.org/log4j/1.2/apidocs/org/apache/log4j/MDC.html">MDC</a> (Mapped diagnostic context) to inject a unique transaction/correlation id to every incoming request, like so: </p>
</li>
</ul>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#windows-docs-logging-__codelineno-0-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">org.slf4j.MDC</span><span class="p">;</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#windows-docs-logging-__codelineno-0-2"></a><span class="kn">import</span><span class="w"> </span><span class="nn">java.util.UUID</span><span class="p">;</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#windows-docs-logging-__codelineno-0-3"></a><span class="n">Class</span><span class="w"> </span><span class="n">LoggingAspect</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="c1">//interceptor</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#windows-docs-logging-__codelineno-0-4"></a>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#windows-docs-logging-__codelineno-0-5"></a><span class="w">    </span><span class="nd">@Before</span><span class="p">(</span><span class="n">value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;execution(* *.*(..))&quot;</span><span class="p">)</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#windows-docs-logging-__codelineno-0-6"></a><span class="w">    </span><span class="n">func</span><span class="w"> </span><span class="nf">before</span><span class="p">(...)</span><span class="w"> </span><span class="p">{</span>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#windows-docs-logging-__codelineno-0-7"></a><span class="w">        </span><span class="n">transactionId</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">generateTransactionId</span><span class="p">();</span>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#windows-docs-logging-__codelineno-0-8"></a><span class="w">        </span><span class="n">MDC</span><span class="p">.</span><span class="na">put</span><span class="p">(</span><span class="n">CORRELATION_ID</span><span class="p">,</span><span class="w"> </span><span class="n">transactionId</span><span class="p">);</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#windows-docs-logging-__codelineno-0-9"></a><span class="w">    </span><span class="p">}</span>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#windows-docs-logging-__codelineno-0-10"></a>
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#windows-docs-logging-__codelineno-0-11"></a><span class="w">    </span><span class="n">func</span><span class="w"> </span><span class="nf">generateTransactionId</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#windows-docs-logging-__codelineno-0-12"></a><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">UUID</span><span class="p">.</span><span class="na">randomUUID</span><span class="p">().</span><span class="na">toString</span><span class="p">();</span>
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#windows-docs-logging-__codelineno-0-13"></a><span class="w">    </span><span class="p">}</span>
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#windows-docs-logging-__codelineno-0-14"></a><span class="p">}</span>
</code></pre></div></section><section class="print-page" id="windows-docs-monitoring"><h1 id="windows-docs-monitoring-monitoring">Monitoring<a class="headerlink" href="#windows-docs-monitoring-monitoring" title="Permanent link">&para;</a></h1>
<p>Prometheus, a <a href="https://www.cncf.io/projects/">graduated CNCF project</a> is by far the most popular monitoring system with native integration into Kubernetes. Prometheus collects metrics around containers, pods, nodes, and clusters. Additionally, Prometheus leverages AlertsManager which lets you program alerts to warn you if something in your cluster is going wrong. Prometheus stores the metric data as a time series data identified by metric name and key/value pairs. Prometheus includes away to query using a language called PromQL, which is short for Prometheus Query Language. </p>
<p>The high level architecture of Prometheus metrics collection is shown below:</p>
<p><img alt="Prometheus Metrics collection" src="../windows/docs/images/prom.png" /></p>
<p>Prometheus uses a pull mechanism and scrapes metrics from targets using exporters and from the Kubernetes API using the <a href="https://github.com/kubernetes/kube-state-metrics">kube state metrics</a>. This means applications and services must expose a HTTP(S) endpoint containing Prometheus formatted metrics. Prometheus will then, as per its configuration, periodically pull metrics from these HTTP(S) endpoints.</p>
<p>An exporter lets you consume third party metrics as Prometheus formatted metrics. A Prometheus exporter is typically deployed on each node. For a complete list of exporters please refer to the Prometheus <a href="https://prometheus.io/docs/instrumenting/exporters/">exporters</a>. While <a href="https://github.com/prometheus/node_exporter">node exporter</a> is suited for exporting host hardware and OS metrics for linux nodes, it wont work for Windows nodes. </p>
<p>In a <strong>mixed node EKS cluster with Windows nodes</strong> when you use the stable <a href="https://github.com/prometheus-community/helm-charts">Prometheus helm chart</a>, you will see failed pods on the Windows nodes, as this exporter is not intended for Windows. You will need to treat the Windows worker pool separate and instead install the <a href="https://github.com/prometheus-community/windows_exporter">Windows exporter</a> on the Windows worker node group. </p>
<p>In order to setup Prometheus monitoring for Windows nodes, you need to download and install the WMI exporter on the Windows server itself and then setup the targets inside the scrape configuration of the Prometheus configuration file.
The <a href="https://github.com/prometheus-community/windows_exporter/releases">releases page</a> provides all available .msi installers, with respective feature sets and bug fixes. The installer will setup the windows_exporter as a Windows service, as well as create an exception in the Windows firewall. If the installer is run without any parameters, the exporter will run with default settings for enabled collectors, ports, etc.</p>
<p>You can check out the <strong>scheduling best practices</strong> section of this guide which suggests the use of taints/tolerations or RuntimeClass to selectively deploy node exporter only to linux nodes, while the Windows exporter is installed on Windows nodes as you bootstrap the node or using a configuration management tool of your choice (example chef, Ansible, SSM etc).</p>
<p>Note that, unlike the linux nodes where the node exporter is installed as a daemonset , on Windows nodes the WMI exporter is installed on the host itself. The exporter will export metrics such as the CPU usage, the memory and the disk I/O usage and can also be used to monitor IIS sites and applications, the network interfaces and services. </p>
<p>The windows_exporter will expose all metrics from enabled collectors by default. This is the recommended way to collect metrics to avoid errors. However, for advanced use the windows_exporter can be passed an optional list of collectors to filter metrics. The collect[] parameter, in the Prometheus configuration lets you do that.</p>
<p>The default install steps for Windows include downloading and starting the exporter as a service during the bootstrapping process with arguments, such as the collectors you want to filter.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#windows-docs-monitoring-__codelineno-0-1"></a><span class="p">&gt;</span> <span class="n">Powershell</span> <span class="nb">Invoke-WebRequest</span> <span class="n">https</span><span class="p">://</span><span class="n">github</span><span class="p">.</span><span class="n">com</span><span class="p">/</span><span class="n">prometheus-community</span><span class="p">/</span><span class="n">windows_exporter</span><span class="p">/</span><span class="n">releases</span><span class="p">/</span><span class="n">download</span><span class="p">/</span><span class="n">v0</span><span class="p">.</span><span class="n">13</span><span class="p">.</span><span class="n">0</span><span class="p">/</span><span class="n">windows_exporter</span><span class="p">-</span><span class="n">0</span><span class="p">.</span><span class="n">13</span><span class="p">.</span><span class="n">0-amd64</span><span class="p">.</span><span class="n">msi</span> <span class="n">-OutFile</span> <span class="p">&lt;</span><span class="n">DOWNLOADPATH</span><span class="p">&gt;</span> 
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#windows-docs-monitoring-__codelineno-0-2"></a>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#windows-docs-monitoring-__codelineno-0-3"></a><span class="p">&gt;</span> <span class="n">msiexec</span> <span class="p">/</span><span class="n">i</span> <span class="p">&lt;</span><span class="n">DOWNLOADPATH</span><span class="p">&gt;</span> <span class="n">ENABLED_COLLECTORS</span><span class="p">=</span><span class="s2">&quot;cpu,cs,logical_disk,net,os,system,container,memory&quot;</span>
</code></pre></div>
<p>By default, the metrics can be scraped at the /metrics endpoint on port 9182.
At this point, Prometheus can consume the metrics by adding the following scrape_config to the Prometheus configuration </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#windows-docs-monitoring-__codelineno-1-1"></a><span class="nt">scrape_configs</span><span class="p">:</span>
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#windows-docs-monitoring-__codelineno-1-2"></a><span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">job_name</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;prometheus&quot;</span>
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#windows-docs-monitoring-__codelineno-1-3"></a><span class="w">      </span><span class="nt">static_configs</span><span class="p">:</span><span class="w"> </span>
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#windows-docs-monitoring-__codelineno-1-4"></a><span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">targets</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&#39;localhost:9090&#39;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#windows-docs-monitoring-__codelineno-1-5"></a><span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">...</span>
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#windows-docs-monitoring-__codelineno-1-6"></a><span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">job_name</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;wmi_exporter&quot;</span>
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#windows-docs-monitoring-__codelineno-1-7"></a><span class="w">      </span><span class="nt">scrape_interval</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10s</span>
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#windows-docs-monitoring-__codelineno-1-8"></a><span class="w">      </span><span class="nt">static_configs</span><span class="p">:</span><span class="w"> </span>
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#windows-docs-monitoring-__codelineno-1-9"></a><span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">targets</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&#39;&lt;windows-node1-ip&gt;:9182&#39;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&#39;&lt;windows-node2-ip&gt;:9182&#39;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">...</span><span class="p p-Indicator">]</span>
</code></pre></div>
<p>Prometheus configuration is reloaded using </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#windows-docs-monitoring-__codelineno-2-1"></a>&gt;<span class="w"> </span>ps<span class="w"> </span>aux<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>prometheus
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#windows-docs-monitoring-__codelineno-2-2"></a>&gt;<span class="w"> </span><span class="nb">kill</span><span class="w"> </span>HUP<span class="w"> </span>&lt;PID&gt;<span class="w"> </span>
</code></pre></div>
<p>A better and recommended way to add targets is to use a  Custom Resource Definition called ServiceMonitor, which comes as part of the <a href="https://github.com/prometheus-operator/kube-prometheus/releases">Prometheus operator</a>] that provides the definition for a ServiceMonitor Object and a controller that will activate the ServiceMonitors we define and automatically build the required Prometheus configuration. </p>
<p>The ServiceMonitor, which declaratively specifies how groups of Kubernetes services should be monitored, is used to define an application you wish to scrape metrics from within Kubernetes. Within the ServiceMonitor we specify the Kubernetes labels that the operator can use to identify the Kubernetes Service which in turn identifies the Pods, that we wish to monitor. </p>
<p>In order to leverage the ServiceMonitor, create an Endpoint object pointing to specific Windows targets, a headless service and a ServiceMontor for the Windows nodes.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#windows-docs-monitoring-__codelineno-3-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#windows-docs-monitoring-__codelineno-3-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Endpoints</span>
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#windows-docs-monitoring-__codelineno-3-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-3-4" name="__codelineno-3-4" href="#windows-docs-monitoring-__codelineno-3-4"></a><span class="w">  </span><span class="nt">labels</span><span class="p">:</span>
<a id="__codelineno-3-5" name="__codelineno-3-5" href="#windows-docs-monitoring-__codelineno-3-5"></a><span class="w">    </span><span class="nt">k8s-app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">wmiexporter</span>
<a id="__codelineno-3-6" name="__codelineno-3-6" href="#windows-docs-monitoring-__codelineno-3-6"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">wmiexporter</span>
<a id="__codelineno-3-7" name="__codelineno-3-7" href="#windows-docs-monitoring-__codelineno-3-7"></a><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">kube-system</span>
<a id="__codelineno-3-8" name="__codelineno-3-8" href="#windows-docs-monitoring-__codelineno-3-8"></a><span class="nt">subsets</span><span class="p">:</span>
<a id="__codelineno-3-9" name="__codelineno-3-9" href="#windows-docs-monitoring-__codelineno-3-9"></a><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">addresses</span><span class="p">:</span>
<a id="__codelineno-3-10" name="__codelineno-3-10" href="#windows-docs-monitoring-__codelineno-3-10"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">ip</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">NODE-ONE-IP</span>
<a id="__codelineno-3-11" name="__codelineno-3-11" href="#windows-docs-monitoring-__codelineno-3-11"></a><span class="w">    </span><span class="nt">targetRef</span><span class="p">:</span>
<a id="__codelineno-3-12" name="__codelineno-3-12" href="#windows-docs-monitoring-__codelineno-3-12"></a><span class="w">      </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Node</span>
<a id="__codelineno-3-13" name="__codelineno-3-13" href="#windows-docs-monitoring-__codelineno-3-13"></a><span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">NODE-ONE-NAME</span>
<a id="__codelineno-3-14" name="__codelineno-3-14" href="#windows-docs-monitoring-__codelineno-3-14"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">ip</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">NODE-TWO-IP</span>
<a id="__codelineno-3-15" name="__codelineno-3-15" href="#windows-docs-monitoring-__codelineno-3-15"></a><span class="w">    </span><span class="nt">targetRef</span><span class="p">:</span>
<a id="__codelineno-3-16" name="__codelineno-3-16" href="#windows-docs-monitoring-__codelineno-3-16"></a><span class="w">      </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Node</span>
<a id="__codelineno-3-17" name="__codelineno-3-17" href="#windows-docs-monitoring-__codelineno-3-17"></a><span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">NODE-TWO-NAME</span>
<a id="__codelineno-3-18" name="__codelineno-3-18" href="#windows-docs-monitoring-__codelineno-3-18"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">ip</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">NODE-THREE-IP</span>
<a id="__codelineno-3-19" name="__codelineno-3-19" href="#windows-docs-monitoring-__codelineno-3-19"></a><span class="w">    </span><span class="nt">targetRef</span><span class="p">:</span>
<a id="__codelineno-3-20" name="__codelineno-3-20" href="#windows-docs-monitoring-__codelineno-3-20"></a><span class="w">      </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Node</span>
<a id="__codelineno-3-21" name="__codelineno-3-21" href="#windows-docs-monitoring-__codelineno-3-21"></a><span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">NODE-THREE-NAME</span>
<a id="__codelineno-3-22" name="__codelineno-3-22" href="#windows-docs-monitoring-__codelineno-3-22"></a><span class="w">  </span><span class="nt">ports</span><span class="p">:</span>
<a id="__codelineno-3-23" name="__codelineno-3-23" href="#windows-docs-monitoring-__codelineno-3-23"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">http-metrics</span>
<a id="__codelineno-3-24" name="__codelineno-3-24" href="#windows-docs-monitoring-__codelineno-3-24"></a><span class="w">    </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">9182</span>
<a id="__codelineno-3-25" name="__codelineno-3-25" href="#windows-docs-monitoring-__codelineno-3-25"></a><span class="w">    </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">TCP</span>
<a id="__codelineno-3-26" name="__codelineno-3-26" href="#windows-docs-monitoring-__codelineno-3-26"></a>
<a id="__codelineno-3-27" name="__codelineno-3-27" href="#windows-docs-monitoring-__codelineno-3-27"></a><span class="nn">---</span>
<a id="__codelineno-3-28" name="__codelineno-3-28" href="#windows-docs-monitoring-__codelineno-3-28"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<a id="__codelineno-3-29" name="__codelineno-3-29" href="#windows-docs-monitoring-__codelineno-3-29"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Service</span><span class="w"> </span><span class="c1">##Headless Service</span>
<a id="__codelineno-3-30" name="__codelineno-3-30" href="#windows-docs-monitoring-__codelineno-3-30"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-3-31" name="__codelineno-3-31" href="#windows-docs-monitoring-__codelineno-3-31"></a><span class="w">  </span><span class="nt">labels</span><span class="p">:</span>
<a id="__codelineno-3-32" name="__codelineno-3-32" href="#windows-docs-monitoring-__codelineno-3-32"></a><span class="w">    </span><span class="nt">k8s-app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">wmiexporter</span>
<a id="__codelineno-3-33" name="__codelineno-3-33" href="#windows-docs-monitoring-__codelineno-3-33"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">wmiexporter</span>
<a id="__codelineno-3-34" name="__codelineno-3-34" href="#windows-docs-monitoring-__codelineno-3-34"></a><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">kube-system</span>
<a id="__codelineno-3-35" name="__codelineno-3-35" href="#windows-docs-monitoring-__codelineno-3-35"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-3-36" name="__codelineno-3-36" href="#windows-docs-monitoring-__codelineno-3-36"></a><span class="w">  </span><span class="nt">clusterIP</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">None</span>
<a id="__codelineno-3-37" name="__codelineno-3-37" href="#windows-docs-monitoring-__codelineno-3-37"></a><span class="w">  </span><span class="nt">ports</span><span class="p">:</span>
<a id="__codelineno-3-38" name="__codelineno-3-38" href="#windows-docs-monitoring-__codelineno-3-38"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">http-metrics</span>
<a id="__codelineno-3-39" name="__codelineno-3-39" href="#windows-docs-monitoring-__codelineno-3-39"></a><span class="w">    </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">9182</span>
<a id="__codelineno-3-40" name="__codelineno-3-40" href="#windows-docs-monitoring-__codelineno-3-40"></a><span class="w">    </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">TCP</span>
<a id="__codelineno-3-41" name="__codelineno-3-41" href="#windows-docs-monitoring-__codelineno-3-41"></a><span class="w">    </span><span class="nt">targetPort</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">9182</span>
<a id="__codelineno-3-42" name="__codelineno-3-42" href="#windows-docs-monitoring-__codelineno-3-42"></a><span class="w">  </span><span class="nt">sessionAffinity</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">None</span>
<a id="__codelineno-3-43" name="__codelineno-3-43" href="#windows-docs-monitoring-__codelineno-3-43"></a><span class="w">  </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ClusterIP</span>
<a id="__codelineno-3-44" name="__codelineno-3-44" href="#windows-docs-monitoring-__codelineno-3-44"></a>
<a id="__codelineno-3-45" name="__codelineno-3-45" href="#windows-docs-monitoring-__codelineno-3-45"></a><span class="nn">---</span>
<a id="__codelineno-3-46" name="__codelineno-3-46" href="#windows-docs-monitoring-__codelineno-3-46"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">monitoring.coreos.com/v1</span>
<a id="__codelineno-3-47" name="__codelineno-3-47" href="#windows-docs-monitoring-__codelineno-3-47"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ServiceMonitor</span><span class="w"> </span><span class="c1">##Custom ServiceMonitor Object</span>
<a id="__codelineno-3-48" name="__codelineno-3-48" href="#windows-docs-monitoring-__codelineno-3-48"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-3-49" name="__codelineno-3-49" href="#windows-docs-monitoring-__codelineno-3-49"></a><span class="w">  </span><span class="nt">labels</span><span class="p">:</span>
<a id="__codelineno-3-50" name="__codelineno-3-50" href="#windows-docs-monitoring-__codelineno-3-50"></a><span class="w">    </span><span class="nt">k8s-app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">wmiexporter</span>
<a id="__codelineno-3-51" name="__codelineno-3-51" href="#windows-docs-monitoring-__codelineno-3-51"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">wmiexporter</span>
<a id="__codelineno-3-52" name="__codelineno-3-52" href="#windows-docs-monitoring-__codelineno-3-52"></a><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">monitoring</span>
<a id="__codelineno-3-53" name="__codelineno-3-53" href="#windows-docs-monitoring-__codelineno-3-53"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-3-54" name="__codelineno-3-54" href="#windows-docs-monitoring-__codelineno-3-54"></a><span class="w">  </span><span class="nt">endpoints</span><span class="p">:</span>
<a id="__codelineno-3-55" name="__codelineno-3-55" href="#windows-docs-monitoring-__codelineno-3-55"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">interval</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">30s</span>
<a id="__codelineno-3-56" name="__codelineno-3-56" href="#windows-docs-monitoring-__codelineno-3-56"></a><span class="w">    </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">http-metrics</span>
<a id="__codelineno-3-57" name="__codelineno-3-57" href="#windows-docs-monitoring-__codelineno-3-57"></a><span class="w">  </span><span class="nt">jobLabel</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">k8s-app</span>
<a id="__codelineno-3-58" name="__codelineno-3-58" href="#windows-docs-monitoring-__codelineno-3-58"></a><span class="w">  </span><span class="nt">namespaceSelector</span><span class="p">:</span>
<a id="__codelineno-3-59" name="__codelineno-3-59" href="#windows-docs-monitoring-__codelineno-3-59"></a><span class="w">    </span><span class="nt">matchNames</span><span class="p">:</span>
<a id="__codelineno-3-60" name="__codelineno-3-60" href="#windows-docs-monitoring-__codelineno-3-60"></a><span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">kube-system</span>
<a id="__codelineno-3-61" name="__codelineno-3-61" href="#windows-docs-monitoring-__codelineno-3-61"></a><span class="w">  </span><span class="nt">selector</span><span class="p">:</span>
<a id="__codelineno-3-62" name="__codelineno-3-62" href="#windows-docs-monitoring-__codelineno-3-62"></a><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span>
<a id="__codelineno-3-63" name="__codelineno-3-63" href="#windows-docs-monitoring-__codelineno-3-63"></a><span class="w">      </span><span class="nt">k8s-app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">wmiexporter</span>
</code></pre></div>
<p>For more details on the operator and the usage of ServiceMonitor, checkout the official <a href="https://github.com/prometheus-operator/kube-prometheus">operator</a> documentation. Note that Prometheus does support dynamic target discovery using many <a href="https://prometheus.io/blog/2015/06/01/advanced-service-discovery/">service discovery</a> options.</p></section><section class="print-page" id="windows-docs-networking"><h1 id="windows-docs-networking-windows-networking">Windows Networking<a class="headerlink" href="#windows-docs-networking-windows-networking" title="Permanent link">&para;</a></h1>
<h2 id="windows-docs-networking-windows-container-networking-overview">Windows Container Networking Overview<a class="headerlink" href="#windows-docs-networking-windows-container-networking-overview" title="Permanent link">&para;</a></h2>
<p>Windows containers are fundamentally different than Linux containers. Linux containers use Linux constructs like namespaces, the union file system, and cgroups. On Windows, those constructs are abstracted from Docker by the <a href="https://github.com/microsoft/hcsshim">Host Compute Service (HCS)</a>. HCS acts as an API layer that sits above the container implementation on Windows. Windows containers also leverage the Host Network Service (HNS) that defines the network topology on a node. </p>
<p><img alt="" src="../windows/docs/images/windows-networking.png" /></p>
<p>From a networking perspective, HCS and HNS make Windows containers function like virtual machines. For example, each container has a virtual network adapter (vNIC) that is connected to a Hyper-V virtual switch (vSwitch) as shown in the diagram above.</p>
<h2 id="windows-docs-networking-ip-address-management">IP Address Management<a class="headerlink" href="#windows-docs-networking-ip-address-management" title="Permanent link">&para;</a></h2>
<p>A node in Amazon EKS uses it's Elastic Network Interface (ENI) to connect to an AWS VPC network. Presently, <strong>only a single ENI per Windows worker node is supported</strong>. The IP address management for Windows nodes is performed by <a href="https://github.com/aws/amazon-vpc-resource-controller-k8s">VPC Resource Controller</a> which runs in control plane. More details about the workflow for IP address management of Windows nodes can be found <a href="https://github.com/aws/amazon-vpc-resource-controller-k8s#windows-ipv4-address-management">here</a>.</p>
<p>The number of pods that a Windows worker node can support is dictated by the size of the node and the number of available IPv4 addresses. You can calculate the IPv4 address available on the node as below:
- By default, only secondary IPv4 addresses are assigned to the ENI. In such a case-
  <div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#windows-docs-networking-__codelineno-0-1"></a>Total IPv4 addresses available for Pods = Number of supported IPv4 addresses per interface - 1
</code></pre></div>
  We subtract one from the total count since one IPv4 addresses will be used as the primary address of the ENI and hence cannot be allocated to the Pods.
- If the cluster has been configured for high pod density by enabling <a href="#networking-prefix-mode-index_windows">prefix delegation feature</a> then-
  <div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#windows-docs-networking-__codelineno-1-1"></a>Total IPv4 addresses available for Pods = (Number of supported IPv4 addresses per interface - 1) * 16
</code></pre></div>
  Here, instead of allocating secondary IPv4 addresses, VPC Resource Controller will allocate <code>/28 prefixes</code> and therefore, the overall number of available IPv4 addresses will be boosted 16 times.</p>
<p>Using the formula above, we can calculate max pods for an m5.large instance as below-
- By default, when running in secondary IP mode-
  <div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#windows-docs-networking-__codelineno-2-1"></a>10 secondary IPv4 addresses per ENI - 1 = 9 available IPv4 addresses
</code></pre></div>
- When using <code>prefix delegation</code>-
  <div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#windows-docs-networking-__codelineno-3-1"></a>(10 secondary IPv4 addresses per ENI - 1) * 16 = 144 available IPv4 addresses
</code></pre></div></p>
<p>For more information on how many IP addresses an instance type can support, see <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#AvailableIpPerENI">IP addresses per network interface per instance type</a>. </p>
<hr />
<p>Another key consideration is the flow of network traffic. With Windows there is a risk of port exhaustion on nodes with more than 100 services. When this condition arises, the nodes will start throwing errors with the following message:</p>
<p><strong>"Policy creation failed: hcnCreateLoadBalancer failed in Win32: The specified port already exists."</strong></p>
<p>To address this issue, we leverage Direct Server Return (DSR). DSR is an implementation of asymmetric network load distribution. In other words, the request and response traffic use different network paths. This feature speeds up communication between pods and reduces the risk of port exhaustion. We therefore recommend enabling DSR on Windows nodes. </p>
<p>DSR is enabled by default in Windows Server SAC EKS Optimized AMIs. For Windows Server 2019 LTSC EKS Optimized AMIs, you will need to enable it during instance provisioning using the script below and by using Windows Server 2019 Full or Core as the amiFamily in the <code>eksctl</code> nodeGroup. See <a href="https://eksctl.io/usage/custom-ami-support/">eksctl custom AMI</a> for additional information. </p>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#windows-docs-networking-__codelineno-4-1"></a><span class="nt">nodeGroups</span><span class="p">:</span>
<a id="__codelineno-4-2" name="__codelineno-4-2" href="#windows-docs-networking-__codelineno-4-2"></a><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">windows-ng</span>
<a id="__codelineno-4-3" name="__codelineno-4-3" href="#windows-docs-networking-__codelineno-4-3"></a><span class="w">  </span><span class="nt">instanceType</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">c5.xlarge</span>
<a id="__codelineno-4-4" name="__codelineno-4-4" href="#windows-docs-networking-__codelineno-4-4"></a><span class="w">  </span><span class="nt">minSize</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<a id="__codelineno-4-5" name="__codelineno-4-5" href="#windows-docs-networking-__codelineno-4-5"></a><span class="w">  </span><span class="nt">volumeSize</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">50</span>
<a id="__codelineno-4-6" name="__codelineno-4-6" href="#windows-docs-networking-__codelineno-4-6"></a><span class="w">  </span><span class="nt">amiFamily</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">WindowsServer2019CoreContainer</span>
<a id="__codelineno-4-7" name="__codelineno-4-7" href="#windows-docs-networking-__codelineno-4-7"></a><span class="w">  </span><span class="nt">ssh</span><span class="p">:</span>
<a id="__codelineno-4-8" name="__codelineno-4-8" href="#windows-docs-networking-__codelineno-4-8"></a><span class="w">    </span><span class="nt">allow</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
</code></pre></div>
In order to utilize DSR in Windows Server 2019 and above, you will need to specify the following <a href="https://kubernetes.io/docs/setup/production-environment/windows/intro-windows-in-kubernetes/#load-balancing-and-services"><strong>kube-proxy</strong></a> flags during instance startup.  You can do this by adjusting the userdata script associated with the <a href="https://docs.aws.amazon.com/eks/latest/userguide/launch-windows-workers.html">self-managed node groups Launch Template</a>.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#windows-docs-networking-__codelineno-5-1"></a><span class="p">&lt;</span><span class="n">powershell</span><span class="p">&gt;</span>
<a id="__codelineno-5-2" name="__codelineno-5-2" href="#windows-docs-networking-__codelineno-5-2"></a><span class="no">[string]</span><span class="nv">$EKSBinDir</span> <span class="p">=</span> <span class="s2">&quot;$env:ProgramFiles\Amazon\EKS&quot;</span>
<a id="__codelineno-5-3" name="__codelineno-5-3" href="#windows-docs-networking-__codelineno-5-3"></a><span class="no">[string]</span><span class="nv">$EKSBootstrapScriptName</span> <span class="p">=</span> <span class="s1">&#39;Start-EKSBootstrap.ps1&#39;</span>
<a id="__codelineno-5-4" name="__codelineno-5-4" href="#windows-docs-networking-__codelineno-5-4"></a><span class="no">[string]</span><span class="nv">$EKSBootstrapScriptFile</span> <span class="p">=</span> <span class="s2">&quot;$EKSBinDir\$EKSBootstrapScriptName&quot;</span>
<a id="__codelineno-5-5" name="__codelineno-5-5" href="#windows-docs-networking-__codelineno-5-5"></a><span class="p">(</span><span class="nb">Get-Content</span> <span class="nv">$EKSBootstrapScriptFile</span><span class="p">).</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;&quot;--proxy-mode=kernelspace&quot;,&#39;</span><span class="p">,</span> <span class="s1">&#39;&quot;--proxy-mode=kernelspace&quot;, &quot;--feature-gates WinDSR=true&quot;, &quot;--enable-dsr&quot;,&#39;</span><span class="p">)</span> <span class="p">|</span> <span class="nb">Set-Content</span> <span class="nv">$EKSBootstrapScriptFile</span> 
<a id="__codelineno-5-6" name="__codelineno-5-6" href="#windows-docs-networking-__codelineno-5-6"></a><span class="p">&amp;</span> <span class="nv">$EKSBootstrapScriptFile</span> <span class="n">-EKSClusterName</span> <span class="s2">&quot;eks-windows&quot;</span> <span class="n">-APIServerEndpoint</span> <span class="s2">&quot;https://&lt;REPLACE-EKS-CLUSTER-CONFIG-API-SERVER&gt;&quot;</span> <span class="n">-Base64ClusterCA</span> <span class="s2">&quot;&lt;REPLACE-EKSCLUSTER-CONFIG-DETAILS-CA&gt;&quot;</span> <span class="n">-DNSClusterIP</span> <span class="s2">&quot;172.20.0.10&quot;</span> <span class="n">-KubeletExtraArgs</span> <span class="s2">&quot;--node-labels=alpha.eksctl.io/cluster-name=eks-windows,alpha.eksctl.io/nodegroup-name=windows-ng-ltsc2019 --register-with-taints=&quot;</span> <span class="n">3</span><span class="p">&gt;&amp;</span><span class="n">1</span> <span class="n">4</span><span class="p">&gt;&amp;</span><span class="n">1</span> <span class="n">5</span><span class="p">&gt;&amp;</span><span class="n">1</span> <span class="n">6</span><span class="p">&gt;&amp;</span><span class="n">1</span>
<a id="__codelineno-5-7" name="__codelineno-5-7" href="#windows-docs-networking-__codelineno-5-7"></a><span class="p">&lt;/</span><span class="n">powershell</span><span class="p">&gt;</span>
</code></pre></div>
<p>DSR enablement can be verified following the instructions in the <a href="https://techcommunity.microsoft.com/t5/networking-blog/direct-server-return-dsr-in-a-nutshell/ba-p/693710">Microsoft Networking blog</a> and the <a href="https://catalog.us-east-1.prod.workshops.aws/workshops/1de8014a-d598-4cb5-a119-801576492564/en-US/module1-eks/lab3-handling-mixed-clusters">Windows Containers on AWS Lab</a>.</p>
<p><img alt="" src="../windows/docs/images/dsr.png" /></p>
<p>Using an older versions of Windows will increase the risk of port exhaustion as those versions do not support DSR. </p>
<h2 id="windows-docs-networking-container-network-interface-cni-options">Container Network Interface (CNI) options<a class="headerlink" href="#windows-docs-networking-container-network-interface-cni-options" title="Permanent link">&para;</a></h2>
<p>The AWSVPC CNI is the de facto CNI plugin for Windows and Linux worker nodes. While the AWSVPC CNI satisfies the needs of many customers, still there may be times when you need to consider alternatives like an overlay network to avoid IP exhaustion. In these cases, the Calico CNI can be used in place of the AWSVPC CNI. <a href="https://www.projectcalico.org/">Project Calico</a> is open source software that was developed by <a href="https://www.tigera.io/">Tigera</a>. That software includes a CNI that works with EKS. Instructions for installing Calico CNI in EKS can be found on the <a href="https://docs.projectcalico.org/getting-started/kubernetes/managed-public-cloud/eks">Project Calico EKS installation</a> page.</p>
<h2 id="windows-docs-networking-network-polices">Network Polices<a class="headerlink" href="#windows-docs-networking-network-polices" title="Permanent link">&para;</a></h2>
<p>It is considered a best practice to change from the default mode of open communication between pods on your Kubernetes cluster to limiting access based on network polices. The open source <a href="https://www.tigera.io/tigera-products/calico/">Project Calico</a> has strong support for network polices that work with both Linux and Windows nodes. This feature is separate and not dependent on using the Calico CNI. We therefore recommend installing Calico and using it for network policy management. </p>
<p>Instructions for installing Calico in EKS can be found on the <a href="https://docs.aws.amazon.com/eks/latest/userguide/calico.html">Installing Calico on Amazon EKS</a> page.</p>
<p>In addition, the advice provided in the <a href="https://aws.github.io/aws-eks-best-practices/security/docs/network/">Amazon EKS Best Practices Guide for Security - Network Section</a> applies equally to EKS clusters with Windows worker nodes, however, some features like "Security Groups for Pods" are not supported by Windows at this time.</p></section><section class="print-page" id="windows-docs-oom"><h1 id="windows-docs-oom-avoiding-oom-errors">Avoiding OOM errors<a class="headerlink" href="#windows-docs-oom-avoiding-oom-errors" title="Permanent link">&para;</a></h1>
<p>Windows does not have an out-of-memory process killer as Linux does. Windows always treats all user-mode memory allocations as virtual, and pagefiles are mandatory. The net effect is that Windows won't reach out of memory conditions the same way Linux does. Processes will page to disk instead of being subject to out of memory (OOM) termination. If memory is over-provisioned and all physical memory is exhausted, then paging can slow down performance.</p>
<h2 id="windows-docs-oom-reserving-system-and-kubelet-memory">Reserving system and kubelet memory<a class="headerlink" href="#windows-docs-oom-reserving-system-and-kubelet-memory" title="Permanent link">&para;</a></h2>
<p>Different from Linux where <code>--kubelet-reserve</code> <strong>capture</strong> resource reservation for kubernetes system daemons like kubelet, container runtime, etc; and <code>--system-reserve</code> <strong>capture</strong> resource reservation for OS system daemons like sshd, udev and etc. On <strong>Windows</strong> these flags do not <strong>capture</strong> and <strong>set</strong> memory limits on <strong>kubelet</strong> or <strong>processes</strong> running on the node.</p>
<p>However, you can combine these flags to manage <strong>NodeAllocatable</strong> to reduce Capacity on the node with Pod manifest <strong>memory resource limit</strong> to control memory allocation per pod. Using this strategy you have a better control of memory allocation as well as a mechanism to minimize out-of-memory (OOM) on Windows nodes.</p>
<p>On Windows nodes, a best practice is to reserve at least 2GB of memory for the OS and process. Use <code>--kubelet-reserve</code> and/or <code>--system-reserve</code> to reduce NodeAllocatable.</p>
<p>Following the <a href="https://docs.aws.amazon.com/eks/latest/userguide/launch-windows-workers.html">Amazon EKS Self-managed Windows nodes</a> documentation, use the CloudFormation template to launch a new Windows node group with customizations to kubelet configuration. The CloudFormation has an element called <code>BootstrapArguments</code> which is the same as <code>KubeletExtraArgs</code>. Use with the following flags and values:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#windows-docs-oom-__codelineno-0-1"></a>--kube-reserved<span class="w"> </span><span class="nv">memory</span><span class="o">=</span><span class="m">0</span>.5Gi,ephemeral-storage<span class="o">=</span>1Gi<span class="w"> </span>--system-reserved<span class="w"> </span><span class="nv">memory</span><span class="o">=</span><span class="m">1</span>.5Gi,ephemeral-storage<span class="o">=</span>1Gi<span class="w"> </span>--eviction-hard<span class="w"> </span>memory.available&lt;200Mi,nodefs.available&lt;<span class="m">10</span>%<span class="s2">&quot;</span>
</code></pre></div>
<p>If eksctl is the deployment tool, check the following documentation to customize the kubelet configuration https://eksctl.io/usage/customizing-the-kubelet/</p>
<h2 id="windows-docs-oom-windows-container-memory-requirements">Windows container memory requirements<a class="headerlink" href="#windows-docs-oom-windows-container-memory-requirements" title="Permanent link">&para;</a></h2>
<p>As per <a href="https://docs.microsoft.com/en-us/virtualization/windowscontainers/deploy-containers/system-requirements">Microsoft documentation</a>, a Windows Server base image for NANO requires at least 30MB, whereas Server Core requires 45MB. These numbers grow as you add Windows components such as the .NET Framework, Web Services as IIS and applications.</p>
<p>It is essential for you to know the minimum amount of memory required by your Windows container image, i.e. the base image plus its application layers, and set it as the container's resources/requests in the pod specification. You should also set a limit to avoid pods to consume all the available node memory in case of an application issue.</p>
<p>In the example below, when the Kubernetes scheduler tries to place a pod on a node, the pod's requests are used to determine which node has sufficient resources available for scheduling.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#windows-docs-oom-__codelineno-1-1"></a><span class="w"> </span><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#windows-docs-oom-__codelineno-1-2"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">iis</span>
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#windows-docs-oom-__codelineno-1-3"></a><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">mcr.microsoft.com/windows/servercore/iis:windowsservercore-ltsc2019</span>
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#windows-docs-oom-__codelineno-1-4"></a><span class="w">    </span><span class="nt">resources</span><span class="p">:</span>
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#windows-docs-oom-__codelineno-1-5"></a><span class="w">      </span><span class="nt">limits</span><span class="p">:</span>
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#windows-docs-oom-__codelineno-1-6"></a><span class="w">        </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#windows-docs-oom-__codelineno-1-7"></a><span class="w">        </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">800Mi</span>
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#windows-docs-oom-__codelineno-1-8"></a><span class="w">      </span><span class="nt">requests</span><span class="p">:</span>
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#windows-docs-oom-__codelineno-1-9"></a><span class="w">        </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">.1</span>
<a id="__codelineno-1-10" name="__codelineno-1-10" href="#windows-docs-oom-__codelineno-1-10"></a><span class="w">        </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">128Mi</span>
</code></pre></div>
<h2 id="windows-docs-oom-conclusion">Conclusion<a class="headerlink" href="#windows-docs-oom-conclusion" title="Permanent link">&para;</a></h2>
<p>Using this approach minimizes the risks of memory exhaustion but does not prevent it happen. Using Amazon CloudWatch Metrics, you can set up alerts and remediations in case of memory exhaustion occurs.</p></section><section class="print-page" id="windows-docs-patching"><h1 id="windows-docs-patching-patching-windows-servers-and-containers">Patching Windows Servers and Containers<a class="headerlink" href="#windows-docs-patching-patching-windows-servers-and-containers" title="Permanent link">&para;</a></h1>
<p>Patching Windows Server is a standard management task for Windows Administrators. This can be accomplished using different tools like Amazon System Manager - Patch Manager, WSUS, System Center Configuration Manager, and many others. However, Windows nodes in an Amazon EKS cluster should not be treated as an ordinary Windows servers. They should be treated as an immutable server. Simply put, avoid updating an existing node, just launch a new one based on an new updated AMI.</p>
<p>Using <a href="https://aws.amazon.com/image-builder/">EC2 Image Builder</a> you can automate AMIs build, by creating recipes and adding components.</p>
<p>The following example shows <strong>components</strong>, which can be pre-existing ones built by AWS (Amazon-managed) as well as the components you create (Owned by me). Pay close attention to the Amazon-managed component called <strong>update-windows</strong>, this updates Windows Server before generating the AMI through the EC2 Image Builder pipeline.</p>
<p><img alt="" src="../windows/docs/images/associated-components.png" /></p>
<p>EC2 Image Builder allows you to build AMI's based off Amazon Managed Public AMIs and customize them to meet your business requirements. You can then associate those AMIs with Launch Templates which allows you to link a new AMI to the Auto Scaling Group created by the EKS Nodegroup. After that is complete, you can begin terminating the existing Windows Nodes and new ones will be launched based on the new updated AMI.</p>
<h2 id="windows-docs-patching-pushing-and-pulling-windows-images">Pushing and pulling Windows images<a class="headerlink" href="#windows-docs-patching-pushing-and-pulling-windows-images" title="Permanent link">&para;</a></h2>
<p>Amazon publishes EKS optimized AMIs that include two cached Windows container images.  </p>
<div class="codehilite"><pre><span></span><code>mcr.microsoft.com/windows/servercore
mcr.microsoft.com/windows/nanoserver
</code></pre></div>

<p><img alt="" src="../windows/docs/images/images.png" /></p>
<p>Cached images are updated following the updates on the main OS. When Microsoft releases a new Windows update that directly affects the Windows container base image, the update will be launched as an ordinary Windows Update on the main OS. Keeping the environment up-to-date offers a more secure environment at the Node and Container level.</p>
<p>The size of a Windows container image influences push/pull operations which can lead to slow container startup times. <a href="https://aws.amazon.com/blogs/containers/speeding-up-windows-container-launch-times-with-ec2-image-builder-and-image-cache-strategy/">Caching Windows container images</a> allows the expensive I/O operations (file extraction) to occur on the AMI build creation instead of the container launch. As a result, all the necessary image layers will be extracted on the AMI and will be ready to be used, speeding up the time a Windows container launches and can start accepting traffic. During a push operation, only the layers that compose your image are uploaded to the repository.</p>
<p>The following example shows that on the Amazon ECR the <strong>fluentd-windows-sac2004</strong> images have only <strong>390.18MB</strong>. This is the amount of upload that happened during the push operation.</p>
<p>The following example shows a <a href="https://github.com/fluent/fluentd-docker-image/blob/master/v1.14/windows-ltsc2019/Dockerfile">fluentd Windows ltsc</a> image pushed to an Amazon ECR repository.  The size of the layer stored in ECR is <strong>533.05MB</strong>.</p>
<p><img alt="" src="../windows/docs/images/ecr-image.png" /></p>
<p>The output below from <code>docker image ls</code> , the size of the fluentd v1.14-windows-ltsc2019-1 is <strong>6.96GB</strong> on disk, but that doesn't mean it downloaded and extracted that amount of data.</p>
<p>In practice, during the pull operation only the <strong>compressed 533.05MB</strong> will be downloaded and extracted.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#windows-docs-patching-__codelineno-0-1"></a>REPOSITORY<span class="w">                                                              </span>TAG<span class="w">                        </span>IMAGE<span class="w"> </span>ID<span class="w">       </span>CREATED<span class="w">         </span>SIZE
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#windows-docs-patching-__codelineno-0-2"></a><span class="m">111122223333</span>.dkr.ecr.us-east-1.amazonaws.com/fluentd-windows-coreltsc<span class="w">   </span>latest<span class="w">                     </span>721afca2c725<span class="w">   </span><span class="m">7</span><span class="w"> </span>weeks<span class="w"> </span>ago<span class="w">     </span><span class="m">6</span>.96GB
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#windows-docs-patching-__codelineno-0-3"></a>fluent/fluentd<span class="w">                                                          </span>v1.14-windows-ltsc2019-1<span class="w">   </span>721afca2c725<span class="w">   </span><span class="m">7</span><span class="w"> </span>weeks<span class="w"> </span>ago<span class="w">     </span><span class="m">6</span>.96GB
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#windows-docs-patching-__codelineno-0-4"></a>amazonaws.com/eks/pause-windows<span class="w">                                         </span>latest<span class="w">                     </span>6392f69ae6e7<span class="w">   </span><span class="m">10</span><span class="w"> </span>months<span class="w"> </span>ago<span class="w">   </span>255MB
</code></pre></div>
<p>The size column shows the overall size of image, 6.96GB. Breaking it down:</p>
<ul>
<li>Windows Server Core 2019 LTSC Base image = 5.74GB</li>
<li>Fluentd Uncompressed Base Image = 6.96GB</li>
<li>Difference on disk = 1.2GB</li>
<li>Fluentd <a href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/repository-info.html">compressed final image ECR</a> = 533.05MB</li>
</ul>
<p>The base image already exists on the local disk, resulting in the total amount on disk being 1.2GB additional. The next time you see the amount of GBs in the size column, don't worry too much, likely more than 70% is already on disk as a cached container image. </p>
<h2 id="windows-docs-patching-reference">Reference<a class="headerlink" href="#windows-docs-patching-reference" title="Permanent link">&para;</a></h2>
<p><a href="https://aws.amazon.com/blogs/containers/speeding-up-windows-container-launch-times-with-ec2-image-builder-and-image-cache-strategy/">Speeding up Windows container launch times with EC2 Image builder and image cache strategy</a></p></section><section class="print-page" id="windows-docs-scheduling"><h1 id="windows-docs-scheduling-running-heterogeneous-workloads">Running Heterogeneous workloads¶<a class="headerlink" href="#windows-docs-scheduling-running-heterogeneous-workloads" title="Permanent link">&para;</a></h1>
<p>Kubernetes has support for heterogeneous clusters where you can have a mixture of Linux and Windows nodes in the same cluster. Within that cluster, you can have a mixture of Pods that run on Linux and Pods that run on Windows. You can even run multiple versions of Windows in the same cluster. However, there are several factors (as mentioned below) that will need to be accounted for when making this decision.</p>
<h1 id="windows-docs-scheduling-assigning-pods-to-nodes-best-practices">Assigning PODs to Nodes Best practices<a class="headerlink" href="#windows-docs-scheduling-assigning-pods-to-nodes-best-practices" title="Permanent link">&para;</a></h1>
<p>In order to keep Linux and Windows workloads on their respective OS-specific nodes, you need to use some combination of node selectors and taints/tolerations. The main goal of scheduling workloads in a heterogeneous environment is to avoid breaking compatibility for existing Linux workloads.</p>
<h2 id="windows-docs-scheduling-ensuring-os-specific-workloads-land-on-the-appropriate-container-host">Ensuring OS-specific workloads land on the appropriate container host<a class="headerlink" href="#windows-docs-scheduling-ensuring-os-specific-workloads-land-on-the-appropriate-container-host" title="Permanent link">&para;</a></h2>
<p>Users can ensure Windows containers can be scheduled on the appropriate host using nodeSelectors. All Kubernetes nodes today have the following default labels:</p>
<div class="codehilite"><pre><span></span><code>kubernetes.io/os = [windows|linux]
kubernetes.io/arch = [amd64|arm64|...]
</code></pre></div>

<p>If a Pod specification does not include a nodeSelector like <code>"kubernetes.io/os": windows</code>, the Pod may be scheduled on any host, Windows or Linux. This can be problematic since a Windows container can only run on Windows and a Linux container can only run on Linux. </p>
<p>In Enterprise environments, it's not uncommon to have a large number of pre-existing deployments for Linux containers, as well as an ecosystem of off-the-shelf configurations, like Helm charts. In these situations, you may be hesitant to make changes to a deployment's nodeSelectors. <strong>The alternative is to use Taints</strong>.</p>
<p>For example: <code>--register-with-taints='os=windows:NoSchedule'</code></p>
<p>If you are using EKS, eksctl offers ways to apply taints through clusterConfig:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#windows-docs-scheduling-__codelineno-0-1"></a><span class="nt">NodeGroups</span><span class="p">:</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#windows-docs-scheduling-__codelineno-0-2"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">windows-ng</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#windows-docs-scheduling-__codelineno-0-3"></a><span class="w">    </span><span class="nt">amiFamily</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">WindowsServer2022FullContainer</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#windows-docs-scheduling-__codelineno-0-4"></a><span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">...</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#windows-docs-scheduling-__codelineno-0-5"></a><span class="w">    </span><span class="nt">labels</span><span class="p">:</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#windows-docs-scheduling-__codelineno-0-6"></a><span class="w">      </span><span class="nt">nodeclass</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">windows2022</span>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#windows-docs-scheduling-__codelineno-0-7"></a><span class="w">    </span><span class="nt">taints</span><span class="p">:</span>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#windows-docs-scheduling-__codelineno-0-8"></a><span class="w">      </span><span class="nt">os</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;windows:NoSchedule&quot;</span>
</code></pre></div>
<p>Adding a taint to all Windows nodes, the scheduler will not schedule pods on those nodes unless they tolerate the taint. Pod manifest example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#windows-docs-scheduling-__codelineno-1-1"></a><span class="nt">nodeSelector</span><span class="p">:</span>
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#windows-docs-scheduling-__codelineno-1-2"></a><span class="w">    </span><span class="nt">kubernetes.io/os</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">windows</span>
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#windows-docs-scheduling-__codelineno-1-3"></a><span class="nt">tolerations</span><span class="p">:</span>
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#windows-docs-scheduling-__codelineno-1-4"></a><span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;os&quot;</span>
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#windows-docs-scheduling-__codelineno-1-5"></a><span class="w">      </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Equal&quot;</span>
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#windows-docs-scheduling-__codelineno-1-6"></a><span class="w">      </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;windows&quot;</span>
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#windows-docs-scheduling-__codelineno-1-7"></a><span class="w">      </span><span class="nt">effect</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;NoSchedule&quot;</span>
</code></pre></div>
<h2 id="windows-docs-scheduling-handling-multiple-windows-build-in-the-same-cluster">Handling multiple Windows build in the same cluster<a class="headerlink" href="#windows-docs-scheduling-handling-multiple-windows-build-in-the-same-cluster" title="Permanent link">&para;</a></h2>
<p>The Windows container base image used by each pod must match the same kernel build version as the node. If you want to use multiple Windows Server builds in the same cluster, then you should set additional node labels, nodeSelectors or leverage a label called <strong>windows-build</strong>.</p>
<p>Kubernetes 1.17 automatically adds a new label <strong>node.kubernetes.io/windows-build</strong> to simplify the management of multiple Windows build in the same cluster. If you're running an older version, then it's recommended to add this label manually to Windows nodes.</p>
<p>This label reflects the Windows major, minor, and build number that need to match for compatibility. Below are values used today for each Windows Server version.</p>
<p>It's important to note that Windows Server is moving to the Long-Term Servicing Channel (LTSC) as the primary release channel. The Windows Server Semi-Annual Channel (SAC) was retired on August 9, 2022. There will be no future SAC releases of Windows Server.</p>
<table>
<thead>
<tr>
<th>Product Name</th>
<th>Build Number(s)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Server full 2022 LTSC</td>
<td>10.0.20348</td>
</tr>
<tr>
<td>Server core 2019 LTSC</td>
<td>10.0.17763</td>
</tr>
</tbody>
</table>
<p>It is possible to check the OS build version through the following command:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#windows-docs-scheduling-__codelineno-2-1"></a>kubectl<span class="w"> </span>get<span class="w"> </span>nodes<span class="w"> </span>-o<span class="w"> </span>wide
</code></pre></div>
<p>The KERNEL-VERSION output matches the Windows OS build version.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#windows-docs-scheduling-__codelineno-3-1"></a>NAME<span class="w">                          </span>STATUS<span class="w">   </span>ROLES<span class="w">    </span>AGE<span class="w">   </span>VERSION<span class="w">                </span>INTERNAL-IP<span class="w">   </span>EXTERNAL-IP<span class="w">     </span>OS-IMAGE<span class="w">                         </span>KERNEL-VERSION<span class="w">                  </span>CONTAINER-RUNTIME
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#windows-docs-scheduling-__codelineno-3-2"></a>ip-10-10-2-235.ec2.internal<span class="w">   </span>Ready<span class="w">    </span>&lt;none&gt;<span class="w">   </span>23m<span class="w">   </span>v1.24.7-eks-fb459a0<span class="w">    </span><span class="m">10</span>.10.2.235<span class="w">   </span><span class="m">3</span>.236.30.157<span class="w">    </span>Windows<span class="w"> </span>Server<span class="w"> </span><span class="m">2022</span><span class="w"> </span>Datacenter<span class="w">   </span><span class="m">10</span>.0.20348.1607<span class="w">                 </span>containerd://1.6.6
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#windows-docs-scheduling-__codelineno-3-3"></a>ip-10-10-31-27.ec2.internal<span class="w">   </span>Ready<span class="w">    </span>&lt;none&gt;<span class="w">   </span>23m<span class="w">   </span>v1.24.7-eks-fb459a0<span class="w">    </span><span class="m">10</span>.10.31.27<span class="w">   </span><span class="m">44</span>.204.218.24<span class="w">   </span>Windows<span class="w"> </span>Server<span class="w"> </span><span class="m">2019</span><span class="w"> </span>Datacenter<span class="w">   </span><span class="m">10</span>.0.17763.4131<span class="w">                 </span>containerd://1.6.6
<a id="__codelineno-3-4" name="__codelineno-3-4" href="#windows-docs-scheduling-__codelineno-3-4"></a>ip-10-10-7-54.ec2.internal<span class="w">    </span>Ready<span class="w">    </span>&lt;none&gt;<span class="w">   </span>31m<span class="w">   </span>v1.24.11-eks-a59e1f0<span class="w">   </span><span class="m">10</span>.10.7.54<span class="w">    </span><span class="m">3</span>.227.8.172<span class="w">     </span>Amazon<span class="w"> </span>Linux<span class="w"> </span><span class="m">2</span><span class="w">                   </span><span class="m">5</span>.10.173-154.642.amzn2.x86_64<span class="w">   </span>containerd://1.6.19
</code></pre></div>
<p>The example below applies an additional nodeSelector to the pod manifest in order to match the correct Windows-build version when running different Windows node groups OS versions.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#windows-docs-scheduling-__codelineno-4-1"></a><span class="nt">nodeSelector</span><span class="p">:</span>
<a id="__codelineno-4-2" name="__codelineno-4-2" href="#windows-docs-scheduling-__codelineno-4-2"></a><span class="w">    </span><span class="nt">kubernetes.io/os</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">windows</span>
<a id="__codelineno-4-3" name="__codelineno-4-3" href="#windows-docs-scheduling-__codelineno-4-3"></a><span class="w">    </span><span class="nt">node.kubernetes.io/windows-build</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;10.0.20348&#39;</span>
<a id="__codelineno-4-4" name="__codelineno-4-4" href="#windows-docs-scheduling-__codelineno-4-4"></a><span class="nt">tolerations</span><span class="p">:</span>
<a id="__codelineno-4-5" name="__codelineno-4-5" href="#windows-docs-scheduling-__codelineno-4-5"></a><span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;os&quot;</span>
<a id="__codelineno-4-6" name="__codelineno-4-6" href="#windows-docs-scheduling-__codelineno-4-6"></a><span class="w">    </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Equal&quot;</span>
<a id="__codelineno-4-7" name="__codelineno-4-7" href="#windows-docs-scheduling-__codelineno-4-7"></a><span class="w">    </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;windows&quot;</span>
<a id="__codelineno-4-8" name="__codelineno-4-8" href="#windows-docs-scheduling-__codelineno-4-8"></a><span class="w">    </span><span class="nt">effect</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;NoSchedule&quot;</span>
</code></pre></div>
<h2 id="windows-docs-scheduling-simplifying-nodeselector-and-toleration-in-pod-manifests-using-runtimeclass">Simplifying NodeSelector and Toleration in Pod manifests using RuntimeClass<a class="headerlink" href="#windows-docs-scheduling-simplifying-nodeselector-and-toleration-in-pod-manifests-using-runtimeclass" title="Permanent link">&para;</a></h2>
<p>You can also make use of RuntimeClass to simplify the process of using taints and tolerations. This can be accomplished by creating a RuntimeClass object which is used to encapsulate these taints and tolerations.</p>
<p>Create a RuntimeClass by running the following manifest:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#windows-docs-scheduling-__codelineno-5-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">node.k8s.io/v1beta1</span>
<a id="__codelineno-5-2" name="__codelineno-5-2" href="#windows-docs-scheduling-__codelineno-5-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">RuntimeClass</span>
<a id="__codelineno-5-3" name="__codelineno-5-3" href="#windows-docs-scheduling-__codelineno-5-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-5-4" name="__codelineno-5-4" href="#windows-docs-scheduling-__codelineno-5-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">windows-2022</span>
<a id="__codelineno-5-5" name="__codelineno-5-5" href="#windows-docs-scheduling-__codelineno-5-5"></a><span class="nt">handler</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;docker&#39;</span>
<a id="__codelineno-5-6" name="__codelineno-5-6" href="#windows-docs-scheduling-__codelineno-5-6"></a><span class="nt">scheduling</span><span class="p">:</span>
<a id="__codelineno-5-7" name="__codelineno-5-7" href="#windows-docs-scheduling-__codelineno-5-7"></a><span class="w">  </span><span class="nt">nodeSelector</span><span class="p">:</span>
<a id="__codelineno-5-8" name="__codelineno-5-8" href="#windows-docs-scheduling-__codelineno-5-8"></a><span class="w">    </span><span class="nt">kubernetes.io/os</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;windows&#39;</span>
<a id="__codelineno-5-9" name="__codelineno-5-9" href="#windows-docs-scheduling-__codelineno-5-9"></a><span class="w">    </span><span class="nt">kubernetes.io/arch</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;amd64&#39;</span>
<a id="__codelineno-5-10" name="__codelineno-5-10" href="#windows-docs-scheduling-__codelineno-5-10"></a><span class="w">    </span><span class="nt">node.kubernetes.io/windows-build</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;10.0.20348&#39;</span>
<a id="__codelineno-5-11" name="__codelineno-5-11" href="#windows-docs-scheduling-__codelineno-5-11"></a><span class="w">  </span><span class="nt">tolerations</span><span class="p">:</span>
<a id="__codelineno-5-12" name="__codelineno-5-12" href="#windows-docs-scheduling-__codelineno-5-12"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">effect</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">NoSchedule</span>
<a id="__codelineno-5-13" name="__codelineno-5-13" href="#windows-docs-scheduling-__codelineno-5-13"></a><span class="w">    </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">os</span>
<a id="__codelineno-5-14" name="__codelineno-5-14" href="#windows-docs-scheduling-__codelineno-5-14"></a><span class="w">    </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Equal</span>
<a id="__codelineno-5-15" name="__codelineno-5-15" href="#windows-docs-scheduling-__codelineno-5-15"></a><span class="w">    </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;windows&quot;</span>
</code></pre></div>
<p>Once the Runtimeclass is created, assign it using as a Spec on the Pod manifest:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#windows-docs-scheduling-__codelineno-6-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">apps/v1</span>
<a id="__codelineno-6-2" name="__codelineno-6-2" href="#windows-docs-scheduling-__codelineno-6-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deployment</span>
<a id="__codelineno-6-3" name="__codelineno-6-3" href="#windows-docs-scheduling-__codelineno-6-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-6-4" name="__codelineno-6-4" href="#windows-docs-scheduling-__codelineno-6-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">iis-2022</span>
<a id="__codelineno-6-5" name="__codelineno-6-5" href="#windows-docs-scheduling-__codelineno-6-5"></a><span class="w">  </span><span class="nt">labels</span><span class="p">:</span>
<a id="__codelineno-6-6" name="__codelineno-6-6" href="#windows-docs-scheduling-__codelineno-6-6"></a><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">iis-2022</span>
<a id="__codelineno-6-7" name="__codelineno-6-7" href="#windows-docs-scheduling-__codelineno-6-7"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-6-8" name="__codelineno-6-8" href="#windows-docs-scheduling-__codelineno-6-8"></a><span class="w">  </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<a id="__codelineno-6-9" name="__codelineno-6-9" href="#windows-docs-scheduling-__codelineno-6-9"></a><span class="w">  </span><span class="nt">template</span><span class="p">:</span>
<a id="__codelineno-6-10" name="__codelineno-6-10" href="#windows-docs-scheduling-__codelineno-6-10"></a><span class="w">    </span><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-6-11" name="__codelineno-6-11" href="#windows-docs-scheduling-__codelineno-6-11"></a><span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">iis-2022</span>
<a id="__codelineno-6-12" name="__codelineno-6-12" href="#windows-docs-scheduling-__codelineno-6-12"></a><span class="w">      </span><span class="nt">labels</span><span class="p">:</span>
<a id="__codelineno-6-13" name="__codelineno-6-13" href="#windows-docs-scheduling-__codelineno-6-13"></a><span class="w">        </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">iis-2022</span>
<a id="__codelineno-6-14" name="__codelineno-6-14" href="#windows-docs-scheduling-__codelineno-6-14"></a><span class="w">    </span><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-6-15" name="__codelineno-6-15" href="#windows-docs-scheduling-__codelineno-6-15"></a><span class="w">      </span><span class="nt">runtimeClassName</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">windows-2022</span>
<a id="__codelineno-6-16" name="__codelineno-6-16" href="#windows-docs-scheduling-__codelineno-6-16"></a><span class="w">      </span><span class="nt">containers</span><span class="p">:</span>
<a id="__codelineno-6-17" name="__codelineno-6-17" href="#windows-docs-scheduling-__codelineno-6-17"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">iis</span>
</code></pre></div>
<h2 id="windows-docs-scheduling-managed-node-group-support">Managed Node Group Support<a class="headerlink" href="#windows-docs-scheduling-managed-node-group-support" title="Permanent link">&para;</a></h2>
<p>To help customers run their Windows applications in a more streamlined manner, AWS launched the support for Amazon <a href="https://aws.amazon.com/about-aws/whats-new/2022/12/amazon-eks-automated-provisioning-lifecycle-management-windows-containers/">EKS Managed Node Group (MNG) support for Windows containers</a> on December 15, 2022. To help align operations teams, <a href="https://docs.aws.amazon.com/eks/latest/userguide/managed-node-groups.html">Windows MNGs</a> are enabled using the same workflows and tools as <a href="https://docs.aws.amazon.com/eks/latest/userguide/managed-node-groups.html">Linux MNGs</a>. Full and core AMI (Amazon Machine Image) family versions of Windows Server 2019 and 2022 are supported. </p>
<p>Following AMI families are supported for Managed Node Groups(MNG)s.</p>
<table>
<thead>
<tr>
<th>AMI Family</th>
</tr>
</thead>
<tbody>
<tr>
<td>WINDOWS_CORE_2019_x86_64</td>
</tr>
<tr>
<td>WINDOWS_FULL_2019_x86_64</td>
</tr>
<tr>
<td>WINDOWS_CORE_2022_x86_64</td>
</tr>
<tr>
<td>WINDOWS_FULL_2022_x86_64</td>
</tr>
</tbody>
</table>
<h2 id="windows-docs-scheduling-additional-documentations">Additional documentations<a class="headerlink" href="#windows-docs-scheduling-additional-documentations" title="Permanent link">&para;</a></h2>
<p>AWS Official Documentation:
https://docs.aws.amazon.com/eks/latest/userguide/windows-support.html</p>
<p>To better understand how Pod Networking (CNI) works, check the following link: https://docs.aws.amazon.com/eks/latest/userguide/pod-networking.html</p>
<p>AWS Blog on Deploying Managed Node Group for Windows on EKS:
https://aws.amazon.com/blogs/containers/deploying-amazon-eks-windows-managed-node-groups/</p></section><section class="print-page" id="windows-docs-security"><h1 id="windows-docs-security-pod-security-contexts">Pod Security Contexts<a class="headerlink" href="#windows-docs-security-pod-security-contexts" title="Permanent link">&para;</a></h1>
<p><strong>Pod Security Policies (PSP)</strong> and <strong>Pod Security Standards (PSS)</strong> are two main ways of enforcing security in Kubernetes. Note that PodSecurityPolicy is deprecated as of Kubernetes v1.21, and will be removed in v1.25 and Pod Security Standard (PSS) is the Kubernetes recommended approach for enforcing security going forward.</p>
<p>A Pod Security Policy (PSP) is a native solution in Kubernetes to implement security policies. PSP is a cluster-level resource that controls security-sensitive aspects of the Pod specification. Using Pod Security Policy you can define a set of conditions that Pods must meet to be accepted by the cluster.
The PSP feature has been available from the early days of Kubernetes and is designed to block misconfigured pods from being created on a given cluster.</p>
<p>For more information on Pod Security Policies please reference the Kubernetes <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/">documentation</a>. According to the <a href="https://kubernetes.io/docs/reference/using-api/deprecation-policy/">Kubernetes deprecation policy</a>, older versions will stop getting support nine months after the deprecation of the feature.</p>
<p>On the other hand, Pod Security Standards (PSS) which is the recommended security approach and typically implemented using Security Contexts are defined as part of the Pod and container specifications in the Pod manifest. PSS is the official standard that the Kubernetes project team has defined to address the security-related best practices for Pods. It defines policies such as baseline (minimally restrictive, default), privileged (unrestrictive) and restricted (most restrictive).</p>
<p>We recommend starting with the baseline profile. PSS baseline profile provides a solid balance between security and potential friction, requiring a minimal list of exceptions, it serves as a good starting point for workload security. If you are currently using PSPs we recommend switching to PSS. More details on the PSS policies can be found in the Kubernetes <a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/">documentation</a>. These policies can be enforced with several tools including those from <a href="https://www.openpolicyagent.org/">OPA</a> and <a href="https://kyverno.io/">Kyverno</a>. For example, Kyverno provides the full collection of PSS policies <a href="https://kyverno.io/policies/pod-security/">here</a>.</p>
<p>Security context settings allow one to give privileges to select processes, use program profiles to restrict capabilities to individual programs, allow privilege escalation, filter system calls, among other things.</p>
<p>Windows pods in Kubernetes have some limitations and differentiators from standard Linux-based workloads when it comes to security contexts.</p>
<p>Windows uses a Job object per container with a system namespace filter to contain all processes in a container and provide logical isolation from the host. There is no way to run a Windows container without the namespace filtering in place. This means that system privileges cannot be asserted in the context of the host, and thus privileged containers are not available on Windows.</p>
<p>The following <code>windowsOptions</code> are the only documented <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.20/#windowssecuritycontextoptions-v1-core">Windows Security Context options</a> while the rest are general <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.21/#securitycontext-v1-core (https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.21/#securitycontext-v1-core)">Security Context options</a></p>
<p>For a list of security context attributes that are supported in Windows vs linux, please refer to the official documentation <a href="https://kubernetes.io/docs/setup/production-environment/windows/_print/#v1-container">here</a>.</p>
<p>The Pod specific settings are applied to all containers. If unspecified, the options from the PodSecurityContext will be used. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence.</p>
<p>For example, runAsUserName setting for Pods and containers which is a Windows option is a rough equivalent of the Linux-specific runAsUser setting and in the following manifest, the pod specific security context is applied to all containers</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#windows-docs-security-__codelineno-0-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#windows-docs-security-__codelineno-0-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Pod</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#windows-docs-security-__codelineno-0-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#windows-docs-security-__codelineno-0-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">run-as-username-pod-demo</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#windows-docs-security-__codelineno-0-5"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#windows-docs-security-__codelineno-0-6"></a><span class="w">  </span><span class="nt">securityContext</span><span class="p">:</span>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#windows-docs-security-__codelineno-0-7"></a><span class="w">    </span><span class="nt">windowsOptions</span><span class="p">:</span>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#windows-docs-security-__codelineno-0-8"></a><span class="w">      </span><span class="nt">runAsUserName</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;ContainerUser&quot;</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#windows-docs-security-__codelineno-0-9"></a><span class="w">  </span><span class="nt">containers</span><span class="p">:</span>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#windows-docs-security-__codelineno-0-10"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">run-as-username-demo</span>
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#windows-docs-security-__codelineno-0-11"></a><span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">...</span>
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#windows-docs-security-__codelineno-0-12"></a><span class="w">  </span><span class="nt">nodeSelector</span><span class="p">:</span>
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#windows-docs-security-__codelineno-0-13"></a><span class="w">    </span><span class="nt">kubernetes.io/os</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">windows</span>
</code></pre></div>
<p>Whereas in the following, the container level security context overrides the pod level security context.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#windows-docs-security-__codelineno-1-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#windows-docs-security-__codelineno-1-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Pod</span>
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#windows-docs-security-__codelineno-1-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#windows-docs-security-__codelineno-1-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">run-as-username-container-demo</span>
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#windows-docs-security-__codelineno-1-5"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#windows-docs-security-__codelineno-1-6"></a><span class="w">  </span><span class="nt">securityContext</span><span class="p">:</span>
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#windows-docs-security-__codelineno-1-7"></a><span class="w">    </span><span class="nt">windowsOptions</span><span class="p">:</span>
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#windows-docs-security-__codelineno-1-8"></a><span class="w">      </span><span class="nt">runAsUserName</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;ContainerUser&quot;</span>
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#windows-docs-security-__codelineno-1-9"></a><span class="w">  </span><span class="nt">containers</span><span class="p">:</span>
<a id="__codelineno-1-10" name="__codelineno-1-10" href="#windows-docs-security-__codelineno-1-10"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">run-as-username-demo</span>
<a id="__codelineno-1-11" name="__codelineno-1-11" href="#windows-docs-security-__codelineno-1-11"></a><span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">..</span>
<a id="__codelineno-1-12" name="__codelineno-1-12" href="#windows-docs-security-__codelineno-1-12"></a><span class="w">    </span><span class="nt">securityContext</span><span class="p">:</span>
<a id="__codelineno-1-13" name="__codelineno-1-13" href="#windows-docs-security-__codelineno-1-13"></a><span class="w">        </span><span class="nt">windowsOptions</span><span class="p">:</span>
<a id="__codelineno-1-14" name="__codelineno-1-14" href="#windows-docs-security-__codelineno-1-14"></a><span class="w">            </span><span class="nt">runAsUserName</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;ContainerAdministrator&quot;</span>
<a id="__codelineno-1-15" name="__codelineno-1-15" href="#windows-docs-security-__codelineno-1-15"></a><span class="w">  </span><span class="nt">nodeSelector</span><span class="p">:</span>
<a id="__codelineno-1-16" name="__codelineno-1-16" href="#windows-docs-security-__codelineno-1-16"></a><span class="w">    </span><span class="nt">kubernetes.io/os</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">windows</span>
</code></pre></div>
<p>Examples of acceptable values for the runAsUserName field: ContainerAdministrator, ContainerUser, NT AUTHORITY\NETWORK SERVICE, NT AUTHORITY\LOCAL SERVICE</p>
<p>It is generally a good idea to run your containers with ContainerUser for Windows pods. The users are not shared between the container and host but the ContainerAdministrator does have additional privileges with in the container. Note that, there are username <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-runasusername/#windows-username-limitations">limitations</a> to be aware of.</p>
<p>A good example of when to use ContainerAdministrator is to set PATH. You can use the USER directive to do that, like so:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#windows-docs-security-__codelineno-2-1"></a>USER<span class="w"> </span>ContainerAdministrator
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#windows-docs-security-__codelineno-2-2"></a>RUN<span class="w"> </span>setx<span class="w"> </span>/M<span class="w"> </span>PATH<span class="w"> </span><span class="s2">&quot;%PATH%;C:/your/path&quot;</span>
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#windows-docs-security-__codelineno-2-3"></a>USER<span class="w"> </span>ContainerUser
</code></pre></div>
<p>Also note that, secrets are written in clear text on the node's volume (as compared to tmpfs/in-memory on linux). This means you have to do two things</p>
<ul>
<li>Use file ACLs to secure the secrets file location</li>
<li>Use volume-level encryption using <a href="https://docs.microsoft.com/en-us/windows/security/information-protection/bitlocker/bitlocker-how-to-deploy-on-windows-server">BitLocker</a></li>
</ul></section><section class="print-page" id="windows-docs-storage"><h1 id="windows-docs-storage-persistent-storage-options">Persistent storage options<a class="headerlink" href="#windows-docs-storage-persistent-storage-options" title="Permanent link">&para;</a></h1>
<h2 id="windows-docs-storage-what-is-an-in-tree-vs-out-of-tree-volume-plugin">What is an in-tree vs. out-of-tree volume plugin?<a class="headerlink" href="#windows-docs-storage-what-is-an-in-tree-vs-out-of-tree-volume-plugin" title="Permanent link">&para;</a></h2>
<p>Before the introduction of the Container Storage Interface (CSI), all volume plugins were in-tree meaning they were built, linked, compiled, and shipped with the core Kubernetes binaries and extend the core Kubernetes API. This meant that adding a new storage system to Kubernetes (a volume plugin) required checking code into the core Kubernetes code repository.</p>
<p>Out-of-tree volume plugins are developed independently of the Kubernetes code base, and are deployed (installed) on Kubernetes clusters as extensions. This gives vendors the ability to update drivers out-of-band, i.e. separately from the Kubernetes release cycle. This is largely possible because Kubernetes has created a storage interface or CSI that provides vendors a standard way of interfacing with k8s. </p>
<p>You can check more about Amazon Elastic Kubernetes Services (EKS) storage classes and CSI Drivers on https://docs.aws.amazon.com/eks/latest/userguide/storage.html</p>
<h2 id="windows-docs-storage-in-tree-volume-plugin-for-windows">In-tree Volume Plugin for Windows<a class="headerlink" href="#windows-docs-storage-in-tree-volume-plugin-for-windows" title="Permanent link">&para;</a></h2>
<p>Kubernetes volumes enable applications, with data persistence requirements, to be deployed on Kubernetes. The management of persistent volumes consists of provisioning/de-provisioning/resizing of volumes, attaching/detaching a volume to/from a Kubernetes node, and mounting/dismounting a volume to/from individual containers in a pod. The code for implementing these volume management actions for a specific storage back-end or protocol is shipped in the form of a Kubernetes volume plugin <strong>(In-tree Volume Plugins)</strong>. On Amazon Elastic Kubernetes Services (EKS) the following class of Kubernetes volume plugins are supported on Windows:</p>
<p><em>In-tree Volume Plugin:</em> <a href="https://kubernetes.io/docs/concepts/storage/volumes/#awselasticblockstore">awsElasticBlockStore</a></p>
<p>In order to use In-tree volume plugin on Windows nodes, it is necessary to create an additional StorageClass to use NTFS as the fsType. On EKS, the default StorageClass uses ext4 as the default fsType. </p>
<p>A StorageClass provides a way for administrators to describe the "classes" of storage they offer. Different classes might map to quality-of-service levels, backup policies, or arbitrary policies determined by the cluster administrators. Kubernetes is unopinionated about what classes represent. This concept is sometimes called "profiles" in other storage systems.</p>
<p>You can check it by running the following command:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#windows-docs-storage-__codelineno-0-1"></a>kubectl<span class="w"> </span>describe<span class="w"> </span>storageclass<span class="w"> </span>gp2
</code></pre></div>
<p>Output:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#windows-docs-storage-__codelineno-1-1"></a>Name:<span class="w">            </span>gp2
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#windows-docs-storage-__codelineno-1-2"></a>IsDefaultClass:<span class="w">  </span>Yes
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#windows-docs-storage-__codelineno-1-3"></a>Annotations:<span class="w">     </span>kubectl.kubernetes.io/last-applied-configuration<span class="o">={</span><span class="s2">&quot;apiVersion&quot;</span>:<span class="s2">&quot;storage.k8s.io/v1&quot;</span>,<span class="s2">&quot;kind&quot;</span>:<span class="s2">&quot;StorageClas</span>
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#windows-docs-storage-__codelineno-1-4"></a><span class="s2">&quot;</span>,<span class="s2">&quot;metadata&quot;</span>:<span class="o">{</span><span class="s2">&quot;annotations&quot;</span>:<span class="o">{</span><span class="s2">&quot;storageclass.kubernetes.io/is-default-class&quot;</span>:<span class="s2">&quot;true&quot;</span><span class="o">}</span>,<span class="s2">&quot;name&quot;</span>:<span class="s2">&quot;gp2&quot;</span><span class="o">}</span>,<span class="s2">&quot;parameters&quot;</span>:<span class="o">{</span><span class="s2">&quot;fsType&quot;</span>
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#windows-docs-storage-__codelineno-1-5"></a><span class="s2">&quot;ext4&quot;</span>,<span class="s2">&quot;type&quot;</span>:<span class="s2">&quot;gp2&quot;</span><span class="o">}</span>,<span class="s2">&quot;provisioner&quot;</span>:<span class="s2">&quot;kubernetes.io/aws-ebs&quot;</span>,<span class="s2">&quot;volumeBindingMode&quot;</span>:<span class="s2">&quot;WaitForFirstConsumer&quot;</span><span class="o">}</span>
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#windows-docs-storage-__codelineno-1-6"></a>,storageclass.kubernetes.io/is-default-class<span class="o">=</span><span class="nb">true</span>
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#windows-docs-storage-__codelineno-1-7"></a>Provisioner:<span class="w">           </span>kubernetes.io/aws-ebs
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#windows-docs-storage-__codelineno-1-8"></a>Parameters:<span class="w">            </span><span class="nv">fsType</span><span class="o">=</span>ext4,type<span class="o">=</span>gp2
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#windows-docs-storage-__codelineno-1-9"></a>AllowVolumeExpansion:<span class="w">  </span>&lt;unset&gt;
<a id="__codelineno-1-10" name="__codelineno-1-10" href="#windows-docs-storage-__codelineno-1-10"></a>MountOptions:<span class="w">          </span>&lt;none&gt;
<a id="__codelineno-1-11" name="__codelineno-1-11" href="#windows-docs-storage-__codelineno-1-11"></a>ReclaimPolicy:<span class="w">         </span>Delete
<a id="__codelineno-1-12" name="__codelineno-1-12" href="#windows-docs-storage-__codelineno-1-12"></a>VolumeBindingMode:<span class="w">     </span>WaitForFirstConsumer
<a id="__codelineno-1-13" name="__codelineno-1-13" href="#windows-docs-storage-__codelineno-1-13"></a>Events:<span class="w">                </span>&lt;none&gt;
</code></pre></div>
<p>To create the new StorageClass to support <strong>NTFS</strong>, use the following manifest:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#windows-docs-storage-__codelineno-2-1"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">StorageClass</span>
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#windows-docs-storage-__codelineno-2-2"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">storage.k8s.io/v1</span>
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#windows-docs-storage-__codelineno-2-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#windows-docs-storage-__codelineno-2-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gp2-windows</span>
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#windows-docs-storage-__codelineno-2-5"></a><span class="nt">provisioner</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">kubernetes.io/aws-ebs</span>
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#windows-docs-storage-__codelineno-2-6"></a><span class="nt">parameters</span><span class="p">:</span>
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#windows-docs-storage-__codelineno-2-7"></a><span class="w">  </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gp2</span>
<a id="__codelineno-2-8" name="__codelineno-2-8" href="#windows-docs-storage-__codelineno-2-8"></a><span class="w">  </span><span class="nt">fsType</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ntfs</span>
<a id="__codelineno-2-9" name="__codelineno-2-9" href="#windows-docs-storage-__codelineno-2-9"></a><span class="nt">volumeBindingMode</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">WaitForFirstConsumer</span>
</code></pre></div>
<p>Create the StorageClass by running the following command:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#windows-docs-storage-__codelineno-3-1"></a>kubectl<span class="w"> </span>apply<span class="w"> </span>-f<span class="w"> </span>NTFSStorageClass.yaml
</code></pre></div>
<p>The next step is to create a Persistent Volume Claim (PVC).</p>
<p>A PersistentVolume (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using PVC. It is a resource in the cluster just like a node is a cluster resource. This API object captures the details of the implementation of the storage, be that NFS, iSCSI, or a cloud-provider-specific storage system.</p>
<p>A PersistentVolumeClaim (PVC) is a request for storage by a user. Claims can request specific size and access modes (e.g., they can be mounted ReadWriteOnce, ReadOnlyMany or ReadWriteMany).</p>
<p>Users need PersistentVolumes with different attributes, such as performance, for different use cases. Cluster administrators need to be able to offer a variety of PersistentVolumes that differ in more ways than just size and access modes, without exposing users to the details of how those volumes are implemented. For these needs, there is the StorageClass resource.</p>
<p>In the example below, the PVC has been created within the namespace windows.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#windows-docs-storage-__codelineno-4-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<a id="__codelineno-4-2" name="__codelineno-4-2" href="#windows-docs-storage-__codelineno-4-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">PersistentVolumeClaim</span>
<a id="__codelineno-4-3" name="__codelineno-4-3" href="#windows-docs-storage-__codelineno-4-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-4-4" name="__codelineno-4-4" href="#windows-docs-storage-__codelineno-4-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ebs-windows-pv-claim</span>
<a id="__codelineno-4-5" name="__codelineno-4-5" href="#windows-docs-storage-__codelineno-4-5"></a><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">windows</span>
<a id="__codelineno-4-6" name="__codelineno-4-6" href="#windows-docs-storage-__codelineno-4-6"></a><span class="nt">spec</span><span class="p">:</span><span class="w"> </span>
<a id="__codelineno-4-7" name="__codelineno-4-7" href="#windows-docs-storage-__codelineno-4-7"></a><span class="w">  </span><span class="nt">accessModes</span><span class="p">:</span>
<a id="__codelineno-4-8" name="__codelineno-4-8" href="#windows-docs-storage-__codelineno-4-8"></a><span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ReadWriteOnce</span>
<a id="__codelineno-4-9" name="__codelineno-4-9" href="#windows-docs-storage-__codelineno-4-9"></a><span class="w">  </span><span class="nt">storageClassName</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gp2-windows</span>
<a id="__codelineno-4-10" name="__codelineno-4-10" href="#windows-docs-storage-__codelineno-4-10"></a><span class="w">  </span><span class="nt">resources</span><span class="p">:</span><span class="w"> </span>
<a id="__codelineno-4-11" name="__codelineno-4-11" href="#windows-docs-storage-__codelineno-4-11"></a><span class="w">    </span><span class="nt">requests</span><span class="p">:</span>
<a id="__codelineno-4-12" name="__codelineno-4-12" href="#windows-docs-storage-__codelineno-4-12"></a><span class="w">      </span><span class="nt">storage</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1Gi</span>
</code></pre></div>
<p>Create the PVC by running the following command:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#windows-docs-storage-__codelineno-5-1"></a>kubectl<span class="w"> </span>apply<span class="w"> </span>-f<span class="w"> </span>persistent-volume-claim.yaml
</code></pre></div>
<p>The following manifest creates a Windows Pod, setup the VolumeMount as <code>C:\Data</code> and uses the PVC as the attached storage on <code>C:\Data</code>. </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#windows-docs-storage-__codelineno-6-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">apps/v1</span>
<a id="__codelineno-6-2" name="__codelineno-6-2" href="#windows-docs-storage-__codelineno-6-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deployment</span>
<a id="__codelineno-6-3" name="__codelineno-6-3" href="#windows-docs-storage-__codelineno-6-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-6-4" name="__codelineno-6-4" href="#windows-docs-storage-__codelineno-6-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">windows-server-ltsc2019</span>
<a id="__codelineno-6-5" name="__codelineno-6-5" href="#windows-docs-storage-__codelineno-6-5"></a><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">windows</span>
<a id="__codelineno-6-6" name="__codelineno-6-6" href="#windows-docs-storage-__codelineno-6-6"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-6-7" name="__codelineno-6-7" href="#windows-docs-storage-__codelineno-6-7"></a><span class="w">  </span><span class="nt">selector</span><span class="p">:</span>
<a id="__codelineno-6-8" name="__codelineno-6-8" href="#windows-docs-storage-__codelineno-6-8"></a><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span>
<a id="__codelineno-6-9" name="__codelineno-6-9" href="#windows-docs-storage-__codelineno-6-9"></a><span class="w">      </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">windows-server-ltsc2019</span>
<a id="__codelineno-6-10" name="__codelineno-6-10" href="#windows-docs-storage-__codelineno-6-10"></a><span class="w">      </span><span class="nt">tier</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">backend</span>
<a id="__codelineno-6-11" name="__codelineno-6-11" href="#windows-docs-storage-__codelineno-6-11"></a><span class="w">      </span><span class="nt">track</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">stable</span>
<a id="__codelineno-6-12" name="__codelineno-6-12" href="#windows-docs-storage-__codelineno-6-12"></a><span class="w">  </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<a id="__codelineno-6-13" name="__codelineno-6-13" href="#windows-docs-storage-__codelineno-6-13"></a><span class="w">  </span><span class="nt">template</span><span class="p">:</span>
<a id="__codelineno-6-14" name="__codelineno-6-14" href="#windows-docs-storage-__codelineno-6-14"></a><span class="w">    </span><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-6-15" name="__codelineno-6-15" href="#windows-docs-storage-__codelineno-6-15"></a><span class="w">      </span><span class="nt">labels</span><span class="p">:</span>
<a id="__codelineno-6-16" name="__codelineno-6-16" href="#windows-docs-storage-__codelineno-6-16"></a><span class="w">        </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">windows-server-ltsc2019</span>
<a id="__codelineno-6-17" name="__codelineno-6-17" href="#windows-docs-storage-__codelineno-6-17"></a><span class="w">        </span><span class="nt">tier</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">backend</span>
<a id="__codelineno-6-18" name="__codelineno-6-18" href="#windows-docs-storage-__codelineno-6-18"></a><span class="w">        </span><span class="nt">track</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">stable</span>
<a id="__codelineno-6-19" name="__codelineno-6-19" href="#windows-docs-storage-__codelineno-6-19"></a><span class="w">    </span><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-6-20" name="__codelineno-6-20" href="#windows-docs-storage-__codelineno-6-20"></a><span class="w">      </span><span class="nt">containers</span><span class="p">:</span>
<a id="__codelineno-6-21" name="__codelineno-6-21" href="#windows-docs-storage-__codelineno-6-21"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">windows-server-ltsc2019</span>
<a id="__codelineno-6-22" name="__codelineno-6-22" href="#windows-docs-storage-__codelineno-6-22"></a><span class="w">        </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">mcr.microsoft.com/windows/servercore:ltsc2019</span>
<a id="__codelineno-6-23" name="__codelineno-6-23" href="#windows-docs-storage-__codelineno-6-23"></a><span class="w">        </span><span class="nt">ports</span><span class="p">:</span>
<a id="__codelineno-6-24" name="__codelineno-6-24" href="#windows-docs-storage-__codelineno-6-24"></a><span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">http</span>
<a id="__codelineno-6-25" name="__codelineno-6-25" href="#windows-docs-storage-__codelineno-6-25"></a><span class="w">          </span><span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">80</span>
<a id="__codelineno-6-26" name="__codelineno-6-26" href="#windows-docs-storage-__codelineno-6-26"></a><span class="w">        </span><span class="nt">imagePullPolicy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">IfNotPresent</span>
<a id="__codelineno-6-27" name="__codelineno-6-27" href="#windows-docs-storage-__codelineno-6-27"></a><span class="w">        </span><span class="nt">volumeMounts</span><span class="p">:</span>
<a id="__codelineno-6-28" name="__codelineno-6-28" href="#windows-docs-storage-__codelineno-6-28"></a><span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;C:\\data&quot;</span>
<a id="__codelineno-6-29" name="__codelineno-6-29" href="#windows-docs-storage-__codelineno-6-29"></a><span class="w">          </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">test-volume</span>
<a id="__codelineno-6-30" name="__codelineno-6-30" href="#windows-docs-storage-__codelineno-6-30"></a><span class="w">      </span><span class="nt">volumes</span><span class="p">:</span>
<a id="__codelineno-6-31" name="__codelineno-6-31" href="#windows-docs-storage-__codelineno-6-31"></a><span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">test-volume</span>
<a id="__codelineno-6-32" name="__codelineno-6-32" href="#windows-docs-storage-__codelineno-6-32"></a><span class="w">          </span><span class="nt">persistentVolumeClaim</span><span class="p">:</span>
<a id="__codelineno-6-33" name="__codelineno-6-33" href="#windows-docs-storage-__codelineno-6-33"></a><span class="w">            </span><span class="nt">claimName</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ebs-windows-pv-claim</span>
<a id="__codelineno-6-34" name="__codelineno-6-34" href="#windows-docs-storage-__codelineno-6-34"></a><span class="w">      </span><span class="nt">nodeSelector</span><span class="p">:</span>
<a id="__codelineno-6-35" name="__codelineno-6-35" href="#windows-docs-storage-__codelineno-6-35"></a><span class="w">        </span><span class="nt">kubernetes.io/os</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">windows</span>
<a id="__codelineno-6-36" name="__codelineno-6-36" href="#windows-docs-storage-__codelineno-6-36"></a><span class="w">        </span><span class="nt">node.kubernetes.io/windows-build</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;10.0.17763&#39;</span>
</code></pre></div>
<p>Test the results by accessing the Windows pod via PowerShell:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#windows-docs-storage-__codelineno-7-1"></a>kubectl<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>-it<span class="w"> </span>podname<span class="w"> </span>powershell<span class="w"> </span>-n<span class="w"> </span>windows
</code></pre></div>
<p>Inside the Windows Pod, run: <code>ls</code></p>
<p>Output:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1" href="#windows-docs-storage-__codelineno-8-1"></a>PS<span class="w"> </span>C:<span class="se">\&gt;</span><span class="w"> </span>ls
<a id="__codelineno-8-2" name="__codelineno-8-2" href="#windows-docs-storage-__codelineno-8-2"></a>
<a id="__codelineno-8-3" name="__codelineno-8-3" href="#windows-docs-storage-__codelineno-8-3"></a>
<a id="__codelineno-8-4" name="__codelineno-8-4" href="#windows-docs-storage-__codelineno-8-4"></a><span class="w">    </span>Directory:<span class="w"> </span>C:<span class="se">\</span>
<a id="__codelineno-8-5" name="__codelineno-8-5" href="#windows-docs-storage-__codelineno-8-5"></a>
<a id="__codelineno-8-6" name="__codelineno-8-6" href="#windows-docs-storage-__codelineno-8-6"></a>
<a id="__codelineno-8-7" name="__codelineno-8-7" href="#windows-docs-storage-__codelineno-8-7"></a>Mode<span class="w">                 </span>LastWriteTime<span class="w">         </span>Length<span class="w"> </span>Name
<a id="__codelineno-8-8" name="__codelineno-8-8" href="#windows-docs-storage-__codelineno-8-8"></a>----<span class="w">                 </span>-------------<span class="w">         </span>------<span class="w"> </span>----
<a id="__codelineno-8-9" name="__codelineno-8-9" href="#windows-docs-storage-__codelineno-8-9"></a>d-----<span class="w">          </span><span class="m">3</span>/8/2021<span class="w">   </span><span class="m">1</span>:54<span class="w"> </span>PM<span class="w">                </span>data
<a id="__codelineno-8-10" name="__codelineno-8-10" href="#windows-docs-storage-__codelineno-8-10"></a>d-----<span class="w">          </span><span class="m">3</span>/8/2021<span class="w">   </span><span class="m">3</span>:37<span class="w"> </span>PM<span class="w">                </span>inetpub
<a id="__codelineno-8-11" name="__codelineno-8-11" href="#windows-docs-storage-__codelineno-8-11"></a>d-r---<span class="w">          </span><span class="m">1</span>/9/2021<span class="w">   </span><span class="m">7</span>:26<span class="w"> </span>AM<span class="w">                </span>Program<span class="w"> </span>Files
<a id="__codelineno-8-12" name="__codelineno-8-12" href="#windows-docs-storage-__codelineno-8-12"></a>d-----<span class="w">          </span><span class="m">1</span>/9/2021<span class="w">   </span><span class="m">7</span>:18<span class="w"> </span>AM<span class="w">                </span>Program<span class="w"> </span>Files<span class="w"> </span><span class="o">(</span>x86<span class="o">)</span>
<a id="__codelineno-8-13" name="__codelineno-8-13" href="#windows-docs-storage-__codelineno-8-13"></a>d-r---<span class="w">          </span><span class="m">1</span>/9/2021<span class="w">   </span><span class="m">7</span>:28<span class="w"> </span>AM<span class="w">                </span>Users
<a id="__codelineno-8-14" name="__codelineno-8-14" href="#windows-docs-storage-__codelineno-8-14"></a>d-----<span class="w">          </span><span class="m">3</span>/8/2021<span class="w">   </span><span class="m">3</span>:36<span class="w"> </span>PM<span class="w">                </span>var
<a id="__codelineno-8-15" name="__codelineno-8-15" href="#windows-docs-storage-__codelineno-8-15"></a>d-----<span class="w">          </span><span class="m">3</span>/8/2021<span class="w">   </span><span class="m">3</span>:36<span class="w"> </span>PM<span class="w">                </span>Windows
<a id="__codelineno-8-16" name="__codelineno-8-16" href="#windows-docs-storage-__codelineno-8-16"></a>-a----<span class="w">         </span><span class="m">12</span>/7/2019<span class="w">   </span><span class="m">4</span>:20<span class="w"> </span>AM<span class="w">           </span><span class="m">5510</span><span class="w"> </span>License.txt
</code></pre></div>
<p>The <strong>data directory</strong> is provided by the EBS volume.</p>
<h2 id="windows-docs-storage-out-of-tree-for-windows">Out-of-tree for Windows<a class="headerlink" href="#windows-docs-storage-out-of-tree-for-windows" title="Permanent link">&para;</a></h2>
<p>Code associated with CSI plugins ship as out-of-tree scripts and binaries that are typically distributed as container images and deployed using standard Kubernetes constructs like DaemonSets and StatefulSets. CSI plugins handle a wide range of volume management actions in Kubernetes. CSI plugins typically consist of node plugins (that run on each node as a DaemonSet) and controller plugins.</p>
<p>CSI node plugins (especially those associated with persistent volumes exposed as either block devices or over a shared file-system) need to perform various privileged operations like scanning of disk devices, mounting of file systems, etc. These operations differ for each host operating system. For Linux worker nodes, containerized CSI node plugins are typically deployed as privileged containers. For Windows worker nodes, privileged operations for containerized CSI node plugins is supported using <a href="https://github.com/kubernetes-csi/csi-proxy">csi-proxy</a>, a community-managed, stand-alone binary that needs to be pre-installed on each Windows node. </p>
<p><a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-windows-ami.html">The Amazon EKS Optimized Windows AMI</a> includes CSI-proxy starting from April 2022. Customers can use the <a href="https://github.com/kubernetes-csi/csi-driver-smb">SMB CSI Driver</a> on Windows nodes to access <a href="https://aws.amazon.com/fsx/windows/">Amazon FSx for Windows File Server</a>, <a href="https://aws.amazon.com/fsx/netapp-ontap/">Amazon FSx for NetApp ONTAP SMB Shares</a>, and/or <a href="https://aws.amazon.com/storagegateway/file/">AWS Storage Gateway – File Gateway</a>.</p>
<p>The following <a href="https://aws.amazon.com/blogs/modernizing-with-aws/using-smb-csi-driver-on-amazon-eks-windows-nodes/">blog</a> has implementation details on how to setup SMB CSI Driver to use Amazon FSx for Windows File Server as a persistent storage for Windows Pods.</p>
<h2 id="windows-docs-storage-amazon-fsx-for-windows-file-server">Amazon FSx for Windows File Server<a class="headerlink" href="#windows-docs-storage-amazon-fsx-for-windows-file-server" title="Permanent link">&para;</a></h2>
<p>An option is to use Amazon FSx for Windows File Server through an SMB feature called <a href="https://docs.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/persistent-storage">SMB Global Mapping</a> which makes it possible to mount a SMB share on the host, then pass directories on that share into a container. The container doesn't need to be configured with a specific server, share, username or password - that's all handled on the host instead. The container will work the same as if it had local storage.</p>
<blockquote>
<p>The SMB Global Mapping is transparent to the orchestrator, and it is mounted through HostPath which can <strong>imply in secure concerns</strong>.</p>
</blockquote>
<p>In the example below,  the path <code>G:\Directory\app-state</code> is an SMB share on the Windows Node.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-9-1" name="__codelineno-9-1" href="#windows-docs-storage-__codelineno-9-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<a id="__codelineno-9-2" name="__codelineno-9-2" href="#windows-docs-storage-__codelineno-9-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Pod</span>
<a id="__codelineno-9-3" name="__codelineno-9-3" href="#windows-docs-storage-__codelineno-9-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-9-4" name="__codelineno-9-4" href="#windows-docs-storage-__codelineno-9-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">test-fsx</span>
<a id="__codelineno-9-5" name="__codelineno-9-5" href="#windows-docs-storage-__codelineno-9-5"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-9-6" name="__codelineno-9-6" href="#windows-docs-storage-__codelineno-9-6"></a><span class="w">  </span><span class="nt">containers</span><span class="p">:</span>
<a id="__codelineno-9-7" name="__codelineno-9-7" href="#windows-docs-storage-__codelineno-9-7"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">test-fsx</span>
<a id="__codelineno-9-8" name="__codelineno-9-8" href="#windows-docs-storage-__codelineno-9-8"></a><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">mcr.microsoft.com/windows/servercore:ltsc2019</span>
<a id="__codelineno-9-9" name="__codelineno-9-9" href="#windows-docs-storage-__codelineno-9-9"></a><span class="w">    </span><span class="nt">command</span><span class="p">:</span>
<a id="__codelineno-9-10" name="__codelineno-9-10" href="#windows-docs-storage-__codelineno-9-10"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">powershell.exe</span>
<a id="__codelineno-9-11" name="__codelineno-9-11" href="#windows-docs-storage-__codelineno-9-11"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">-command</span>
<a id="__codelineno-9-12" name="__codelineno-9-12" href="#windows-docs-storage-__codelineno-9-12"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&quot;Add-WindowsFeature</span><span class="nv"> </span><span class="s">Web-Server;</span><span class="nv"> </span><span class="s">Invoke-WebRequest</span><span class="nv"> </span><span class="s">-UseBasicParsing</span><span class="nv"> </span><span class="s">-Uri</span><span class="nv"> </span><span class="s">&#39;https://dotnetbinaries.blob.core.windows.net/servicemonitor/2.0.1.6/ServiceMonitor.exe&#39;</span><span class="nv"> </span><span class="s">-OutFile</span><span class="nv"> </span><span class="s">&#39;C:\\ServiceMonitor.exe&#39;;</span><span class="nv"> </span><span class="s">echo</span><span class="nv"> </span><span class="s">&#39;&lt;html&gt;&lt;body&gt;&lt;br/&gt;&lt;br/&gt;&lt;marquee&gt;&lt;H1&gt;Hello</span><span class="nv"> </span><span class="s">EKS!!!&lt;H1&gt;&lt;marquee&gt;&lt;/body&gt;&lt;html&gt;&#39;</span><span class="nv"> </span><span class="s">&gt;</span><span class="nv"> </span><span class="s">C:\\inetpub\\wwwroot\\default.html;</span><span class="nv"> </span><span class="s">C:\\ServiceMonitor.exe</span><span class="nv"> </span><span class="s">&#39;w3svc&#39;;</span><span class="nv"> </span><span class="s">&quot;</span>
<a id="__codelineno-9-13" name="__codelineno-9-13" href="#windows-docs-storage-__codelineno-9-13"></a><span class="w">    </span><span class="nt">volumeMounts</span><span class="p">:</span>
<a id="__codelineno-9-14" name="__codelineno-9-14" href="#windows-docs-storage-__codelineno-9-14"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">C:\dotnetapp\app-state</span>
<a id="__codelineno-9-15" name="__codelineno-9-15" href="#windows-docs-storage-__codelineno-9-15"></a><span class="w">        </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">test-mount</span>
<a id="__codelineno-9-16" name="__codelineno-9-16" href="#windows-docs-storage-__codelineno-9-16"></a><span class="w">  </span><span class="nt">volumes</span><span class="p">:</span>
<a id="__codelineno-9-17" name="__codelineno-9-17" href="#windows-docs-storage-__codelineno-9-17"></a><span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">test-mount</span>
<a id="__codelineno-9-18" name="__codelineno-9-18" href="#windows-docs-storage-__codelineno-9-18"></a><span class="w">      </span><span class="nt">hostPath</span><span class="p">:</span><span class="w"> </span>
<a id="__codelineno-9-19" name="__codelineno-9-19" href="#windows-docs-storage-__codelineno-9-19"></a><span class="w">        </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">G:\Directory\app-state</span>
<a id="__codelineno-9-20" name="__codelineno-9-20" href="#windows-docs-storage-__codelineno-9-20"></a><span class="w">        </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Directory</span>
<a id="__codelineno-9-21" name="__codelineno-9-21" href="#windows-docs-storage-__codelineno-9-21"></a><span class="w">  </span><span class="nt">nodeSelector</span><span class="p">:</span>
<a id="__codelineno-9-22" name="__codelineno-9-22" href="#windows-docs-storage-__codelineno-9-22"></a><span class="w">      </span><span class="nt">beta.kubernetes.io/os</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">windows</span>
<a id="__codelineno-9-23" name="__codelineno-9-23" href="#windows-docs-storage-__codelineno-9-23"></a><span class="w">      </span><span class="nt">beta.kubernetes.io/arch</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">amd64</span>
</code></pre></div>
<p>The following <a href="https://aws.amazon.com/blogs/containers/using-amazon-fsx-for-windows-file-server-on-eks-windows-containers/">blog</a> has implementation details on how to setup Amazon FSx for Windows File Server as a persistent storage for Windows Pods. </p></section><h1 class='nav-section-title-end'>Ended: Windows Containers</h1>
                        <h1 class='nav-section-title' id='section-networking'>
                            Networking <a class='headerlink' href='#section-networking' title='Permanent link'>↵</a>
                        </h1>
                        <section class="print-page" id="networking-index"><h1 id="networking-index-amazon-eks-best-practices-guide-for-networking">Amazon EKS Best Practices Guide for Networking<a class="headerlink" href="#networking-index-amazon-eks-best-practices-guide-for-networking" title="Permanent link">&para;</a></h1>
<p>It is critical to understand Kubernetes networking to operate your cluster and applications efficiently. Pod networking, also called the cluster networking, is the center of Kubernetes networking. Kubernetes supports <a href="https://github.com/containernetworking/cni">Container Network Interface</a> (CNI) plugins for cluster networking. </p>
<p>Amazon EKS officially supports <a href="https://docs.aws.amazon.com/vpc/latest/userguide/what-is-amazon-vpc.html">Amazon Virtual Private Cloud (VPC)</a> CNI plugin to implement Kubernetes Pod networking. The VPC CNI provides native integration with AWS VPC and works in underlay mode. In underlay mode, Pods and hosts are located at the same network layer and share the network namespace. The IP address of the Pod is consistent from the cluster and VPC perspective. </p>
<p>This guide introduces the <a href="https://github.com/aws/amazon-vpc-cni-k8s">Amazon VPC Container Network Interface</a><a href="https://github.com/aws/amazon-vpc-cni-k8s">(VPC CNI)</a> in the context of Kubernetes cluster networking. The VPC CNI is the default networking plugin supported by EKS and hence is the focus of the guide. The VPC CNI is highly configurable to support different use cases. This guide further includes dedicated sections on different VPC CNI use cases, operating modes, sub-components, followed by the recommendations.</p>
<p>Amazon EKS runs upstream Kubernetes and is certified Kubernetes conformant. Although you can use alternate CNI plugins, this guide does not provide recommendations for managing alternate CNIs. Check the <a href="https://docs.aws.amazon.com/eks/latest/userguide/alternate-cni-plugins.html">EKS Alternate CNI</a> documentation for a list of partners and resources for managing alternate CNIs effectively.</p>
<h2 id="networking-index-kubernetes-networking-model">Kubernetes Networking Model<a class="headerlink" href="#networking-index-kubernetes-networking-model" title="Permanent link">&para;</a></h2>
<p>Kubernetes sets the following requirements on cluster networking:</p>
<ul>
<li>Pods scheduled on the same node must be able to communicate with other Pods without using NAT (Network Address Translation).</li>
<li>All system daemons (background processes, for example, <a href="https://kubernetes.io/docs/concepts/overview/components/">kubelet</a>) running on a particular node can communicate with the Pods running on the same node.</li>
<li>Pods that use the <a href="https://docs.docker.com/network/host/">host network</a> must be able to contact all other Pods on all other nodes without using NAT.</li>
</ul>
<p>See <a href="https://kubernetes.io/docs/concepts/services-networking/#the-kubernetes-network-model">the Kubernetes network model</a> for details on what Kubernetes expects from compatible networking implementations. The following figure illustrates the relationship between Pod network namespaces and the host network namespace.</p>
<p><img alt="illustration of host network and 2 pod network namespaces" src="../networking/index/image.png" /></p>
<h2 id="networking-index-container-networking-interface-cni">Container Networking Interface (CNI)<a class="headerlink" href="#networking-index-container-networking-interface-cni" title="Permanent link">&para;</a></h2>
<p>Kubernetes supports CNI specifications and plugins to implement Kubernetes network model. A CNI consists of a <a href="https://github.com/containernetworking/cni/blob/main/SPEC.md">specification</a> (current version 1.0.0) and libraries for writing plugins to configure network interfaces in containers, along with a number of supported plugins. CNI concerns itself only with network connectivity of containers and removing allocated resources when the container is deleted. </p>
<p>The CNI plugin is enabled by passing kubelet the <code>--network-plugin=cni</code> command-line option. Kubelet reads a file from <code>--cni-conf-dir</code> (default /etc/cni/net.d) and uses the CNI configuration from that file to set up each Pod’s network. The CNI configuration file must match the CNI specification (minimum v0.4.0) and any required CNI plugins referenced by the configuration must be present in the <code>--cni-bin-dir</code> directory (default /opt/cni/bin). If there are multiple CNI configuration files in the directory, <em>the kubelet uses the configuration file that comes first by name in lexicographic order</em>.</p>
<h2 id="networking-index-amazon-virtual-private-cloud-vpc-cni">Amazon Virtual Private Cloud (VPC) CNI<a class="headerlink" href="#networking-index-amazon-virtual-private-cloud-vpc-cni" title="Permanent link">&para;</a></h2>
<p>The AWS-provided VPC CNI is the default networking add-on for EKS clusters. VPC CNI add-on is installed by default when you provision EKS clusters. VPC CNI runs on Kubernetes worker nodes. The VPC CNI add-on consists of the CNI binary and the IP Address Management (ipamd) plugin. The CNI assigns an IP address from the VPC network to a Pod. The ipamd manages AWS Elastic Networking Interfaces (ENIs) to each Kubernetes node and maintains the warm pool of IPs. The VPC CNI provides configuration options for pre-allocation of ENIs and IP addresses for fast Pod startup times. Refer to <a href="#networking-vpc-cni">Amazon VPC CNI</a> for recommended plugin management best practices.</p>
<p>Amazon EKS recommends you specify subnets in at least two availability zones when you create a cluster. Amazon VPC CNI allocates IP addresses to Pods from the node subnets. We strongly recommend checking the subnets for available IP addresses. Please consider <a href="#networking-subnets">VPC and Subnet</a> recommendations before deploying EKS clusters. </p>
<p>Amazon VPC CNI allocates a warm pool of ENIs and secondary IP addresses from the subnet attached to the node’s primary ENI. This mode of VPC CNI is called the "<a href="#networking-vpc-cni">secondary IP mode</a>." The number of IP addresses and hence the number of Pods (Pod density) is defined by the number of ENIs and the IP address per ENI (limits) as defined by the instance type. The secondary mode is the default and works well for small clusters with smaller instance types. Please consider using <a href="#networking-prefix-mode-index_linux">prefix mode</a> if you are experiencing pod density challenges. You can also increase the available IP addresses on node for Pods by assigning prefixes to ENIs.</p>
<p>Amazon VPC CNI natively integrates with AWS VPC and allows users to apply existing AWS VPC networking and security best practices for building Kubernetes clusters. This includes the ability to use VPC flow logs, VPC routing policies, and security groups for network traffic isolation. By default, the Amazon VPC CNI applies security group associated with the primary ENI on the node to the Pods. Consider enabling <a href="#networking-sgpp">security groups for Pods</a> when you would like to assign different network rules for a Pod.</p>
<p>By default, VPC CNI assigns IP addresses to Pods from the subnet assigned to the primary ENI of a node. It is common to experience a shortage of IPv4 addresses when running large clusters with thousands of workloads. AWS VPC allows you to extend available IPs by <a href="https://docs.aws.amazon.com/vpc/latest/userguide/configure-your-vpc.html#add-cidr-block-restrictions">assigning a secondary CIDRs</a> to work around exhaustion of IPv4 CIDR blocks. AWS VPC CNI allows you to use a different subnet CIDR range for Pods. This feature of VPC CNI is called <a href="#networking-custom-networking">custom networking</a>. You might consider using custom networking to use 100.64.0.0/10 and 198.19.0.0/16 CIDRs (CG-NAT) with EKS. This effectively allows you to create an environment where Pods no longer consume any RFC1918 IP addresses from your VPC.</p>
<p>Custom networking is one option to address the IPv4 address exhaustion problem, but it requires operational overhead. We recommend IPv6 clusters over custom networking to resolve this problem. Specifically, we recommend migrating to <a href="#networking-ipv6">IPv6 clusters</a> if you have completely exhausted all available IPv4 address space for your VPC. Evaluate your organization’s plans to support IPv6, and consider if investing in IPv6 may have more long-term value. </p>
<p>EKS’s support for IPv6 is focused on solving the IP exhaustion problem caused by a limited IPv4 address space. In response to customer issues with IPv4 exhaustion, EKS has prioritized IPv6-only Pods over dual-stack Pods. That is, Pods may be able to access IPv4 resources, but they are not assigned an IPv4 address from VPC CIDR range. The VPC CNI assigns IPv6 addresses to Pods from the AWS managed VPC IPv6 CIDR block. </p>
<h2 id="networking-index-subnet-calculator">Subnet Calculator<a class="headerlink" href="#networking-index-subnet-calculator" title="Permanent link">&para;</a></h2>
<p>This project includes a <a href="../networking/subnet-calc/subnet-calc.xlsx">Subnet Calculator Excel Document</a>. This calculator document simulates the IP address consumption of a specified workload under different ENI configuration options, such as <code>WARM_IP_TARGET</code> and <code>WARM_ENI_TARGET</code>. The document includes two sheets, a first for Warm ENI mode, and a second for Warm IP mode. Review the <a href="#networking-vpc-cni">VPC CNI guidance</a> for more information on these modes. </p>
<p>Inputs:
- Subnet CIDR Size
- Warm ENI Target <em>or</em> Warm IP Target
- List of instances
    - type, number, and number of workload pods scheduled per instance</p>
<p>Outputs:
- Total number of pods hosted
- Number of Subnet IPs consumed
- Number of Subnet IPs remaining
- Instance Level Details
    - Number of Warm IPs/ENIs per instance
    - Number of Active IPs/ENIs per instance</p></section><section class="print-page" id="networking-subnets"><h1 id="networking-subnets-vpc-and-subnet-considerations">VPC and Subnet Considerations<a class="headerlink" href="#networking-subnets-vpc-and-subnet-considerations" title="Permanent link">&para;</a></h1>
<p>Operating an EKS cluster requires knowledge of AWS VPC networking, in addition to Kubernetes networking.</p>
<p>We recommend you understand the EKS control plane communication mechanisms before you start designing your VPC or deploying clusters into existing VPCs.</p>
<p>Refer to <a href="https://docs.aws.amazon.com/eks/latest/userguide/network_reqs.html">Cluster VPC considerations</a> and <a href="https://docs.aws.amazon.com/eks/latest/userguide/sec-group-reqs.html">Amazon EKS security group considerations</a> when architecting a VPC and subnets to be used with EKS.</p>
<h2 id="networking-subnets-overview">Overview<a class="headerlink" href="#networking-subnets-overview" title="Permanent link">&para;</a></h2>
<h3 id="networking-subnets-eks-cluster-architecture">EKS Cluster Architecture<a class="headerlink" href="#networking-subnets-eks-cluster-architecture" title="Permanent link">&para;</a></h3>
<p>An EKS cluster consists of two VPCs: </p>
<ul>
<li>An AWS-managed VPC that hosts the Kubernetes control plane. This VPC does not appear in the customer account. </li>
<li>A customer-managed VPC that hosts the Kubernetes nodes. This is where containers run, as well as other customer-managed AWS infrastructure such as load balancers used by the cluster. This VPC appears in the customer account. You need to create customer-managed VPC prior creating a cluster. The eksctl creates a VPC if you do not provide one.</li>
</ul>
<p>The nodes in the customer VPC need the ability to connect to the managed API server endpoint in the AWS VPC. This allows the nodes to register with the Kubernetes control plane and receive requests to run application Pods.</p>
<p>The nodes connect to the EKS control plane through (a) an EKS public endpoint or (b) a Cross-Account <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html">elastic network interfaces</a>  (X-ENI) managed by EKS. When a cluster is created, you need to specify at least two VPC subnets. EKS places a X-ENI in each  subnet specified during cluster create (also called cluster subnets). The Kubernetes API server uses these Cross-Account ENIs to communicate with nodes deployed on the customer-managed cluster VPC subnets. </p>
<p><img alt="general illustration of cluster networking, including load balancer, nodes, and pods." src="../networking/subnets/image.png" /></p>
<p>As the node starts, the EKS bootstrap script is executed and Kubernetes node configuration files are installed. As part of the boot process on each instance, the container runtime agents, kubelet, and Kubernetes node agents are launched.</p>
<p>To register a node, Kubelet contacts the Kubernetes cluster endpoint. It establishes a connection with either the public endpoint outside of the VPC or the private endpoint within the VPC. Kubelet receives API instructions and provides status updates and heartbeats to the endpoint on a regular basis.</p>
<h3 id="networking-subnets-eks-control-plane-communication">EKS Control Plane Communication<a class="headerlink" href="#networking-subnets-eks-control-plane-communication" title="Permanent link">&para;</a></h3>
<p>EKS has two ways to control access to the <a href="https://docs.aws.amazon.com/eks/latest/userguide/cluster-endpoint.html">cluster endpoint</a>. Endpoint access control lets you choose whether the endpoint can be reached from the public internet or only through your VPC. You can turn on the public endpoint (which is the default), the private endpoint, or both at once. </p>
<p>The configuration of the cluster API endpoint determines the path that nodes take to communicate to the control plane. Note that these endpoint settings can be changed at any time through the EKS console or API.</p>
<h4 id="networking-subnets-public-endpoint">Public Endpoint<a class="headerlink" href="#networking-subnets-public-endpoint" title="Permanent link">&para;</a></h4>
<p>This is the default behavior for new Amazon EKS clusters. When only the public endpoint for the cluster is enabled, Kubernetes API requests that originate from within your cluster’s VPC (such as worker node to control plane communication) leave the VPC, but not Amazon’s network. In order for nodes to connect to the control plane, they must have a public IP address and a route to an internet gateway or a route to a NAT gateway where they can use the public IP address of the NAT gateway.</p>
<h4 id="networking-subnets-public-and-private-endpoint">Public and Private Endpoint<a class="headerlink" href="#networking-subnets-public-and-private-endpoint" title="Permanent link">&para;</a></h4>
<p>When both the public and private endpoints are enabled, Kubernetes API requests from within the VPC communicate to the control plane via the X-ENIs within your VPC. Your cluster API server is accessible from the internet.</p>
<h4 id="networking-subnets-private-endpoint">Private Endpoint<a class="headerlink" href="#networking-subnets-private-endpoint" title="Permanent link">&para;</a></h4>
<p>There is no public access to your API server from the internet when only private endpoint is enabled. All traffic to your cluster API server must come from within your cluster’s VPC or a connected network. The nodes communicate to API server via X-ENIs within your VPC. Note that cluster management tools must have access to the private endpoint. Learn more about <a href="https://aws.amazon.com/premiumsupport/knowledge-center/eks-private-cluster-endpoint-vpc/">how to connect to a private Amazon EKS cluster endpoint from outside the Amazon VPC.</a></p>
<p>Note that the cluster's API server endpoint is resolved by public DNS servers to a private IP address from the VPC. In the past, the endpoint could only be resolved from within the VPC.</p>
<h3 id="networking-subnets-vpc-configurations">VPC configurations<a class="headerlink" href="#networking-subnets-vpc-configurations" title="Permanent link">&para;</a></h3>
<p>Amazon VPC supports IPv4 and IPv6 addressing. Amazon EKS supports IPv4 by default. A VPC must have an IPv4 CIDR block associated with it. You can optionally associate multiple IPv4 <a href="http://en.wikipedia.org/wiki/CIDR_notation">Classless Inter-Domain Routing</a> (CIDR) blocks and multiple IPv6 CIDR blocks to your VPC. When you create a VPC, you must specify an IPv4 CIDR block for the VPC from the private IPv4 address ranges as specified in <a href="http://www.faqs.org/rfcs/rfc1918.html">RFC 1918</a>. The allowed block size is between a <code>/16</code> prefix (65,536 IP addresses) and <code>/28</code> prefix (16 IP addresses). </p>
<p>When creating a new VPC, you can attach a single IPv6 CIDR block, and up to five when changing an existing VPC. The prefix length of the CIDR block is fixed at /64 for IPv6 VPCs, which has <a href="https://www.ripe.net/about-us/press-centre/understanding-ip-addressing#:~:text=IPv6%20Relative%20Network%20Sizes%20%20%20%2F128%20,Minimum%20IPv6%20allocation%20%201%20more%20rows%20">more than one trillion IP addresses</a>. You can request an IPv6 CIDR block from the pool of IPv6 addresses maintained by Amazon.</p>
<p>Amazon EKS clusters support both IPv4 and IPv6. By default, EKS clusters use IPv4 IP. Specifying IPv6 at cluster creation time will enable the use IPv6 clusters. IPv6 clusters require dual-stack VPCs and subnets.</p>
<p>Amazon EKS recommends you use at least two subnets that are in different Availability Zones during cluster creation. The subnets you pass in during cluster creation are known as cluster subnets. When you create a cluster, Amazon EKS creates up to 4 cross account (x-account or x-ENIs) ENIs in the subnets that you specify. The x-ENIs are always deployed and are used for cluster administration traffic such as log delivery, exec, and proxy. Please refer to the EKS user guide for complete <a href="https://docs.aws.amazon.com/eks/latest/userguide/network_reqs.html#network-requirements-subnets">VPC and subnet requirement</a> details. </p>
<p>Kubernetes worker nodes can run in the cluster subnets, but it is not recommended. During <a href="https://aws.github.io/aws-eks-best-practices/upgrades/#verify-available-ip-addresses">cluster upgrades</a> Amazon EKS provisions additional ENIs in the cluster subnets. When your cluster scales out, worker nodes and pods may consume the available IPs in the cluster subnet. Hence in order to make sure there are enough available IPs you might want to consider using dedicated cluster subnets with /28 netmask.</p>
<p>Kubernetes worker nodes can run in either a public or a private subnet. Whether a subnet is public or private refers to whether traffic within the subnet is routed through an <a href="https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html">internet gateway</a>. Public subnets have a route table entry to the internet through the internet gateway, but private subnets don't.</p>
<p>The traffic that originates somewhere else and reaches your nodes is called <em>ingress</em>. Traffic that originates from the nodes and leaves the network is called <em>egress</em>. Nodes with public or elastic IP addresses (EIPs) within a subnet configured with an internet gateway allow ingress from outside of the VPC. Private subnets usually include a <a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html">NAT gateway</a>, which only allows ingress traffic to the nodes from within the VPC while still allowing traffic <em>from</em> the nodes to leave the VPC (<em>egress</em>).</p>
<p>In the IPv6 world, every address is internet routable. The IPv6 addresses associated with the nodes and pods are public. Private subnets are supported by implementing an <a href="https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html">egress-only internet gateways (EIGW)</a> in a VPC, allowing outbound traffic while blocking all incoming traffic. Best practices for implementing IPv6 subnets can be found in the <a href="https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario2.html">VPC user guide</a>.</p>
<h3 id="networking-subnets-you-can-configure-vpc-and-subnets-in-three-different-ways">You can configure VPC and Subnets in three different ways:<a class="headerlink" href="#networking-subnets-you-can-configure-vpc-and-subnets-in-three-different-ways" title="Permanent link">&para;</a></h3>
<h4 id="networking-subnets-using-only-public-subnets">Using only public subnets<a class="headerlink" href="#networking-subnets-using-only-public-subnets" title="Permanent link">&para;</a></h4>
<p>In the same public subnets, both nodes and ingress resources (such as load balancers) are created. Tag the public subnet with <a href="http://kubernetes.io/role/elb"><code>kubernetes.io/role/elb</code></a> to construct load balancers that face the internet. In this configuration, the cluster endpoint can be configured to be public, private, or both (public and private).</p>
<h4 id="networking-subnets-using-private-and-public-subnets">Using private and public subnets<a class="headerlink" href="#networking-subnets-using-private-and-public-subnets" title="Permanent link">&para;</a></h4>
<p>Nodes are created on private subnets, whereas Ingress resources are instantiated in public subnets. You can enable public, private, or both (public and private) access to the cluster endpoint. Depending on the configuration of the cluster endpoint, node traffic will enter via the NAT gateway or the ENI.</p>
<h4 id="networking-subnets-using-only-private-subnets">Using only private subnets<a class="headerlink" href="#networking-subnets-using-only-private-subnets" title="Permanent link">&para;</a></h4>
<p>Both nodes and ingress are created in private subnets. Using the <a href="http://kubernetes.io/role/internal-elb:1"><code>kubernetes.io/role/internal-elb</code></a> subnet tag to construct internal load balancers. Accessing your cluster's endpoint will require a VPN connection. You must activate <a href="https://docs.aws.amazon.com/vpc/latest/userguide/endpoint-service.html">AWS PrivateLink</a> for EC2 and all Amazon ECR and S3 repositories. Only the private endpoint of the cluster should be enabled. We suggest going through the <a href="https://docs.aws.amazon.com/eks/latest/userguide/private-clusters.html">EKS private cluster requirements</a> before provisioning private clusters.</p>
<h3 id="networking-subnets-communication-across-vpcs">Communication across VPCs<a class="headerlink" href="#networking-subnets-communication-across-vpcs" title="Permanent link">&para;</a></h3>
<p>There are many scenarios when you require multiple VPCs and separate EKS clusters deployed to these VPCs. </p>
<p>You can use <a href="https://aws.amazon.com/vpc/lattice/">Amazon VPC Lattice</a> to consistently and securely connect services across multiple VPCs and accounts (without requiring additional connectivity to be provided by services like VPC peering, AWS PrivateLink or AWS Transit Gateway). Learn more <a href="https://aws.amazon.com/blogs/networking-and-content-delivery/build-secure-multi-account-multi-vpc-connectivity-for-your-applications-with-amazon-vpc-lattice/">here</a>.</p>
<p><img alt="Amazon VPC Lattice, traffic flow" src="../networking/subnets/vpc-lattice.gif" /></p>
<p>Amazon VPC Lattice operates in the link-local address space in IPv4 and IPv6, providing connectivity between services that may have overlapping IPv4 addresses. For operational efficiency, we strongly recommend deploying EKS clusters and nodes to IP ranges that do not overlap. In case your infrastructure includes VPCs with overlapping IP ranges, you need to architect your network accordingly. We suggest <a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html#nat-gateway-basics">Private NAT Gateway</a>, or VPC CNI in <a href="#networking-custom-networking">custom networking</a> mode in conjunction with <a href="https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-transit-gateway.html">transit gateway</a> to integrate workloads on EKS to solve overlapping CIDR challenges while preserving routable RFC1918 IP addresses. </p>
<p><img alt="Private Nat Gateway with Custom Networking, traffic flow" src="../networking/subnets/private-nat-gw.gif" /></p>
<p>Consider utilizing <a href="https://docs.aws.amazon.com/vpc/latest/privatelink/privatelink-share-your-services.html">AWS PrivateLink</a>, also known as an endpoint service, if you are the service provider and would want to share your Kubernetes service and ingress (either ALB or NLB) with your customer VPC in separate accounts. </p>
<h3 id="networking-subnets-sharing-vpc-across-multiple-accounts">Sharing VPC across multiple accounts<a class="headerlink" href="#networking-subnets-sharing-vpc-across-multiple-accounts" title="Permanent link">&para;</a></h3>
<p>Many enterprises adopted shared Amazon VPCs as a means to streamline network administration, reduce costs and improve security across multiple AWS Accounts in an AWS Organization. They utilize AWS Resource Access Manager (RAM) to securely share supported <a href="https://docs.aws.amazon.com/ram/latest/userguide/shareable.html">AWS resources</a> with individual AWS Accounts, organizational units (OUs) or entire AWS Organization.</p>
<p>You can deploy Amazon EKS clusters, managed node groups and other supporting AWS resources (like LoadBalancers, security groups, end points, etc.,) in shared VPC Subnets from an another AWS Account using AWS RAM. Below figure depicts an example highlevel architecture. This allows central networking teams control over the networking constructs like VPCs, Subnets, etc., while allowing application or platform teams to deploy Amazon EKS clusters in their respective AWS Accounts. A complete walkthrough of this scenario is available at this <a href="https://github.com/aws-samples/eks-shared-subnets">github repository</a>.</p>
<p><img alt="Deploying Amazon EKS in VPC Shared Subnets across AWS Accounts." src="../networking/subnets/eks-shared-subnets.png" /></p>
<h4 id="networking-subnets-considerations-when-using-shared-subnets">Considerations when using Shared Subnets<a class="headerlink" href="#networking-subnets-considerations-when-using-shared-subnets" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p>Amazon EKS clusters and worker nodes can be created within shared subnets that are all part of the same VPC. Amazon EKS does not support the creation of clusters across multiple VPCs. </p>
</li>
<li>
<p>Amazon EKS uses AWS VPC Security Groups (SGs) to control the traffic between the Kubernetes control plane and the cluster's worker nodes. Security groups are also used to control the traffic between worker nodes, and other VPC resources, and external IP addresses. You must create these security groups in the application/participant account. Ensure that the security groups you intend to use for your pods are also located in the participant account. You can configure the inbound and outbound rules within your security groups to permit the necessary traffic to and from security groups located in the Central VPC account.</p>
</li>
<li>
<p>Create IAM roles and associated policies within the participant account where your Amazon EKS cluster resides. These IAM roles and policies are essential for granting the necessary permissions to Kubernetes clusters managed by Amazon EKS, as well as to the nodes and pods running on Fargate. The permissions enable Amazon EKS to make calls to other AWS services on your behalf.</p>
</li>
<li>
<p>You can follow following approaches to allow cross Account access to AWS resources like Amazon S3 buckets, Dynamodb tables, etc., from k8s pods:</p>
<ul>
<li>
<p><strong>Resource based policy approach</strong>: If the AWS service supports resource policies, you can add appropriate resource based policy to allow cross account access to IAM Roles assigned to the kubernetes pods. In this scenario, OIDC provider, IAM Roles, and permission policies exist in the application account. To find AWS Services that support Resource based policies, refer <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_aws-services-that-work-with-iam.html">AWS services that work with IAM</a> and look for the services that have Yes in the Resource Based column.</p>
</li>
<li>
<p><strong>OIDC Provider approach</strong>: IAM resources like OIDC Provider, IAM Roles, Permission, and Trust policies will be created in other participant AWS Account where the resources exists. These roles will be assigned to Kubernetes pods in application account, so that they can access cross account resources. Refer <a href="https://aws.amazon.com/blogs/containers/cross-account-iam-roles-for-kubernetes-service-accounts/">Cross account IAM roles for Kubernetes service accounts</a> blog for a complete walkthrough of this approach.</p>
</li>
</ul>
</li>
<li>
<p>You can deploy the Amazon Elastic Loadbalancer (ELB) resources (ALB or NLB) to route traffic to k8s pods either in application or central networking accounts. Refer to <a href="https://aws.amazon.com/blogs/containers/expose-amazon-eks-pods-through-cross-account-load-balancer/">Expose Amazon EKS Pods Through Cross-Account Load Balancer</a> walkthrough for detailed instructions on deploying the ELB resources in central networking account. This option offers enhanced flexibility, as it grants the Central Networking account full control over the security configuration of the Load Balancer resources.</p>
</li>
<li>
<p>When using <code>custom networking feature</code> of Amazon VPC CNI, you need to use the Availability Zone (AZ) ID mappings listed in the central networking account to create each <code>ENIConfig</code>. This is due to random mapping of physical AZs to the AZ names in each AWS account.</p>
</li>
</ul>
<h3 id="networking-subnets-security-groups">Security Groups<a class="headerlink" href="#networking-subnets-security-groups" title="Permanent link">&para;</a></h3>
<p>A <a href="https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html"><em>security group</em></a> controls the traffic that is allowed to reach and leave the resources that it is associated with. Amazon EKS uses security groups to manage the communication between the <a href="https://docs.aws.amazon.com/eks/latest/userguide/sec-group-reqs.html">control plane and nodes</a>. When you create a cluster, Amazon EKS creates a security group that's named <code>eks-cluster-sg-my-cluster-uniqueID</code>. EKS associates these security groups to the managed ENIs and the nodes. The default rules allow all traffic to flow freely between your cluster and nodes, and allows all outbound traffic to any destination. </p>
<p>When you create a cluster, you can specify your own security groups. Please see <a href="https://docs.aws.amazon.com/eks/latest/userguide/sec-group-reqs.html">recommendation for security groups</a> when you specify own security groups. </p>
<h2 id="networking-subnets-recommendations">Recommendations<a class="headerlink" href="#networking-subnets-recommendations" title="Permanent link">&para;</a></h2>
<h3 id="networking-subnets-consider-multi-az-deployment">Consider Multi-AZ  Deployment<a class="headerlink" href="#networking-subnets-consider-multi-az-deployment" title="Permanent link">&para;</a></h3>
<p>AWS Regions provide multiple physically separated and isolated Availability Zones (AZ), which are connected with low-latency, high-throughput, and highly redundant networking. With Availability Zones, you can design and operate applications that automatically fail over between Availability Zones without interruption. Amazon EKS strongly recommends deploying EKS clusters to multiple availability zones. Please consider specifying subnets in at least two availability zones when you create the cluster.</p>
<p>Kubelet running on nodes automatically adds labels to the node object such as <a href="http://topology.kubernetes.io/region=us-west-2,topology.kubernetes.io/zone=us-west-2d"><code>topology.kubernetes.io/region=us-west-2</code>, and <code>topology.kubernetes.io/zone=us-west-2d</code></a>. We recommend to use node labels in conjunction with <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/">Pod topology spread constraints</a> to control how Pods are spread across zones. These hints enable Kubernetes <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/">scheduler</a> to place Pods for better expected availability, reducing the risk that a correlated failure affects your whole workload. Please refer <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector">Assigning nodes to Pods</a> to see examples for node selector and AZ spread constraints.</p>
<p>You can define the subnets or availability zones when you create nodes. The nodes are placed in cluster subnets if no subnets are configured. EKS support for managed node groups automatically spreads the nodes across multiple availability zones on available capacity. <a href="https://karpenter.sh/">Karpenter</a>will honor the AZ spread placement by scaling nodes to specified AZs if workloads define topology spread limits.</p>
<p>AWS Elastic Load Balancers are managed by the AWS Load Balancer Controller for a Kubernetes cluster. It provisions an Application Load Balancer (ALB) for Kubernetes ingress resources and a Network Load Balancer (NLB) for Kubernetes services of type Loadbalancer. The Elastic Load Balancer controller uses <a href="https://aws.amazon.com/premiumsupport/knowledge-center/eks-vpc-subnet-discovery/">tags</a> to discover the subnets. ELB controller requires a minimum of two availability zones (AZs) to provision ingress resource successfully. Consider setting subnets in at least two AZs to take advantage of geographic redundancy's safety and reliability. </p>
<h3 id="networking-subnets-deploy-nodes-to-private-subnets">Deploy Nodes to Private Subnets<a class="headerlink" href="#networking-subnets-deploy-nodes-to-private-subnets" title="Permanent link">&para;</a></h3>
<p>A VPC including both private and public subnets is the ideal method for deploying Kubernetes workloads on EKS. Consider setting a minimum of two public subnets and two private subnets in two distinct availability zones. The related route table of a public subnet contains a route to an internet gateway . Pods are able to interact with the Internet via a NAT gateway. Private subnets are supported by <a href="https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html">egress-only internet gateways</a> in the IPv6 environment (EIGW).</p>
<p>Instantiating nodes in private subnets offers maximal control over traffic to the nodes and is effective for the vast majority of Kubernetes applications. Ingress resources (like as load balancers) are instantiated in public subnets and route traffic to Pods operating on private subnets.</p>
<p>Consider private only mode if you demand strict security and network isolation. In this configuration, three private subnets are deployed in distinct Availability Zones within the AWS Region's VPC. The resources deployed to the subnets cannot access the internet, nor can the internet access the resources in the subnets. In order for your Kubernetes application to access other AWS services, you must configure PrivateLink interfaces and/or gateway endpoints. You may setup internal load balancers to redirect traffic to Pods using AWS Load Balancer Controller. The private subnets must be tagged (<a href="http://kubernetes.io/role/internal-elb"><code>kubernetes.io/role/internal-elb: 1</code></a>) for the controller to provision load balancers. For nodes to register with the cluster, the cluster endpoint must be set to private mode.  Please visit <a href="https://docs.aws.amazon.com/eks/latest/userguide/private-clusters.html">private cluster guide</a> for complete requirements and considerations.</p>
<h3 id="networking-subnets-consider-public-and-private-mode-for-cluster-endpoint">Consider Public and Private Mode for Cluster Endpoint<a class="headerlink" href="#networking-subnets-consider-public-and-private-mode-for-cluster-endpoint" title="Permanent link">&para;</a></h3>
<p>Amazon EKS offers public-only, public-and-private, and private-only cluster endpoint modes. The default mode is public-only, however we recommend configuring cluster endpoint in public and private mode. This option allows Kubernetes API calls within your cluster's VPC (such as node-to-control-plane communication) to utilize the private VPC endpoint and traffic to remain within your cluster's VPC. Your cluster API server, on the other hand, can be reached from the internet. However, we strongly recommend limiting the CIDR blocks that can use the public endpoint. <a href="https://docs.aws.amazon.com/eks/latest/userguide/cluster-endpoint.html#modify-endpoint-access">Learn how to configure public and private endpoint access, including limiting CIDR blocks.</a></p>
<p>We suggest a private-only endpoint when you need security and network isolation. We recommend using either of the options listed in the <a href="https://docs.aws.amazon.com/eks/latest/userguide/cluster-endpoint.html#private-access">EKS user guide</a> to connect to an API server privately.</p>
<h3 id="networking-subnets-configure-security-groups-carefully">Configure Security Groups Carefully<a class="headerlink" href="#networking-subnets-configure-security-groups-carefully" title="Permanent link">&para;</a></h3>
<p>Amazon EKS supports using custom security groups. Any custom security groups must allow communication between nodes and the Kubernetes control plane. Please check <a href="https://docs.aws.amazon.com/eks/latest/userguide/sec-group-reqs.html">port requirements</a> and configure rules manually when your organization doesn't allow for open communication. </p>
<p>EKS applies the custom security groups that you provide during cluster creation to the managed interfaces (X-ENIs). However, it does not immediately associate them with nodes. While creating node groups, it is strongly recommended to <a href="https://eksctl.io/usage/schema/#nodeGroups-securityGroups">associate custom security groups</a> manually. Please consider enabling <a href="https://karpenter.sh/docs/concepts/node-templates/#specsecuritygroupselector">securityGroupSelector</a> to enable Karpenter node template discovery of custom security groups during autoscaling of nodes. </p>
<p>We strongly recommend creating a security group to allow all inter-node communication traffic. During the bootstrap process, nodes require outbound Internet connectivity to access the cluster endpoint. Evaluate outward access requirements, such as on-premise connection and container registry access, and set rules appropriately. Before putting changes into production, we strongly suggest that you check connections carefully in your development environment.</p>
<h3 id="networking-subnets-deploy-nat-gateways-in-each-availability-zone">Deploy NAT Gateways in each Availability Zone<a class="headerlink" href="#networking-subnets-deploy-nat-gateways-in-each-availability-zone" title="Permanent link">&para;</a></h3>
<p>If you deploy nodes in private subnets (IPv4 and IPv6), consider creating a NAT Gateway in each Availability Zone (AZ) to ensure zone-independent architecture and reduce cross AZ expenditures. Each NAT gateway in an AZ is implemented with redundancy.</p>
<h3 id="networking-subnets-use-cloud9-to-access-private-clusters">Use Cloud9 to access Private Clusters<a class="headerlink" href="#networking-subnets-use-cloud9-to-access-private-clusters" title="Permanent link">&para;</a></h3>
<p>AWS Cloud9 is a web-based IDE than can run securely in Private Subnets without ingress access, using AWS Systems Manager. Egress can also be disabled on the Cloud9 instance. <a href="https://aws.amazon.com/blogs/security/isolating-network-access-to-your-aws-cloud9-environments/">Learn more about using Cloud9 to access private clusters and subnets.</a></p>
<p><img alt="illustration of AWS Cloud9 console connecting to no-ingress EC2 instance." src="../networking/subnets/image-2.jpg" /></p></section><section class="print-page" id="networking-vpc-cni"><h1 id="networking-vpc-cni-amazon-vpc-cni">Amazon VPC CNI<a class="headerlink" href="#networking-vpc-cni-amazon-vpc-cni" title="Permanent link">&para;</a></h1>
<iframe width="560" height="315" src="https://www.youtube.com/embed/RBE3yk2UlYA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

<p>Amazon EKS implements cluster networking through the <a href="https://github.com/aws/amazon-vpc-cni-k8s">Amazon VPC Container Network Interface</a><a href="https://github.com/aws/amazon-vpc-cni-k8s">(VPC CNI)</a> plugin. The CNI plugin allows Kubernetes Pods to have the same IP address as they do on the VPC network. More specifically, all containers inside the Pod share a network namespace, and they can communicate with each-other using local ports.</p>
<p>Amazon VPC CNI has two components:</p>
<ul>
<li>CNI Binary, which will setup Pod network to enable Pod-to-Pod communication. The CNI binary runs on a node root file system and is invoked by the kubelet when a new Pod gets added to, or an existing Pod removed from the node.</li>
<li>ipamd, a long-running node-local IP Address Management (IPAM) daemon and is responsible for:</li>
<li>managing ENIs on a node, and</li>
<li>maintaining a warm-pool of available IP addresses or prefix</li>
</ul>
<p>When an instance is created, EC2 creates and attaches a primary ENI associated with a primary subnet. The primary subnet may be public or private. The Pods that run in hostNetwork mode use the primary IP address assigned to the node primary ENI and share the same network namespace as the host.</p>
<p>The CNI plugin manages <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html">Elastic Network Interfaces (ENI)</a> on the node. When a node is provisioned, the CNI plugin automatically allocates a pool of slots (IPs or Prefix’s) from the node’s subnet to the primary ENI. This pool is known as the <em>warm pool</em>, and its size is determined by the node’s instance type. Depending on CNI settings, a slot may be an IP address or a prefix. When a slot on an ENI has been assigned, the CNI may attach additional ENIs with warm pool of slots to the nodes. These additional ENIs are called Secondary ENIs. Each ENI can only support a certain number of slots, based on instance type. The CNI attaches more ENIs to instances based on the number of slots needed, which usually corresponds to the number of Pods. This process continues until the node can no longer support additional ENI. The CNI also pre-allocates “warm” ENIs and slots for faster Pod startup. Note each instance type has a maximum number of ENIs that may be attached. This is one constraint on Pod density (number of Pods per node), in addition to compute resources.</p>
<p><img alt="flow chart illustrating procedure when new ENI delegated prefix is needed" src="../networking/vpc-cni/image.png" /></p>
<p>The maximum number of network interfaces, and the maximum number of slots that you can use varies by the type of EC2 Instance. Since each Pod consumes an IP address on a slot, the number of Pods you can run on a particular EC2 Instance depends on how many ENIs can be attached to it and how many slots each ENI supports. We suggest setting the maximum Pods per EKS user guide to avoid exhaustion of the instance’s CPU and memory resources. Pods using <code>hostNetwork</code> are excluded from this calculation. You may consider using a script called <a href="https://github.com/awslabs/amazon-eks-ami/blob/master/files/max-pods-calculator.sh">max-pod-calculator.sh</a> to calculate EKS’s recommended maximum Pods for a given instance type.</p>
<h2 id="networking-vpc-cni-overview">Overview<a class="headerlink" href="#networking-vpc-cni-overview" title="Permanent link">&para;</a></h2>
<p>Secondary IP mode is the default mode for VPC CNI. This guide provides a generic overview of VPC CNI behavior when Secondary IP mode is enabled. The functionality of ipamd (allocation of IP addresses) may vary depending on the configuration settings for VPC CNI, such as <a href="#networking-prefix-mode-index_linux">Prefix Mode</a>, <a href="#networking-sgpp">Security Groups Per Pod</a>, and <a href="#networking-custom-networking">Custom Networking</a>.</p>
<p>The Amazon VPC CNI is deployed as a Kubernetes Daemonset named aws-node on worker nodes. When a worker node is provisioned, it has a default ENI, called the primary ENI, attached to it. The CNI allocates a warm pool of ENIs and secondary IP addresses from the subnet attached to the node’s primary ENI. By default, ipamd attempts to allocate an additional ENI to the node. The IPAMD allocates additional ENI when a single Pod is scheduled and assigned a secondary IP address from the primary ENI. This "warm" ENI enables faster Pod networking. As the pool of secondary IP addresses runs out, the CNI adds another ENI to assign more.</p>
<p>The number of ENIs and IP addresses in a pool are configured through environment variables called <a href="https://github.com/aws/amazon-vpc-cni-k8s/blob/master/docs/eni-and-ip-target.md">WARM_ENI_TARGET, WARM_IP_TARGET, MINIMUM_IP_TARGET</a>. The <code>aws-node</code> Daemonset will periodically check that a sufficient number of ENIs are attached. A sufficient number of ENIs are attached when all of the <code>WARM_ENI_TARGET</code>, or <code>WARM_IP_TARGET</code> and <code>MINIMUM_IP_TARGET</code> conditions are met. If there are insufficient ENIs attached, the CNI will make an API call to EC2 to attach more until <code>MAX_ENI</code> limit is reached.</p>
<ul>
<li><code>WARM_ENI_TARGET</code> - Integer, Values &gt;0 indicate requirement Enabled</li>
<li>The number of Warm ENIs to be maintained. An ENI is “warm” when it is attached as a secondary ENI to a node, but it is not in use by any Pod. More specifically, no IP addresses of the ENI have been associated with a Pod.</li>
<li>Example: Consider an instance with 2 ENIs, each ENI supporting 5 IP addresses. WARM_ENI_TARGET is set to 1. If exactly 5 IP addresses are associated with the instance, the CNI maintains 2 ENIs attached to the instance. The first ENI is in use, and all 5 possible IP addresses of this ENI are used. The second ENI is “warm” with all 5 IP addresses in pool. If another Pod is launched on the instance, a 6th IP address will be needed. The CNI will assign this 6th Pod an IP address from the second ENI and from 5 IPs from the pool. The second ENI is now in use, and no longer in a “warm” status. The CNI will allocate a 3rd ENI to maintain at least 1 warm ENI.</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The warm ENIs still consume IP addresses from the CIDR of your VPC. IP addresses are “unused” or “warm” until they are associated with a workload, such as a Pod.</p>
</div>
<ul>
<li><code>WARM_IP_TARGET</code>, Integer, Values &gt;0 indicate requirement Enabled</li>
<li>The number of Warm IP addresses to be maintained. A Warm IP is available on an actively attached ENI, but has not been assigned to a Pod. In other words, the number of Warm IPs available is the number of IPs that may be assigned to a Pod without requiring an additional ENI.</li>
<li>Example: Consider an instance with 1 ENI, each ENI supporting 20 IP addresses. WARM_IP_TARGET is set to 5. WARM_ENI_TARGET is set to 0. Only 1 ENI will be attached until a 16th IP address is needed. Then, the CNI will attach a second ENI, consuming 20 possible addresses from the subnet CIDR.</li>
<li><code>MINIMUM_IP_TARGET</code>, Integer, Values &gt;0 indicate requirement Enabled</li>
<li>The minimum number of IP addresses to be allocated at any time. This is commonly used to front-load the assignment of multiple ENIs at instance launch.</li>
<li>Example: Consider a newly launched instance. It has 1 ENI and each ENI supports 10 IP addresses. MINIMUM_IP_TARGET is set to 100. The ENI immediately attaches 9 more ENIs for a total of 100 addresses. This happens regardless of any WARM_IP_TARGET or WARM_ENI_TARGET values.</li>
</ul>
<p>This project includes a <a href="../networking/subnet-calc/subnet-calc.xlsx">Subnet Calculator Excel Document</a>. This calculator document simulates the IP address consumption of a specified workload under different ENI configuration options, such as <code>WARM_IP_TARGET</code> and <code>WARM_ENI_TARGET</code>.</p>
<p><img alt="illustration of components involved in assigning an IP address to a pod" src="../networking/vpc-cni/image-2.png" /></p>
<p>When Kubelet receives an add Pod request, the CNI binary queries ipamd for an available IP address, which ipamd then provides to the Pod. The CNI binary wires up the host and Pod network.</p>
<p>Pods deployed on a node are, by default, assigned to the same security groups as the primary ENI. Alternatively, Pods may be configured with different security groups.</p>
<p><img alt="second illustration of components involved in assigning an IP address to a pod" src="../networking/vpc-cni/image-3.png" /></p>
<p>As the pool of IP addresses is depleted, the plugin automatically attaches another elastic network interface to the instance and allocates another set of secondary IP addresses to that interface. This process continues until the node can no longer support additional elastic network interfaces.</p>
<p><img alt="third illustration of components involved in assigning an IP address to a pod" src="../networking/vpc-cni/image-4.png" /></p>
<p>When a Pod is deleted, VPC CNI places the Pod’s IP address in a 30-second cool down cache. The IPs in a cool down cache are not assigned to new Pods. When the cooling-off period is over, VPC CNI moves Pod IP back to the warm pool. The cooling-off period prevents Pod IP addresses from being recycled prematurely and allows kube-proxy on all cluster nodes to finish updating the iptables rules. When the number of IPs or ENIs exceeds the number of warm pool settings, the ipamd plugin returns IPs and ENIs to the VPC.</p>
<p>As described above in Secondary IP mode, each Pod receives one secondary private IP address from one of the ENIs attached to an instance. Since each Pod uses an IP address, the number of Pods you can run on a particular EC2 Instance depends on how many ENIs can be attached to it and how many IP addresses it supports. The VPC CNI checks the <a href="https://github.com/aws/amazon-vpc-resource-controller-k8s/blob/master/pkg/aws/vpc/limits.go">limits</a> file to find out how many ENIs and IP addresses are allowed for each type of instance.</p>
<p>You can use the following formula to determine maximum number of Pods you can deploy on a node.</p>
<p><code>(Number of network interfaces for the instance type × (the number of IP addresses per network interface - 1)) + 2</code></p>
<p>The +2 indicates Pods that require host networking, such as kube-proxy and VPC CNI. Amazon EKS requires kube-proxy and VPC CNI to be operating on each node, and these requirements are factored into the max-pods value. If you want to run additional host networking pods, consider updating the max-pods value.</p>
<p>The +2 indicates Kubernetes Pods that use host networking, such as kube-proxy and VPC CNI. Amazon EKS requires kube-proxy and VPC CNI to be running on every node and are calculated towards max-pods. Consider updating max-pods if you plan to run more host networking Pods. You can specify <code>--kubelet-extra-args "—max-pods=110"</code> as user data in the launch template.</p>
<p>As an example, on a cluster with 3 c5.large nodes (3 ENIs and max 10 IPs per ENI), when the cluster starts up and has 2 CoreDNS pods, the CNI will consume 49 IP addresses and keeps them in warm pool. The warm pool enables faster Pod launches when the application is deployed.</p>
<p>Node 1 (with CoreDNS pod): 2 ENIs, 20 IPs assigned</p>
<p>Node 2 (with CoreDNS pod): 2 ENIs, 20 IPs assigned</p>
<p>Node 3 (no Pod): 1 ENI. 10 IPs assigned.</p>
<p>Keep in mind that infrastructure pods, often running as daemon sets, each contribute to the max-pod count. These can include:</p>
<ul>
<li>CoreDNS</li>
<li>Amazon Elastic LoadBalancer</li>
<li>Operational pods for metrics-server</li>
</ul>
<p>We suggest that you plan your infrastructure by combining these Pods' capacities. For a list of the maximum number of Pods supported by each instance type, see <a href="https://github.com/awslabs/amazon-eks-ami/blob/master/files/eni-max-pods.txt">eni-max-Pods.txt</a> on GitHub.</p>
<p><img alt="illustration of multiple ENIs attached to a node" src="../networking/vpc-cni/image-5.png" /></p>
<h2 id="networking-vpc-cni-recommendations">Recommendations<a class="headerlink" href="#networking-vpc-cni-recommendations" title="Permanent link">&para;</a></h2>
<h3 id="networking-vpc-cni-deploy-vpc-cni-managed-add-on">Deploy VPC CNI Managed Add-On<a class="headerlink" href="#networking-vpc-cni-deploy-vpc-cni-managed-add-on" title="Permanent link">&para;</a></h3>
<p>When you provision a cluster, Amazon EKS installs VPC CNI automatically. Amazon EKS nevertheless supports managed add-ons that enable the cluster to interact with underlying AWS resources such as computing, storage, and networking. We highly recommend that you deploy clusters with managed add-ons including VPC CNI.</p>
<p>Amazon EKS managed add-on offer VPC CNI installation and management for Amazon EKS clusters. Amazon EKS add-ons include the latest security patches, bug fixes, and are validated by AWS to work with Amazon EKS. The VPC CNI add-on enables you to continuously ensure the security and stability of your Amazon EKS clusters and decrease the amount of effort required to install, configure, and update add-ons. Additionally, a managed add-on can be added, updated, or deleted via the Amazon EKS API, AWS Management Console, AWS CLI, and eksctl.</p>
<p>You can find the managed fields of VPC CNI using <code>--show-managed-fields</code> flag with the <code>kubectl get</code> command.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#networking-vpc-cni-__codelineno-0-1"></a>kubectl get daemonset aws-node --show-managed-fields -n kube-system -o yaml
</code></pre></div>
<p>Managed add-ons prevents configuration drift by automatically overwriting configurations every 15 minutes. This means that any changes to managed add-ons, made via the Kubernetes API after add-on creation, will overwrite by the automated drift-prevention process and also set to defaults during add-on update process.</p>
<p>The fields managed by EKS are listed under managedFields with manager as EKS. Fields managed by EKS include service account, image, image url, liveness probe, readiness probe, labels, volumes, and volume mounts.</p>
<div class="admonition info">
<p class="admonition-title">Info</p>
</div>
<p>The most frequently used fields such as WARM_ENI_TARGET, WARM_IP_TARGET, and MINIMUM_IP_TARGET are not managed and will not be reconciled. The changes to these fields will be preserved upon updating of the add-on.</p>
<p>We suggest testing the add-on behavior in your non-production clusters for a specific configuration before updating production clusters. Additionally, follow the steps in the EKS user guide for <a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-add-ons.html">add-on</a> configurations.</p>
<h4 id="networking-vpc-cni-migrate-to-managed-add-on">Migrate to Managed Add-On<a class="headerlink" href="#networking-vpc-cni-migrate-to-managed-add-on" title="Permanent link">&para;</a></h4>
<p>You will manage the version compatibility and update the security patches of self-managed VPC CNI. To update a self-managed add-on, you must use the Kubernetes APIs and instructions outlined in the <a href="https://docs.aws.amazon.com/eks/latest/userguide/managing-vpc-cni.html#updating-vpc-cni-add-on">EKS user guide</a>. We recommend migrating to a managed add-on for existing EKS clusters and highly suggest creating a backup of your current CNI settings prior to migration. To configure managed add-ons, you can utilize the Amazon EKS API, AWS Management Console, or AWS Command Line Interface.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#networking-vpc-cni-__codelineno-1-1"></a>kubectl apply view-last-applied daemonset aws-node -n kube-system &gt; aws-k8s-cni-old.yaml
</code></pre></div>
<p>Amazon EKS will replace the CNI configuration settings if the field is listed as managed with default settings. We caution against modifying the managed fields. The add-on does not reconcile configuration fields such as the <em>warm</em> environment variables and CNI modes. The Pods and applications will continue to run while you migrate to a managed CNI.</p>
<h4 id="networking-vpc-cni-backup-cni-settings-before-update">Backup CNI Settings Before Update<a class="headerlink" href="#networking-vpc-cni-backup-cni-settings-before-update" title="Permanent link">&para;</a></h4>
<p>VPC CNI runs on customer data plane (nodes), and hence Amazon EKS does not automatically update the add-on (managed and self-managed) when new versions are released or after you <a href="https://docs.aws.amazon.com/eks/latest/userguide/update-cluster.html">update your cluster</a> to a new Kubernetes minor version. To update the add-on for an existing cluster, you must trigger an update via update-addon API or clicking update now link in the EKS console for add-ons. If you have deployed self-managed add-on, follow steps mentioned under <a href="https://docs.aws.amazon.com/eks/latest/userguide/managing-vpc-cni.html#updating-vpc-cni-add-on">updating self-managed VPC CNI add-on.</a></p>
<p>We strongly recommend that you update one minor version at a time. For example, if your current minor version is <code>1.9</code> and you want to update to <code>1.11</code>, you should update to the latest patch version of <code>1.10</code> first, then update to the latest patch version of <code>1.11</code>.</p>
<p>Perform an inspection of the aws-node Daemonset before updating Amazon VPC CNI. Take a backup of existing settings. If using a managed add-on, confirm that you have not updated any settings that Amazon EKS might override. We recommend a post update hook in your automation workflow or a manual apply step after an add-on update.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#networking-vpc-cni-__codelineno-2-1"></a>kubectl apply view-last-applied daemonset aws-node -n kube-system &gt; aws-k8s-cni-old.yaml
</code></pre></div>
<p>For a self-managed add-on, compare the backup with <code>releases</code> on GitHub to see the available versions and familiarize yourself with the changes in the version that you want to update to. We recommend using Helm to manage self-managed add-ons and leverage values files to apply settings. Any update operations involving Daemonset delete will result in application downtime and must be avoided.</p>
<h3 id="networking-vpc-cni-understand-security-context">Understand Security Context<a class="headerlink" href="#networking-vpc-cni-understand-security-context" title="Permanent link">&para;</a></h3>
<p>We strongly suggest you to understand the security contexts configured for managing VPC CNI efficiently. Amazon VPC CNI has two components CNI binary and ipamd (aws-node) Daemonset. The CNI runs as a binary on a node and has access to node root file system, also has privileged access as it deals with iptables at the node level. The CNI binary is invoked by the kubelet when Pods gets added or removed.</p>
<p>The aws-node Daemonset is a long-running process responsible for IP address management at the node level. The aws-node runs in <code>hostNetwork</code> mode and allows access to the loopback device, and network activity of other pods on the same node. The aws-node init-container runs in privileged mode and mounts the CRI socket allowing the Daemonset to monitor IP usage by the Pods running on the node. Amazon EKS is working to remove the privileged requirement of aws-node init container. Additionally, the aws-node needs to update NAT entries and to load the iptables modules and hence runs with NET_ADMIN privileges.</p>
<p>Amazon EKS recommends deploying the security policies as defined by the aws-node manifest for IP management for the Pods and networking settings. Please consider updating to the latest version of VPC CNI. Furthermore, please consider opening a <a href="https://github.com/aws/amazon-vpc-cni-k8s/issues">GitHub issue</a> if you have a specific security requirement.</p>
<h3 id="networking-vpc-cni-use-separate-iam-role-for-cni">Use separate IAM role for CNI<a class="headerlink" href="#networking-vpc-cni-use-separate-iam-role-for-cni" title="Permanent link">&para;</a></h3>
<p>The AWS VPC CNI requires AWS Identity and Access Management (IAM) permissions. The CNI policy needs to be set up before the IAM role can be used. You can use <a href="https://console.aws.amazon.com/iam/home#/policies/arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy%24jsonEditor"><code>AmazonEKS_CNI_Policy</code></a>, which is an AWS managed policy for IPv4 clusters. AmazonEKS CNI managed policy only has permissions for IPv4 clusters. You must create a separate IAM policy for IPv6 clusters with the permissions listed <a href="https://docs.aws.amazon.com/eks/latest/userguide/cni-iam-role.html#cni-iam-role-create-ipv6-policy">here</a>.</p>
<p>By default, VPC CNI inherits the <a href="https://docs.aws.amazon.com/eks/latest/userguide/create-node-role.html">Amazon EKS node IAM role</a> (both managed and self-managed node groups).</p>
<p>Configuring a separate IAM role with the relevant policies for Amazon VPC CNI is <strong>strongly</strong> recommended. If not, the pods of Amazon VPC CNI gets the permission assigned to the node IAM role and have access to the instance profile assigned to the node.</p>
<p>The VPC CNI plugin creates and configures a service account called aws-node. By default, the service account binds to the Amazon EKS node IAM role with Amazon EKS CNI policy attached. To use the separate IAM role, we recommend that you <a href="https://docs.aws.amazon.com/eks/latest/userguide/cni-iam-role.html#cni-iam-role-create-role">create a new service account</a> with Amazon EKS CNI policy attached. To use a new service account you must <a href="https://docs.aws.amazon.com/eks/latest/userguide/cni-iam-role.html#cni-iam-role-redeploy-pods">redeploy the CNI pods</a>. Consider specifying a <code>--service-account-role-arn</code> for VPC CNI managed add-on when creating new clusters. Make sure you remove Amazon EKS CNI policy for both IPv4 and IPv6 from Amazon EKS node role.</p>
<p>It is advised that you <a href="https://aws.github.io/aws-eks-best-practices/security/docs/iam/#restrict-access-to-the-instance-profile-assigned-to-the-worker-node">block access instance metadata</a> to minimize the blast radius of security breach.</p>
<h3 id="networking-vpc-cni-handle-livenessreadiness-probe-failures">Handle Liveness/Readiness Probe failures<a class="headerlink" href="#networking-vpc-cni-handle-livenessreadiness-probe-failures" title="Permanent link">&para;</a></h3>
<p>We advise increasing the liveness and readiness probe timeout values (default <code>timeoutSeconds: 10</code>) for EKS 1.20 an later clusters to prevent probe failures from causing your application's Pod to become stuck in a containerCreating state. This problem has been seen in data-intensive and batch-processing clusters. High CPU use causes aws-node probe health failures, leading to unfulfilled Pod CPU requests. In addition to modifying the probe timeout, ensure that the CPU resource requests (default <code>CPU: 25m</code>) for aws-node are correctly configured. We do not suggest updating the settings unless your node is having issues.</p>
<p>We highly encourage you to run sudo <code>bash /opt/cni/bin/aws-cni-support.sh</code> on a node while you engage Amazon EKS support. The script will assist in evaluating kubelet logs and memory utilization on the node. Please consider installing SSM Agent on Amazon EKS worker nodes to run the script.</p>
<h3 id="networking-vpc-cni-configure-iptables-forward-policy-on-non-eks-optimized-ami-instances">Configure IPTables Forward Policy on non-EKS Optimized AMI Instances<a class="headerlink" href="#networking-vpc-cni-configure-iptables-forward-policy-on-non-eks-optimized-ami-instances" title="Permanent link">&para;</a></h3>
<p>If you are using custom AMI, make sure to set iptables forward policy to ACCEPT under <a href="https://github.com/awslabs/amazon-eks-ami/blob/master/files/kubelet.service#L8">kubelet.service</a>. Many systems set the iptables forward policy to DROP.  You can build custom AMI using <a href="https://packer.io/intro/why.html">HashiCorp Packer</a> and a build specification with resources and configuration scripts from the <a href="https://github.com/awslabs/amazon-eks-ami">Amazon EKS AMI repository on AWS GitHub</a>. You can update the <a href="https://github.com/awslabs/amazon-eks-ami/blob/master/files/kubelet.service#L8">kubelet.service</a> and follow the instructions specified <a href="https://aws.amazon.com/premiumsupport/knowledge-center/eks-custom-linux-ami/">here</a> to create a custom AMI.</p>
<h3 id="networking-vpc-cni-routinely-upgrade-cni-version">Routinely Upgrade CNI Version<a class="headerlink" href="#networking-vpc-cni-routinely-upgrade-cni-version" title="Permanent link">&para;</a></h3>
<p>The VPC CNI is backward compatible. The latest version works with all Amazon EKS supported Kubernetes versions. Additionally, the VPC CNI is offered as an EKS add-on (see “Deploy VPC CNI Managed Add-On” above). While EKS add-ons orchestrates upgrades of add-ons, it will not automatically upgrade add-ons like the CNI because they run on the data plane. You are responsible for upgrading the VPC CNI add-on following managed and self-managed worker node upgrades.</p></section><section class="print-page" id="networking-ip-optimization-strategies"><h1 id="networking-ip-optimization-strategies-optimizing-ip-address-utilization">Optimizing IP Address Utilization<a class="headerlink" href="#networking-ip-optimization-strategies-optimizing-ip-address-utilization" title="Permanent link">&para;</a></h1>
<p>Containerized environments are growing in scale at a rapid pace, thanks to application modernization. This means that more and more worker nodes and pods are being deployed.</p>
<p>The <a href="#networking-vpc-cni">Amazon VPC CNI</a> plugin assigns each pod an IP address from the VPC's CIDR(s). This approach provides full visibility of the Pod addresses with tools such as VPC Flow Logs and other monitoring solutions. Depending on your workload type this can cause a substantial number of IP addresses to be consumed by the pods.</p>
<p>When designing your AWS networking architecture, it is important to optimize Amazon EKS IP consumption at the VPC and at the node level. This will help you mitigate IP exhaustion issues and increase the pod density per node.</p>
<p>In this section, we will discuss techniques that can help you achieve these goals.</p>
<h2 id="networking-ip-optimization-strategies-optimize-node-level-ip-consumption">Optimize node-level IP consumption<a class="headerlink" href="#networking-ip-optimization-strategies-optimize-node-level-ip-consumption" title="Permanent link">&para;</a></h2>
<p><a href="https://docs.aws.amazon.com/eks/latest/userguide/cni-increase-ip-addresses.html">Prefix delegation</a> is a feature of Amazon Virtual Private Cloud (Amazon VPC) that allows you to assign IPv4 or IPv6 prefixes to your Amazon Elastic Compute Cloud (Amazon EC2) instances. It increases the IP addresses per network interface (ENI), which increases the pod density per node and improves your compute efficiency. Prefix delegation is also supported with Custom Networking. </p>
<p>For detailed information please see <a href="#networking-prefix-mode-index_linux">Prefix Delegation with Linux nodes</a> and <a href="#networking-prefix-mode-index_windows">Prefix Delegation with Windows nodes</a> sections.</p>
<h2 id="networking-ip-optimization-strategies-mitigate-ip-exhaustion">Mitigate IP exhaustion<a class="headerlink" href="#networking-ip-optimization-strategies-mitigate-ip-exhaustion" title="Permanent link">&para;</a></h2>
<p>To prevent your clusters from consuming all available IP addresses, we strongly recommend sizing your VPCs and subnets with growth in mind. </p>
<p>Adopting <a href="#networking-ipv6">IPv6</a> is a great way to avoid these problems from the very beginning. However, for organizations whose scalability needs exceed the initial planning and cannot adopt IPv6, improving the VPC design is the recommended response to IP address exhaustion. The most commonly used technique among Amazon EKS customers is adding non-routable Secondary CIDRs to the VPC and configuring the VPC CNI to use this additional IP space when allocating IP addresses to Pods. This is commonly referred to as <a href="#networking-custom-networking">Custom Networking</a>. </p>
<p>We will cover which variables of the Amazon VPC CNI you can use to optimize the warm pool of IPs assigned to your nodes. We will close this section with some other architectural patterns that are not intrinsic to Amazon EKS but can help mitigate IP exhaustion.</p>
<h3 id="networking-ip-optimization-strategies-use-ipv6-recommended">Use IPv6 (recommended)<a class="headerlink" href="#networking-ip-optimization-strategies-use-ipv6-recommended" title="Permanent link">&para;</a></h3>
<p>Adopting IPv6 is the easiest way to work around the RFC1918 limitations; we strongly recommend you consider adopting IPv6 as your first option when choosing a network architecture. IPv6 provides a significantly larger total IP address space, and cluster administrators can focus on migrating and scaling applications without devoting effort towards working around IPv4 limits.</p>
<p>Amazon EKS clusters support both IPv4 and IPv6. By default, EKS clusters use IPv4 address space. Specifying an IPv6 based address space at cluster creation time will enable the use of IPv6. In an IPv6 EKS cluster, pods and services receive IPv6 addresses while <strong>maintaining the ability for legacy IPv4 endpoints to connect to services running on IPv6 clusters and vice versa</strong>. All the pod-to-pod communication within a cluster always occurs over IPv6. Within a VPC (/56), the IPv6 CIDR block size for IPv6 subnets is fixed at /64. This provides 2^64 (approximately 18 quintillion) IPv6 addresses allowing to scale your deployments on EKS.</p>
<p>For detailed information please see the <a href="#networking-ipv6">Running IPv6 EKS Clusters</a> section and for hands-on experience please see the <a href="https://catalog.workshops.aws/ipv6-on-aws/en-US/lab-6">Understanding IPv6 on Amazon EKS</a> section of the <a href="https://catalog.workshops.aws/ipv6-on-aws/en-US">Get hands-on with IPv6 workshop</a>.</p>
<p><img alt="EKS Cluster in IPv6 Mode, traffic flow" src="../networking/ip-optimization-strategies/ipv6.gif" /></p>
<h3 id="networking-ip-optimization-strategies-optimize-ip-consumption-in-ipv4-clusters">Optimize IP consumption in IPv4 clusters<a class="headerlink" href="#networking-ip-optimization-strategies-optimize-ip-consumption-in-ipv4-clusters" title="Permanent link">&para;</a></h3>
<p>This section is dedicated to customers that are running legacy applications, and/or are not ready to migrate to IPv6. While we encourage all organizations to migrate to IPv6 as soon as possible, we recognize that some may still need to look into alternative approaches to scale their container workloads with IPv4. For this reason, we will also walk you through the architectural patterns to optimize IPv4 (RFC1918) address space consumption with Amazon EKS clusters.</p>
<h4 id="networking-ip-optimization-strategies-plan-for-growth">Plan for Growth<a class="headerlink" href="#networking-ip-optimization-strategies-plan-for-growth" title="Permanent link">&para;</a></h4>
<p>As a first line of defense against IP exhaustion, we strongly recommend to size your IPv4 VPCs and subnets with growth in mind, to prevent your clusters to consume all the available IP addresses. You will not be able to create new Pods or nodes if the subnets don’t have enough available IP addresses. </p>
<p>Before building VPC and subnets, it is advised to work backwards from the required workload scale. For example, when clusters are built using <a href="https://eksctl.io/">eksctl</a> (a simple CLI tool for creating and managing clusters on EKS) /19 subnets are created by default. A netmask of /19 is suitable for the majority of workload types allowing more than 8000 addresses to be allocated.</p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>When you size VPCs and subnets, there might be a number of elements (other than pods and nodes) which can consume IP addresses, for example Load Balancers, RDS Databases and other in-vpc services. </p>
</div>
<p>Additionally, Amazon EKS, can create up to 4 elastic network interfaces (X-ENI) that are required to allow communication towards the control plane (more info <a href="#networking-subnets">here</a>). During cluster upgrades, Amazon EKS creates new X-ENIs and deletes the old ones when the upgrade is successful. For this reason we recommend a netmask of at least /28 (16 IP addresses) for subnets associated with an EKS cluster.</p>
<p>You can use the <a href="../networking/subnet-calc/subnet-calc.xlsx">sample EKS Subnet Calculator</a> spreadsheet to plan for your network. The spreadsheet calculates IP usage based on workloads and VPC ENI configuration. The IP usage is compared to an IPv4 subnet to determine if the configuration and subnet size is sufficient for your workload. Keep in mind that, if subnets in your VPC run out of available IP addresses, we suggest <a href="https://docs.aws.amazon.com/vpc/latest/userguide/working-with-subnets.html#create-subnets">creating a new subnet</a> using the VPC’s original CIDR blocks. Notice that now <a href="https://aws.amazon.com/about-aws/whats-new/2023/10/amazon-eks-modification-cluster-subnets-security/">Amazon EKS now allows modification of cluster subnets and security groups</a>.</p>
<h4 id="networking-ip-optimization-strategies-expand-the-ip-space">Expand the IP space<a class="headerlink" href="#networking-ip-optimization-strategies-expand-the-ip-space" title="Permanent link">&para;</a></h4>
<p>If you are about to exhaust the RFC1918 IP space, you can use the <a href="#networking-custom-networking">Custom Networking</a> pattern to conserve routable IPs by scheduling Pods inside dedicated additional subnets. 
While custom networking will accept valid VPC range for secondary CIDR range, we recommend that you use CIDRs (/16) from the CG-NAT space, i.e. <code>100.64.0.0/10</code> or <code>198.19.0.0/16</code> as those are less likely to be used in a corporate setting than RFC1918 ranges. </p>
<p>For detailed information please see the dedicated section for <a href="#networking-custom-networking">Custom Networking</a>.</p>
<p><img alt="Custom Networking, traffic flow" src="../networking/ip-optimization-strategies/custom-networking.gif" /></p>
<h4 id="networking-ip-optimization-strategies-optimize-the-ips-warm-pool">Optimize the IPs warm pool<a class="headerlink" href="#networking-ip-optimization-strategies-optimize-the-ips-warm-pool" title="Permanent link">&para;</a></h4>
<p>With the default configuration, the VPC CNI keeps an entire ENI (and associated IPs) in the warm pool. This may consume a large number of IPs, especially on larger instance types.</p>
<p>If your cluster subnet has a limited number of IP addresses available, scrutinize these VPC CNI configuration environment variables:</p>
<ul>
<li><code>WARM_IP_TARGET</code> </li>
<li><code>MINIMUM_IP_TARGET</code></li>
<li><code>WARM_ENI_TARGET</code></li>
</ul>
<p>You can configure the value of <code>MINIMUM_IP_TARGET</code> to closely match the number of Pods you expect to run on your nodes. Doing so will ensure that as Pods get created, and the CNI can assign IP addresses from the warm pool without calling the EC2 API.</p>
<p>Please be mindful that setting the value of <code>WARM_IP_TARGET</code> too low, will cause additional calls to the EC2 API, and that might cause throttling of the requests. For large clusters use along with <code>MINIMUM_IP_TARGET</code> to avoid throttling of the requests.</p>
<p>To configure these options, you can download the <code>aws-k8s-cni.yaml</code> manifest and set the environment variables. At the time of writing, the latest release is located <a href="https://github.com/aws/amazon-vpc-cni-k8s/blob/master/config/master/aws-k8s-cni.yaml">here</a>. Check the version of the configuration value matches the installed VPC CNI version.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>These settings will be reset to defaults when you update the CNI. Please take a backup of the CNI, before you update it. Review the configuration settings to determine if you need to reapply them after update is successful.</p>
</div>
<p>You can adjust the CNI parameters on the fly without downtime for your existing applications, but you should choose values that will support your scalability needs. For example, if you're working with batch workloads, we recommend updating the default <code>WARM_ENI_TARGET</code> to match the Pod scale needs. Setting <code>WARM_ENI_TARGET</code> to a high value always maintains the warm IP pool required to run large batch workloads and hence avoid data processing delays. </p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Improving your VPC design is the recommended response to IP address exhaustion. Consider solutions like IPv6 and Secondary CIDRs. Adjusting these values to minimize the number of Warm IPs should be a temporary solution after other options are excluded. Misconfiguring these values may interfere with cluster operation. </p>
<p><strong>Before making any changes to a production system</strong>, be sure to review the considerations on <a href="https://github.com/aws/amazon-vpc-cni-k8s/blob/master/docs/eni-and-ip-target.md">this page</a>.</p>
</div>
<h4 id="networking-ip-optimization-strategies-monitor-ip-address-inventory">Monitor IP Address Inventory<a class="headerlink" href="#networking-ip-optimization-strategies-monitor-ip-address-inventory" title="Permanent link">&para;</a></h4>
<p>In addition to the solutions described above, it is also important to have visibility over IP utilization. You can monitor the IP addresses inventory of subnets using <a href="https://docs.aws.amazon.com/eks/latest/userguide/cni-metrics-helper.html">CNI Metrics Helper</a>. Some of the metrics available are:</p>
<ul>
<li>maximum number of ENIs the cluster can support</li>
<li>number of ENIs already allocated</li>
<li>number of IP addresses currently assigned to Pods</li>
<li>total and maximum number of IP address available</li>
</ul>
<p>You can also set <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html">CloudWatch alarms</a> to get notified if a subnet is running out of IP addresses. Please visit EKS user guide for install instructions of <a href="https://docs.aws.amazon.com/eks/latest/userguide/cni-metrics-helper.html">CNI metrics helper</a> </p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Make sure <code>DISABLE_METRICS</code> variable for VPC CNI is set to false.</p>
</div>
<h4 id="networking-ip-optimization-strategies-further-considerations">Further considerations<a class="headerlink" href="#networking-ip-optimization-strategies-further-considerations" title="Permanent link">&para;</a></h4>
<p>There are other architectural patterns not intrinsic to Amazon EKS that can help with IP exhaustion. For example, you can <a href="#networking-subnets-communication-across-vpcs">optimize communication across VPCs</a> or <a href="#networking-subnets-sharing-vpc-across-multiple-accounts">share a VPC across multiple accounts</a> to limit the IPv4 address allocation. </p>
<p>Learn more about these patterns here:</p>
<ul>
<li><a href="https://aws.amazon.com/blogs/networking-and-content-delivery/designing-hyperscale-amazon-vpc-networks/">Designing hyperscale Amazon VPC networks</a>,</li>
<li><a href="https://aws.amazon.com/blogs/networking-and-content-delivery/build-secure-multi-account-multi-vpc-connectivity-for-your-applications-with-amazon-vpc-lattice/">Build secure multi-account multi-VPC connectivity with Amazon VPC Lattice</a>.</li>
</ul></section><section class="print-page" id="networking-ipv6"><h1 id="networking-ipv6-running-ipv6-eks-clusters">Running IPv6 EKS Clusters<a class="headerlink" href="#networking-ipv6-running-ipv6-eks-clusters" title="Permanent link">&para;</a></h1>
<iframe width="560" height="315" src="https://www.youtube.com/embed/zdXpTT0bZXo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

<p>EKS in IPv6 mode solves the IPv4 exhaustion challenge often manifested in large scale EKS clusters. EKS’s support for IPv6 is focused on resolving the IPv4 exhaustion problem, which stems from the limited size of the IPv4 address space. This is a significant concern raised by a number of our customers and is distinct from Kubernetes’ “<a href="https://kubernetes.io/docs/concepts/services-networking/dual-stack/">IPv4/IPv6 dual-stack</a>” feature.
EKS/IPv6 will also provide the flexability to inter-connect network boundaries using IPv6 CIDRs hence minimizing the chances to suffer from CIDR overlap, therefor solving a 2-Fold problem (In-Cluster, Cross-Cluster).
When deploying EKS clusters in IPv6 mode (--ip-family ipv6), the action is not a reversible. In simple words EKS IPv6 support is enabled for the entire lifetime of your cluster.</p>
<p>In an IPv6 EKS cluster, Pods and Services will receive IPv6 addresses while maintaining compatibility with legacy IPv4 Endpoints. This includes the ability for external IPv4 endpoints to access in-cluster services, and Pods to access external IPv4 endpoints.</p>
<p>Amazon EKS IPv6 support leverages the native VPC IPv6 capabilities. Each VPC is allocated with an IPv4 address prefix (CIDR block size can be from /16 to /28) and a unique /56 IPv6 address prefix (fixed) from within Amazon’s GUA (Global Unicast Address); you can assign a /64 address prefix to each subnet in your VPC. IPv4 features, like Route Tables, Network Access Control Lists, Peering, and DNS resolution, work the same way in an IPv6 enabled VPC. The VPC is then referred as dual-stack VPC, following dual-stack subnets, the following diagram depict the IPV4&amp;IPv6 VPC foundation pattern that support EKS/IPv6 based clusters:</p>
<p><img alt="Dual Stack VPC, mandatory foundation for EKS cluster in IPv6 mode" src="../networking/ipv6/eks-ipv6-foundation.png" /></p>
<p>In the IPv6 world, every address is internet routable. By default, VPC allocates IPv6 CIDR from the public GUA range. VPCs do not support assigning private IPv6 addresses from the <a href="https://en.wikipedia.org/wiki/Unique_local_address">Unique Local Address (ULA)</a> range as defined by RFC 4193 (fd00::/8 or fc00::/8). This is true even when you would like to assign an IPv6 CIDR owned by you. Egressing to the internet from Private Subnets is supported by implementing an egress-only internet gateway (EIGW) in a VPC, allowing outbound traffic while blocking all incoming traffic. 
The following diagram depict a Pod IPv6 Internet egress flow inside an EKS/IPv6 cluster:</p>
<p><img alt="Dual Stack VPC, EKS Cluster in IPv6 Mode, Pods in private subnets egressing to Internet IPv6 endpoints" src="../networking/ipv6/eks-egress-ipv6.png" /></p>
<p>Best practices for implementing IPv6 subnets can be found in the <a href="https://docs.aws.amazon.com/whitepapers/latest/ipv6-on-aws/IPv6-on-AWS.html">VPC user guide</a>.</p>
<p>In an IPv6 EKS cluster, nodes and Pods receive public IPv6 addresses. EKS assigns IPv6 addresses to services based on Unique Local IPv6 Unicast Addresses (ULA). The ULA Service CIDR for an IPv6 cluster is automatically assigned during the cluster creation stage and cannot be specified, unlike IPv4. The following diagram depict an EKS/IPv6 based cluster control-plane &amp; data-plan foundation pattern:</p>
<p><img alt="Dual Stack VPC, EKS Cluster in IPv6 Mode, control plane ULA, data plane IPv6 GUA for EC2 &amp; Pods" src="../networking/ipv6/eks-cluster-ipv6-foundation.png" /></p>
<h2 id="networking-ipv6-overview">Overview<a class="headerlink" href="#networking-ipv6-overview" title="Permanent link">&para;</a></h2>
<p>EKS/IPv6 is only supported in prefix mode (VPC-CNI Plug-in ENI IP assign mode). Learn more on <a href="https://aws.github.io/aws-eks-best-practices/networking/prefix-mode/index_linux/">Prefix 
Mode</a>.</p>
<blockquote>
<p>Prefix assignment only works on Nitro-based EC2 instances, hence EKS/IPv6 is only supported when the cluster data-plane uses EC2 Nitro-based instances. </p>
</blockquote>
<p>In simple words an IPv6 prefix of /80 (Per worker-node) will yield ~10^14 IPv6 addresses, the limiting factor will no longer be IPs but Pod density (Resources wise).</p>
<p>IPv6 prefix assignment only occurs at the EKS worker-node bootstrap time.
This behaviour is known to mitigate scenarios where high Pod churn EKS/IPv4 clusters are often delayed in Pod scheduling due to throttled API calls generated by the VPC CNI plug-in (ipamd) aimed to allocate Private IPv4 addresses in a timely fashion. It is also known to make the VPC-CNI plug-in advanced knobs tuning <a href="https://github.com/aws/amazon-vpc-cni-k8s#warm_ip_target">WARM_IP/ENI<em>, MINIMUM_IP</em></a> unnecessarily.</p>
<p>The following diagram zooms into an IPv6 worker-node Elastic Network Interface (ENI):</p>
<p><img alt="illustration of worker subnet, including primary ENI with multiple IPv6 Addresses" src="../networking/ipv6/image-2.png" /></p>
<p>Every EKS worker-node is assigned with IPv4 and IPv6 addresses, along with corresponding DNS entries. For a given worker-node, only a single IPv4 address from the dual-stack subnet is consumed. EKS support for IPv6 enables you to communicate with IPv4 endpoints (AWS, on-premise, internet) through a highly opinionated egress-only IPv4 model. EKS implements a host-local CNI plugin, secondary to the VPC CNI plugin, which allocates and configures an IPv4 address for a Pod. The CNI plugin configures a host-specific non-routable IPv4 address for a Pod from the 169.254.172.0/22 range. The IPv4 address assigned to the Pod is <em>unique to the worker-node</em> and is <em>not advertised beyond the worker-node</em>. 169.254.172.0/22 provides up to 1024 unique IPv4 addresses which can support large instance types.</p>
<p>The following diagram depict the flow of an IPv6 Pod connecting to an IPv4 endpoint outside the cluster boundary (non-internet):</p>
<p><img alt="EKS/IPv6, IPv4 egress-only flow" src="../networking/ipv6/eks-ipv4-snat-cni.png" /></p>
<p>In the above diagram Pods will perform a DNS lookup for the endpoint and, upon receiving an IPv4 “A” response, Pod’s node-only unique IPv4 address is translated through source network address translation (SNAT) to the Private IPv4 (VPC) address of the primary network interface attached to the EC2 Worker-node.</p>
<p>EKS/IPv6 Pods will also need to connect to IPv4 endpoints over the internet using public IPv4 Addresses, to achieve that a similar flow exists.
The following diagram depict the flow of an IPv6 Pod connecting to an IPv4 endpoint outside the cluster boundary (internet routable):  </p>
<p><img alt="EKS/IPv6, IPv4 Internet egress-only flow" src="../networking/ipv6/eks-ipv4-snat-cni-internet.png" /></p>
<p>In the above diagram Pods will perform a DNS lookup for the endpoint and, upon receiving an IPv4 “A” response, Pod’s node-only unique IPv4 address is translated through source network address translation (SNAT) to the Private IPv4 (VPC) address of the primary network interface attached to the EC2 Worker-node. The Pod IPv4 Address (Source IPv4: EC2 Primary IP) is then routed to the IPv4 NAT Gateway where the EC2 Primary IP is translated (SNAT) into a valid internet routable IPv4 Public IP Address (NAT Gateway Assigned Public IP).</p>
<p>Any Pod-to-Pod communication across the nodes always uses an IPv6 address. VPC CNI configures iptables to handle IPv6 while blocking any IPv4 connections.</p>
<p>Kubernetes services will receive only IPv6 addresses (ClusterIP) from Unique <a href="https://datatracker.ietf.org/doc/html/rfc4193">Local IPv6 Unicast Addresses (ULA)</a>. The ULA Service CIDR for an IPv6 cluster is automatically assigned during EKS cluster creation stage and cannot be modified. The following diagram depict the Pod to Kubernetes Service flow:</p>
<p><img alt="EKS/IPv6, IPv6 Pod to IPv6 k8s service (ClusterIP ULA) flow" src="../networking/ipv6/Pod-to-service-ipv6.png" /></p>
<p>Services are exposed to the internet using an AWS load balancer. The load balancer receives public IPv4 and IPv6 addresses, a.k.a dual-stack load balancer. For IPv4 clients accessing IPv6 cluster kubernetes services, the load balancer does IPv4 to IPv6 translation.</p>
<p>Amazon EKS recommends running worker nodes and Pods in private subnets. You can create public load balancers in the public subnets that load balance traffic to Pods running on nodes that are in private subnets.
The following diagram depict an internet IPv4 user accessing an EKS/IPv6 Ingress based service:</p>
<p><img alt="Internet IPv4 user to EKS/IPv6 Ingress service" src="../networking/ipv6/ipv4-internet-to-eks-ipv6.png" /></p>
<blockquote>
<p>Note: The above pattern requires to deploy the <a href="https://kubernetes-sigs.github.io/aws-load-balancer-controller">most recent version</a> of the AWS load balancer controller</p>
</blockquote>
<h3 id="networking-ipv6-eks-control-plane-data-plane-communication">EKS Control Plane &lt;-&gt; Data Plane communication<a class="headerlink" href="#networking-ipv6-eks-control-plane-data-plane-communication" title="Permanent link">&para;</a></h3>
<p>EKS will provision Cross-Account ENIs (X-ENIs) in dual stack mode (IPv4/IPv6). Kubernetes node components such as kubelet and kube-proxy are configured to support dual stack. Kubelet and kube-proxy run in a hostNetwork mode and bind to both IPv4 and IPv6 addresses attached to the primary network interface of a node. The Kubernetes api-server communicates to Pods and node components via the X-ENIs is IPv6 based. Pods communicate with the api-servers via the X-ENIs, and Pod to api-server communication always uses IPv6 mode.</p>
<p><img alt="illustration of cluster including X-ENIs" src="../networking/ipv6/image-5.png" /></p>
<h2 id="networking-ipv6-recommendations">Recommendations<a class="headerlink" href="#networking-ipv6-recommendations" title="Permanent link">&para;</a></h2>
<h3 id="networking-ipv6-maintain-access-to-ipv4-eks-apis">Maintain Access to IPv4 EKS APIs<a class="headerlink" href="#networking-ipv6-maintain-access-to-ipv4-eks-apis" title="Permanent link">&para;</a></h3>
<p>EKS APIs are accessible by IPv4 only. This also includes the Cluster API Endpoint. You will not be able to access cluster endpoints and APIs from an IPv6 only network. It is required that your network supports (1) an IPv6 transition mechanism such as NAT64/DNS64 that facilitates communication between IPv6 and IPv4 hosts and (2) a DNS service that supports translations of IPv4 endpoints.</p>
<h3 id="networking-ipv6-schedule-based-on-compute-resources">Schedule Based on Compute Resources<a class="headerlink" href="#networking-ipv6-schedule-based-on-compute-resources" title="Permanent link">&para;</a></h3>
<p>A single IPv6 prefix is sufficient to run many Pods on a single node. This also effectively removes ENI and IP limitations on the maximum number of Pods on a node. Although IPv6 removes direct dependency on max-Pods, when using prefix attachments with smaller instance types like the m5.large, you’re likely to exhaust the instance’s CPU and memory resources long before you exhaust its IP addresses. You must set the EKS recommended maximum Pod value by hand if you are using self-managed node groups or a managed node group with a custom AMI ID.</p>
<p>You can use the following formula to determine the maximum number of Pods you can deploy on a node for a IPv6 EKS cluster.</p>
<ul>
<li>
<p>((Number of network interfaces for instance type (number of prefixes per network interface-1)* 16) + 2</p>
</li>
<li>
<p>((3 ENIs)<em>((10 secondary IPs per ENI-1)</em> 16)) + 2 = 460 (real)</p>
</li>
</ul>
<p>Managed node groups automatically calculate the maximum number of Pods for you. Avoid changing EKS’s recommended value for the maximum number of Pods to avoid Pod scheduling failures due to resource limitations.</p>
<h3 id="networking-ipv6-evaluate-purpose-of-existing-custom-networking">Evaluate Purpose of Existing Custom Networking<a class="headerlink" href="#networking-ipv6-evaluate-purpose-of-existing-custom-networking" title="Permanent link">&para;</a></h3>
<p>If <a href="https://aws.github.io/aws-eks-best-practices/networking/custom-networking/">custom networking</a> is currently enabled, Amazon EKS recommends re-evaluating your need for it with IPv6. If you chose to use custom networking to address the IPv4 exhaustion issue, it is no longer necessary with IPv6. If you are utilizing custom networking to satisfy a security requirement, such as a separate network for nodes and Pods, you are encouraged to submit an <a href="https://github.com/aws/containers-roadmap/issues">EKS roadmap request</a>.</p>
<h3 id="networking-ipv6-fargate-pods-in-eksipv6-cluster">Fargate Pods in EKS/IPv6 Cluster<a class="headerlink" href="#networking-ipv6-fargate-pods-in-eksipv6-cluster" title="Permanent link">&para;</a></h3>
<p>EKS supports IPv6 for Pods running on Fargate. Pods running on Fargate will consume IPv6 and VPC Routable Private IPv4 addresses carved from the VPC CIDR ranges (IPv4&amp;IPv6). In simple words your EKS/Fargate Pods cluster wide density will be limited to the available IPv4 and IPv6 addresses. It is recommended to size your dual-stack subnets/VPC CIDRs for future growth. You will not be able to schedule new Fargate Pods if the underlying subnet does not contain an available IPv4 address, irrespective of IPv6 available addresses.</p>
<h3 id="networking-ipv6-deploy-the-aws-load-balancer-controller-lbc">Deploy the AWS Load Balancer Controller (LBC)<a class="headerlink" href="#networking-ipv6-deploy-the-aws-load-balancer-controller-lbc" title="Permanent link">&para;</a></h3>
<p><strong>The upstream in-tree Kubernetes service controller does not support IPv6</strong>. We recommend using the <a href="https://kubernetes-sigs.github.io/aws-load-balancer-controller">most recent version</a> of the AWS Load Balancer Controller add-on. The LBC will only deploy a dual-stack NLB or a dual-stack ALB upon consuming corresponding kubernetes service/ingress definition annotated with: <code>"alb.ingress.kubernetes.io/ip-address-type: dualstack"</code> and <code>"alb.ingress.kubernetes.io/target-type: ip"</code> </p>
<p>AWS Network Load Balancer does not support dual-stack UDP protocol address types. If you have strong requirements for low-latency, real-time streaming, online gaming, and IoT, we recommend running IPv4 clusters. To learn more about managing health checks for UDP services, please refer to <a href="https://aws.amazon.com/blogs/containers/how-to-route-udp-traffic-into-kubernetes/">“How to route UDP traffic into Kubernetes”</a>.</p>
<h3 id="networking-ipv6-identify-dependencies-on-imdsv2">Identify Dependencies on IMDSv2<a class="headerlink" href="#networking-ipv6-identify-dependencies-on-imdsv2" title="Permanent link">&para;</a></h3>
<p>EKS in IPv6 mode does not support IMDSv2 endpoints (yet). Please open a support ticket if IMDSv2 is a blocker for you to migrate to EKS/IPv6.</p></section><section class="print-page" id="networking-custom-networking"><h1 id="networking-custom-networking-custom-networking">Custom Networking<a class="headerlink" href="#networking-custom-networking-custom-networking" title="Permanent link">&para;</a></h1>
<p>By default, Amazon VPC CNI will assign Pods an IP address selected from the primary subnet.  The primary subnet is the subnet CIDR that the primary ENI is attached to, usually the subnet of the node/host.</p>
<p>If the subnet CIDR is too small, the CNI may not be able to acquire enough secondary IP addresses to assign to your Pods. This is a common challenge for EKS IPv4 clusters.</p>
<p>Custom networking is one solution to this problem.</p>
<p>Custom networking addresses the IP exhaustion issue by assigning the node and Pod IPs from secondary VPC address spaces (CIDR). Custom networking support supports ENIConfig custom resource. The ENIConfig includes an alternate subnet CIDR range (carved from a secondary VPC CIDR), along with the security group(s) that the Pods will belong to. When custom networking is enabled, the VPC CNI creates secondary ENIs in the subnet  defined under ENIConfig. The CNI assigns Pods an IP addresses from a CIDR range defined in a ENIConfig CRD. </p>
<p>Since the primary ENI is not used by custom networking, the maximum number of Pods you can run on a node is lower. The host network Pods continue to use IP address assigned to the primary ENI. Additionally, the primary ENI is used to handle source network translation and route Pods traffic outside the node. </p>
<h2 id="networking-custom-networking-example-configuration">Example Configuration<a class="headerlink" href="#networking-custom-networking-example-configuration" title="Permanent link">&para;</a></h2>
<p>While custom networking will accept valid VPC range for secondary CIDR range, we recommend that you use CIDRs (/16) from the CG-NAT space, i.e. 100.64.0.0/10 or 198.19.0.0/16 as those are less likely to be used in a corporate setting than other RFC1918 ranges. For additional information about the permitted and restricted CIDR block associations you can use with your VPC, see <a href="https://docs.aws.amazon.com/vpc/latest/userguide/configure-your-vpc.html#add-cidr-block-restrictions">IPv4 CIDR block association restrictions</a> in the VPC and subnet sizing section of the VPC documentation.</p>
<p>As shown in the diagram below, the primary Elastic Network Interface (<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html">ENI</a>) of the worker node still uses the primary VPC CIDR range (in this case 10.0.0.0/16) but the secondary ENIs use the secondary VPC CIDR Range (in this case 100.64.0.0/16). Now, in order to have the Pods use the 100.64.0.0/16 CIDR range, you must configure the CNI plugin to use custom networking. You can follow through the steps as documented <a href="https://docs.aws.amazon.com/eks/latest/userguide/cni-custom-network.html">here</a>.</p>
<p><img alt="illustration of pods on secondary subnet" src="../networking/custom-networking/image.png" /></p>
<p>If you want the CNI to use custom networking, set the <code>AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG</code> environment variable to <code>true</code>.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#networking-custom-networking-__codelineno-0-1"></a>kubectl set env daemonset aws-node -n kube-system AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG=true
</code></pre></div>
<p>When <code>AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG=true</code>, the CNI will assign Pod IP address from a subnet defined in <code>ENIConfig</code>. The <code>ENIConfig</code> custom resource is used to define the subnet in which Pods will be scheduled.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#networking-custom-networking-__codelineno-1-1"></a>apiVersion : crd.k8s.amazonaws.com/v1alpha1
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#networking-custom-networking-__codelineno-1-2"></a>kind : ENIConfig
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#networking-custom-networking-__codelineno-1-3"></a>metadata:
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#networking-custom-networking-__codelineno-1-4"></a>  name: us-west-2a
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#networking-custom-networking-__codelineno-1-5"></a>spec: 
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#networking-custom-networking-__codelineno-1-6"></a>  securityGroups:
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#networking-custom-networking-__codelineno-1-7"></a>    - sg-0dff111a1d11c1c11
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#networking-custom-networking-__codelineno-1-8"></a>  subnet: subnet-011b111c1f11fdf11
</code></pre></div>
<p>Upon creating the <code>ENIconfig</code> custom resources, you will need to create new worker nodes and drain the existing nodes. The existing worker nodes and Pods will remain unaffected. </p>
<h2 id="networking-custom-networking-recommendations">Recommendations<a class="headerlink" href="#networking-custom-networking-recommendations" title="Permanent link">&para;</a></h2>
<h3 id="networking-custom-networking-use-custom-networking-when">Use Custom Networking When<a class="headerlink" href="#networking-custom-networking-use-custom-networking-when" title="Permanent link">&para;</a></h3>
<p>We recommend you to consider custom networking if you are dealing with IPv4 exhaustion and can’t use IPv6 yet. Amazon EKS support for <a href="https://datatracker.ietf.org/doc/html/rfc6598">RFC6598</a> space enables you to scale Pods beyond <a href="https://datatracker.ietf.org/doc/html/rfc1918">RFC1918</a> address exhaustion challenges. Please consider using prefix delegation with custom networking to increase the Pods density on a node. </p>
<p>You might consider custom networking if you have a security requirement to run Pods on a different network with different security group requirements. When custom networking enabled, the pods use different subnet or security groups as defined in the ENIConfig than the node's primary network interface.</p>
<p>Custom networking is indeed an ideal option for deploying multiple EKS clusters and applications to connect on-premise datacenter services. You can increase the number of private addresses (RFC1918) accessible to EKS in your VPC for services such as Amazon Elastic Load Balancing and NAT-GW, while using non-routable CG-NAT space for your Pods across multiple clusters. Custom networking with the <a href="https://aws.amazon.com/transit-gateway/">transit gateway</a> and a Shared Services VPC (including NAT gateways across several Availability Zones for high availability) enables you to deliver scalable and predictable traffic flows. This <a href="https://aws.amazon.com/blogs/containers/eks-vpc-routable-ip-address-conservation/">blog post</a> describes an architectural pattern that is one of the most recommended ways to connect EKS Pods to a datacenter network using custom networking.</p>
<h3 id="networking-custom-networking-avoid-custom-networking-when">Avoid Custom Networking When<a class="headerlink" href="#networking-custom-networking-avoid-custom-networking-when" title="Permanent link">&para;</a></h3>
<h4 id="networking-custom-networking-ready-to-implement-ipv6">Ready to Implement IPv6<a class="headerlink" href="#networking-custom-networking-ready-to-implement-ipv6" title="Permanent link">&para;</a></h4>
<p>Custom networking can mitigate IP exhaustion issues, but it requires additional operational overhead. If you are currently deploying a dual-stack (IPv4/IPv6) VPC or if your plan includes IPv6 support, we recommend implementing IPv6 clusters instead. You can set up IPv6 EKS clusters and migrate your apps. In an IPv6 EKS cluster, both Kubernetes and Pods get an IPv6 address and can communicate in and out to both IPv4 and IPv6 endpoints. Please review best practices for <a href="#networking-ipv6">Running IPv6 EKS Clusters</a>.</p>
<h4 id="networking-custom-networking-exhausted-cg-nat-space">Exhausted CG-NAT Space<a class="headerlink" href="#networking-custom-networking-exhausted-cg-nat-space" title="Permanent link">&para;</a></h4>
<p>Furthermore, if you're currently utilizing CIDRs from the CG-NAT space or are unable to link a secondary CIDR with your cluster VPC, you may need to explore other options, such as using an alternative CNI. We strongly recommend that you either obtain commercial support or possess the in-house knowledge to debug and submit patches to the open source CNI plugin project. Refer <a href="https://docs.aws.amazon.com/eks/latest/userguide/alternate-cni-plugins.html">Alternate CNI Plugins</a> user guide for more details.</p>
<h4 id="networking-custom-networking-use-private-nat-gateway">Use Private NAT Gateway<a class="headerlink" href="#networking-custom-networking-use-private-nat-gateway" title="Permanent link">&para;</a></h4>
<p>Amazon VPC now offers <a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html">private NAT gateway</a> capabilities. Amazon's private NAT Gateway enables instances in private subnets to connect to other VPCs and on-premises networks with overlapping CIDRs. Consider utilizing the method described on this <a href="https://aws.amazon.com/blogs/containers/addressing-ipv4-address-exhaustion-in-amazon-eks-clusters-using-private-nat-gateways/">blog post</a> to employ a private NAT gateway to overcome communication issues for the EKS workloads caused by overlapping CIDRs, a significant complaint expressed by our clients. Custom networking cannot address the overlapping CIDR difficulties on its own, and it adds to the configuration challenges.</p>
<p>The network architecture used in this blog post implementation follows the recommendations under <a href="https://docs.aws.amazon.com/vpc/latest/userguide/nat-gateway-scenarios.html#private-nat-overlapping-networks">Enable communication between overlapping networks</a> in Amazon VPC documentation. As demonstrated in this blog post, you may expand the usage of private NAT Gateway in conjunction with RFC6598 addresses to manage customers' private IP exhaustion issues. The EKS clusters, worker nodes are deployed in the non-routable 100.64.0.0/16 VPC secondary CIDR range, whereas the private NAT gateway, NAT gateway are deployed to the routable RFC1918 CIDR ranges. The blog explains how a transit gateway is used to connect VPCs in order to facilitate communication across VPCs with overlapping non-routable CIDR ranges. For use cases in which EKS resources in a VPC's non-routable address range need to communicate with other VPCs that do not have overlapping address ranges, customers have the option of using VPC Peering to interconnect such VPCs. This method could provide potential cost savings as all data transit within an Availability Zone via a VPC peering connection is now free.</p>
<p><img alt="illustration of network traffic using private NAT gateway" src="../networking/custom-networking/image-3.png" /></p>
<h4 id="networking-custom-networking-unique-network-for-nodes-and-pods">Unique network for nodes and Pods<a class="headerlink" href="#networking-custom-networking-unique-network-for-nodes-and-pods" title="Permanent link">&para;</a></h4>
<p>If you need to isolate your nodes and Pods to a specific network for security reasons, we recommend that you deploy nodes and Pods to a subnet from a larger secondary CIDR block (e.g. 100.64.0.0/8). Following the installation of the new CIDR in your VPC, you can deploy another node group using the secondary CIDR and drain the original nodes to automatically redeploy the pods to the new worker nodes. For more information on how to implement this, see this <a href="https://aws.amazon.com/blogs/containers/optimize-ip-addresses-usage-by-pods-in-your-amazon-eks-cluster/">blog</a> post.</p>
<p>Custom networking is not used in the setup represented in the diagram below. Rather, Kubernetes worker nodes are deployed on subnets from your VPC's secondary VPC CIDR range, such as 100.64.0.0/10. You can keep the EKS cluster running (the control plane will remain on the original subnet/s), but the nodes and Pods will be moved to a secondary subnet/s. This is yet another, albeit unconventional, technique to mitigate the danger of IP exhaustion in a VPC. We propose draining the old nodes before redeploying the pods to the new worker nodes.</p>
<p><img alt="illustration of worker nodes on secondary subnet" src="../networking/custom-networking/image-2.png" /></p>
<h3 id="networking-custom-networking-automate-configuration-with-availability-zone-labels">Automate Configuration with Availability Zone Labels<a class="headerlink" href="#networking-custom-networking-automate-configuration-with-availability-zone-labels" title="Permanent link">&para;</a></h3>
<p>You can enable Kubernetes to automatically apply the corresponding ENIConfig for the worker node Availability Zone (AZ).</p>
<p>Kubernetes automatically adds the tag <a href="http://topology.kubernetes.io/zone"><code>topology.kubernetes.io/zone</code></a> to your worker nodes. Amazon EKS recommends using the availability zone as your ENI config name when you only have one secondary subnet (alternate CIDR) per AZ. Note that tag <code>failure-domain.beta.kubernetes.io/zone</code> is deprecated and replaced with the tag <code>topology.kubernetes.io/zone</code>.</p>
<ol>
<li>Set <code>name</code> field to the Availability Zone of your VPC.</li>
<li>Enable automatic configuration with this command:</li>
</ol>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#networking-custom-networking-__codelineno-2-1"></a>kubectl set env daemonset aws-node -n kube-system AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG=true
</code></pre></div>
<p>if you have multiple secondary subnets per availability zone, you need create a specific <code>ENI_CONFIG_LABEL_DEF</code>. You might consider configuring <code>ENI_CONFIG_LABEL_DEF</code> as <a href="http://k8s.amazonaws.com/eniConfig"><code>k8s.amazonaws.com/eniConfig</code></a> and label nodes with custom eniConfig names, such as <a href="http://k8s.amazonaws.com/eniConfig=us-west-2a-subnet-1"><code>k8s.amazonaws.com/eniConfig=us-west-2a-subnet-1</code></a> and <a href="http://k8s.amazonaws.com/eniConfig=us-west-2a-subnet-2"><code>k8s.amazonaws.com/eniConfig=us-west-2a-subnet-2</code></a>.</p>
<h3 id="networking-custom-networking-replace-pods-when-configuring-secondary-networking">Replace Pods when Configuring Secondary Networking<a class="headerlink" href="#networking-custom-networking-replace-pods-when-configuring-secondary-networking" title="Permanent link">&para;</a></h3>
<p>Enabling custom networking does not modify existing nodes. Custom networking is a disruptive action. Rather than doing a rolling replacement of all the worker nodes in your cluster after enabling custom networking, we suggest updating the AWS CloudFormation template in the <a href="https://docs.aws.amazon.com/eks/latest/userguide/getting-started.html">EKS Getting Started Guide</a> with a custom resource that calls a Lambda function to update the <code>aws-node</code> Daemonset with the environment variable to enable custom networking before the worker nodes are provisioned.</p>
<p>If you had any nodes in your cluster with running Pods before you switched to the custom CNI networking feature, you should cordon and <a href="https://aws.amazon.com/premiumsupport/knowledge-center/eks-worker-node-actions/">drain the nodes</a> to gracefully shutdown the Pods and then terminate the nodes. Only new nodes matching the ENIConfig label or annotations use custom networking, and hence the Pods scheduled on these new nodes can be assigned an IP from secondary CIDR. </p>
<h3 id="networking-custom-networking-calculate-max-pods-per-node">Calculate Max Pods per Node<a class="headerlink" href="#networking-custom-networking-calculate-max-pods-per-node" title="Permanent link">&para;</a></h3>
<p>Since the node’s primary ENI is no longer used to assign Pod IP addresses, there is a decrease in the number of Pods you can run on a given EC2 instance type. To work around this limitation you can use prefix assignment with custom networking. With prefix assignment, each secondary IP is replaced with a /28 prefix on secondary ENIs. </p>
<p>Consider the maximum number of Pods for an m5.large instance with custom networking.</p>
<p>The maximum number of Pods you can run without prefix assignment is 29</p>
<ul>
<li>((3 ENIs - 1) * (10 secondary IPs per ENI - 1)) + 2 = 20</li>
</ul>
<p>Enabling prefix attachments increases the number of Pods to 290.</p>
<ul>
<li>(((3 ENIs - 1) * ((10 secondary IPs per ENI - 1) * 16)) + 2 = 290</li>
</ul>
<p>However, we suggest setting max-pods to 110 rather than 290 because the instance has a rather small number of virtual CPUs. On bigger instances, EKS recommends a max pods value of 250. When utilizing prefix attachments with smaller instance types (e.g. m5.large), it is possible that you will exhaust the instance's CPU and memory resources well before its IP addresses.</p>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>When the CNI prefix allocates a /28 prefix to an ENI, it has to be a contiguous block of IP addresses. If the subnet that the prefix is generated from is highly fragmented, the prefix attachment may fail. You can mitigate this from happening by creating a new dedicated VPC for the cluster or by reserving subnet a set of CIDR exclusively for prefix attachments. Visit <a href="https://docs.aws.amazon.com/vpc/latest/userguide/subnet-cidr-reservation.html">Subnet CIDR reservations</a> for more information on this topic.</p>
</div>
<h3 id="networking-custom-networking-identify-existing-usage-of-cg-nat-space">Identify Existing Usage of CG-NAT Space<a class="headerlink" href="#networking-custom-networking-identify-existing-usage-of-cg-nat-space" title="Permanent link">&para;</a></h3>
<p>Custom networking allows you to mitigate IP exhaustion issue, however it can’t solve all the challenges. If you already using CG-NAT space for your cluster, or simply don’t have the ability to associate a secondary CIDR with your cluster VPC, we suggest you to explore other options, like using an alternate CNI or moving to IPv6 clusters.</p></section><section class="print-page" id="networking-prefix-mode-index_linux"><h1 id="networking-prefix-mode-index_linux-prefix-mode-for-linux">Prefix Mode for Linux<a class="headerlink" href="#networking-prefix-mode-index_linux-prefix-mode-for-linux" title="Permanent link">&para;</a></h1>
<p>Amazon VPC CNI assigns network prefixes to <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-prefix-eni.html">Amazon EC2 network interfaces</a> to increase the number of IP addresses available to nodes and increase pod density per node. You can configure version 1.9.0 or later of the Amazon VPC CNI add-on to assign IPv4 and IPv6 CIDRs instead of assigning individual secondary IP addresses to network interfaces. </p>
<p>Prefix mode is enabled by default on IPv6 clusters and is the only option supported. The VPC CNI assigns a /80 IPv6 prefix to a slot on an ENI. Please refer to the <a href="#networking-ipv6">IPv6 section of this guide</a> for further information. </p>
<p>With prefix assignment mode, the maximum number of elastic network interfaces per instance type remains the same, but you can now configure Amazon VPC CNI to assign /28 (16 IP addresses) IPv4 address prefixes, instead of assigning individual IPv4 addresses to the slots on network interfaces. When <code>ENABLE_PREFIX_DELEGATION</code> is set to true VPC CNI allocates an IP address to a Pod from the prefix assigned to an ENI.  Please follow the instructions mentioned in the <a href="https://docs.aws.amazon.com/eks/latest/userguide/cni-increase-ip-addresses.html">EKS user guide</a> to enable Prefix IP mode. </p>
<p><img alt="illustration of two worker subnets, comparing ENI secondary IPvs to ENIs with delegated prefixes" src="../networking/prefix-mode/image.png" /></p>
<p>The maximum number of IP addresses that you can assign to a network interface depends on the instance type. Each prefix that you assign to a network interface counts as one IP address. For example, a <code>c5.large</code> instance has a limit of <code>10</code> IPv4 addresses per network interface. Each network interface for this instance has a primary IPv4 address. If a network interface has no secondary IPv4 addresses, you can assign up to 9 prefixes to the network interface. For each additional IPv4 address that you assign to a network interface, you can assign one less prefix to the network interface. Review the AWS EC2 documentation on <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#AvailableIpPerENI">IP addresses per network interface per instance type</a> and <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-prefix-eni.html">assigning prefixes to network interfaces.</a></p>
<p>During worker node initialization, the VPC CNI assigns one or more prefixes to the primary ENI. The CNI pre-allocates a prefix for faster pod startup by maintaining a warm pool. The number of prefixes to be held in warm pool can be controlled by setting environment variables.</p>
<ul>
<li><code>WARM_PREFIX_TARGET</code>, the number of prefixes to be allocated in excess of current need.</li>
<li><code>WARM_IP_TARGET</code>, the number of IP addresses to be allocated in excess of current need.</li>
<li><code>MINIMUM_IP_TARGET</code>, the minimum number of IP addresses to be available at any time.</li>
<li><code>WARM_IP_TARGET</code> and <code>MINIMUM_IP_TARGET</code> if set will override <code>WARM_PREFIX_TARGET</code>.</li>
</ul>
<p>As more Pods scheduled additional prefixes will be requested for the existing ENI. First, the VPC CNI attempts to allocate a new prefix to an existing ENI. If the ENI is at capacity, the VPC CNI attempts to allocate a new ENI to the node. New ENIs will be attached until the maximum ENI limit (defined by the instance type) is reached. When a new ENI is attached, ipamd will allocate one or more prefixes needed to maintain the <code>WARM_PREFIX_TARGET</code>, <code>WARM_IP_TARGET</code>, and <code>MINIMUM_IP_TARGET</code> setting.</p>
<p><img alt="flow chart of procedure for assigning IP to pod" src="../networking/prefix-mode/image-2.jpeg" /></p>
<h2 id="networking-prefix-mode-index_linux-recommendations">Recommendations<a class="headerlink" href="#networking-prefix-mode-index_linux-recommendations" title="Permanent link">&para;</a></h2>
<h3 id="networking-prefix-mode-index_linux-use-prefix-mode-when">Use Prefix Mode when<a class="headerlink" href="#networking-prefix-mode-index_linux-use-prefix-mode-when" title="Permanent link">&para;</a></h3>
<p>Use prefix mode if you are experiencing Pod density issue on the worker nodes. To avoid VPC CNI errors, we recommend examining the subnets for contiguous block of addresses for /28 prefix before migrate to prefix mode. Please refer “<a href="https://docs.aws.amazon.com/vpc/latest/userguide/subnet-cidr-reservation.html">Use Subnet Reservations to Avoid Subnet Fragmentation (IPv4)</a>” section for Subnet reservation details. </p>
<p>For backward compatibility, the <a href="https://github.com/awslabs/amazon-eks-ami/blob/master/files/eni-max-pods.txt">max-pods</a> limit is set to support secondary IP mode. To increase the pod density, please specify the <code>max-pods</code> value to Kubelet and <code>--use-max-pods=false</code> as the user data for the nodes. You may consider using the <a href="https://github.com/awslabs/amazon-eks-ami/blob/master/files/max-pods-calculator.sh">max-pod-calculator.sh</a> script to calculate EKS’s recommended maximum number of pods for a given instance type. Refer to the EKS <a href="https://docs.aws.amazon.com/eks/latest/userguide/cni-increase-ip-addresses.html">user guide</a> for example user data.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#networking-prefix-mode-index_linux-__codelineno-0-1"></a>./max-pods-calculator.sh --instance-type m5.large --cni-version ``1.9``.0 --cni-prefix-delegation-enabled
</code></pre></div>
<p>Prefix assignment mode is especially relevant for users of <a href="https://docs.aws.amazon.com/eks/latest/userguide/cni-custom-network.html">CNI custom networking</a> where the primary ENI is not used for pods. With prefix assignment, you can still attach more IPs on nearly every Nitro instance type, even without the primary ENI used for pods.</p>
<h3 id="networking-prefix-mode-index_linux-avoid-prefix-mode-when">Avoid Prefix Mode when<a class="headerlink" href="#networking-prefix-mode-index_linux-avoid-prefix-mode-when" title="Permanent link">&para;</a></h3>
<p>If your subnet is very fragmented and has insufficient available IP addresses to create /28 prefixes, avoid using prefix mode. The prefix attachment may fail if the subnet from which the prefix is produced is fragmented (a heavily used subnet with scattered secondary IP addresses). This problem may be avoided by creating a new subnet and reserving a prefix.</p>
<p>In prefix mode, the security group assigned to the worker nodes is shared by the Pods. Consider using <a href="#networking-sgpp">Security groups for Pods</a>if you have a security requirement to achieve compliance by running applications with varying network security requirements on shared compute resources.</p>
<h3 id="networking-prefix-mode-index_linux-use-similar-instance-types-in-the-same-node-group">Use Similar Instance Types in the same Node Group<a class="headerlink" href="#networking-prefix-mode-index_linux-use-similar-instance-types-in-the-same-node-group" title="Permanent link">&para;</a></h3>
<p>Your node group may contain instances of many types. If an instance has a low maximum pod count, that value is applied to all nodes in the node group. Consider using similar instance types in a node group to maximize node use. We recommend configuring <a href="https://karpenter.sh/docs/concepts/provisioners/">node.kubernetes.io/instance-type</a> in the requirements part of the provisioner API if you are using Karpenter for automated node scaling.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The maximum pod count for all nodes in a particular node group is defined by the <em>lowest</em> maximum pod count of any single instance type in the node group.</p>
</div>
<h3 id="networking-prefix-mode-index_linux-configure-warm_prefix_target-to-conserve-ipv4-addresses">Configure <code>WARM_PREFIX_TARGET</code> to conserve IPv4 addresses<a class="headerlink" href="#networking-prefix-mode-index_linux-configure-warm_prefix_target-to-conserve-ipv4-addresses" title="Permanent link">&para;</a></h3>
<p>The <a href="https://github.com/aws/amazon-vpc-cni-k8s/blob/master/config/v1.9/aws-k8s-cni.yaml#L158">installation manifest’s</a> default value for <code>WARM_PREFIX_TARGET</code> is 1. In most cases, the recommended value of 1 for <code>WARM_PREFIX_TARGET</code> will provide a good mix of fast pod launch times while minimizing unused IP addresses assigned to the instance.</p>
<p>If you have a need to further conserve IPv4 addresses per node use <code>WARM_IP_TARGET</code> and <code>MINIMUM_IP_TARGET</code> settings, which override <code>WARM_PREFIX_TARGET</code> when configured. By setting <code>WARM_IP_TARGET</code> to a value less than 16, you can prevent the CNI from keeping an entire excess prefix attached.</p>
<h3 id="networking-prefix-mode-index_linux-prefer-allocating-new-prefixes-over-attaching-a-new-eni">Prefer allocating new prefixes over attaching a new ENI<a class="headerlink" href="#networking-prefix-mode-index_linux-prefer-allocating-new-prefixes-over-attaching-a-new-eni" title="Permanent link">&para;</a></h3>
<p>Allocating an additional prefix to an existing ENI is a faster EC2 API operation than creating and attaching a new ENI to the instance. Using prefixes improves performance while being frugal with IPv4 address allocation. Attaching a prefix typically completes in under a second, whereas attaching a new ENI can take up to 10 seconds. For most use cases, the CNI will only need a single ENI per worker node when running in prefix mode. If you can afford (in the worst case) up to 15 unused IPs per node, we strongly recommend using the newer prefix assignment networking mode, and realizing the performance and efficiency gains that come with it.</p>
<h3 id="networking-prefix-mode-index_linux-use-subnet-reservations-to-avoid-subnet-fragmentation-ipv4">Use Subnet Reservations to Avoid Subnet Fragmentation (IPv4)<a class="headerlink" href="#networking-prefix-mode-index_linux-use-subnet-reservations-to-avoid-subnet-fragmentation-ipv4" title="Permanent link">&para;</a></h3>
<p>When EC2 allocates a /28 IPv4 prefix to an ENI, it has to be a contiguous block of IP addresses from your subnet. If the subnet that the prefix is generated from is fragmented (a highly used subnet with scattered secondary IP addresses), the prefix attachment may fail, and you will see the following error message in the VPC CNI logs:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#networking-prefix-mode-index_linux-__codelineno-1-1"></a>failed to allocate a private IP/Prefix address: InsufficientCidrBlocks: There are not enough free cidr blocks in the specified subnet to satisfy the request.
</code></pre></div>
<p>To avoid fragmentation and have sufficient contiguous space for creating prefixes, you may use <a href="https://docs.aws.amazon.com/vpc/latest/userguide/subnet-cidr-reservation.html#work-with-subnet-cidr-reservations">VPC Subnet CIDR reservations</a> to reserve IP space within a subnet for exclusive use by prefixes. Once you create a reservation, the VPC CNI plugin will call EC2 APIs to assign prefixes that are automatically allocated from the reserved space.</p>
<p>It is recommended to create a new subnet, reserve space for prefixes, and enable prefix assignment with VPC CNI for worker nodes running in that subnet. If the new subnet is dedicated only to Pods running in your EKS cluster with VPC CNI prefix assignment enabled, then you can skip the prefix reservation step.</p>
<h3 id="networking-prefix-mode-index_linux-avoid-downgrading-vpc-cni">Avoid downgrading VPC CNI<a class="headerlink" href="#networking-prefix-mode-index_linux-avoid-downgrading-vpc-cni" title="Permanent link">&para;</a></h3>
<p>Prefix mode works with VPC CNI version 1.9.0 and later. Downgrading of the Amazon VPC CNI add-on to a version lower than 1.9.0 must be avoided once the prefix mode is enabled and prefixes are assigned to ENIs. You must delete and recreate nodes if you decide to downgrade the VPC CNI.</p>
<h3 id="networking-prefix-mode-index_linux-replace-all-nodes-during-the-transition-to-prefix-delegation">Replace all nodes during the transition to Prefix Delegation<a class="headerlink" href="#networking-prefix-mode-index_linux-replace-all-nodes-during-the-transition-to-prefix-delegation" title="Permanent link">&para;</a></h3>
<p>It is highly recommended that you create new node groups to increase the number of available IP addresses rather than doing rolling replacement of existing worker nodes. Cordon and drain all the existing nodes to safely evict all of your existing Pods. To prevent service disruptions, we suggest implementing <a href="https://kubernetes.io/docs/tasks/run-application/configure-pdb">Pod Disruption Budgets</a> on your production clusters for critical workloads. Pods on new nodes will be assigned an IP from a prefix assigned to an ENI. After you confirm the Pods are running, you can delete the old nodes and node groups. If you are using managed node groups, please follow steps mentioned here to safely <a href="https://docs.aws.amazon.com/eks/latest/userguide/delete-managed-node-group.html">delete a node group</a>.</p></section><section class="print-page" id="networking-prefix-mode-index_windows"><h1 id="networking-prefix-mode-index_windows-prefix-mode-for-windows">Prefix Mode for Windows<a class="headerlink" href="#networking-prefix-mode-index_windows-prefix-mode-for-windows" title="Permanent link">&para;</a></h1>
<p>In Amazon EKS, each Pod that runs on a Windows host is assigned a secondary IP address by the <a href="https://github.com/aws/amazon-vpc-resource-controller-k8s">VPC resource controller</a> by default. This IP address is a VPC-routable address that is allocated from the host's subnet. On Linux, each ENI attached to the instance has multiple slots that can be populated by a secondary IP address or a /28 CIDR (a prefix). Windows hosts, however, only support a single ENI and its available slots. Using only secondary IP addresses can artifically limit the number of pods you can run on a Windows host, even when there is an abundance of IP addresses available for assignment.</p>
<p>In order to increase the pod density on Windows hosts, especially when using smaller instance types, you can enable <strong>Prefix Delegation</strong> for Windows nodes. When prefix delegation is enabled, /28 IPv4 prefixes are assigned to ENI slots rather than secondary IP addresses. Prefix delegation can be enabled by adding the <code>enable-windows-prefix-delegation: "true"</code> entry to the <code>amazon-vpc-cni</code> config map. This is the same config map where you need to set <code>enable-windows-ipam: "true"</code> entry for enabling Windows support.</p>
<p>Please follow the instructions mentioned in the <a href="https://docs.aws.amazon.com/eks/latest/userguide/cni-increase-ip-addresses.html">EKS user guide</a> to enable Prefix Delegation mode for Windows nodes.</p>
<p><img alt="illustration of two worker subnets, comparing ENI secondary IPvs to ENIs with delegated prefixes" src="../networking/prefix-mode/windows-1.jpg" /></p>
<p>Figure: Comparison of Secondary IP mode with Prefix Delegation mode </p>
<p>The maximum number of IP addresses you can assign to a network interface depends on the instance type and its size. Each prefix assigned to a network interface consumes an available slot. For example, a <code>c5.large</code> instance has a limit of <code>10</code> slots per network interface. The first slot on a network interface is always consumed by the interface's primary IP address, leaving you with 9 slots for prefixes and/or secondary IP addresses. If these  slots are assigned prefixes, the node can support (9 * 16) 144 IP address whereas if they're assigned secondary IP addresses it can only support 9 IP addresses. See the documentation on <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#AvailableIpPerENI">IP addresses per network interface per instance type</a> and <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-prefix-eni.html">assigning prefixes to network interfaces</a> for further information.</p>
<p>During worker node initialization, the VPC Resource Controller assigns one or more prefixes to the primary ENI for faster pod startup by maintaining a warm pool of the IP addresses. The number of prefixes to be held in warm pool can be controlled by setting the following configuration parameters in <code>amazon-vpc-cni</code> config map.</p>
<ul>
<li><code>warm-prefix-target</code>, the number of prefixes to be allocated in excess of current need.</li>
<li><code>warm-ip-target</code>, the number of IP addresses to be allocated in excess of current need.</li>
<li><code>minimum-ip-target</code>, the minimum number of IP addresses to be available at any time.</li>
<li><code>warm-ip-target</code> and/or <code>minimum-ip-target</code> if set will override <code>warm-prefix-target</code>.</li>
</ul>
<p>As more Pods are scheduled on the node, additional prefixes will be requested for the existing ENI. When a Pod is scheduled on the node, VPC Resource Controller would first try to assign an IPv4 address from the existing prefixes on the node. If that is not possible, then a new IPv4 prefix will be requested as long as the subnet has the required capacity.</p>
<p><img alt="flow chart of procedure for assigning IP to pod" src="../networking/prefix-mode/windows-2.jpg" /></p>
<p>Figure: Workflow during assignment of IPv4 address to the Pod</p>
<h2 id="networking-prefix-mode-index_windows-recommendations">Recommendations<a class="headerlink" href="#networking-prefix-mode-index_windows-recommendations" title="Permanent link">&para;</a></h2>
<h3 id="networking-prefix-mode-index_windows-use-prefix-delegation-when">Use Prefix Delegation when<a class="headerlink" href="#networking-prefix-mode-index_windows-use-prefix-delegation-when" title="Permanent link">&para;</a></h3>
<p>Use prefix delegation if you are experiencing Pod density issues on the worker nodes. To avoid errors, we recommend examining the subnets for contiguous block of addresses for /28 prefix before migrating to prefix mode. Please refer “<a href="https://docs.aws.amazon.com/vpc/latest/userguide/subnet-cidr-reservation.html">Use Subnet Reservations to Avoid Subnet Fragmentation (IPv4)</a>” section for Subnet reservation details. </p>
<p>By default, the <code>max-pods</code> on Windows nodes is set to <code>110</code>. For the vast majority of instance types, this should be sufficient. If you want to increase or decrease this limit, then add the following to the bootstrap command in your user data:
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#networking-prefix-mode-index_windows-__codelineno-0-1"></a>-KubeletExtraArgs &#39;--max-pods=example-value&#39;
</code></pre></div>
For more details about the bootstrap configuration parameters for Windows nodes, please visit the documentation <a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-windows-ami.html#bootstrap-script-configuration-parameters">here</a>.</p>
<h3 id="networking-prefix-mode-index_windows-avoid-prefix-delegation-when">Avoid Prefix Delegation when<a class="headerlink" href="#networking-prefix-mode-index_windows-avoid-prefix-delegation-when" title="Permanent link">&para;</a></h3>
<p>If your subnet is very fragmented and has insufficient available IP addresses to create /28 prefixes, avoid using prefix mode. The prefix attachment may fail if the subnet from which the prefix is produced is fragmented (a heavily used subnet with scattered secondary IP addresses). This problem may be avoided by creating a new subnet and reserving a prefix.</p>
<h3 id="networking-prefix-mode-index_windows-configure-parameters-for-prefix-delegation-to-conserve-ipv4-addresses">Configure parameters for prefix delegation to conserve IPv4 addresses<a class="headerlink" href="#networking-prefix-mode-index_windows-configure-parameters-for-prefix-delegation-to-conserve-ipv4-addresses" title="Permanent link">&para;</a></h3>
<p><code>warm-prefix-target</code>, <code>warm-ip-target</code>, and <code>minimum-ip-target</code> can be used to fine tune the behaviour of pre-scaling and dynamic scaling with prefixes. By default, the following values are used:
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#networking-prefix-mode-index_windows-__codelineno-1-1"></a>warm-ip-target: &quot;1&quot;
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#networking-prefix-mode-index_windows-__codelineno-1-2"></a>minimum-ip-target: &quot;3&quot;
</code></pre></div>
By fine tuning these configuration parameters, you can achieve an optimal balance of conserving the IP addresses and ensuring decreased Pod latency due to assignment of IP address. For more information about these configuration parameters, visit the documentation <a href="https://github.com/aws/amazon-vpc-resource-controller-k8s/blob/master/docs/windows/prefix_delegation_config_options.md">here</a>.</p>
<h3 id="networking-prefix-mode-index_windows-use-subnet-reservations-to-avoid-subnet-fragmentation-ipv4">Use Subnet Reservations to Avoid Subnet Fragmentation (IPv4)<a class="headerlink" href="#networking-prefix-mode-index_windows-use-subnet-reservations-to-avoid-subnet-fragmentation-ipv4" title="Permanent link">&para;</a></h3>
<p>When EC2 allocates a /28 IPv4 prefix to an ENI, it has to be a contiguous block of IP addresses from your subnet. If the subnet that the prefix is generated from is fragmented (a highly used subnet with scattered secondary IP addresses), the prefix attachment may fail, and you will see the following node event:
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#networking-prefix-mode-index_windows-__codelineno-2-1"></a>InsufficientCidrBlocks: The specified subnet does not have enough free cidr blocks to satisfy the request
</code></pre></div>
To avoid fragmentation and have sufficient contiguous space for creating prefixes, use <a href="https://docs.aws.amazon.com/vpc/latest/userguide/subnet-cidr-reservation.html#work-with-subnet-cidr-reservations">VPC Subnet CIDR reservations</a> to reserve IP space within a subnet for exclusive use by prefixes. Once you create a reservation, the IP addresses from the reserved blocks will not be assigned to other resources. That way, VPC Resource Controller will be able to get available prefixes during the assignment call to the node ENI.</p>
<p>It is recommended to create a new subnet, reserve space for prefixes, and enable prefix assignment for worker nodes running in that subnet. If the new subnet is dedicated only to Pods running in your EKS cluster with prefix delegation enabled, then you can skip the prefix reservation step.</p>
<h3 id="networking-prefix-mode-index_windows-replace-all-nodes-when-migrating-from-secondary-ip-mode-to-prefix-delegation-mode-or-vice-versa">Replace all nodes when migrating from Secondary IP mode to Prefix Delegation mode or vice versa<a class="headerlink" href="#networking-prefix-mode-index_windows-replace-all-nodes-when-migrating-from-secondary-ip-mode-to-prefix-delegation-mode-or-vice-versa" title="Permanent link">&para;</a></h3>
<p>It is highly recommended that you create new node groups to increase the number of available IP addresses rather than doing rolling replacement of existing worker nodes.</p>
<p>When using self-managed node groups, the steps for transition would be:</p>
<ul>
<li>Increase the capacity in your cluster such that the new nodes would be able to accomodate your workloads</li>
<li>Enable/Disable the Prefix Delegation feature for Windows</li>
<li>Cordon and drain all the existing nodes to safely evict all of your existing Pods. To prevent service disruptions, we suggest implementing <a href="https://kubernetes.io/docs/tasks/run-application/configure-pdb">Pod Disruption Budgets</a> on your production clusters for critical workloads.</li>
<li>After you confirm the Pods are running, you can delete the old nodes and node groups. Pods on new nodes will be assigned an IPv4 address from a prefix assigned to the node ENI.</li>
</ul>
<p>When using managed node groups, the steps for transition would be:</p>
<ul>
<li>Enable/Disable the Prefix Delegation feature for Windows</li>
<li>Update the node group using the steps mentioned <a href="https://docs.aws.amazon.com/eks/latest/userguide/update-managed-node-group.html">here</a>. This performs similar steps as above but are managed by EKS.</li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Run all Pods on a node in the same mode</p>
</div>
<p>For Windows, we recommend that you avoid running Pods in both secondary IP mode and prefix delegation mode at the same time. Such a situation can arise when you migrate from secondary IP mode to prefix delegation mode or vice versa with running Windows workloads.</p>
<p>While this will not impact your running Pods, there can be inconsistency with respect to the node's IP address capacity. For example, consider that a t3.xlarge node which has 14 slots for secondary IPv4 addresses. If you are running 10 Pods, then 10 slots on the ENI will be consumed by secondary IP addresses. After you enable prefix delegation the capacity advertised to the kube-api server would be (14 slots * 16 ip addresses per prefix) 244 but the actual capacity at that moment would be (4 remaining slots * 16 addresses per prefix) 64. This inconsistency between the amount of capacity advertised and the actual amount of capacity (remaining slots) can cause issues if you run more Pods than there are IP addresses available for assignment.</p>
<p>That being said, you can use the migration strategy as described above to safely transition your Pods from secondary IP address to addresses obtained from prefixes. When toggling between the modes, the Pods will continue running normally and:</p>
<ul>
<li>When toggling from secondary IP mode to prefix delegation mode, the secondary IP addresses assigned to the running pods will not be released. Prefixes will be assigned to the free slots. Once a pod is terminated, the secondary IP and slot it was using will be released.</li>
<li>When toggling from prefix delegation mode to secondary IP mode, a prefix will be released when all the IPs within its range are no longer allocated to pods. If any IP from the prefix is assigned to a pod then that prefix will be kept until the pods are terminated.</li>
</ul>
<h3 id="networking-prefix-mode-index_windows-debugging-issues-with-prefix-delegation">Debugging Issues with Prefix Delegation<a class="headerlink" href="#networking-prefix-mode-index_windows-debugging-issues-with-prefix-delegation" title="Permanent link">&para;</a></h3>
<p>You can use our debugging guide <a href="https://github.com/aws/amazon-vpc-resource-controller-k8s/blob/master/docs/troubleshooting.md">here</a> to deep dive into the issue you are facing with prefix delegation on Windows.</p></section><section class="print-page" id="networking-sgpp"><h1 id="networking-sgpp-security-groups-per-pod">Security Groups Per Pod<a class="headerlink" href="#networking-sgpp-security-groups-per-pod" title="Permanent link">&para;</a></h1>
<p>An AWS security group acts as a virtual firewall for EC2 instances to control inbound and outbound traffic. By default, the Amazon VPC CNI will use security groups associated with the primary ENI on the node. More specifically, every ENI associated with the instance will have the same EC2 Security Groups. Thus, every Pod on a node shares the same security groups as the node it runs on. </p>
<p>As seen in the image below, all application Pods operating on worker nodes will have access to the RDS database service (considering RDS inbound allows node security group). Security groups are too coarse grained because they apply to all Pods running on a node. Security groups for Pods provides network segmentation for workloads which is an essential part a good defense in depth strategy.</p>
<p><img alt="illustration of node with security group connecting to RDS" src="../networking/sgpp/image.png" />
With security groups for Pods, you can improve compute efficiency by running applications with varying network security requirements on shared compute resources. Multiple types of security rules, such as Pod-to-Pod and Pod-to-External AWS services, can be defined in a single place with EC2 security groups and applied to workloads with Kubernetes native APIs. The image below shows security groups applied at the Pod level and how they simplify your application deployment and node architecture. The Pod can now access Amazon RDS database.</p>
<p><img alt="illustration of pod and node with different security groups connecting to RDS" src="../networking/sgpp/image-2.png" /></p>
<p>You can enable security groups for Pods by setting <code>ENABLE_POD_ENI=true</code> for VPC CNI. Once enabled, the “<a href="https://github.com/aws/amazon-vpc-resource-controller-k8s">VPC Resource Controller</a>“ running on the control plane (managed by EKS) creates and attaches a trunk interface called “aws-k8s-trunk-eni“ to the node. The trunk interface acts as a standard network interface attached to the instance. To manage trunk interfaces, you must add the <code>AmazonEKSVPCResourceController</code> managed policy to the cluster role that goes with your Amazon EKS cluster.</p>
<p>The controller also creates branch interfaces named "aws-k8s-branch-eni" and associates them with the trunk interface. Pods are assigned a security group using the <a href="https://github.com/aws/amazon-vpc-resource-controller-k8s/blob/master/config/crd/bases/vpcresources.k8s.aws_securitygrouppolicies.yaml">SecurityGroupPolicy</a> custom resource and are associated with a branch interface. Since security groups are specified with network interfaces, we are now able to schedule Pods requiring specific security groups on these additional network interfaces. Review the<a href="https://docs.aws.amazon.com/eks/latest/userguide/security-groups-for-pods.html">EKS User Guide Section on Security Groups for Pods,</a> including deployment prerequisites.</p>
<p><img alt="illustration of worker subnet with security groups associated with ENIs" src="../networking/sgpp/image-3.png" /></p>
<p>Branch interface capacity is <em>additive</em> to existing instance type limits for secondary IP addresses. Pods that use security groups are not accounted for in the max-pods formula and when you use security group for pods you need to consider raising the max-pods value or be ok with running fewer pods than the node can actually support.</p>
<p>A m5.large can have up to 9 branch network interfaces and up to 27 secondary IP addresses assigned to its standard network interfaces. As shown in the example below, the default max-pods for a m5.large is 29, and EKS counts the Pods that use security groups towards the maximum Pods. Please see the <a href="https://docs.aws.amazon.com/eks/latest/userguide/cni-increase-ip-addresses.html">EKS user guide</a> for instructions on how to change the max-pods for nodes.</p>
<p>When security groups for Pods are used in combination with <a href="https://docs.aws.amazon.com/eks/latest/userguide/cni-custom-network.html">custom networking</a>, the security group defined in security groups for Pods is used rather than the security group specified in the ENIConfig. As a result, when custom networking is enabled, carefully assess security group ordering while using security groups per Pod.</p>
<h2 id="networking-sgpp-recommendations">Recommendations<a class="headerlink" href="#networking-sgpp-recommendations" title="Permanent link">&para;</a></h2>
<h3 id="networking-sgpp-disable-tcp-early-demux-for-liveness-probe">Disable TCP Early Demux for Liveness Probe<a class="headerlink" href="#networking-sgpp-disable-tcp-early-demux-for-liveness-probe" title="Permanent link">&para;</a></h3>
<p>If are you using liveness or readiness probes, you also need to disable TCP early demux, so that the kubelet can connect to Pods on branch network interfaces via TCP. This is only required in strict mode. To do this run the following command:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#networking-sgpp-__codelineno-0-1"></a>kubectl edit daemonset aws-node -n kube-system
</code></pre></div>
<p>Under the <code>initContainer</code> section, change the value for <code>DISABLE_TCP_EARLY_DEMUX</code> to <code>true.</code></p>
<h3 id="networking-sgpp-use-security-group-for-pods-to-leverage-existing-aws-configuration-investment">Use Security Group For Pods to leverage existing AWS configuration investment.<a class="headerlink" href="#networking-sgpp-use-security-group-for-pods-to-leverage-existing-aws-configuration-investment" title="Permanent link">&para;</a></h3>
<p>Security groups makes it easier to restrict network access to VPC resources, such as RDS databases or EC2 instances. One clear advantage of security groups per Pod is the opportunity to reuse existing AWS security group resources. 
If you are using security groups as a network firewall to limit access to your AWS services, we propose applying security groups to Pods using branch ENIs. Consider using security groups for Pods if you are transferring apps from EC2 instances to EKS and limit access to other AWS services with security groups.</p>
<h3 id="networking-sgpp-configure-pod-security-group-enforcing-mode">Configure Pod Security Group Enforcing Mode<a class="headerlink" href="#networking-sgpp-configure-pod-security-group-enforcing-mode" title="Permanent link">&para;</a></h3>
<p>Amazon VPC CNI plugin version 1.11 added a new setting named <code>POD_SECURITY_GROUP_ENFORCING_MODE</code> (“enforcing mode”). The enforcing mode controls both which security groups apply to the pod, and if source NAT is enabled. You may specify the enforcing mode as either strict or standard. Strict is the default, reflecting the previous behavior of the VPC CNI with <code>ENABLE_POD_ENI</code> set to <code>true</code>. </p>
<p>In Strict Mode, only the branch ENI security groups are enforced. The source NAT is also disabled. </p>
<p>In Standard Mode, the security groups associated with both the primary ENI and branch ENI (associated with the pod) are applied. Network traffic must comply with both security groups. </p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Any mode change will only impact newly launched Pods. Existing Pods will use the mode that was configured when the Pod was created. Customers will need to recycle existing Pods with security groups if they want to change the traffic behavior.</p>
</div>
<h3 id="networking-sgpp-enforcing-mode-use-strict-mode-for-isolating-pod-and-node-traffic">Enforcing Mode: Use Strict mode for isolating pod and node traffic:<a class="headerlink" href="#networking-sgpp-enforcing-mode-use-strict-mode-for-isolating-pod-and-node-traffic" title="Permanent link">&para;</a></h3>
<p>By default, security groups for Pods is set to "strict mode." Use this setting if you must completely separate Pod traffic from the rest of the node's traffic. In strict mode, the source NAT is turned off so the branch ENI outbound security groups can be used. </p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>When strict mode is enabled, all outbound traffic from a pod will leave the node and enter the VPC network. Traffic between pods on the same node will go over the VPC. This increases VPC traffic and limits node-based features. The NodeLocal DNSCache is not supported with strict mode. </p>
</div>
<h3 id="networking-sgpp-enforcing-mode-use-standard-mode-in-the-following-situations">Enforcing Mode: Use Standard mode in the following situations<a class="headerlink" href="#networking-sgpp-enforcing-mode-use-standard-mode-in-the-following-situations" title="Permanent link">&para;</a></h3>
<p><strong>Client source IP visible to the containers in the Pod</strong></p>
<p>If you need to keep the client source IP visible to the containers in the Pod, consider setting <code>POD_SECURITY_GROUP_ENFORCING_MODE</code> to <code>standard</code>. Kubernetes services support externalTrafficPolicy=local to support preservation of the client source IP (default type cluster). You can now run Kubernetes services of type NodePort and LoadBalancer using instance targets with an externalTrafficPolicy set to Local in the standard mode. <code>Local</code> preserves the client source IP and avoids a second hop for LoadBalancer and NodePort type Services.</p>
<p><strong>Deploying NodeLocal DNSCache</strong></p>
<p>When using security groups for pods, configure standard mode to support Pods that use <a href="https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/">NodeLocal DNSCache</a>. NodeLocal DNSCache improves Cluster DNS performance by running a DNS caching agent on cluster nodes as a DaemonSet. This will help the pods that have the highest DNS QPS requirements to query local kube-dns/CoreDNS having a local cache, which will improve the latency.</p>
<p>NodeLocal DNSCache is not supported in strict mode as all network traffic, even to the node, enters the VPC. </p>
<p><strong>Supporting Kubernetes Network Policy</strong></p>
<p>We recommend using standard enforcing mode when using network policy with Pods that have associated security groups.</p>
<p>We strongly recommend to utilize security groups for Pods to limit network-level access to AWS services that are not part of a cluster. Consider network policies to restrict network traffic between Pods inside a cluster, often known as East/West traffic. </p>
<h3 id="networking-sgpp-identify-incompatibilities-with-security-groups-per-pod">Identify Incompatibilities with Security Groups per Pod<a class="headerlink" href="#networking-sgpp-identify-incompatibilities-with-security-groups-per-pod" title="Permanent link">&para;</a></h3>
<p>Windows-based and non-nitro instances do not support security groups for Pods. To utilize security groups with Pods, the instances must be tagged with isTrunkingEnabled. Use network policies to manage access between Pods rather than security groups if your Pods do not depend on any AWS services within or outside of your VPC.</p>
<h3 id="networking-sgpp-use-security-groups-per-pod-to-efficiently-control-traffic-to-aws-services">Use Security Groups per Pod to efficiently control traffic to AWS Services<a class="headerlink" href="#networking-sgpp-use-security-groups-per-pod-to-efficiently-control-traffic-to-aws-services" title="Permanent link">&para;</a></h3>
<p>If an application running within the EKS cluster has to communicate with another resource within the VPC, e.g. an RDS database, then consider using SGs for pods. While there are policy engines that allow you to specify an CIDR or a DNS name, they are a less optimal choice when communicating with AWS services that have endpoints that reside within a VPC.</p>
<p>In contrast, Kubernetes <a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/">network policies</a> provide a mechanism for controlling ingress and egress traffic both within and outside the cluster. Kubernetes network policies should be considered if your application has limited dependencies on other AWS services. You may configure network policies that specify egress rules based on CIDR ranges to limit access to AWS services as opposed to AWS native semantics like SGs. You may use Kubernetes network policies to control network traffic between Pods (often referred to as East/West traffic) and between Pods and external services. Kubernetes network policies are implemented at OSI levels 3 and 4. </p>
<p>Amazon EKS allows you to use network policy engines such as <a href="https://projectcalico.docs.tigera.io/getting-started/kubernetes/managed-public-cloud/eks">Calico</a> and <a href="https://docs.cilium.io/en/stable/intro/">Cilium</a>. By default, the network policy engines are not installed. Please check the respective install guides for instructions on how to set up. For more information on how to use network policy, see <a href="https://aws.github.io/aws-eks-best-practices/security/docs/network/#network-policy">EKS Security best practices</a>. The DNS hostnames feature is available in the enterprise versions of network policy engines, which could be useful for controlling traffic between Kubernetes Services/Pods and resources that run outside of AWS. Also, you can consider DNS hostname support for AWS services that don't support security groups by default.</p>
<h3 id="networking-sgpp-tag-a-single-security-group-to-use-aws-loadbalancer-controller">Tag a single Security Group to use AWS Loadbalancer Controller<a class="headerlink" href="#networking-sgpp-tag-a-single-security-group-to-use-aws-loadbalancer-controller" title="Permanent link">&para;</a></h3>
<p>When many security groups are allocated to a Pod, Amazon EKS recommends tagging a single security group with <a href="http://kubernetes.io/cluster/$name"><code>kubernetes.io/cluster/$name</code></a> shared or owned. The tag allows the AWS Loadbalancer Controller to update the rules of security groups to route traffic to the Pods. If just one security group is given to a Pod, the assignment of a tag is optional. Permissions set in a security group are additive, therefore tagging a single security group is sufficient for the loadbalancer controller to locate and reconcile the rules. It also helps to adhere to the <a href="https://docs.aws.amazon.com/vpc/latest/userguide/amazon-vpc-limits.html#vpc-limits-security-groups">default quotas</a> defined by security groups.</p>
<h3 id="networking-sgpp-configure-nat-for-outbound-traffic">Configure NAT for Outbound Traffic<a class="headerlink" href="#networking-sgpp-configure-nat-for-outbound-traffic" title="Permanent link">&para;</a></h3>
<p>Source NAT is disabled for outbound traffic from Pods that are assigned security groups. For Pods using security groups that require access the internet launch worker nodes on private subnets configured with a NAT gateway or instance and enable <a href="https://docs.aws.amazon.com/eks/latest/userguide/external-snat.html">external SNAT</a> in the CNI.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#networking-sgpp-__codelineno-1-1"></a>kubectl set env daemonset -n kube-system aws-node AWS_VPC_K8S_CNI_EXTERNALSNAT=true
</code></pre></div>
<h3 id="networking-sgpp-deploy-pods-with-security-groups-to-private-subnets">Deploy Pods with Security Groups to Private Subnets<a class="headerlink" href="#networking-sgpp-deploy-pods-with-security-groups-to-private-subnets" title="Permanent link">&para;</a></h3>
<p>Pods that are assigned security groups must be run on nodes that are deployed on to private subnets. Note that Pods with assigned security groups deployed to public subnets will not able to access the internet.</p>
<h3 id="networking-sgpp-verify-terminationgraceperiodseconds-in-pod-specification-file">Verify <em>terminationGracePeriodSeconds</em> in Pod Specification File<a class="headerlink" href="#networking-sgpp-verify-terminationgraceperiodseconds-in-pod-specification-file" title="Permanent link">&para;</a></h3>
<p>Ensure that <code>terminationGracePeriodSeconds</code> is non-zero in your Pod specification file (default 30 seconds). This is essential in order for Amazon VPC CNI to delete the Pod network from the worker node. When set to zero, the CNI plugin does not remove the Pod network from the host, and the branch ENI is not effectively cleaned up.</p>
<h3 id="networking-sgpp-using-security-groups-for-pods-with-fargate">Using Security Groups for Pods with Fargate<a class="headerlink" href="#networking-sgpp-using-security-groups-for-pods-with-fargate" title="Permanent link">&para;</a></h3>
<p>Security groups for Pods that run on Fargate work very similarly to Pods that run on EC2 worker nodes. For example, you have to create the security group before referencing it in the SecurityGroupPolicy you associate with your Fargate Pod. By default, the <a href="https://docs.aws.amazon.com/eks/latest/userguide/sec-group-reqs.html">cluster security group</a> is assiged to all Fargate Pods when you don't explicitly assign a SecurityGroupPolicy to a Fargate Pod. For simplicity's sake, you may want to add the cluster security group to a Fagate Pod's SecurityGroupPolicy otherwise you will have to add the minimum security group rules to your security group. You can find the cluster security group using the describe-cluster API.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#networking-sgpp-__codelineno-2-1"></a><span class="w"> </span>aws<span class="w"> </span>eks<span class="w"> </span>describe-cluster<span class="w"> </span>--name<span class="w"> </span>CLUSTER_NAME<span class="w"> </span>--query<span class="w"> </span><span class="s1">&#39;cluster.resourcesVpcConfig.clusterSecurityGroupId&#39;</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#networking-sgpp-__codelineno-3-1"></a>cat<span class="w"> </span>&gt;my-fargate-sg-policy.yaml<span class="w"> </span><span class="s">&lt;&lt;EOF</span>
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#networking-sgpp-__codelineno-3-2"></a><span class="s">apiVersion: vpcresources.k8s.aws/v1beta1</span>
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#networking-sgpp-__codelineno-3-3"></a><span class="s">kind: SecurityGroupPolicy</span>
<a id="__codelineno-3-4" name="__codelineno-3-4" href="#networking-sgpp-__codelineno-3-4"></a><span class="s">metadata:</span>
<a id="__codelineno-3-5" name="__codelineno-3-5" href="#networking-sgpp-__codelineno-3-5"></a><span class="s">  name: my-fargate-sg-policy</span>
<a id="__codelineno-3-6" name="__codelineno-3-6" href="#networking-sgpp-__codelineno-3-6"></a><span class="s">  namespace: my-fargate-namespace</span>
<a id="__codelineno-3-7" name="__codelineno-3-7" href="#networking-sgpp-__codelineno-3-7"></a><span class="s">spec:</span>
<a id="__codelineno-3-8" name="__codelineno-3-8" href="#networking-sgpp-__codelineno-3-8"></a><span class="s">  podSelector: </span>
<a id="__codelineno-3-9" name="__codelineno-3-9" href="#networking-sgpp-__codelineno-3-9"></a><span class="s">    matchLabels:</span>
<a id="__codelineno-3-10" name="__codelineno-3-10" href="#networking-sgpp-__codelineno-3-10"></a><span class="s">      role: my-fargate-role</span>
<a id="__codelineno-3-11" name="__codelineno-3-11" href="#networking-sgpp-__codelineno-3-11"></a><span class="s">  securityGroups:</span>
<a id="__codelineno-3-12" name="__codelineno-3-12" href="#networking-sgpp-__codelineno-3-12"></a><span class="s">    groupIds:</span>
<a id="__codelineno-3-13" name="__codelineno-3-13" href="#networking-sgpp-__codelineno-3-13"></a><span class="s">      - cluster_security_group_id</span>
<a id="__codelineno-3-14" name="__codelineno-3-14" href="#networking-sgpp-__codelineno-3-14"></a><span class="s">      - my_fargate_pod_security_group_id</span>
<a id="__codelineno-3-15" name="__codelineno-3-15" href="#networking-sgpp-__codelineno-3-15"></a><span class="s">EOF</span>
</code></pre></div>
<p>The minimum security group rules are listed <a href="https://docs.aws.amazon.com/eks/latest/userguide/sec-group-reqs.html">here</a>. These rules allow Fargate Pods to communicate with in-cluster services like kube-apiserver, kubelet, and CoreDNS. You also need add rules to allow inbound and outbound connections to and from your Fargate Pod. This will allow your Pod to communicate with other Pods or resources in your VPC. Additionally, you have to include rules for Fargate to pull container images from Amazon ECR or other container registries such as DockerHub. For more information, see AWS IP address ranges in the <a href="https://docs.aws.amazon.com/general/latest/gr/aws-ip-ranges.html">AWS General Reference</a>. </p>
<p>You can use the below commands to find the security groups applied to a Fargate Pod. </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#networking-sgpp-__codelineno-4-1"></a>kubectl<span class="w"> </span>get<span class="w"> </span>pod<span class="w"> </span>FARGATE_POD<span class="w"> </span>-o<span class="w"> </span><span class="nv">jsonpath</span><span class="o">=</span><span class="s1">&#39;{.metadata.annotations.vpc\.amazonaws\.com/pod-eni}{&quot;\n&quot;}&#39;</span>
</code></pre></div>
<p>Note down the eniId from above command. </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#networking-sgpp-__codelineno-5-1"></a>aws<span class="w"> </span>ec2<span class="w"> </span>describe-network-interfaces<span class="w"> </span>--network-interface-ids<span class="w"> </span>ENI_ID<span class="w"> </span>--query<span class="w"> </span><span class="s1">&#39;NetworkInterfaces[*].Groups[*]&#39;</span>
</code></pre></div>
<p>Existing Fargate pods must be deleted and recreated in order for new security groups to be applied. For instance, the following command initiates the deployment of the example-app. To update specific pods, you can change the namespace and deployment name in the below command.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#networking-sgpp-__codelineno-6-1"></a>kubectl<span class="w"> </span>rollout<span class="w"> </span>restart<span class="w"> </span>-n<span class="w"> </span>example-ns<span class="w"> </span>deployment<span class="w"> </span>example-pod
</code></pre></div></section><section class="print-page" id="networking-loadbalancing-loadbalancing"><h1 id="networking-loadbalancing-loadbalancing-avoiding-errors-timeouts-with-kubernetes-applications-and-aws-load-balancers">Avoiding Errors &amp; Timeouts with Kubernetes Applications and AWS Load Balancers<a class="headerlink" href="#networking-loadbalancing-loadbalancing-avoiding-errors-timeouts-with-kubernetes-applications-and-aws-load-balancers" title="Permanent link">&para;</a></h1>
<p>After creating the necessary Kubernetes resources (Service, Deployment, Ingress, etc), your pods should be able to receive traffic from your clients through an Elastic Load Balancer. However, you may find that errors, timeouts, or connection resets are being generated when you make changes to the application or Kubernetes environment. Those changes could trigger an application deployment or a scaling action (either manual or automatic).</p>
<p>Unfortunately, those errors may be generated even when your application is not logging problems. This is because the Kubernetes systems controlling the resources in your cluster may be running faster than the AWS systems that control the Load Balancer's target registration and health. Your Pods may also start receiving traffic before your application is ready to receive requests.</p>
<p>Lets review the process through which a pod becomes Ready and how traffic can be routed into the pods.</p>
<h2 id="networking-loadbalancing-loadbalancing-pod-readiness">Pod Readiness<a class="headerlink" href="#networking-loadbalancing-loadbalancing-pod-readiness" title="Permanent link">&para;</a></h2>
<p>This diagram from a <a href="https://www.youtube.com/watch?v=Vw9GmSeomFg">2019 Kubecon talk</a>, shows the steps taken for a pod to become Ready and receive traffic for a <code>LoadBalancer</code> service:
<img alt="readiness.png" src="../networking/loadbalancing/readiness.png" />
<em><a href="https://www.youtube.com/watch?v=Vw9GmSeomFg">Ready? A Deep Dive into Pod Readiness Gates for Service Health... - Minhan Xia &amp; Ping Zou</a></em><br />
When a pod that is a member of a NodePort Service is created, Kubernetes will go through the following steps:</p>
<ol>
<li>The pod is created on the Kubernetes control plane (i.e. from a <code>kubectl</code> command or scaling action).</li>
<li>The pod is scheduled by the <code>kube-scheduler</code> and is assigned to a node in the cluster. </li>
<li>The kubelet running on the assigned node will receive the update (via <code>watch</code>) and will communicate with it’s local Container Runtime to start the containers specified in the pod. <ol>
<li>When the containers have started running (and optionally pass <code>ReadinessProbes</code>), the kubelet will update the pod status to <code>Ready</code> by sending an update to the <code>kube-apiserver</code></li>
</ol>
</li>
<li>The Endpoint Controller will receive an update (via <code>watch</code>) that there is a new pod that is <code>Ready</code> to be added to the Endpoints list for the Service and will add the pod IP/Port tuples to the appropriate endpoints array.</li>
<li><code>kube-proxy</code> receives an update (via <code>watch</code>) that there is a new IP/Port to add to the iptables rules for the Service.<ol>
<li>The local iptables rules on the worker node will be updated with the additional target pod for the NodePort Service.</li>
</ol>
</li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When using an Ingress resource and Ingress Controller (like the AWS Load Balancer Controller) step 5 is handled by the relevant controller instead of <code>kube-proxy</code>. The controller will then take the necessary configuration steps (such as registering/deregistering the target to a load balancer) to allow traffic to flow as expected.</p>
</div>
<p><a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination">When a pod is terminated</a>, or changes to a not-ready state, a similar process occurs. The API server will receive either an update from a controller, kubelet, or kubectl client to terminate the pod. Steps 3-5 continue from there but will remove the Pod IP/Tuple from the endpoints list and iptables rules rather than insert.</p>
<h3 id="networking-loadbalancing-loadbalancing-impact-on-deployments">Impact on Deployments<a class="headerlink" href="#networking-loadbalancing-loadbalancing-impact-on-deployments" title="Permanent link">&para;</a></h3>
<p>Below is a diagram showing the steps taken when an application deployment triggers the replacement of pods:
<img alt="deployments.png" src="../networking/loadbalancing/deployments.png" />
<em><a href="https://www.youtube.com/watch?v=Vw9GmSeomFg">Ready? A Deep Dive into Pod Readiness Gates for Service Health... - Minhan Xia &amp; Ping Zou</a></em><br />
Of note in this diagram is that the second Pod will not be deployed until the first pod has reached the “Ready” state. Steps 4 and 5 from the previous section will also happen in parallel with the deployment actions above.</p>
<p>This means that the actions to propagate the new pod status may still be in-progress when the Deployment controller moves on to the next pods. Since this process also terminates the older version of pods, this could lead to a situation where the pods have reached a Ready status but those changes are still being propagated and the older versions of the pod have been terminated. </p>
<p>This problem is exacerbated when working with load balancers from Cloud Providers like AWS as the Kubernetes systems described above do not, by default, take into account registration times or health checks on the Load Balancer. <strong>This means the Deployment update could completely cycle through the pods, but the Load Balancer has not finished performing the health checks or registrating the new Pods which could cause an outage.</strong></p>
<p>A similar problem occurs when a pod is terminated. Depending on the Load Balancer configuration the Pod may take a minute or two to deregister and stop taking new requests. <strong>Kubernetes does not delay rolling deployments for this deregistration, this can lead to a state where the Load Balancer is still sending traffic to the IP/port for a target Pod that has already been terminated.</strong></p>
<p>To avoid these problems we can add configuration to ensure the Kubernetes systems take actions more in line with the AWS Load Balancer behavior.</p>
<h2 id="networking-loadbalancing-loadbalancing-recommendations">Recommendations<a class="headerlink" href="#networking-loadbalancing-loadbalancing-recommendations" title="Permanent link">&para;</a></h2>
<h3 id="networking-loadbalancing-loadbalancing-use-ip-target-type-load-balancers">Use IP Target-Type Load Balancers<a class="headerlink" href="#networking-loadbalancing-loadbalancing-use-ip-target-type-load-balancers" title="Permanent link">&para;</a></h3>
<p>When creating a <code>LoadBalancer</code> type service the traffic is sent from the load balancer to <em>any node in the cluster</em> via <strong>Instance target type</strong> registration. Each node then redirects the traffic from the <code>NodePort</code> to a Pod/IP tuple in the Service’s Endpoints array, this target could be running on a separate worker node</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Remember that array should only have “Ready” pods</p>
</div>
<p><img alt="nodeport.png" src="../networking/loadbalancing/nodeport.png" /></p>
<p>This adds an additional hop to your request, and adds complexity to the Load Balancer configuration. For example, if the Load Balancer above was configured with session affinity, that affinity could only hold between the load balancer and the backend node (depending on the affinity configuration). </p>
<p>Since the Load Balancer is not communicating with the backend Pod directly, controlling the traffic flow and timing with the Kubernetes systems becomes more difficult.</p>
<p>When using the <a href="https://github.com/kubernetes-sigs/aws-load-balancer-controller">AWS Load Balancer Controller</a>, <strong>IP target type</strong> can be used to register the Pod IP/Port tuples with the Load Balancer directly:
<img alt="ip.png" src="../networking/loadbalancing/ip.png" /><br />
This simplifies the traffic path from the Load Balancer to the target Pods. This means when a new target is registered we can be sure that target is a “Ready” Pod IP and port, the health checks from the load balancer will hit the Pod Directly, and when reviewing VPC flow logs or monitoring utilities traffic between the Load Balancer and the Pods will be easily traceable.</p>
<p>Using IP registration also allows us to control the timing and configuration of the traffic directly against the backend Pods, rather than trying to manage connections through the <code>NodePort</code> rules as well.</p>
<h3 id="networking-loadbalancing-loadbalancing-utilize-pod-readiness-gates">Utilize Pod Readiness Gates<a class="headerlink" href="#networking-loadbalancing-loadbalancing-utilize-pod-readiness-gates" title="Permanent link">&para;</a></h3>
<p><a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-readiness-gate">Pod Readiness Gates</a> are additional requirements that must be met before the Pod is allowed to reach the “Ready” state.</p>
<blockquote>
<p><a href="https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.4/deploy/pod_readiness_gate/">[...] the AWS Load Balancer controller can set the readiness condition on the pods that constitute your ingress or service backend. The condition status on a pod will be set to <code>True</code> only when the corresponding target in the ALB/NLB target group shows a health state of »Healthy«. This prevents the rolling update of a deployment from terminating old pods until the newly created pods are »Healthy« in the ALB/NLB target group and ready to take traffic.</a></p>
</blockquote>
<p>The readiness gates ensure that Kubernetes doesn’t move “too fast” when creating new replicas during a deployment and avoids the situation where Kubernetes has completed the deployment but the new Pods have not completed registration.</p>
<p>To enable these you will need to:</p>
<ol>
<li>Deploy the latest version of the <a href="https://github.com/kubernetes-sigs/aws-load-balancer-controller">AWS Load Balancer Controller</a> (<strong><a href="https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.4/deploy/upgrade/migrate_v1_v2/"><em>Refer to the documentation if upgrading older versions</em></a></strong>)</li>
<li><a href="https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.4/deploy/pod_readiness_gate/">Label the namespace where the target pods are running</a> with the <code>elbv2.k8s.aws/pod-readiness-gate-inject: enabled</code> label to inject Pod Readiness Gates automatically.</li>
<li>To ensure all of your pods in a namespace get the readiness gate config, you need create your Ingress or Service and label the namespace <strong><em>before</em></strong> creating the pods.</li>
</ol>
<h3 id="networking-loadbalancing-loadbalancing-ensure-pods-are-deregistered-from-load-balancers-before-termination">Ensure Pods are Deregistered from Load Balancers <em>before</em> Termination<a class="headerlink" href="#networking-loadbalancing-loadbalancing-ensure-pods-are-deregistered-from-load-balancers-before-termination" title="Permanent link">&para;</a></h3>
<p>When a pod is terminated steps 4 and 5 from the pod readiness section occur at the same time that the container processes receive the termination signals. This means that if your container is able to shut down quickly it may shut down faster than the Load Balancer is able to deregister the target. To avoid this situation adjust the Pod spec with:</p>
<ol>
<li>Add a <code>preStop</code> lifecycle hook to allow the application to deregister and gracefully close connections. This hook is called immediately before a container is terminated due to an API request or management event such as a liveness/startup probe failure, preemption, resource contention and others. Critically, <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination">this hook is called and allowed to complete <strong>before</strong> the termination signals are sent</a>, provided the grace period is long enough to accommodate the execution.</li>
</ol>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#networking-loadbalancing-loadbalancing-__codelineno-0-1"></a>        lifecycle:
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#networking-loadbalancing-loadbalancing-__codelineno-0-2"></a>          preStop:
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#networking-loadbalancing-loadbalancing-__codelineno-0-3"></a>            exec:
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#networking-loadbalancing-loadbalancing-__codelineno-0-4"></a>              command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;sleep 180&quot;] 
</code></pre></div>
<p>A simple sleep command like the one above can be used to introduce a short delay between when the pod is marked <code>Terminating</code> (and Load Balancer deregistration begins) and when the termination signal is sent to the container process. If needed this hook can also be leveraged for more advanced application termination/shutdown procedures.</p>
<ol>
<li>Extend the <code>terminationGracePeriodSeconds</code> to accommodate the entire <code>prestop</code> execution time, as well as the time your application takes to gracefully respond to the termination signal. In the example below the grace period is extended to 200s which allows the entire <code>sleep 180</code> command to complete and then an extra 20s just to be sure my app can shutdown gracefully.</li>
</ol>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#networking-loadbalancing-loadbalancing-__codelineno-1-1"></a>    spec:
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#networking-loadbalancing-loadbalancing-__codelineno-1-2"></a>      terminationGracePeriodSeconds: 200
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#networking-loadbalancing-loadbalancing-__codelineno-1-3"></a>      containers:
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#networking-loadbalancing-loadbalancing-__codelineno-1-4"></a>      - name: webapp
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#networking-loadbalancing-loadbalancing-__codelineno-1-5"></a>        image: webapp-st:v1.3
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#networking-loadbalancing-loadbalancing-__codelineno-1-6"></a>        [...]
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#networking-loadbalancing-loadbalancing-__codelineno-1-7"></a>        lifecycle:
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#networking-loadbalancing-loadbalancing-__codelineno-1-8"></a>          preStop:
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#networking-loadbalancing-loadbalancing-__codelineno-1-9"></a>            exec:
<a id="__codelineno-1-10" name="__codelineno-1-10" href="#networking-loadbalancing-loadbalancing-__codelineno-1-10"></a>              command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;sleep 180&quot;] 
</code></pre></div>
<h3 id="networking-loadbalancing-loadbalancing-ensure-pods-have-readiness-probes">Ensure Pods have Readiness Probes<a class="headerlink" href="#networking-loadbalancing-loadbalancing-ensure-pods-have-readiness-probes" title="Permanent link">&para;</a></h3>
<p>When creating Pods in Kubernetes the default Readiness state is “Ready”, however most applications take a moment or two to instantiate and become ready for requests. <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/">You can define a <code>readinessProbe</code> in the Pod spec</a> with an exec command or network request that is used to determine if the application has completed its start up and is ready for traffic. </p>
<p>Pods that are created with a <code>readinessProbe</code> defined start in a “NotReady” state, and only change to “Ready” when the <code>readinessProbe</code> is successful. This ensures that applications are not put “in-service” until the application has completed startup.</p>
<p>Liveness probes are recommended to allow for application restarts when entering a broken state, e.g. deadlocks, however care should be taken with stateful applications as liveness failures will trigger a restart of the application. <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-startup-probes">Startup probes</a> can also be leveraged for applications that are slow to start.</p>
<p>The below probes use HTTP probes against port 80 to check when the web application becomes ready (the same probe configuration is also used for the liveness probe):</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#networking-loadbalancing-loadbalancing-__codelineno-2-1"></a>        [...]
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#networking-loadbalancing-loadbalancing-__codelineno-2-2"></a>        ports:
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#networking-loadbalancing-loadbalancing-__codelineno-2-3"></a>        - containerPort: 80
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#networking-loadbalancing-loadbalancing-__codelineno-2-4"></a>        livenessProbe:
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#networking-loadbalancing-loadbalancing-__codelineno-2-5"></a>          httpGet:
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#networking-loadbalancing-loadbalancing-__codelineno-2-6"></a>            path: /
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#networking-loadbalancing-loadbalancing-__codelineno-2-7"></a>            port: 80
<a id="__codelineno-2-8" name="__codelineno-2-8" href="#networking-loadbalancing-loadbalancing-__codelineno-2-8"></a>          failureThreshold: 1
<a id="__codelineno-2-9" name="__codelineno-2-9" href="#networking-loadbalancing-loadbalancing-__codelineno-2-9"></a>          periodSeconds: 10
<a id="__codelineno-2-10" name="__codelineno-2-10" href="#networking-loadbalancing-loadbalancing-__codelineno-2-10"></a>          initialDelaySeconds: 5
<a id="__codelineno-2-11" name="__codelineno-2-11" href="#networking-loadbalancing-loadbalancing-__codelineno-2-11"></a>        readinessProbe:
<a id="__codelineno-2-12" name="__codelineno-2-12" href="#networking-loadbalancing-loadbalancing-__codelineno-2-12"></a>          httpGet:
<a id="__codelineno-2-13" name="__codelineno-2-13" href="#networking-loadbalancing-loadbalancing-__codelineno-2-13"></a>            path: /
<a id="__codelineno-2-14" name="__codelineno-2-14" href="#networking-loadbalancing-loadbalancing-__codelineno-2-14"></a>            port: 80
<a id="__codelineno-2-15" name="__codelineno-2-15" href="#networking-loadbalancing-loadbalancing-__codelineno-2-15"></a>          periodSeconds: 5
<a id="__codelineno-2-16" name="__codelineno-2-16" href="#networking-loadbalancing-loadbalancing-__codelineno-2-16"></a>        [...]
</code></pre></div>
<h3 id="networking-loadbalancing-loadbalancing-configure-a-pod-disruption-budget">Configure a Pod Disruption Budget<a class="headerlink" href="#networking-loadbalancing-loadbalancing-configure-a-pod-disruption-budget" title="Permanent link">&para;</a></h3>
<p>A <a href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#pod-disruption-budgets">Pod Disruption Budget (PDB)</a> limits the number of Pods of a replicated application that are down simultaneously from <a href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#voluntary-and-involuntary-disruptions">voluntary disruptions</a>. For example, a quorum-based application would like to ensure that the number of replicas running is never brought below the number needed for a quorum. A web front end might want to ensure that the number of replicas serving load never falls below a certain percentage of the total.</p>
<p>The PDB will protect the application against things like the nodes being drained, or application deployments. The PDB ensures that a minimum number or percentage of pods remain available while taking these actions. </p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>PDB’s will NOT protect the application against involuntary disruptions like a failure in the host OS or loss of network connectivity. </p>
</div>
<p>The example below ensures that there is always at least 1 Pod available with the label <code>app: echoserver</code>. <a href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/#think-about-how-your-application-reacts-to-disruptions">You can configure the correct replica count for your application or use a percentage</a>:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#networking-loadbalancing-loadbalancing-__codelineno-3-1"></a>apiVersion: policy/v1beta1
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#networking-loadbalancing-loadbalancing-__codelineno-3-2"></a>kind: PodDisruptionBudget
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#networking-loadbalancing-loadbalancing-__codelineno-3-3"></a>metadata:
<a id="__codelineno-3-4" name="__codelineno-3-4" href="#networking-loadbalancing-loadbalancing-__codelineno-3-4"></a>  name: echoserver-pdb
<a id="__codelineno-3-5" name="__codelineno-3-5" href="#networking-loadbalancing-loadbalancing-__codelineno-3-5"></a>  namespace: echoserver
<a id="__codelineno-3-6" name="__codelineno-3-6" href="#networking-loadbalancing-loadbalancing-__codelineno-3-6"></a>spec:
<a id="__codelineno-3-7" name="__codelineno-3-7" href="#networking-loadbalancing-loadbalancing-__codelineno-3-7"></a>  minAvailable: 1
<a id="__codelineno-3-8" name="__codelineno-3-8" href="#networking-loadbalancing-loadbalancing-__codelineno-3-8"></a>  selector:
<a id="__codelineno-3-9" name="__codelineno-3-9" href="#networking-loadbalancing-loadbalancing-__codelineno-3-9"></a>    matchLabels:
<a id="__codelineno-3-10" name="__codelineno-3-10" href="#networking-loadbalancing-loadbalancing-__codelineno-3-10"></a>      app: echoserver
</code></pre></div>
<h3 id="networking-loadbalancing-loadbalancing-gracefully-handle-termination-signals">Gracefully handle Termination Signals<a class="headerlink" href="#networking-loadbalancing-loadbalancing-gracefully-handle-termination-signals" title="Permanent link">&para;</a></h3>
<p>When a pod is Terminated the application running inside the container will receive two <a href="https://www.gnu.org/software/libc/manual/html_node/Standard-Signals.html">Signals</a>. The first is the <a href="https://www.gnu.org/software/libc/manual/html_node/Termination-Signals.html"><code>SIGTERM</code> signal</a>, which is a “polite” request that the process cease execution. This signal can be blocked or the application could simply ignore this signal, so after the <code>terminationGracePeriodSeconds</code> has elapsed the application will receive the <a href="https://www.gnu.org/software/libc/manual/html_node/Termination-Signals.html"><code>SIGKILL</code> signal</a>. <code>SIGKILL</code> is used to forcibly stop a process, it cannot be <a href="https://man7.org/linux/man-pages/man7/signal.7.html">blocked, handled or ignored</a>, and is therefore always fatal.</p>
<p>These Signals are used by the container runtime to trigger your application to shutdown. The <code>SIGTERM</code> signal will also be sent <strong>after</strong> the <code>preStop</code> hook has executed. With the above configuration the <code>preStop</code> hook will ensure the pod has been deregistered from the Load Balancer, so the application can then gracefully closes any remaining open connections when the <code>SIGTERM</code> signal is received. </p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><a href="https://petermalmgren.com/signal-handling-docker/">Signal handling in container environments can be complicated when using “wrapper scripts” for the entrypoint of your application</a> as the script will be PID 1 and may not forward the signal to your application.</p>
</div>
<h3 id="networking-loadbalancing-loadbalancing-be-wary-of-the-deregistration-delay">Be Wary of the Deregistration Delay<a class="headerlink" href="#networking-loadbalancing-loadbalancing-be-wary-of-the-deregistration-delay" title="Permanent link">&para;</a></h3>
<p>Elastic Load Balancing stops sending requests to targets that are deregistering. By default, Elastic Load Balancing waits 300 seconds before completing the deregistration process, which can help in-flight requests to the target to complete. To change the amount of time that Elastic Load Balancing waits, update the deregistration delay value.
The initial state of a deregistering target is <code>draining</code>. After the deregistration delay elapses, the deregistration process completes and the state of the target is <code>unused</code>. If the target is part of an Auto Scaling group, it can be terminated and replaced.</p>
<p>If a deregistering target has no in-flight requests and no active connections, Elastic Load Balancing immediately completes the deregistration process, without waiting for the deregistration delay to elapse. </p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Even though target deregistration is complete, the status of the target is displayed as <code>draining</code> until the deregistration delay timeout expires. After the timeout expires, the target transitions to an <code>unused</code> state.</p>
</div>
<p><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#deregistration-delay">If a deregistering target terminates the connection before the deregistration delay elapses, the client receives a 500-level error response</a>.</p>
<p>This can be configured using annotations on the Ingress resource using the<a href="https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.4/guide/ingress/annotations/#target-group-attributes"><code>alb.ingress.kubernetes.io/target-group-attributes</code> annotation</a>. Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#networking-loadbalancing-loadbalancing-__codelineno-4-1"></a>apiVersion: networking.k8s.io/v1
<a id="__codelineno-4-2" name="__codelineno-4-2" href="#networking-loadbalancing-loadbalancing-__codelineno-4-2"></a>kind: Ingress
<a id="__codelineno-4-3" name="__codelineno-4-3" href="#networking-loadbalancing-loadbalancing-__codelineno-4-3"></a>metadata:
<a id="__codelineno-4-4" name="__codelineno-4-4" href="#networking-loadbalancing-loadbalancing-__codelineno-4-4"></a>  name: echoserver-ip
<a id="__codelineno-4-5" name="__codelineno-4-5" href="#networking-loadbalancing-loadbalancing-__codelineno-4-5"></a>  namespace: echoserver
<a id="__codelineno-4-6" name="__codelineno-4-6" href="#networking-loadbalancing-loadbalancing-__codelineno-4-6"></a>  annotations:
<a id="__codelineno-4-7" name="__codelineno-4-7" href="#networking-loadbalancing-loadbalancing-__codelineno-4-7"></a>    alb.ingress.kubernetes.io/scheme: internet-facing
<a id="__codelineno-4-8" name="__codelineno-4-8" href="#networking-loadbalancing-loadbalancing-__codelineno-4-8"></a>    alb.ingress.kubernetes.io/target-type: ip
<a id="__codelineno-4-9" name="__codelineno-4-9" href="#networking-loadbalancing-loadbalancing-__codelineno-4-9"></a>    alb.ingress.kubernetes.io/load-balancer-name: echoserver-ip
<a id="__codelineno-4-10" name="__codelineno-4-10" href="#networking-loadbalancing-loadbalancing-__codelineno-4-10"></a>    alb.ingress.kubernetes.io/target-group-attributes: deregistration_delay.timeout_seconds=30
<a id="__codelineno-4-11" name="__codelineno-4-11" href="#networking-loadbalancing-loadbalancing-__codelineno-4-11"></a>spec:
<a id="__codelineno-4-12" name="__codelineno-4-12" href="#networking-loadbalancing-loadbalancing-__codelineno-4-12"></a>  ingressClassName: alb
<a id="__codelineno-4-13" name="__codelineno-4-13" href="#networking-loadbalancing-loadbalancing-__codelineno-4-13"></a>  rules:
<a id="__codelineno-4-14" name="__codelineno-4-14" href="#networking-loadbalancing-loadbalancing-__codelineno-4-14"></a>    - host: echoserver.example.com
<a id="__codelineno-4-15" name="__codelineno-4-15" href="#networking-loadbalancing-loadbalancing-__codelineno-4-15"></a>      http:
<a id="__codelineno-4-16" name="__codelineno-4-16" href="#networking-loadbalancing-loadbalancing-__codelineno-4-16"></a>        paths:
<a id="__codelineno-4-17" name="__codelineno-4-17" href="#networking-loadbalancing-loadbalancing-__codelineno-4-17"></a>          - path: /
<a id="__codelineno-4-18" name="__codelineno-4-18" href="#networking-loadbalancing-loadbalancing-__codelineno-4-18"></a>            pathType: Exact
<a id="__codelineno-4-19" name="__codelineno-4-19" href="#networking-loadbalancing-loadbalancing-__codelineno-4-19"></a>            backend:
<a id="__codelineno-4-20" name="__codelineno-4-20" href="#networking-loadbalancing-loadbalancing-__codelineno-4-20"></a>              service:
<a id="__codelineno-4-21" name="__codelineno-4-21" href="#networking-loadbalancing-loadbalancing-__codelineno-4-21"></a>                name: echoserver-service
<a id="__codelineno-4-22" name="__codelineno-4-22" href="#networking-loadbalancing-loadbalancing-__codelineno-4-22"></a>                port:
<a id="__codelineno-4-23" name="__codelineno-4-23" href="#networking-loadbalancing-loadbalancing-__codelineno-4-23"></a>                  number: 8080
</code></pre></div></section><section class="print-page" id="networking-monitoring"><h1 id="networking-monitoring-monitoring-eks-workloads-for-network-performance-issues">Monitoring EKS workloads for Network performance issues<a class="headerlink" href="#networking-monitoring-monitoring-eks-workloads-for-network-performance-issues" title="Permanent link">&para;</a></h1>
<h2 id="networking-monitoring-monitoring-coredns-traffic-for-dns-throttling-issues">Monitoring CoreDNS traffic for DNS throttling issues<a class="headerlink" href="#networking-monitoring-monitoring-coredns-traffic-for-dns-throttling-issues" title="Permanent link">&para;</a></h2>
<p>Running DNS intensive workloads can sometimes experience intermittent CoreDNS failures due to DNS throttling, and this can impact applications where you may encounter occasional UnknownHostException errors.</p>
<p>The Deployment for CoreDNS has an anti-affinity policy that instructs the Kubernetes scheduler to run instances of CoreDNS on separate worker nodes in the cluster, i.e. it should avoid co-locating replicas on the same worker node. This effectively reduces the number of DNS queries per network interface because traffic from each replica is routed through a different ENI. If you notice that DNS queries are being throttled because of the 1024 packets per second limit, you can 1) try increasing the number of CoreDNS replicas or 2) implement <a href="https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/">NodeLocal DNSCache</a>. See <a href="https://aws.github.io/aws-eks-best-practices/reliability/docs/dataplane/#monitor-coredns-metrics">Monitor CoreDNS Metrics</a> for further information.</p>
<h3 id="networking-monitoring-challenge">Challenge<a class="headerlink" href="#networking-monitoring-challenge" title="Permanent link">&para;</a></h3>
<ul>
<li>Packet drop happens in seconds and it can be tricky for us to properly monitor these patterns to determine if DNS throttling is actually happening.</li>
<li>DNS queries are throttled at the elastic network interface level. So, throttled queries don't appear in the query logging. </li>
<li>Flow logs do not capture all IP traffic. E.g. Traffic generated by instances when they contact the Amazon DNS server. If you use your own DNS server, then all traffic to that DNS server is logged</li>
</ul>
<h3 id="networking-monitoring-solution">Solution<a class="headerlink" href="#networking-monitoring-solution" title="Permanent link">&para;</a></h3>
<p>An easy way to identify the DNS throttling issues in worker nodes is by capturing <code>linklocal_allowance_exceeded</code> metric. The <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html#linux-metrics-enabled-by-CloudWatch-agent">linklocal_allowance_exceeded</a> is number of packets dropped because the PPS of the traffic to local proxy services exceeded the maximum for the network interface. This impacts traffic to the DNS service, the Instance Metadata Service, and the Amazon Time Sync Service. Instead of tracking this event real-time, we can stream this metric to <a href="https://aws.amazon.com/prometheus/">Amazon Managed Service for Prometheus</a> as well and can have them visualized in <a href="https://aws.amazon.com/grafana/">Amazon Managed Grafana</a></p>
<h2 id="networking-monitoring-monitoring-dns-query-delays-using-conntrack-metrics">Monitoring DNS query delays using Conntrack metrics<a class="headerlink" href="#networking-monitoring-monitoring-dns-query-delays-using-conntrack-metrics" title="Permanent link">&para;</a></h2>
<p>Another metric that can help in monitoring the CoreDNS throttling / query delay are <code>conntrack_allowance_available</code> and <code>conntrack_allowance_exceeded</code>.
Connectivity failures caused by exceeding Connections Tracked allowances can have a larger impact than those resulting from exceeding other allowances. When relying on TCP to transfer data, packets that are queued or dropped due to exceeding EC2 instance network allowances, such as Bandwidth, PPS, etc., are typically handled gracefully thanks to TCP’s congestion control capabilities. Impacted flows will be slowed down, and lost packets will be retransmitted. However, when an instance exceeds its Connections Tracked allowance, no new connections can be established until some of the existing ones are closed to make room for new connections. </p>
<p><code>conntrack_allowance_available</code> and <code>conntrack_allowance_exceeded</code> helps customers in monitoring the connections tracked allowance which varies for every instance. These network performance metrics give customers visibility into the number of packets queued or dropped when an instance’s networking allowances, such as Network Bandwidth, Packets-Per-Second (PPS), Connections Tracked, and Link-local service access (Amazon DNS, Instance Meta Data Service, Amazon Time Sync) are exceeded</p>
<p><code>conntrack_allowance_available</code> is the number of tracked connections that can be established by the instance before hitting the Connections Tracked allowance of that instance type (supported for nitro-based instance only). 
<code>conntrack_allowance_exceeded</code> is the number of packets dropped because connection tracking exceeded the maximum for the instance and new connections could not be established. </p>
<h2 id="networking-monitoring-other-important-network-performance-metrics">Other important Network performance metrics<a class="headerlink" href="#networking-monitoring-other-important-network-performance-metrics" title="Permanent link">&para;</a></h2>
<p>Other important network performance metrics include:</p>
<p><code>bw_in_allowance_exceeded</code> (ideal value of the metric should be zero) is the number of packets queued and/or dropped because the inbound aggregate bandwidth exceeded the maximum for the instance</p>
<p><code>bw_out_allowance_exceeded</code> (ideal value of the metric should be zero) is the number of packets queued and/or dropped because the outbound aggregate bandwidth exceeded the maximum for the instance</p>
<p><code>pps_allowance_exceeded</code> (ideal value of the metric should be zero) is the number of packets queued and/or dropped because the bidirectional PPS exceeded the maximum for the instance</p>
<h2 id="networking-monitoring-capturing-the-metrics-to-monitor-workloads-for-network-performance-issues">Capturing the metrics to monitor workloads for network performance issues<a class="headerlink" href="#networking-monitoring-capturing-the-metrics-to-monitor-workloads-for-network-performance-issues" title="Permanent link">&para;</a></h2>
<p>The Elastic Network Adapter (ENA ) driver publishes network performance metrics discussed above from the instances where they are enabled. All the network performance metrics can be published to CloudWatch using the CloudWatch agent. Please refer to the <a href="https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-ec2-instance-level-network-performance-metrics-uncover-new-insights/">blog</a> for more information.</p>
<p>Let's now capture the metrics discussed above, store them in Amazon Managed Service for Prometheus and visualize using Amazon Managed Grafana</p>
<h3 id="networking-monitoring-prerequisites">Prerequisites<a class="headerlink" href="#networking-monitoring-prerequisites" title="Permanent link">&para;</a></h3>
<ul>
<li>ethtool - Ensure the worker nodes have ethtool installed</li>
<li>An AMP workspace configured in your AWS account. For instructions, see <a href="https://docs.aws.amazon.com/prometheus/latest/userguide/AMP-onboard-create-workspace.html">Create a workspace</a> in the AMP User Guide.</li>
<li>Amazon Managed Grafana Workspace</li>
</ul>
<h3 id="networking-monitoring-deploying-prometheus-ethtool-exporter">Deploying Prometheus ethtool exporter<a class="headerlink" href="#networking-monitoring-deploying-prometheus-ethtool-exporter" title="Permanent link">&para;</a></h3>
<p>The deployment contains a python script that pulls information from ethtool and publishes it in prometheus format.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#networking-monitoring-__codelineno-0-1"></a>kubectl apply -f https://raw.githubusercontent.com/Showmax/prometheus-ethtool-exporter/master/deploy/k8s-daemonset.yaml
</code></pre></div>
<h3 id="networking-monitoring-deploy-the-adot-collector-to-scrape-the-ethtool-metrics-and-store-in-amazon-managed-service-for-prometheus-workspace">Deploy the ADOT collector to scrape the ethtool metrics and store in Amazon Managed Service for Prometheus workspace<a class="headerlink" href="#networking-monitoring-deploy-the-adot-collector-to-scrape-the-ethtool-metrics-and-store-in-amazon-managed-service-for-prometheus-workspace" title="Permanent link">&para;</a></h3>
<p>Each cluster where you install AWS Distro for OpenTelemetry (ADOT) must have this role to grant your AWS service account permissions to store metrics into Amazon Managed Service for Prometheus. Follow these steps to create and associate your IAM role to your Amazon EKS service account using IRSA:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#networking-monitoring-__codelineno-1-1"></a>eksctl create iamserviceaccount --name adot-collector --namespace default --cluster &lt;CLUSTER_NAME&gt; --attach-policy-arn arn:aws:iam::aws:policy/AmazonPrometheusRemoteWriteAccess --attach-policy-arn arn:aws:iam::aws:policy/AWSXrayWriteOnlyAccess --attach-policy-arn arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy --region &lt;REGION&gt; --approve  --override-existing-serviceaccounts
</code></pre></div>
<p>Let's deploy the ADOT collector to scrape the metrcis from the prometheus ethtool exporter and store it in Amazon Managed Service for Prometheus</p>
<p>The following procedure uses an example YAML file with deployment as the mode value. This is the default mode and deploys the ADOT Collector similarly to a standalone application. This configuration receives OTLP metrics from the sample application and Amazon Managed Service for Prometheus metrics scraped from pods on the cluster</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#networking-monitoring-__codelineno-2-1"></a>curl -o collector-config-amp.yaml https://raw.githubusercontent.com/aws-observability/aws-otel-community/master/sample-configs/operator/collector-config-amp.yaml
</code></pre></div>
<p>In collector-config-amp.yaml, replace the following with your own values:
* mode: deployment
* serviceAccount: adot-collector
* endpoint: "<YOUR_REMOTE_WRITE_ENDPOINT>"
* region: "<YOUR_AWS_REGION>"
* name: adot-collector</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#networking-monitoring-__codelineno-3-1"></a>kubectl apply -f collector-config-amp.yaml 
</code></pre></div>
<p>Once the adot collector is deployed, the metrics will be stored successfully in Amazon Prometheus</p>
<h3 id="networking-monitoring-configure-alert-manager-in-amazon-managed-service-for-prometheus-to-send-notifications">Configure alert manager in Amazon Managed Service for Prometheus to send notifications<a class="headerlink" href="#networking-monitoring-configure-alert-manager-in-amazon-managed-service-for-prometheus-to-send-notifications" title="Permanent link">&para;</a></h3>
<p>Let's configure recording rules and alerting rules to check for the metrics discussed so far. </p>
<p>We will use the <a href="https://github.com/aws-controllers-k8s/prometheusservice-controller">ACK Controller for Amazon Managed Service for Prometheus</a> to provision the alerting and recording rules.</p>
<p>Let's deploy the ACL controller for the Amazon Managed Service for Prometheus service:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#networking-monitoring-__codelineno-4-1"></a>export SERVICE=prometheusservice
<a id="__codelineno-4-2" name="__codelineno-4-2" href="#networking-monitoring-__codelineno-4-2"></a>export RELEASE_VERSION=`curl -sL https://api.github.com/repos/aws-controllers-k8s/$SERVICE-controller/releases/latest | grep &#39;&quot;tag_name&quot;:&#39; | cut -d&#39;&quot;&#39; -f4`
<a id="__codelineno-4-3" name="__codelineno-4-3" href="#networking-monitoring-__codelineno-4-3"></a>export ACK_SYSTEM_NAMESPACE=ack-system
<a id="__codelineno-4-4" name="__codelineno-4-4" href="#networking-monitoring-__codelineno-4-4"></a>export AWS_REGION=us-east-1
<a id="__codelineno-4-5" name="__codelineno-4-5" href="#networking-monitoring-__codelineno-4-5"></a>aws ecr-public get-login-password --region us-east-1 | helm registry login --username AWS --password-stdin public.ecr.aws
<a id="__codelineno-4-6" name="__codelineno-4-6" href="#networking-monitoring-__codelineno-4-6"></a>helm install --create-namespace -n $ACK_SYSTEM_NAMESPACE ack-$SERVICE-controller \
<a id="__codelineno-4-7" name="__codelineno-4-7" href="#networking-monitoring-__codelineno-4-7"></a>oci://public.ecr.aws/aws-controllers-k8s/$SERVICE-chart --version=$RELEASE_VERSION --set=aws.region=$AWS_REGION
</code></pre></div>
<p>Run the command and after a few moments you should see the following message:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#networking-monitoring-__codelineno-5-1"></a>You are now able to create Amazon Managed Service for Prometheus (AMP) resources!
<a id="__codelineno-5-2" name="__codelineno-5-2" href="#networking-monitoring-__codelineno-5-2"></a>
<a id="__codelineno-5-3" name="__codelineno-5-3" href="#networking-monitoring-__codelineno-5-3"></a>The controller is running in &quot;cluster&quot; mode.
<a id="__codelineno-5-4" name="__codelineno-5-4" href="#networking-monitoring-__codelineno-5-4"></a>
<a id="__codelineno-5-5" name="__codelineno-5-5" href="#networking-monitoring-__codelineno-5-5"></a>The controller is configured to manage AWS resources in region: &quot;us-east-1&quot;
<a id="__codelineno-5-6" name="__codelineno-5-6" href="#networking-monitoring-__codelineno-5-6"></a>
<a id="__codelineno-5-7" name="__codelineno-5-7" href="#networking-monitoring-__codelineno-5-7"></a>The ACK controller has been successfully installed and ACK can now be used to provision an Amazon Managed Service for Prometheus workspace.
</code></pre></div>
<p>Let's now create a yaml file for provisioning the alert manager defnition and rule groups.
Save the below file as <code>rulegroup.yaml</code></p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#networking-monitoring-__codelineno-6-1"></a>apiVersion: prometheusservice.services.k8s.aws/v1alpha1
<a id="__codelineno-6-2" name="__codelineno-6-2" href="#networking-monitoring-__codelineno-6-2"></a>kind: RuleGroupsNamespace
<a id="__codelineno-6-3" name="__codelineno-6-3" href="#networking-monitoring-__codelineno-6-3"></a>metadata:
<a id="__codelineno-6-4" name="__codelineno-6-4" href="#networking-monitoring-__codelineno-6-4"></a>   name: default-rule
<a id="__codelineno-6-5" name="__codelineno-6-5" href="#networking-monitoring-__codelineno-6-5"></a>spec:
<a id="__codelineno-6-6" name="__codelineno-6-6" href="#networking-monitoring-__codelineno-6-6"></a>   workspaceID: &lt;Your WORKSPACE-ID&gt;
<a id="__codelineno-6-7" name="__codelineno-6-7" href="#networking-monitoring-__codelineno-6-7"></a>   name: default-rule
<a id="__codelineno-6-8" name="__codelineno-6-8" href="#networking-monitoring-__codelineno-6-8"></a>   configuration: |
<a id="__codelineno-6-9" name="__codelineno-6-9" href="#networking-monitoring-__codelineno-6-9"></a>     groups:
<a id="__codelineno-6-10" name="__codelineno-6-10" href="#networking-monitoring-__codelineno-6-10"></a>     - name: ppsallowance
<a id="__codelineno-6-11" name="__codelineno-6-11" href="#networking-monitoring-__codelineno-6-11"></a>       rules:
<a id="__codelineno-6-12" name="__codelineno-6-12" href="#networking-monitoring-__codelineno-6-12"></a>       - record: metric:pps_allowance_exceeded
<a id="__codelineno-6-13" name="__codelineno-6-13" href="#networking-monitoring-__codelineno-6-13"></a>         expr: rate(node_net_ethtool{device=&quot;eth0&quot;,type=&quot;pps_allowance_exceeded&quot;}[30s])
<a id="__codelineno-6-14" name="__codelineno-6-14" href="#networking-monitoring-__codelineno-6-14"></a>       - alert: PPSAllowanceExceeded
<a id="__codelineno-6-15" name="__codelineno-6-15" href="#networking-monitoring-__codelineno-6-15"></a>         expr: rate(node_net_ethtool{device=&quot;eth0&quot;,type=&quot;pps_allowance_exceeded&quot;} [30s]) &gt; 0
<a id="__codelineno-6-16" name="__codelineno-6-16" href="#networking-monitoring-__codelineno-6-16"></a>         labels:
<a id="__codelineno-6-17" name="__codelineno-6-17" href="#networking-monitoring-__codelineno-6-17"></a>           severity: critical
<a id="__codelineno-6-18" name="__codelineno-6-18" href="#networking-monitoring-__codelineno-6-18"></a>
<a id="__codelineno-6-19" name="__codelineno-6-19" href="#networking-monitoring-__codelineno-6-19"></a>         annotations:
<a id="__codelineno-6-20" name="__codelineno-6-20" href="#networking-monitoring-__codelineno-6-20"></a>           summary: Connections dropped due to total allowance exceeding for the  (instance {{ $labels.instance }})
<a id="__codelineno-6-21" name="__codelineno-6-21" href="#networking-monitoring-__codelineno-6-21"></a>           description: &quot;PPSAllowanceExceeded is greater than 0&quot;
<a id="__codelineno-6-22" name="__codelineno-6-22" href="#networking-monitoring-__codelineno-6-22"></a>     - name: bw_in
<a id="__codelineno-6-23" name="__codelineno-6-23" href="#networking-monitoring-__codelineno-6-23"></a>       rules:
<a id="__codelineno-6-24" name="__codelineno-6-24" href="#networking-monitoring-__codelineno-6-24"></a>       - record: metric:bw_in_allowance_exceeded
<a id="__codelineno-6-25" name="__codelineno-6-25" href="#networking-monitoring-__codelineno-6-25"></a>         expr: rate(node_net_ethtool{device=&quot;eth0&quot;,type=&quot;bw_in_allowance_exceeded&quot;}[30s])
<a id="__codelineno-6-26" name="__codelineno-6-26" href="#networking-monitoring-__codelineno-6-26"></a>       - alert: BWINAllowanceExceeded
<a id="__codelineno-6-27" name="__codelineno-6-27" href="#networking-monitoring-__codelineno-6-27"></a>         expr: rate(node_net_ethtool{device=&quot;eth0&quot;,type=&quot;bw_in_allowance_exceeded&quot;} [30s]) &gt; 0
<a id="__codelineno-6-28" name="__codelineno-6-28" href="#networking-monitoring-__codelineno-6-28"></a>         labels:
<a id="__codelineno-6-29" name="__codelineno-6-29" href="#networking-monitoring-__codelineno-6-29"></a>           severity: critical
<a id="__codelineno-6-30" name="__codelineno-6-30" href="#networking-monitoring-__codelineno-6-30"></a>
<a id="__codelineno-6-31" name="__codelineno-6-31" href="#networking-monitoring-__codelineno-6-31"></a>         annotations:
<a id="__codelineno-6-32" name="__codelineno-6-32" href="#networking-monitoring-__codelineno-6-32"></a>           summary: Connections dropped due to total allowance exceeding for the  (instance {{ $labels.instance }})
<a id="__codelineno-6-33" name="__codelineno-6-33" href="#networking-monitoring-__codelineno-6-33"></a>           description: &quot;BWInAllowanceExceeded is greater than 0&quot;
<a id="__codelineno-6-34" name="__codelineno-6-34" href="#networking-monitoring-__codelineno-6-34"></a>     - name: bw_out
<a id="__codelineno-6-35" name="__codelineno-6-35" href="#networking-monitoring-__codelineno-6-35"></a>       rules:
<a id="__codelineno-6-36" name="__codelineno-6-36" href="#networking-monitoring-__codelineno-6-36"></a>       - record: metric:bw_out_allowance_exceeded
<a id="__codelineno-6-37" name="__codelineno-6-37" href="#networking-monitoring-__codelineno-6-37"></a>         expr: rate(node_net_ethtool{device=&quot;eth0&quot;,type=&quot;bw_out_allowance_exceeded&quot;}[30s])
<a id="__codelineno-6-38" name="__codelineno-6-38" href="#networking-monitoring-__codelineno-6-38"></a>       - alert: BWOutAllowanceExceeded
<a id="__codelineno-6-39" name="__codelineno-6-39" href="#networking-monitoring-__codelineno-6-39"></a>         expr: rate(node_net_ethtool{device=&quot;eth0&quot;,type=&quot;bw_out_allowance_exceeded&quot;} [30s]) &gt; 0
<a id="__codelineno-6-40" name="__codelineno-6-40" href="#networking-monitoring-__codelineno-6-40"></a>         labels:
<a id="__codelineno-6-41" name="__codelineno-6-41" href="#networking-monitoring-__codelineno-6-41"></a>           severity: critical
<a id="__codelineno-6-42" name="__codelineno-6-42" href="#networking-monitoring-__codelineno-6-42"></a>
<a id="__codelineno-6-43" name="__codelineno-6-43" href="#networking-monitoring-__codelineno-6-43"></a>         annotations:
<a id="__codelineno-6-44" name="__codelineno-6-44" href="#networking-monitoring-__codelineno-6-44"></a>           summary: Connections dropped due to total allowance exceeding for the  (instance {{ $labels.instance }})
<a id="__codelineno-6-45" name="__codelineno-6-45" href="#networking-monitoring-__codelineno-6-45"></a>           description: &quot;BWoutAllowanceExceeded is greater than 0&quot;            
<a id="__codelineno-6-46" name="__codelineno-6-46" href="#networking-monitoring-__codelineno-6-46"></a>     - name: conntrack
<a id="__codelineno-6-47" name="__codelineno-6-47" href="#networking-monitoring-__codelineno-6-47"></a>       rules:
<a id="__codelineno-6-48" name="__codelineno-6-48" href="#networking-monitoring-__codelineno-6-48"></a>       - record: metric:conntrack_allowance_exceeded
<a id="__codelineno-6-49" name="__codelineno-6-49" href="#networking-monitoring-__codelineno-6-49"></a>         expr: rate(node_net_ethtool{device=&quot;eth0&quot;,type=&quot;conntrack_allowance_exceeded&quot;}[30s])
<a id="__codelineno-6-50" name="__codelineno-6-50" href="#networking-monitoring-__codelineno-6-50"></a>       - alert: ConntrackAllowanceExceeded
<a id="__codelineno-6-51" name="__codelineno-6-51" href="#networking-monitoring-__codelineno-6-51"></a>         expr: rate(node_net_ethtool{device=&quot;eth0&quot;,type=&quot;conntrack_allowance_exceeded&quot;} [30s]) &gt; 0
<a id="__codelineno-6-52" name="__codelineno-6-52" href="#networking-monitoring-__codelineno-6-52"></a>         labels:
<a id="__codelineno-6-53" name="__codelineno-6-53" href="#networking-monitoring-__codelineno-6-53"></a>           severity: critical
<a id="__codelineno-6-54" name="__codelineno-6-54" href="#networking-monitoring-__codelineno-6-54"></a>
<a id="__codelineno-6-55" name="__codelineno-6-55" href="#networking-monitoring-__codelineno-6-55"></a>         annotations:
<a id="__codelineno-6-56" name="__codelineno-6-56" href="#networking-monitoring-__codelineno-6-56"></a>           summary: Connections dropped due to total allowance exceeding for the  (instance {{ $labels.instance }})
<a id="__codelineno-6-57" name="__codelineno-6-57" href="#networking-monitoring-__codelineno-6-57"></a>           description: &quot;ConnTrackAllowanceExceeded is greater than 0&quot;
<a id="__codelineno-6-58" name="__codelineno-6-58" href="#networking-monitoring-__codelineno-6-58"></a>     - name: linklocal
<a id="__codelineno-6-59" name="__codelineno-6-59" href="#networking-monitoring-__codelineno-6-59"></a>       rules:
<a id="__codelineno-6-60" name="__codelineno-6-60" href="#networking-monitoring-__codelineno-6-60"></a>       - record: metric:linklocal_allowance_exceeded
<a id="__codelineno-6-61" name="__codelineno-6-61" href="#networking-monitoring-__codelineno-6-61"></a>         expr: rate(node_net_ethtool{device=&quot;eth0&quot;,type=&quot;linklocal_allowance_exceeded&quot;}[30s])
<a id="__codelineno-6-62" name="__codelineno-6-62" href="#networking-monitoring-__codelineno-6-62"></a>       - alert: LinkLocalAllowanceExceeded
<a id="__codelineno-6-63" name="__codelineno-6-63" href="#networking-monitoring-__codelineno-6-63"></a>         expr: rate(node_net_ethtool{device=&quot;eth0&quot;,type=&quot;linklocal_allowance_exceeded&quot;} [30s]) &gt; 0
<a id="__codelineno-6-64" name="__codelineno-6-64" href="#networking-monitoring-__codelineno-6-64"></a>         labels:
<a id="__codelineno-6-65" name="__codelineno-6-65" href="#networking-monitoring-__codelineno-6-65"></a>           severity: critical
<a id="__codelineno-6-66" name="__codelineno-6-66" href="#networking-monitoring-__codelineno-6-66"></a>
<a id="__codelineno-6-67" name="__codelineno-6-67" href="#networking-monitoring-__codelineno-6-67"></a>         annotations:
<a id="__codelineno-6-68" name="__codelineno-6-68" href="#networking-monitoring-__codelineno-6-68"></a>           summary: Packets dropped due to PPS rate allowance exceeded for local services  (instance {{ $labels.instance }})
<a id="__codelineno-6-69" name="__codelineno-6-69" href="#networking-monitoring-__codelineno-6-69"></a>           description: &quot;LinkLocalAllowanceExceeded is greater than 0&quot;
</code></pre></div>
<p>Replace Your WORKSPACE-ID with the Workspace ID of the  workspace you are using. </p>
<p>Let's now configure the alert manager definition. Save the below fie as <code>alertmanager.yaml</code></p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#networking-monitoring-__codelineno-7-1"></a>apiVersion: prometheusservice.services.k8s.aws/v1alpha1  
<a id="__codelineno-7-2" name="__codelineno-7-2" href="#networking-monitoring-__codelineno-7-2"></a>kind: AlertManagerDefinition
<a id="__codelineno-7-3" name="__codelineno-7-3" href="#networking-monitoring-__codelineno-7-3"></a>metadata:
<a id="__codelineno-7-4" name="__codelineno-7-4" href="#networking-monitoring-__codelineno-7-4"></a>  name: alert-manager
<a id="__codelineno-7-5" name="__codelineno-7-5" href="#networking-monitoring-__codelineno-7-5"></a>spec:
<a id="__codelineno-7-6" name="__codelineno-7-6" href="#networking-monitoring-__codelineno-7-6"></a>  workspaceID: &lt;Your WORKSPACE-ID &gt;
<a id="__codelineno-7-7" name="__codelineno-7-7" href="#networking-monitoring-__codelineno-7-7"></a>  configuration: |
<a id="__codelineno-7-8" name="__codelineno-7-8" href="#networking-monitoring-__codelineno-7-8"></a>    alertmanager_config: |
<a id="__codelineno-7-9" name="__codelineno-7-9" href="#networking-monitoring-__codelineno-7-9"></a>      route:
<a id="__codelineno-7-10" name="__codelineno-7-10" href="#networking-monitoring-__codelineno-7-10"></a>         receiver: default_receiver
<a id="__codelineno-7-11" name="__codelineno-7-11" href="#networking-monitoring-__codelineno-7-11"></a>       receivers:
<a id="__codelineno-7-12" name="__codelineno-7-12" href="#networking-monitoring-__codelineno-7-12"></a>       - name: default_receiver
<a id="__codelineno-7-13" name="__codelineno-7-13" href="#networking-monitoring-__codelineno-7-13"></a>          sns_configs:
<a id="__codelineno-7-14" name="__codelineno-7-14" href="#networking-monitoring-__codelineno-7-14"></a>          - topic_arn: TOPIC-ARN
<a id="__codelineno-7-15" name="__codelineno-7-15" href="#networking-monitoring-__codelineno-7-15"></a>            sigv4:
<a id="__codelineno-7-16" name="__codelineno-7-16" href="#networking-monitoring-__codelineno-7-16"></a>              region: REGION
<a id="__codelineno-7-17" name="__codelineno-7-17" href="#networking-monitoring-__codelineno-7-17"></a>            message: |
<a id="__codelineno-7-18" name="__codelineno-7-18" href="#networking-monitoring-__codelineno-7-18"></a>              alert_type: {{ .CommonLabels.alertname }}
<a id="__codelineno-7-19" name="__codelineno-7-19" href="#networking-monitoring-__codelineno-7-19"></a>              event_type: {{ .CommonLabels.event_type }}     
</code></pre></div>
<p>Replace You WORKSPACE-ID with the Workspace ID of the new workspace, TOPIC-ARN with the ARN of an <a href="https://aws.amazon.com/sns/">Amazon Simple Notification Service</a> topic where you want to send the alerts, and REGION with the current region of the workload. Make sure that your workspace has permissions to send messages to Amazon SNS.</p>
<h3 id="networking-monitoring-visualize-ethtool-metrics-in-amazon-managed-grafana">Visualize ethtool metrics in Amazon Managed Grafana<a class="headerlink" href="#networking-monitoring-visualize-ethtool-metrics-in-amazon-managed-grafana" title="Permanent link">&para;</a></h3>
<p>Let's visualize the metrics within the Amazon Managed Grafana and build a dashboard. Configure the Amazon Managed Service for Prometheus as a datasource inside the Amazon Managed Grafana console. For instructions, see <a href="https://docs.aws.amazon.com/grafana/latest/userguide/AMP-adding-AWS-config.html">Add Amazon Prometheus as a datasource</a></p>
<p>Let's explore the metrics in Amazon Managed Grafana now:
Click the explore button, and search for ethtool:</p>
<p><img alt="Node_ethtool metrics" src="../networking/monitoring/explore_metrics.png" /></p>
<p>Let's build a dashboard for the linklocal_allowance_exceeded metric by using the query <code>rate(node_net_ethtool{device="eth0",type="linklocal_allowance_exceeded"}[30s])</code>. It will result in the below dashboard. </p>
<p><img alt="linklocal_allowance_exceeded dashboard" src="../networking/monitoring/linklocal.png" /></p>
<p>We can clearly see that there were no packets dropped as the value is zero. </p>
<p>Let's build a dashboard for the conntrack_allowance_exceeded metric by using the query <code>rate(node_net_ethtool{device="eth0",type="conntrack_allowance_exceeded"}[30s])</code>. It will result in the below dashboard. </p>
<p><img alt="conntrack_allowance_exceeded dashboard" src="../networking/monitoring/conntrack.png" /></p>
<p>The metric <code>conntrack_allowance_exceeded</code> can be visualized in CloudWatch, provided you run a cloudwatch agent as described <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Agent-network-performance.html">here</a>. The resulting dashboard in CloudWatch will look like below:</p>
<p><img alt="CW_NW_Performance" src="../networking/monitoring/cw_metrics.png" /></p>
<p>We can clearly see that there were no packets dropped as the value is zero. If you are using Nitro-based instances, you can create a similar dashboard for <code>conntrack_allowance_available</code> and pro-actively monitor the connections in your EC2 instance. You can further extend this by configuring alerts in Amazon Managed Grafana to send notifications to Slack, SNS, Pagerduty etc.</p></section><h1 class='nav-section-title-end'>Ended: Networking</h1>
                        <h1 class='nav-section-title' id='section-scalability'>
                            Scalability <a class='headerlink' href='#section-scalability' title='Permanent link'>↵</a>
                        </h1>
                        <section class="print-page" id="scalability-docs"><h1 id="scalability-docs-eks-scalability-best-practices">EKS Scalability best practices<a class="headerlink" href="#scalability-docs-eks-scalability-best-practices" title="Permanent link">&para;</a></h1>
<p>This guide provides advice for scaling EKS clusters. The goal of scaling an EKS cluster is to maximize the amount of work a single cluster can perform. Using a single, large EKS cluster can reduce operational load compared to using multiple clusters, but it has trade-offs for things like multi-region deployments, tenant isolation, and cluster upgrades. In this document we will focus on how to achieve maximum scalability with a single cluster.</p>
<h2 id="scalability-docs-how-to-use-this-guide">How to use this guide<a class="headerlink" href="#scalability-docs-how-to-use-this-guide" title="Permanent link">&para;</a></h2>
<p>This guide is meant for developers and administrators responsible for creating and managing EKS clusters in AWS. It focuses on some generic Kubernetes scaling practices, but it does not have specifics for self-managed Kubernetes clusters or clusters that run outside of an AWS region with <a href="https://anywhere.eks.amazonaws.com/">EKS Anywhere</a>.</p>
<p>Each topic has a brief overview, followed by recommendations and best practices for operating EKS clusters at scale. Topics do not need to be read in a particular order and recommendations should not be applied without testing and verifying they work in your clusters.</p>
<h2 id="scalability-docs-understanding-scaling-dimensions">Understanding scaling dimensions<a class="headerlink" href="#scalability-docs-understanding-scaling-dimensions" title="Permanent link">&para;</a></h2>
<p>Scalability is different from performance and <a href="https://aws.github.io/aws-eks-best-practices/reliability/docs/">reliability</a>, and all three should be considered when planning your cluster and workload needs. As clusters scale, they need to be monitored, but this guide will not cover monitoring best practices. EKS can scale to large sizes, but you will need to plan how you are going to scale a cluster beyond 300 nodes or 5000 pods. These are not absolute numbers, but they come from collaborating this guide with multiple users, engineers, and support professionals.</p>
<p>Scaling in Kubernetes is multi-dimensional and there are no specific settings or recommendations that work in every situation. The main areas areas where we can provide guidance for scaling include:</p>
<ul>
<li><a href="#scalability-docs-control-plane">Kubernetes Control Plane</a></li>
<li><a href="#scalability-docs-data-plane">Kubernetes Data Plane</a></li>
<li><a href="#scalability-docs-cluster-services">Cluster Services</a></li>
<li><a href="#scalability-docs-workloads">Workloads</a></li>
</ul>
<p><strong>Kubernetes Control Plane</strong> in an EKS cluster includes all of the services AWS runs and scales for you automatically (e.g. Kubernetes API server). Scaling the Control Plane is AWS's responsibility, but using the Control Plane responsibly is your responsibility.</p>
<p><strong>Kubernetes Data Plane</strong> scaling deals with AWS resources that are required for your cluster and workloads, but they are outside of the EKS Control Plane. Resources including EC2 instances, kublet, and storage all need to be scaled as your cluster scales.</p>
<p><strong>Cluster services</strong> are Kubernetes controllers and applications that run inside the cluster and provide functionality for your cluster and workloads. These can be <a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-add-ons.html">EKS Add-ons</a> and also other services or Helm charts you install for compliance and integrations. These services are often depended on by workloads and as your workloads scale your cluster services will need to scale with them.</p>
<p><strong>Workloads</strong> are the reason you have a cluster and should scale horizontally with the cluster. There are integrations and settings that workloads have in Kubernetes that can help the cluster scale. There are also architectural considerations with Kubernetes abstractions such as namespaces and services.</p>
<h2 id="scalability-docs-extra-large-scaling">Extra large scaling<a class="headerlink" href="#scalability-docs-extra-large-scaling" title="Permanent link">&para;</a></h2>
<p>If you are scaling a single cluster beyond 1000 nodes or 50,000 pods we would love to talk to you. We recommend reaching out to your support team or technical account manager to get in touch with specialists who can help you plan and scale beyond the information provided in this guide.</p></section><section class="print-page" id="scalability-docs-control-plane"><h1 id="scalability-docs-control-plane-kubernetes-control-plane">Kubernetes Control Plane<a class="headerlink" href="#scalability-docs-control-plane-kubernetes-control-plane" title="Permanent link">&para;</a></h1>
<p>The Kubernetes control plane consists of the Kubernetes API Server, Kubernetes Controller Manager, Scheduler and other components that are required for Kubernetes to function. Scalability limits of these components are different depending on what you’re running in the cluster, but the areas with the biggest impact to scaling include the Kubernetes version, utilization, and individual Node scaling.</p>
<h2 id="scalability-docs-control-plane-use-eks-124-or-above">Use EKS 1.24 or above<a class="headerlink" href="#scalability-docs-control-plane-use-eks-124-or-above" title="Permanent link">&para;</a></h2>
<p>EKS 1.24 introduced a number of changes and switches the container runtime to <a href="https://containerd.io/">containerd</a> instead of docker. Containerd helps clusters scale by increasing individual node performance by limiting container runtime features to closely align with Kubernetes’ needs. Containerd is available in every supported version of EKS and if you would like to switch to containerd in versions prior to 1.24 please use the <a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html#containerd-bootstrap"><code>--container-runtime</code> bootstrap flag</a>.</p>
<h2 id="scalability-docs-control-plane-limit-workload-and-node-bursting">Limit workload and node bursting<a class="headerlink" href="#scalability-docs-control-plane-limit-workload-and-node-bursting" title="Permanent link">&para;</a></h2>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>To avoid reaching API limits on the control plane you should limit scaling spikes that increase cluster size by double digit percentages at a time (e.g. 1000 nodes to 1100 nodes or 4000 to 4500 pods at once).</p>
</div>
<p>The EKS control plane will automatically scale as your cluster grows, but there are limits on how fast it will scale. When you first create an EKS cluster the Control Plane will not immediately be able to scale to hundreds of nodes or thousands of pods. To read more about how EKS has made scaling improvements see <a href="https://aws.amazon.com/blogs/containers/amazon-eks-control-plane-auto-scaling-enhancements-improve-speed-by-4x/">this blog post</a>.</p>
<p>Scaling large applications requires infrastructure to adapt to become fully ready (e.g. warming load balancers). To control the speed of scaling make sure you are scaling based on the right metrics for your application. CPU and memory scaling may not accurately predict your application constraints and using custom metrics (e.g. requests per second) in Kubernetes Horizontal Pod Autoscaler (HPA) may be a better scaling option.</p>
<p>To use a custom metric see the examples in the <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics">Kubernetes documentation</a>. If you have more advanced scaling needs or need to scale based on external sources (e.g. AWS SQS queue) then use <a href="https://keda.sh">KEDA</a> for event based workload scaling.</p>
<h2 id="scalability-docs-control-plane-scale-nodes-and-pods-down-safely">Scale nodes and pods down safely<a class="headerlink" href="#scalability-docs-control-plane-scale-nodes-and-pods-down-safely" title="Permanent link">&para;</a></h2>
<h3 id="scalability-docs-control-plane-replace-long-running-instances">Replace long running instances<a class="headerlink" href="#scalability-docs-control-plane-replace-long-running-instances" title="Permanent link">&para;</a></h3>
<p>Replacing nodes regularly keeps your cluster healthy by avoiding configuration drift and issues that only happen after extended uptime (e.g. slow memory leaks). Automated replacement will give you good process and practices for node upgrades and security patching. If every node in your cluster is replaced regularly then there is less toil required to maintain separate processes for ongoing maintenance.</p>
<p>Use Karpenter’s <a href="https://aws.github.io/aws-eks-best-practices/karpenter/#use-timers-ttl-to-automatically-delete-nodes-from-the-cluster">time to live (TTL)</a> settings to replace instances after they’ve been running for a specified amount of time. Self managed node groups can use the <code>max-instance-lifetime</code> setting to cycle nodes automatically. Managed node groups do not currently have this feature but you can track the request <a href="https://github.com/aws/containers-roadmap/issues/1190">here on GitHub</a>.</p>
<h3 id="scalability-docs-control-plane-remove-underutilized-nodes">Remove underutilized nodes<a class="headerlink" href="#scalability-docs-control-plane-remove-underutilized-nodes" title="Permanent link">&para;</a></h3>
<p>You can remove nodes when they have no running workloads using the scale down threshold in the Kubernetes Cluster Autoscaler with the <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-does-scale-down-work"><code>--scale-down-utilization-threshold</code></a> or in Karpenter you can use the <code>ttlSecondsAfterEmpty</code> provisioner setting.</p>
<h3 id="scalability-docs-control-plane-use-pod-disruption-budgets-and-safe-node-shutdown">Use pod disruption budgets and safe node shutdown<a class="headerlink" href="#scalability-docs-control-plane-use-pod-disruption-budgets-and-safe-node-shutdown" title="Permanent link">&para;</a></h3>
<p>Removing pods and nodes from a Kubernetes cluster requires controllers to make updates to multiple resources (e.g. EndpointSlices). Doing this frequently or too quickly can cause API server throttling and application outages as changes propogate to controllers. <a href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/">Pod Disruption Budgets</a> are a best practice to slow down churn to protect workload availability as nodes are removed or rescheduled in a cluster.</p>
<h2 id="scalability-docs-control-plane-use-client-side-cache-when-running-kubectl">Use Client-Side Cache when running Kubectl<a class="headerlink" href="#scalability-docs-control-plane-use-client-side-cache-when-running-kubectl" title="Permanent link">&para;</a></h2>
<p>Using the kubectl command inefficiently can add additional load to the Kubernetes API Server. You should avoid running scripts or automation that uses kubectl repeatedly (e.g. in a for loop) or running commands without a local cache.</p>
<p><code>kubectl</code> has a client-side cache that caches discovery information from the cluster to reduce the amount of API calls required. The cache is enabled by default and is refreshed every 10 minutes.</p>
<p>If you run kubectl from a container or without a client-side cache you may run into API throttling issues. It is recommended to retain your cluster cache by mounting the <code>--cache-dir</code> to avoid making uncessesary API calls.</p>
<h2 id="scalability-docs-control-plane-disable-kubectl-compression">Disable kubectl Compression<a class="headerlink" href="#scalability-docs-control-plane-disable-kubectl-compression" title="Permanent link">&para;</a></h2>
<p>Disabling kubectl compression in your kubeconfig file can reduce API and client CPU usage. By default the server will compress data sent to the client to optimize network bandwidth. This adds CPU load on the client and server for every request and disabling compression can reduce the overhead and latency if you have adequate bandwidth. To disable compression you can use the <code>--disable-compression=true</code> flag or set <code>disable-compression: true</code> in your kubeconfig file.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#scalability-docs-control-plane-__codelineno-0-1"></a>apiVersion: v1
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#scalability-docs-control-plane-__codelineno-0-2"></a>clusters:
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#scalability-docs-control-plane-__codelineno-0-3"></a>- cluster:
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#scalability-docs-control-plane-__codelineno-0-4"></a>    server: serverURL
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#scalability-docs-control-plane-__codelineno-0-5"></a>    disable-compression: true
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#scalability-docs-control-plane-__codelineno-0-6"></a>  name: cluster
</code></pre></div>
<h2 id="scalability-docs-control-plane-shard-cluster-autoscaler">Shard Cluster Autoscaler<a class="headerlink" href="#scalability-docs-control-plane-shard-cluster-autoscaler" title="Permanent link">&para;</a></h2>
<p>The <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/proposals/scalability_tests.md">Kubernetes Cluster Autoscaler has been tested</a> to scale up to 1000 nodes. On a large cluster with more than 1000 nodes, it is recommended to run multiple instances of the Cluster Autoscaler in shard mode. Each Cluster Autoscaler instance is configured to scale a set of node groups. The following example shows 2 cluster autoscaling configurations that are configured to each scale 4 node groups.</p>
<p>ClusterAutoscaler-1</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#scalability-docs-control-plane-__codelineno-1-1"></a>autoscalingGroups:
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#scalability-docs-control-plane-__codelineno-1-2"></a>- name: eks-core-node-grp-20220823190924690000000011-80c1660e-030d-476d-cb0d-d04d585a8fcb
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#scalability-docs-control-plane-__codelineno-1-3"></a>  maxSize: 50
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#scalability-docs-control-plane-__codelineno-1-4"></a>  minSize: 2
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#scalability-docs-control-plane-__codelineno-1-5"></a>- name: eks-data_m1-20220824130553925600000011-5ec167fa-ca93-8ca4-53a5-003e1ed8d306
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#scalability-docs-control-plane-__codelineno-1-6"></a>  maxSize: 450
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#scalability-docs-control-plane-__codelineno-1-7"></a>  minSize: 2
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#scalability-docs-control-plane-__codelineno-1-8"></a>- name: eks-data_m2-20220824130733258600000015-aac167fb-8bf7-429d-d032-e195af4e25f5
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#scalability-docs-control-plane-__codelineno-1-9"></a>  maxSize: 450
<a id="__codelineno-1-10" name="__codelineno-1-10" href="#scalability-docs-control-plane-__codelineno-1-10"></a>  minSize: 2
<a id="__codelineno-1-11" name="__codelineno-1-11" href="#scalability-docs-control-plane-__codelineno-1-11"></a>- name: eks-data_m3-20220824130553914900000003-18c167fa-ca7f-23c9-0fea-f9edefbda002
<a id="__codelineno-1-12" name="__codelineno-1-12" href="#scalability-docs-control-plane-__codelineno-1-12"></a>  maxSize: 450
<a id="__codelineno-1-13" name="__codelineno-1-13" href="#scalability-docs-control-plane-__codelineno-1-13"></a>  minSize: 2
</code></pre></div>
<p>ClusterAutoscaler-2</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#scalability-docs-control-plane-__codelineno-2-1"></a>autoscalingGroups:
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#scalability-docs-control-plane-__codelineno-2-2"></a>- name: eks-data_m4-2022082413055392550000000f-5ec167fa-ca86-6b83-ae9d-1e07ade3e7c4
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#scalability-docs-control-plane-__codelineno-2-3"></a>  maxSize: 450
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#scalability-docs-control-plane-__codelineno-2-4"></a>  minSize: 2
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#scalability-docs-control-plane-__codelineno-2-5"></a>- name: eks-data_m5-20220824130744542100000017-02c167fb-a1f7-3d9e-a583-43b4975c050c
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#scalability-docs-control-plane-__codelineno-2-6"></a>  maxSize: 450
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#scalability-docs-control-plane-__codelineno-2-7"></a>  minSize: 2
<a id="__codelineno-2-8" name="__codelineno-2-8" href="#scalability-docs-control-plane-__codelineno-2-8"></a>- name: eks-data_m6-2022082413055392430000000d-9cc167fa-ca94-132a-04ad-e43166cef41f
<a id="__codelineno-2-9" name="__codelineno-2-9" href="#scalability-docs-control-plane-__codelineno-2-9"></a>  maxSize: 450
<a id="__codelineno-2-10" name="__codelineno-2-10" href="#scalability-docs-control-plane-__codelineno-2-10"></a>  minSize: 2
<a id="__codelineno-2-11" name="__codelineno-2-11" href="#scalability-docs-control-plane-__codelineno-2-11"></a>- name: eks-data_m7-20220824130553921000000009-96c167fa-ca91-d767-0427-91c879ddf5af
<a id="__codelineno-2-12" name="__codelineno-2-12" href="#scalability-docs-control-plane-__codelineno-2-12"></a>  maxSize: 450
<a id="__codelineno-2-13" name="__codelineno-2-13" href="#scalability-docs-control-plane-__codelineno-2-13"></a>  minSize: 2
</code></pre></div>
<h2 id="scalability-docs-control-plane-api-priority-and-fairness">API Priority and Fairness<a class="headerlink" href="#scalability-docs-control-plane-api-priority-and-fairness" title="Permanent link">&para;</a></h2>
<p><img alt="" src="../scalability/images/APF.jpg" /></p>
<h3 id="scalability-docs-control-plane-overview">Overview<a class="headerlink" href="#scalability-docs-control-plane-overview" title="Permanent link">&para;</a></h3>
<iframe width="560" height="315" src="https://www.youtube.com/embed/YnPPHBawhE0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

<p>To protect itself from being overloaded during periods of increased requests, the API Server limits the number of inflight requests it can have outstanding at a given time. Once this limit is exceeded, the API Server will start rejecting requests and return a 429 HTTP response code for "Too Many Requests" back to clients. The server dropping requests and having clients try again later is preferable to having no server-side limits on the number of requests and overloading the control plane, which could result in degraded performance or unavailability.</p>
<p>The mechanism used by Kubernetes to configure how these inflights requests are divided among different request types is called <a href="https://kubernetes.io/docs/concepts/cluster-administration/flow-control/">API Priority and Fairness</a>. The API Server configures the total number of inflight requests it can accept by summing together the values specified by the <code>--max-requests-inflight</code> and <code>--max-mutating-requests-inflight</code> flags. EKS uses the default values of 400 and 200 requests for these flags, allowing a total of 600 requests to be dispatched at a given time. However, as it scales the control-plane to larger sizes in response to increased utilization and workload churn, it correspondingly increases the inflight request quota all the way till 2000 (subject to change). APF specifies how these inflight request quota is further sub-divided among different request types. Note that EKS control planes are highly available with at least 2 API Servers registered to each cluster. This means the total number of inflight requests your cluster can handle is twice (or higher if horizontally scaled out further) the inflight quota set per kube-apiserver. This amounts to several thousands of requests/second on the largest EKS clusters.</p>
<p>Two kinds of Kubernetes objects, called PriorityLevelConfigurations and FlowSchemas, configure how the total number of requests is divided between different request types. These objects are maintained by the API Server automatically and EKS uses the default configuration of these objects for the given Kubernetes minor version. PriorityLevelConfigurations represent a fraction of the total number of allowed requests. For example, the workload-high PriorityLevelConfiguration is allocated 98 out of the total of 600 requests. The sum of requests allocated to all PriorityLevelConfigurations will equal 600 (or slightly above 600 because the API Server will round up if a given level is granted a fraction of a request). To check the PriorityLevelConfigurations in your cluster and the number of requests allocated to each, you can run the following command. These are the defaults on EKS 1.24:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#scalability-docs-control-plane-__codelineno-3-1"></a>$ kubectl get --raw /metrics | grep apiserver_flowcontrol_request_concurrency_limit
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#scalability-docs-control-plane-__codelineno-3-2"></a>apiserver_flowcontrol_request_concurrency_limit{priority_level=&quot;catch-all&quot;} 13
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#scalability-docs-control-plane-__codelineno-3-3"></a>apiserver_flowcontrol_request_concurrency_limit{priority_level=&quot;global-default&quot;} 49
<a id="__codelineno-3-4" name="__codelineno-3-4" href="#scalability-docs-control-plane-__codelineno-3-4"></a>apiserver_flowcontrol_request_concurrency_limit{priority_level=&quot;leader-election&quot;} 25
<a id="__codelineno-3-5" name="__codelineno-3-5" href="#scalability-docs-control-plane-__codelineno-3-5"></a>apiserver_flowcontrol_request_concurrency_limit{priority_level=&quot;node-high&quot;} 98
<a id="__codelineno-3-6" name="__codelineno-3-6" href="#scalability-docs-control-plane-__codelineno-3-6"></a>apiserver_flowcontrol_request_concurrency_limit{priority_level=&quot;system&quot;} 74
<a id="__codelineno-3-7" name="__codelineno-3-7" href="#scalability-docs-control-plane-__codelineno-3-7"></a>apiserver_flowcontrol_request_concurrency_limit{priority_level=&quot;workload-high&quot;} 98
<a id="__codelineno-3-8" name="__codelineno-3-8" href="#scalability-docs-control-plane-__codelineno-3-8"></a>apiserver_flowcontrol_request_concurrency_limit{priority_level=&quot;workload-low&quot;} 245
</code></pre></div>
<p>The second type of object are FlowSchemas. API Server requests with a given set of properties are classified under the same FlowSchema. These properties include either the authenticated user or attributes of the request, such as the API group, namespace, or resource. A FlowSchema also specifies which PriorityLevelConfiguration this type of request should map to. The two objects together say, "I want this type of request to count towards this share of inflight requests." When a request hits the API Server, it will check each of its FlowSchemas until it finds one that matches all the required properties. If multiple FlowSchemas match a request, the API Server will choose the FlowSchema with the smallest matching precedence which is specified as a property in the object.</p>
<p>The mapping of FlowSchemas to PriorityLevelConfigurations can be viewed using this command:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#scalability-docs-control-plane-__codelineno-4-1"></a>$ kubectl get flowschemas
<a id="__codelineno-4-2" name="__codelineno-4-2" href="#scalability-docs-control-plane-__codelineno-4-2"></a>NAME                           PRIORITYLEVEL     MATCHINGPRECEDENCE   DISTINGUISHERMETHOD   AGE     MISSINGPL
<a id="__codelineno-4-3" name="__codelineno-4-3" href="#scalability-docs-control-plane-__codelineno-4-3"></a>exempt                         exempt            1                    &lt;none&gt;                7h19m   False
<a id="__codelineno-4-4" name="__codelineno-4-4" href="#scalability-docs-control-plane-__codelineno-4-4"></a>eks-exempt                     exempt            2                    &lt;none&gt;                7h19m   False
<a id="__codelineno-4-5" name="__codelineno-4-5" href="#scalability-docs-control-plane-__codelineno-4-5"></a>probes                         exempt            2                    &lt;none&gt;                7h19m   False
<a id="__codelineno-4-6" name="__codelineno-4-6" href="#scalability-docs-control-plane-__codelineno-4-6"></a>system-leader-election         leader-election   100                  ByUser                7h19m   False
<a id="__codelineno-4-7" name="__codelineno-4-7" href="#scalability-docs-control-plane-__codelineno-4-7"></a>endpoint-controller            workload-high     150                  ByUser                7h19m   False
<a id="__codelineno-4-8" name="__codelineno-4-8" href="#scalability-docs-control-plane-__codelineno-4-8"></a>workload-leader-election       leader-election   200                  ByUser                7h19m   False
<a id="__codelineno-4-9" name="__codelineno-4-9" href="#scalability-docs-control-plane-__codelineno-4-9"></a>system-node-high               node-high         400                  ByUser                7h19m   False
<a id="__codelineno-4-10" name="__codelineno-4-10" href="#scalability-docs-control-plane-__codelineno-4-10"></a>system-nodes                   system            500                  ByUser                7h19m   False
<a id="__codelineno-4-11" name="__codelineno-4-11" href="#scalability-docs-control-plane-__codelineno-4-11"></a>kube-controller-manager        workload-high     800                  ByNamespace           7h19m   False
<a id="__codelineno-4-12" name="__codelineno-4-12" href="#scalability-docs-control-plane-__codelineno-4-12"></a>kube-scheduler                 workload-high     800                  ByNamespace           7h19m   False
<a id="__codelineno-4-13" name="__codelineno-4-13" href="#scalability-docs-control-plane-__codelineno-4-13"></a>kube-system-service-accounts   workload-high     900                  ByNamespace           7h19m   False
<a id="__codelineno-4-14" name="__codelineno-4-14" href="#scalability-docs-control-plane-__codelineno-4-14"></a>eks-workload-high              workload-high     1000                 ByUser                7h14m   False
<a id="__codelineno-4-15" name="__codelineno-4-15" href="#scalability-docs-control-plane-__codelineno-4-15"></a>service-accounts               workload-low      9000                 ByUser                7h19m   False
<a id="__codelineno-4-16" name="__codelineno-4-16" href="#scalability-docs-control-plane-__codelineno-4-16"></a>global-default                 global-default    9900                 ByUser                7h19m   False
<a id="__codelineno-4-17" name="__codelineno-4-17" href="#scalability-docs-control-plane-__codelineno-4-17"></a>catch-all                      catch-all         10000                ByUser                7h19m   False
</code></pre></div>
<p>PriorityLevelConfigurations can have a type of Queue, Reject, or Exempt. For types Queue and Reject, a limit is enforced on the maximum number of inflight requests for that priority level, however, the behavior differs when that limit is reached. For example, the workload-high PriorityLevelConfiguration uses type Queue and has 98 requests available for use by the controller-manager, endpoint-controller, scheduler,eks related controllers and from pods running in the kube-system namespace. Since type Queue is used, the API Server will attempt to keep requests in memory and hope that the number of inflight requests drops below 98 before these requests time out. If a given request times out in the queue or if too many requests are already queued, the API Server has no choice but to drop the request and return the client a 429. Note that queuing may prevent a request from receiving a 429, but it comes with the tradeoff of increased end-to-end latency on the request.</p>
<p>Now consider the catch-all FlowSchema that maps to the catch-all PriorityLevelConfiguration with type Reject. If clients reach the limit of 13 inflight requests, the API Server will not exercise queuing and will drop the requests instantly with a 429 response code. Finally, requests mapping to a PriorityLevelConfiguration with type Exempt will never receive a 429 and always be dispatched immediately. This is used for high-priority requests such as healthz requests or requests coming from the system:masters group.  </p>
<h3 id="scalability-docs-control-plane-monitoring-apf-and-dropped-requests">Monitoring APF and Dropped Requests<a class="headerlink" href="#scalability-docs-control-plane-monitoring-apf-and-dropped-requests" title="Permanent link">&para;</a></h3>
<p>To confirm if any requests are being dropped due to APF, the API Server metrics for <code>apiserver_flowcontrol_rejected_requests_total</code> can be monitored to check the impacted FlowSchemas and PriorityLevelConfigurations. For example, this metric shows that 100 requests from the service-accounts FlowSchema were dropped due to requests timing out in workload-low queues:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#scalability-docs-control-plane-__codelineno-5-1"></a>% kubectl get --raw /metrics | grep apiserver_flowcontrol_rejected_requests_total
<a id="__codelineno-5-2" name="__codelineno-5-2" href="#scalability-docs-control-plane-__codelineno-5-2"></a>apiserver_flowcontrol_rejected_requests_total{flow_schema=&quot;service-accounts&quot;,priority_level=&quot;workload-low&quot;,reason=&quot;time-out&quot;} 100
</code></pre></div>
<p>To check how close a given PriorityLevelConfiguration is to receiving 429s or experiencing increased latency due to queuing, you can compare the difference between the concurrency limit and the concurrency in use. In this example, we have a buffer of 100 requests.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#scalability-docs-control-plane-__codelineno-6-1"></a>% kubectl get --raw /metrics | grep &#39;apiserver_flowcontrol_request_concurrency_limit.*workload-low&#39;
<a id="__codelineno-6-2" name="__codelineno-6-2" href="#scalability-docs-control-plane-__codelineno-6-2"></a>apiserver_flowcontrol_request_concurrency_limit{priority_level=&quot;workload-low&quot;} 245
<a id="__codelineno-6-3" name="__codelineno-6-3" href="#scalability-docs-control-plane-__codelineno-6-3"></a>
<a id="__codelineno-6-4" name="__codelineno-6-4" href="#scalability-docs-control-plane-__codelineno-6-4"></a>% kubectl get --raw /metrics | grep &#39;apiserver_flowcontrol_request_concurrency_in_use.*workload-low&#39;
<a id="__codelineno-6-5" name="__codelineno-6-5" href="#scalability-docs-control-plane-__codelineno-6-5"></a>apiserver_flowcontrol_request_concurrency_in_use{flow_schema=&quot;service-accounts&quot;,priority_level=&quot;workload-low&quot;} 145
</code></pre></div>
<p>To check if a given PriorityLevelConfiguration is experiencing queuing but not necessarily dropped requests, the metric for <code>apiserver_flowcontrol_current_inqueue_requests</code> can be referenced:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#scalability-docs-control-plane-__codelineno-7-1"></a>% kubectl get --raw /metrics | grep &#39;apiserver_flowcontrol_current_inqueue_requests.*workload-low&#39;
<a id="__codelineno-7-2" name="__codelineno-7-2" href="#scalability-docs-control-plane-__codelineno-7-2"></a>apiserver_flowcontrol_current_inqueue_requests{flow_schema=&quot;service-accounts&quot;,priority_level=&quot;workload-low&quot;} 10
</code></pre></div>
<p>Other useful Prometheus metrics include:</p>
<ul>
<li>apiserver_flowcontrol_dispatched_requests_total</li>
<li>apiserver_flowcontrol_request_execution_seconds</li>
<li>apiserver_flowcontrol_request_wait_duration_seconds</li>
</ul>
<p>See the upstream documentation for a complete list of <a href="https://kubernetes.io/docs/concepts/cluster-administration/flow-control/#observability">APF metrics</a>.</p>
<h3 id="scalability-docs-control-plane-preventing-dropped-requests">Preventing Dropped Requests<a class="headerlink" href="#scalability-docs-control-plane-preventing-dropped-requests" title="Permanent link">&para;</a></h3>
<h4 id="scalability-docs-control-plane-prevent-429s-by-changing-your-workload">Prevent 429s by changing your workload<a class="headerlink" href="#scalability-docs-control-plane-prevent-429s-by-changing-your-workload" title="Permanent link">&para;</a></h4>
<p>When APF is dropping requests due to a given PriorityLevelConfiguration exceeding its maximum number of allowed inflight requests, clients in the affected FlowSchemas can decrease the number of requests executing at a given time. This can be accomplished by reducing the total number of requests made over the period where 429s are occurring. Note that long-running requests such as expensive list calls are especially problematic because they count as an inflight request for the entire duration they are executing. Reducing the number of these expensive requests or optimizing the latency of these list calls (for example, by reducing the number of objects fetched per request or switching to using a watch request) can help reduce the total concurrency required by the given workload.</p>
<h4 id="scalability-docs-control-plane-prevent-429s-by-changing-your-apf-settings">Prevent 429s by changing your APF settings<a class="headerlink" href="#scalability-docs-control-plane-prevent-429s-by-changing-your-apf-settings" title="Permanent link">&para;</a></h4>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Only change default APF settings if you know what you are doing. Misconfigured APF settings can result in dropped API Server requests and significant workload disruptions.</p>
</div>
<p>One other approach for preventing dropped requests is changing the default FlowSchemas or PriorityLevelConfigurations installed on EKS clusters. EKS installs the upstream default settings for FlowSchemas and PriorityLevelConfigurations for the given Kubernetes minor version. The API Server will automatically reconcile these objects back to their defaults if modified unless the following annotation on the objects is set to false:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1" href="#scalability-docs-control-plane-__codelineno-8-1"></a>  metadata:
<a id="__codelineno-8-2" name="__codelineno-8-2" href="#scalability-docs-control-plane-__codelineno-8-2"></a>    annotations:
<a id="__codelineno-8-3" name="__codelineno-8-3" href="#scalability-docs-control-plane-__codelineno-8-3"></a>      apf.kubernetes.io/autoupdate-spec: &quot;false&quot;
</code></pre></div>
<p>At a high-level, APF settings can be modified to either:</p>
<ul>
<li>Allocate more inflight capacity to requests you care about.</li>
<li>Isolate non-essential or expensive requests that can starve capacity for other request types.</li>
</ul>
<p>This can be accomplished by either changing the default FlowSchemas and PriorityLevelConfigurations or by creating new objects of these types. Operators can increase the values for assuredConcurrencyShares for the relevant PriorityLevelConfigurations objects to increase the fraction of inflight requests they are allocated. Additionally, the number of requests that can be queued at a given time can also be increased if the application can handle the additional latency caused by requests being queued before they are dispatched.</p>
<p>Alternatively, new FlowSchema and PriorityLevelConfigurations objects can be created that are specific to the customer's workload. Be aware that allocating more assuredConcurrencyShares to either existing PriorityLevelConfigurations or to new PriorityLevelConfigurations will cause the number of requests that can be handled by other buckets to be reduced as the overall limit will stay as 600 inflight per API Server.</p>
<p>When making changes to APF defaults, these metrics should be monitored on a non-production cluster to ensure changing the settings do not cause unintended 429s:</p>
<ol>
<li>The metric for <code>apiserver_flowcontrol_rejected_requests_total</code> should be monitored for all FlowSchemas to ensure that no buckets start to drop requests.</li>
<li>The values for <code>apiserver_flowcontrol_request_concurrency_limit</code> and <code>apiserver_flowcontrol_request_concurrency_in_use</code> should be compared to ensure that the concurrency in use is not at risk for breaching the limit for that priority level.</li>
</ol>
<p>One common use-case for defining a new FlowSchema and PriorityLevelConfiguration is for isolation. Suppose we want to isolate long-running list event calls from pods to their own share of requests. This will prevent important requests from pods using the existing service-accounts FlowSchema from receiving 429s and being starved of request capacity. Recall that the total number of inflight requests is finite, however, this example shows APF settings can be modified to better divide request capacity for the given workload:</p>
<p>Example FlowSchema object to isolate list event requests:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-9-1" name="__codelineno-9-1" href="#scalability-docs-control-plane-__codelineno-9-1"></a>apiVersion: flowcontrol.apiserver.k8s.io/v1beta1
<a id="__codelineno-9-2" name="__codelineno-9-2" href="#scalability-docs-control-plane-__codelineno-9-2"></a>kind: FlowSchema
<a id="__codelineno-9-3" name="__codelineno-9-3" href="#scalability-docs-control-plane-__codelineno-9-3"></a>metadata:
<a id="__codelineno-9-4" name="__codelineno-9-4" href="#scalability-docs-control-plane-__codelineno-9-4"></a>  name: list-events-default-service-accounts
<a id="__codelineno-9-5" name="__codelineno-9-5" href="#scalability-docs-control-plane-__codelineno-9-5"></a>spec:
<a id="__codelineno-9-6" name="__codelineno-9-6" href="#scalability-docs-control-plane-__codelineno-9-6"></a>  distinguisherMethod:
<a id="__codelineno-9-7" name="__codelineno-9-7" href="#scalability-docs-control-plane-__codelineno-9-7"></a>    type: ByUser
<a id="__codelineno-9-8" name="__codelineno-9-8" href="#scalability-docs-control-plane-__codelineno-9-8"></a>  matchingPrecedence: 8000
<a id="__codelineno-9-9" name="__codelineno-9-9" href="#scalability-docs-control-plane-__codelineno-9-9"></a>  priorityLevelConfiguration:
<a id="__codelineno-9-10" name="__codelineno-9-10" href="#scalability-docs-control-plane-__codelineno-9-10"></a>    name: catch-all
<a id="__codelineno-9-11" name="__codelineno-9-11" href="#scalability-docs-control-plane-__codelineno-9-11"></a>  rules:
<a id="__codelineno-9-12" name="__codelineno-9-12" href="#scalability-docs-control-plane-__codelineno-9-12"></a>  - resourceRules:
<a id="__codelineno-9-13" name="__codelineno-9-13" href="#scalability-docs-control-plane-__codelineno-9-13"></a>    - apiGroups:
<a id="__codelineno-9-14" name="__codelineno-9-14" href="#scalability-docs-control-plane-__codelineno-9-14"></a>      - &#39;*&#39;
<a id="__codelineno-9-15" name="__codelineno-9-15" href="#scalability-docs-control-plane-__codelineno-9-15"></a>      namespaces:
<a id="__codelineno-9-16" name="__codelineno-9-16" href="#scalability-docs-control-plane-__codelineno-9-16"></a>      - default
<a id="__codelineno-9-17" name="__codelineno-9-17" href="#scalability-docs-control-plane-__codelineno-9-17"></a>      resources:
<a id="__codelineno-9-18" name="__codelineno-9-18" href="#scalability-docs-control-plane-__codelineno-9-18"></a>      - events
<a id="__codelineno-9-19" name="__codelineno-9-19" href="#scalability-docs-control-plane-__codelineno-9-19"></a>      verbs:
<a id="__codelineno-9-20" name="__codelineno-9-20" href="#scalability-docs-control-plane-__codelineno-9-20"></a>      - list
<a id="__codelineno-9-21" name="__codelineno-9-21" href="#scalability-docs-control-plane-__codelineno-9-21"></a>    subjects:
<a id="__codelineno-9-22" name="__codelineno-9-22" href="#scalability-docs-control-plane-__codelineno-9-22"></a>    - kind: ServiceAccount
<a id="__codelineno-9-23" name="__codelineno-9-23" href="#scalability-docs-control-plane-__codelineno-9-23"></a>      serviceAccount:
<a id="__codelineno-9-24" name="__codelineno-9-24" href="#scalability-docs-control-plane-__codelineno-9-24"></a>        name: default
<a id="__codelineno-9-25" name="__codelineno-9-25" href="#scalability-docs-control-plane-__codelineno-9-25"></a>        namespace: default
</code></pre></div>
<ul>
<li>This FlowSchema captures all list event calls made by service accounts in the default namespace.</li>
<li>The matching precedence 8000 is lower than the value of 9000 used by the existing service-accounts FlowSchema so these list event calls will match list-events-default-service-accounts rather than service-accounts.</li>
<li>We're using the catch-all PriorityLevelConfiguration to isolate these requests. This bucket only allows 13 inflight requests to be used by these long-running list event calls. Pods will start to receive 429s as soon they try to issue more than 13 of these requests concurrently.</li>
</ul>
<h2 id="scalability-docs-control-plane-retreiving-resources-in-the-api-server">Retreiving resources in the API server<a class="headerlink" href="#scalability-docs-control-plane-retreiving-resources-in-the-api-server" title="Permanent link">&para;</a></h2>
<p>Getting information from the API server is an expected behavior for clusters of any size. As you scale the number of resources in the cluster the frequency of requests and volume of data can quickly become a bottleneck for the control plane and will lead to API latency and slowness. Depending on the severity of the latency it cause unexpected downtime if you are not careful.</p>
<p>Being aware of what you are requesting and how often are the first steps to avoiding these types of problems. Here is guidance to limit the volume of queries based on the scaling best practices. Suggestions in this section are provided in order starting with the options that are known to scale the best.</p>
<h3 id="scalability-docs-control-plane-use-shared-informers">Use Shared Informers<a class="headerlink" href="#scalability-docs-control-plane-use-shared-informers" title="Permanent link">&para;</a></h3>
<p>When building controllers and automation that integrate with the Kubernetes API you will often need to get information from Kubernetes resources. If you poll for these resources regularly it can cause a significant load on the API server.</p>
<p>Using an <a href="https://pkg.go.dev/k8s.io/client-go/informers">informer</a> from the client-go library will give you benefits of watching for changes to the resources based on events instead of polling for changes. Informers further reduce the load by using shared cache for the events and changes so multiple controllers watching the same resources do not add additional load.</p>
<p>Controllers should avoid polling cluster wide resources without labels and field selectors especially in large clusters. Each un-filtered poll requires a lot of unnecessary data to be sent from etcd through the API server to be filtered by the client. By filtering based on labels and namespaces you can reduce the amount of work the API server needs to perform to fullfil the request and data sent to the client.</p>
<h3 id="scalability-docs-control-plane-optimize-kubernetes-api-usage">Optimize Kubernetes API usage<a class="headerlink" href="#scalability-docs-control-plane-optimize-kubernetes-api-usage" title="Permanent link">&para;</a></h3>
<p>When calling the Kubernetes API with custom controllers or automation it's important that you limit the calls to only the resources you need. Without limits you can cause unneeded load on the API server and etcd.</p>
<p>It is recommended that you use the watch argument whenever possible. With no arguments the default behavior is to list objects. To use watch instead of list you can append <code>?watch=true</code> to the end of your API request. For example, to get all pods in the default namespace with a watch use:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-10-1" name="__codelineno-10-1" href="#scalability-docs-control-plane-__codelineno-10-1"></a>/api/v1/namespaces/default/pods?watch=true
</code></pre></div>
<p>If you are listing objects you should limit the scope of what you are listing and the amount of data returned. You can limit the returned data by adding <code>limit=500</code> argument to requests. The <code>fieldSelector</code> argument and <code>/namespace/</code> path can be useful to make sure your lists are as narrowly scoped as needed. For example, to list only running pods in the default namespace use the following API path and arguments.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-11-1" name="__codelineno-11-1" href="#scalability-docs-control-plane-__codelineno-11-1"></a>/api/v1/namespaces/default/pods?fieldSelector=status.phase=Running&amp;limit=500
</code></pre></div>
<p>Or list all pods that are running with:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-12-1" name="__codelineno-12-1" href="#scalability-docs-control-plane-__codelineno-12-1"></a>/api/v1/pods?fieldSelector=status.phase=Running&amp;limit=500
</code></pre></div>
<p>Another option to limit listed objects is to use <a href="https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions"><code>resourceVersions</code> which you can read about in the Kubernetes documentation</a>. Without a <code>resourceVersion</code> argument you will receive the most recent version available which requires an etcd quorum read which is the most expensive and slowest read for the database. The resourceVersion depends on what resources you are trying to query and can be found in the <code>metadata.resourseVersion</code> field.</p>
<p>There is a special <code>resourceVersion=0</code> available that will return results from the API server cache. This can reduce etcd load but it does not support pagination.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-13-1" name="__codelineno-13-1" href="#scalability-docs-control-plane-__codelineno-13-1"></a>/api/v1/namespaces/default/pods?resourceVersion=0
</code></pre></div>
<p>If you call the API without any arguments it will be the most resource intensive for the API server and etcd. This call will get all pods in all namespaces without pagination or limiting the scope and require a quorum read from etcd.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-14-1" name="__codelineno-14-1" href="#scalability-docs-control-plane-__codelineno-14-1"></a>/api/v1/pods
</code></pre></div></section><section class="print-page" id="scalability-docs-data-plane"><h1 id="scalability-docs-data-plane-kubernetes-data-plane">Kubernetes Data Plane<a class="headerlink" href="#scalability-docs-data-plane-kubernetes-data-plane" title="Permanent link">&para;</a></h1>
<p>The Kubernetes Data Plane includes EC2 instances, load balancers, storage, and other APIs used by the Kubernetes Control Plane. For organization purposes we grouped <a href="#scalability-docs-cluster-services">cluster services</a> in a separate page and load balancer scaling can be found in the <a href="#scalability-docs-workloads">workloads section</a>. This section will focus on scaling compute resources.</p>
<p>Selecting EC2 instance types is possibly one of the hardest decisions customers face because in clusters with multiple workloads. There is no one-size-fits all solution. Here are some tips to help you avoid common pitfalls with scaling compute.</p>
<h2 id="scalability-docs-data-plane-automatic-node-autoscaling">Automatic node autoscaling<a class="headerlink" href="#scalability-docs-data-plane-automatic-node-autoscaling" title="Permanent link">&para;</a></h2>
<p>We recommend you use node autoscaling that reduces toil and integrates deeply with Kubernetes. <a href="https://docs.aws.amazon.com/eks/latest/userguide/managed-node-groups.html">Managed node groups</a> and <a href="https://karpenter.sh/">Karpenter</a> are recommended for large scale clusters.</p>
<p>Managed node groups will give you the flexibility of Amazon EC2 Auto Scaling groups with added benefits for managed upgrades and configuration. It can be scaled with the <a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler">Kubernetes Cluster Autoscaler</a> and is a common option for clusters that have a variety of compute needs.</p>
<p>Karpenter is an open source, workload-native node autoscaler created by AWS. It scales nodes in a cluster based on the workload requirements for resources (e.g. GPU) and taints and tolerations (e.g. zone spread) without managing node groups. Nodes are created directly from EC2 which avoids default node group quotas—450 nodes per group—and provides greater instance selection flexibility with less operational overhead. We recommend customers use Karpenter when possible.</p>
<h2 id="scalability-docs-data-plane-use-many-different-ec2-instance-types">Use many different EC2 instance types<a class="headerlink" href="#scalability-docs-data-plane-use-many-different-ec2-instance-types" title="Permanent link">&para;</a></h2>
<p>Each AWS region has a limited number of available instances per instance type. If you create a cluster that uses only one instance type and scale the number of nodes beyond the capacity of the region you will receive an error that no instances are available. To avoid this issue you should not arbitrarily limit the type of instances that can be use in your cluster.</p>
<p>Karpenter will use a broad set of compatible instance types by default and will pick an instance at provisioning time based on pending workload requirements, availability, and cost. You can broaden the list of instance types used in the <code>karpenter.k8s.aws/instance-category</code> key of <a href="https://karpenter.sh/docs/concepts/provisioners/#instance-types">the provisioner</a>.</p>
<p>The Kubernetes Cluster Autoscaler requires node groups to be similarly sized so they can be consistently scaled. You should create multiple groups based on CPU and memory size and scale them independently. Use the <a href="https://github.com/aws/amazon-ec2-instance-selector">ec2-instance-selector</a> to identify instances that are similarly sized for your node groups.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#scalability-docs-data-plane-__codelineno-0-1"></a>ec2-instance-selector --service eks --vcpus-min 8 --memory-min 16
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#scalability-docs-data-plane-__codelineno-0-2"></a>a1.2xlarge
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#scalability-docs-data-plane-__codelineno-0-3"></a>a1.4xlarge
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#scalability-docs-data-plane-__codelineno-0-4"></a>a1.metal
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#scalability-docs-data-plane-__codelineno-0-5"></a>c4.4xlarge
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#scalability-docs-data-plane-__codelineno-0-6"></a>c4.8xlarge
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#scalability-docs-data-plane-__codelineno-0-7"></a>c5.12xlarge
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#scalability-docs-data-plane-__codelineno-0-8"></a>c5.18xlarge
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#scalability-docs-data-plane-__codelineno-0-9"></a>c5.24xlarge
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#scalability-docs-data-plane-__codelineno-0-10"></a>c5.2xlarge
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#scalability-docs-data-plane-__codelineno-0-11"></a>c5.4xlarge
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#scalability-docs-data-plane-__codelineno-0-12"></a>c5.9xlarge
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#scalability-docs-data-plane-__codelineno-0-13"></a>c5.metal
</code></pre></div>
<h2 id="scalability-docs-data-plane-prefer-larger-nodes-to-reduce-api-server-load">Prefer larger nodes to reduce API server load<a class="headerlink" href="#scalability-docs-data-plane-prefer-larger-nodes-to-reduce-api-server-load" title="Permanent link">&para;</a></h2>
<p>When deciding what instance types to use, fewer, large nodes will put less load on the Kubernetes Control Plane because there will be fewer kubelets and DaemonSets running. However, large nodes may not be utilized fully like smaller nodes. Node sizes should be evaluated based on your workload availability and scale requirements.</p>
<p>A cluster with three u-24tb1.metal instances (24 TB memory and 448 cores) has 3 kublets, and would be limited to 110 pods per node by default. If your pods use 4 cores each then this might be expected (4 cores x 110 = 440 cores/node). With a 3 node cluster your ability to handle an instance incident would be low because 1 instance outage could impact 1/3 of the cluster. You should specify node requirements and pod spread in your workloads so the Kubernetes scheduler can place workloads properly.</p>
<p>Workloads should define the resources they need and the availability required via taints, tolerations, and <a href="https://kubernetes.io/blog/2020/05/introducing-podtopologyspread/">PodTopologySpread</a>. They should prefer the largest nodes that can be fully utilized and meet availability goals to reduce control plane load, lower operations, and reduce cost.</p>
<p>The Kubernetes Scheduler will automatically try to spread workloads across availability zones and hosts if resources are available. If no capacity is available the Kubernetes Cluster Autoscaler will attempt to add nodes in each Availability Zone evenly. Karpenter will attempt to add nodes as quickly and cheaply as possible unless the workload specifies other requirements.</p>
<p>To force workloads to spread with the scheduler and new nodes to be created across availability zones you should use topologySpreadConstraints:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#scalability-docs-data-plane-__codelineno-1-1"></a>spec:
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#scalability-docs-data-plane-__codelineno-1-2"></a>  topologySpreadConstraints:
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#scalability-docs-data-plane-__codelineno-1-3"></a>    - maxSkew: 3
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#scalability-docs-data-plane-__codelineno-1-4"></a>      topologyKey: &quot;topology.kubernetes.io/zone&quot;
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#scalability-docs-data-plane-__codelineno-1-5"></a>      whenUnsatisfiable: ScheduleAnyway
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#scalability-docs-data-plane-__codelineno-1-6"></a>      labelSelector:
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#scalability-docs-data-plane-__codelineno-1-7"></a>        matchLabels:
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#scalability-docs-data-plane-__codelineno-1-8"></a>          dev: my-deployment
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#scalability-docs-data-plane-__codelineno-1-9"></a>    - maxSkew: 2
<a id="__codelineno-1-10" name="__codelineno-1-10" href="#scalability-docs-data-plane-__codelineno-1-10"></a>      topologyKey: &quot;kubernetes.io/hostname&quot;
<a id="__codelineno-1-11" name="__codelineno-1-11" href="#scalability-docs-data-plane-__codelineno-1-11"></a>      whenUnsatisfiable: ScheduleAnyway
<a id="__codelineno-1-12" name="__codelineno-1-12" href="#scalability-docs-data-plane-__codelineno-1-12"></a>      labelSelector:
<a id="__codelineno-1-13" name="__codelineno-1-13" href="#scalability-docs-data-plane-__codelineno-1-13"></a>        matchLabels:
<a id="__codelineno-1-14" name="__codelineno-1-14" href="#scalability-docs-data-plane-__codelineno-1-14"></a>          dev: my-deployment
</code></pre></div>
<h2 id="scalability-docs-data-plane-use-similar-node-sizes-for-consistent-workload-performance">Use similar node sizes for consistent workload performance<a class="headerlink" href="#scalability-docs-data-plane-use-similar-node-sizes-for-consistent-workload-performance" title="Permanent link">&para;</a></h2>
<p>Workloads should define what size nodes they need to be run on to allow consistent performance and predictable scaling. A workload requesting 500m CPU will perform differently on an instance with 4 cores vs one with 16 cores. Avoid instance types that use burstable CPUs like T series instances.</p>
<p>To make sure your workloads get consistent performance a workload can use the <a href="https://karpenter.sh/docs/concepts/scheduling/#labels">supported Karpenter labels</a> to target specific instances sizes.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#scalability-docs-data-plane-__codelineno-2-1"></a>kind: deployment
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#scalability-docs-data-plane-__codelineno-2-2"></a>...
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#scalability-docs-data-plane-__codelineno-2-3"></a>spec:
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#scalability-docs-data-plane-__codelineno-2-4"></a>  template:
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#scalability-docs-data-plane-__codelineno-2-5"></a>    spec:
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#scalability-docs-data-plane-__codelineno-2-6"></a>    containers:
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#scalability-docs-data-plane-__codelineno-2-7"></a>    nodeSelector:
<a id="__codelineno-2-8" name="__codelineno-2-8" href="#scalability-docs-data-plane-__codelineno-2-8"></a>      karpenter.k8s.aws/instance-size: 8xlarge
</code></pre></div>
<p>Workloads being scheduled in a cluster with the Kubernetes Cluster Autoscaler should match a node selector to node groups based on label matching.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#scalability-docs-data-plane-__codelineno-3-1"></a>spec:
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#scalability-docs-data-plane-__codelineno-3-2"></a>  affinity:
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#scalability-docs-data-plane-__codelineno-3-3"></a>    nodeAffinity:
<a id="__codelineno-3-4" name="__codelineno-3-4" href="#scalability-docs-data-plane-__codelineno-3-4"></a>      requiredDuringSchedulingIgnoredDuringExecution:
<a id="__codelineno-3-5" name="__codelineno-3-5" href="#scalability-docs-data-plane-__codelineno-3-5"></a>        nodeSelectorTerms:
<a id="__codelineno-3-6" name="__codelineno-3-6" href="#scalability-docs-data-plane-__codelineno-3-6"></a>        - matchExpressions:
<a id="__codelineno-3-7" name="__codelineno-3-7" href="#scalability-docs-data-plane-__codelineno-3-7"></a>          - key: eks.amazonaws.com/nodegroup
<a id="__codelineno-3-8" name="__codelineno-3-8" href="#scalability-docs-data-plane-__codelineno-3-8"></a>            operator: In
<a id="__codelineno-3-9" name="__codelineno-3-9" href="#scalability-docs-data-plane-__codelineno-3-9"></a>            values:
<a id="__codelineno-3-10" name="__codelineno-3-10" href="#scalability-docs-data-plane-__codelineno-3-10"></a>            - 8-core-node-group    # match your node group name
</code></pre></div>
<h2 id="scalability-docs-data-plane-use-compute-resources-efficiently">Use compute resources efficiently<a class="headerlink" href="#scalability-docs-data-plane-use-compute-resources-efficiently" title="Permanent link">&para;</a></h2>
<p>Compute resources include EC2 instances and availability zones. Using compute resources effectively will increase your scalability, availability, performance, and reduce your total cost. Efficient resource usage is extremely difficult to predict in an autoscaling environment with multiple applications. <a href="https://karpenter.sh/">Karpenter</a> was created to provision instances on-demand based on the workload needs to maximize utilization and flexibility.</p>
<p>Karpenter allows workloads to declare the type of compute resources it needs without first creating node groups or configuring label taints for specific nodes. See the <a href="https://aws.github.io/aws-eks-best-practices/karpenter/">Karpenter best practices</a> for more information. Consider enabling <a href="https://aws.github.io/aws-eks-best-practices/karpenter/#configure-requestslimits-for-all-non-cpu-resources-when-using-consolidation">consolidation</a> in your Karpenter provisioner to replace nodes that are under utilized.</p>
<h2 id="scalability-docs-data-plane-automate-amazon-machine-image-ami-updates">Automate Amazon Machine Image (AMI) updates<a class="headerlink" href="#scalability-docs-data-plane-automate-amazon-machine-image-ami-updates" title="Permanent link">&para;</a></h2>
<p>Keeping worker node components up to date will make sure you have the latest security patches and compatible features with the Kubernetes API. Updating the kublet is the most important component for Kubernetes functionality, but automating OS, kernel, and locally installed application patches will reduce maintenance as you scale.</p>
<p>It is recommended that you use the latest <a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html">Amazon EKS optimized Amazon Linux 2</a> or <a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami-bottlerocket.html">Amazon EKS optimized Bottlerocket AMI</a> for your node image. Karpenter will automatically use the <a href="https://karpenter.sh/docs/concepts/provisioners/#instance-types">latest available AMI</a> to provision new nodes in the cluster. Managed node groups will update the AMI during a <a href="https://docs.aws.amazon.com/eks/latest/userguide/update-managed-node-group.html">node group update</a> but will not update the AMI ID at node provisioning time.</p>
<p>For Managed Node Groups you need to update the Auto Scaling Group (ASG) launch template with new AMI IDs when they are available for patch releases. AMI minor versions (e.g. 1.23.5 to 1.24.3) will be available in the EKS console and API as <a href="https://docs.aws.amazon.com/eks/latest/userguide/update-managed-node-group.html">upgrades for the node group</a>. Patch release versions (e.g. 1.23.5 to 1.23.6) will not be presented as upgrades for the node groups. If you want to keep your node group up to date with AMI patch releases you need to create new launch template version and let the node group replace instances with the new AMI release.</p>
<p>You can find the latest available AMI from <a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html">this page</a> or use the AWS CLI.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#scalability-docs-data-plane-__codelineno-4-1"></a>aws ssm get-parameter \
<a id="__codelineno-4-2" name="__codelineno-4-2" href="#scalability-docs-data-plane-__codelineno-4-2"></a>  --name /aws/service/eks/optimized-ami/1.24/amazon-linux-2/recommended/image_id \
<a id="__codelineno-4-3" name="__codelineno-4-3" href="#scalability-docs-data-plane-__codelineno-4-3"></a>  --query &quot;Parameter.Value&quot; \
<a id="__codelineno-4-4" name="__codelineno-4-4" href="#scalability-docs-data-plane-__codelineno-4-4"></a>  --output text
</code></pre></div>
<h2 id="scalability-docs-data-plane-use-multiple-ebs-volumes-for-containers">Use multiple EBS volumes for containers<a class="headerlink" href="#scalability-docs-data-plane-use-multiple-ebs-volumes-for-containers" title="Permanent link">&para;</a></h2>
<p>EBS volumes have input/output (I/O) quota based on the type of volume (e.g. gp3) and the size of the disk. If your applications share a single EBS root volume with the host this can exhaust the disk quota for the entire host and cause other applications to wait for available capacity. Applications write to disk if they write files to their overlay partition, mount a local volume from the host, and also when they log to standard out (STDOUT) depending on the logging agent used.</p>
<p>To avoid disk I/O exhaustion you should mount a second volume to the container state folder (e.g. /run/containerd), use separate EBS volumes for workload storage, and disable unnecessary local logging.</p>
<p>To mount a second volume to your EC2 instances using <a href="https://eksctl.io/">eksctl</a> you can use a node group with this configuration:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#scalability-docs-data-plane-__codelineno-5-1"></a>managedNodeGroups:
<a id="__codelineno-5-2" name="__codelineno-5-2" href="#scalability-docs-data-plane-__codelineno-5-2"></a>  - name: al2-workers
<a id="__codelineno-5-3" name="__codelineno-5-3" href="#scalability-docs-data-plane-__codelineno-5-3"></a>    amiFamily: AmazonLinux2
<a id="__codelineno-5-4" name="__codelineno-5-4" href="#scalability-docs-data-plane-__codelineno-5-4"></a>    desiredCapacity: 2
<a id="__codelineno-5-5" name="__codelineno-5-5" href="#scalability-docs-data-plane-__codelineno-5-5"></a>    volumeSize: 80
<a id="__codelineno-5-6" name="__codelineno-5-6" href="#scalability-docs-data-plane-__codelineno-5-6"></a>    additionalVolumes:
<a id="__codelineno-5-7" name="__codelineno-5-7" href="#scalability-docs-data-plane-__codelineno-5-7"></a>      - volumeName: &#39;/dev/sdz&#39;
<a id="__codelineno-5-8" name="__codelineno-5-8" href="#scalability-docs-data-plane-__codelineno-5-8"></a>        volumeSize: 100
<a id="__codelineno-5-9" name="__codelineno-5-9" href="#scalability-docs-data-plane-__codelineno-5-9"></a>    preBootstrapCommands:
<a id="__codelineno-5-10" name="__codelineno-5-10" href="#scalability-docs-data-plane-__codelineno-5-10"></a>      - &quot;systemctl stop containerd&quot;
<a id="__codelineno-5-11" name="__codelineno-5-11" href="#scalability-docs-data-plane-__codelineno-5-11"></a>      - &quot;mkfs -t ext4 /dev/nvme1n1&quot;
<a id="__codelineno-5-12" name="__codelineno-5-12" href="#scalability-docs-data-plane-__codelineno-5-12"></a>      - &quot;rm -rf /var/lib/containerd/*&quot;
<a id="__codelineno-5-13" name="__codelineno-5-13" href="#scalability-docs-data-plane-__codelineno-5-13"></a>      - &quot;mount /dev/nvme1n1 /var/lib/containerd/&quot;
<a id="__codelineno-5-14" name="__codelineno-5-14" href="#scalability-docs-data-plane-__codelineno-5-14"></a>      - &quot;systemctl start containerd&quot;
</code></pre></div>
<p>If you are using terraform to provision your node groups please see examples in <a href="https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/main/examples/node-groups/managed-node-groups/main.tf">EKS Blueprints for terraform</a>. If you are using Karpenter to provision nodes you can use <a href="https://karpenter.sh/docs/concepts/node-templates/#specblockdevicemappings"><code>blockDeviceMappings</code></a> with node user-data to add additional volumes.</p>
<p>To mount an EBS volume directly to your pod you should use the <a href="https://github.com/kubernetes-sigs/aws-ebs-csi-driver">AWS EBS CSI driver</a> and consume a volume with a storage class.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#scalability-docs-data-plane-__codelineno-6-1"></a>---
<a id="__codelineno-6-2" name="__codelineno-6-2" href="#scalability-docs-data-plane-__codelineno-6-2"></a>apiVersion: storage.k8s.io/v1
<a id="__codelineno-6-3" name="__codelineno-6-3" href="#scalability-docs-data-plane-__codelineno-6-3"></a>kind: StorageClass
<a id="__codelineno-6-4" name="__codelineno-6-4" href="#scalability-docs-data-plane-__codelineno-6-4"></a>metadata:
<a id="__codelineno-6-5" name="__codelineno-6-5" href="#scalability-docs-data-plane-__codelineno-6-5"></a>  name: ebs-sc
<a id="__codelineno-6-6" name="__codelineno-6-6" href="#scalability-docs-data-plane-__codelineno-6-6"></a>provisioner: ebs.csi.aws.com
<a id="__codelineno-6-7" name="__codelineno-6-7" href="#scalability-docs-data-plane-__codelineno-6-7"></a>volumeBindingMode: WaitForFirstConsumer
<a id="__codelineno-6-8" name="__codelineno-6-8" href="#scalability-docs-data-plane-__codelineno-6-8"></a>---
<a id="__codelineno-6-9" name="__codelineno-6-9" href="#scalability-docs-data-plane-__codelineno-6-9"></a>apiVersion: v1
<a id="__codelineno-6-10" name="__codelineno-6-10" href="#scalability-docs-data-plane-__codelineno-6-10"></a>kind: PersistentVolumeClaim
<a id="__codelineno-6-11" name="__codelineno-6-11" href="#scalability-docs-data-plane-__codelineno-6-11"></a>metadata:
<a id="__codelineno-6-12" name="__codelineno-6-12" href="#scalability-docs-data-plane-__codelineno-6-12"></a>  name: ebs-claim
<a id="__codelineno-6-13" name="__codelineno-6-13" href="#scalability-docs-data-plane-__codelineno-6-13"></a>spec:
<a id="__codelineno-6-14" name="__codelineno-6-14" href="#scalability-docs-data-plane-__codelineno-6-14"></a>  accessModes:
<a id="__codelineno-6-15" name="__codelineno-6-15" href="#scalability-docs-data-plane-__codelineno-6-15"></a>    - ReadWriteOnce
<a id="__codelineno-6-16" name="__codelineno-6-16" href="#scalability-docs-data-plane-__codelineno-6-16"></a>  storageClassName: ebs-sc
<a id="__codelineno-6-17" name="__codelineno-6-17" href="#scalability-docs-data-plane-__codelineno-6-17"></a>  resources:
<a id="__codelineno-6-18" name="__codelineno-6-18" href="#scalability-docs-data-plane-__codelineno-6-18"></a>    requests:
<a id="__codelineno-6-19" name="__codelineno-6-19" href="#scalability-docs-data-plane-__codelineno-6-19"></a>      storage: 4Gi
<a id="__codelineno-6-20" name="__codelineno-6-20" href="#scalability-docs-data-plane-__codelineno-6-20"></a>---
<a id="__codelineno-6-21" name="__codelineno-6-21" href="#scalability-docs-data-plane-__codelineno-6-21"></a>apiVersion: v1
<a id="__codelineno-6-22" name="__codelineno-6-22" href="#scalability-docs-data-plane-__codelineno-6-22"></a>kind: Pod
<a id="__codelineno-6-23" name="__codelineno-6-23" href="#scalability-docs-data-plane-__codelineno-6-23"></a>metadata:
<a id="__codelineno-6-24" name="__codelineno-6-24" href="#scalability-docs-data-plane-__codelineno-6-24"></a>  name: app
<a id="__codelineno-6-25" name="__codelineno-6-25" href="#scalability-docs-data-plane-__codelineno-6-25"></a>spec:
<a id="__codelineno-6-26" name="__codelineno-6-26" href="#scalability-docs-data-plane-__codelineno-6-26"></a>  containers:
<a id="__codelineno-6-27" name="__codelineno-6-27" href="#scalability-docs-data-plane-__codelineno-6-27"></a>  - name: app
<a id="__codelineno-6-28" name="__codelineno-6-28" href="#scalability-docs-data-plane-__codelineno-6-28"></a>    image: public.ecr.aws/docker/library/nginx
<a id="__codelineno-6-29" name="__codelineno-6-29" href="#scalability-docs-data-plane-__codelineno-6-29"></a>    volumeMounts:
<a id="__codelineno-6-30" name="__codelineno-6-30" href="#scalability-docs-data-plane-__codelineno-6-30"></a>    - name: persistent-storage
<a id="__codelineno-6-31" name="__codelineno-6-31" href="#scalability-docs-data-plane-__codelineno-6-31"></a>      mountPath: /data
<a id="__codelineno-6-32" name="__codelineno-6-32" href="#scalability-docs-data-plane-__codelineno-6-32"></a>  volumes:
<a id="__codelineno-6-33" name="__codelineno-6-33" href="#scalability-docs-data-plane-__codelineno-6-33"></a>  - name: persistent-storage
<a id="__codelineno-6-34" name="__codelineno-6-34" href="#scalability-docs-data-plane-__codelineno-6-34"></a>    persistentVolumeClaim:
<a id="__codelineno-6-35" name="__codelineno-6-35" href="#scalability-docs-data-plane-__codelineno-6-35"></a>      claimName: ebs-claim
</code></pre></div>
<h2 id="scalability-docs-data-plane-avoid-instances-with-low-ebs-attach-limits-if-workloads-use-ebs-volumes">Avoid instances with low EBS attach limits if workloads use EBS volumes<a class="headerlink" href="#scalability-docs-data-plane-avoid-instances-with-low-ebs-attach-limits-if-workloads-use-ebs-volumes" title="Permanent link">&para;</a></h2>
<p>EBS is one of the easiest ways for workloads to have persistent storage, but it also comes with scalability limitations. Each instance type has a maximum number of <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/volume_limits.html">EBS volumes that can be attached</a>. Workloads need to declare what instance types they should run on and limit the number of replicas on a single instance with Kubernetes taints.</p>
<h2 id="scalability-docs-data-plane-disable-unnecessary-logging-to-disk">Disable unnecessary logging to disk<a class="headerlink" href="#scalability-docs-data-plane-disable-unnecessary-logging-to-disk" title="Permanent link">&para;</a></h2>
<p>Avoid unnecessary local logging by not running your applications with debug logging in production and disabling logging that reads and writes to disk frequently. Journald is the local logging service that keeps a log buffer in memory and flushes to disk periodically. Journald is preferred over syslog which logs every line immediately to disk. Disabling syslog also lowers the total amount of storage you need and avoids needing complicated log rotation rules. To disable syslog you can add the following snippet to your cloud-init configuration:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#scalability-docs-data-plane-__codelineno-7-1"></a>runcmd:
<a id="__codelineno-7-2" name="__codelineno-7-2" href="#scalability-docs-data-plane-__codelineno-7-2"></a>  - [ systemctl, disable, --now, syslog.service ]
</code></pre></div>
<h2 id="scalability-docs-data-plane-patch-instances-in-place-when-os-update-speed-is-a-necessity">Patch instances in place when OS update speed is a necessity<a class="headerlink" href="#scalability-docs-data-plane-patch-instances-in-place-when-os-update-speed-is-a-necessity" title="Permanent link">&para;</a></h2>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Patching instances in place should only be done when required. Amazon recommends treating infrastructure as immutable and thoroughly testing updates that are promoted through lower environments the same way applications are. This section applies when that is not possible.</p>
</div>
<p>It takes seconds to install a package on an existing Linux host without disrupting containerized workloads. The package can be installed and validated without cordoning, draining, or replacing the instance.</p>
<p>To replace an instance you first need to create, validate, and distribute new AMIs. The instance needs to have a replacement created, and the old instance needs to be cordoned and drained. Then workloads need to be created on the new instance, verified, and repeated for all instances that need to be patched. It takes hours, days, or weeks to replace instances safely without disrupting workloads.</p>
<p>Amazon recommends using immutable infrastructure that is built, tested, and promoted from an automated, declarative system, but if you have a requirement to patch systems quickly then you will need to patch systems in place and replace them as new AMIs are made available. Because of the large time differential between patching and replacing systems we recommend using <a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html">AWS Systems Manager Patch Manager</a> to automate patching nodes when required to do so.</p>
<p>Patching nodes will allow you to quickly roll out security updates and replace the instances on a regular schedule after your AMI has been updated. If you are using an operating system with a read-only root file system like <a href="https://flatcar-linux.org/">Flatcar Container Linux</a> or <a href="https://github.com/bottlerocket-os/bottlerocket">Bottlerocket OS</a> we recommend using the update operators that work with those operating systems. The <a href="https://github.com/flatcar/flatcar-linux-update-operator">Flatcar Linux update operator</a> and <a href="https://github.com/bottlerocket-os/bottlerocket-update-operator">Bottlerocket update operator</a> will reboot instances to keep nodes up to date automatically.</p></section><section class="print-page" id="scalability-docs-cluster-services"><h1 id="scalability-docs-cluster-services-cluster-services">Cluster Services<a class="headerlink" href="#scalability-docs-cluster-services-cluster-services" title="Permanent link">&para;</a></h1>
<p>Cluster services run inside an EKS cluster, but they are not user workloads. If you have a Linux server you often need to run services like NTP, syslog, and a container runtime to support your workloads. Cluster services are similar, supporting services that help you automate and operate your cluster. In Kubernetes these are usually run in the kube-system namespace and some are run as <a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/">DaemonSets</a>.</p>
<p>Cluster services are expected to have a high up-time and are often critical during outages and for troubleshooting. If a core cluster service is not available you may lose access to data that can help recover or prevent an outage (e.g. high disk utilization). They should run on dedicated compute instances such as a separate node group or AWS Fargate. This will ensure that the cluster services are not impacted on shared instances by workloads that may be scaling up or using more resources.</p>
<h2 id="scalability-docs-cluster-services-scale-coredns">Scale CoreDNS<a class="headerlink" href="#scalability-docs-cluster-services-scale-coredns" title="Permanent link">&para;</a></h2>
<p>Scaling CoreDNS has two primary mechanisms. Reducing the number of calls to the CoreDNS service and increasing the number of replicas.</p>
<h3 id="scalability-docs-cluster-services-reduce-external-queries-by-lowering-ndots">Reduce external queries by lowering ndots<a class="headerlink" href="#scalability-docs-cluster-services-reduce-external-queries-by-lowering-ndots" title="Permanent link">&para;</a></h3>
<p>The ndots setting specifies how many periods (a.k.a. "dots") in a domain name are considered enough to avoid querying DNS. If your application has an ndots setting of 5 (default) and you request resources from an external domain such as api.example.com (2 dots) then CoreDNS will be queried for each search domain defined in /etc/resolv.conf for a more specific domain. By default the following domains will be searched before making an external request.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#scalability-docs-cluster-services-__codelineno-0-1"></a>api.example.&lt;namespace&gt;.svc.cluster.local
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#scalability-docs-cluster-services-__codelineno-0-2"></a>api.example.svc.cluster.local
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#scalability-docs-cluster-services-__codelineno-0-3"></a>api.example.cluster.local
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#scalability-docs-cluster-services-__codelineno-0-4"></a>api.example.&lt;region&gt;.compute.internal
</code></pre></div>
<p>The <code>namespace</code> and <code>region</code> values will be replaced with your workloads namespace and your compute region. You may have additional search domains based on your cluster settings.</p>
<p>You can reduce the number of requests to CoreDNS by <a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-dns-config">lowering the ndots option</a> of your workload or fully qualifying your domain requests by including a trailing . (e.g. <code>api.example.com.</code> ). If your workload connects to external services via DNS we recommend setting ndots to 2 so workloads do not make unnecessary, cluster DNS queries inside the cluster. You can set a different DNS server and search domain if the workload doesn’t require access to services inside the cluster.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#scalability-docs-cluster-services-__codelineno-1-1"></a>spec:
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#scalability-docs-cluster-services-__codelineno-1-2"></a>  dnsPolicy: &quot;None&quot;
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#scalability-docs-cluster-services-__codelineno-1-3"></a>  dnsConfig:
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#scalability-docs-cluster-services-__codelineno-1-4"></a>    options:
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#scalability-docs-cluster-services-__codelineno-1-5"></a>      - name: ndots
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#scalability-docs-cluster-services-__codelineno-1-6"></a>        value: &quot;2&quot;
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#scalability-docs-cluster-services-__codelineno-1-7"></a>      - name: edns0
</code></pre></div>
<p>If you lower ndots to a value that is too low or the domains you are connecting to do not include enough specificity (including trailing .) then it is possible DNS lookups will fail. Make sure you test how this setting will impact your workloads.</p>
<h3 id="scalability-docs-cluster-services-scale-coredns-horizontally">Scale CoreDNS Horizontally<a class="headerlink" href="#scalability-docs-cluster-services-scale-coredns-horizontally" title="Permanent link">&para;</a></h3>
<p>CoreDNS instances can scale by adding additional replicas to the deployment. It's recommended you use <a href="https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/">NodeLocal DNS</a> or the <a href="https://github.com/kubernetes-sigs/cluster-proportional-autoscaler">cluster proportional autoscaler</a> to scale CoreDNS.</p>
<p>NodeLocal DNS will require run one instance per node—as a DaemonSet—which requires more compute resources in the cluster, but it will avoid failed DNS requests and decrease the response time for DNS queries in the cluster. The cluster proportional autoscaler will scale CoreDNS based on the number of nodes or cores in the cluster. This isn’t a direct correlation to request queries, but can be useful depending on your workloads and cluster size. The default proportional scale is to add an additional replica for every 256 cores or 16 nodes in the cluster—whichever happens first.</p>
<h2 id="scalability-docs-cluster-services-scale-kubernetes-metrics-server-vertically">Scale Kubernetes Metrics Server Vertically<a class="headerlink" href="#scalability-docs-cluster-services-scale-kubernetes-metrics-server-vertically" title="Permanent link">&para;</a></h2>
<p>The Kubernetes Metrics Server supports horizontal and vertical scaling. By horizontally scaling the Metrics Server it will be highly available, but it will not scale horizontally to handle more cluster metrics. You will need to vertically scale the Metrics Server based on <a href="https://kubernetes-sigs.github.io/metrics-server/#scaling">their recommendations</a> as nodes and collected metrics are added to the cluster.</p>
<p>The Metrics Server keeps the data it collects, aggregates, and serves in memory. As a cluster grows, the amount of data the Metrics Server stores increases. In large clusters the Metrics Server will require more compute resources than the memory and CPU reservation specified in the default installation. You can use the <a href="https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler">Vertical Pod Autoscaler</a> (VPA) or <a href="https://github.com/kubernetes/autoscaler/tree/master/addon-resizer">Addon Resizer</a> to scale the Metrics Server. The Addon Resizer scales vertically in proportion to worker nodes and VPA scales based on CPU and memory usage.</p>
<h2 id="scalability-docs-cluster-services-coredns-lameduck-duration">CoreDNS lameduck duration<a class="headerlink" href="#scalability-docs-cluster-services-coredns-lameduck-duration" title="Permanent link">&para;</a></h2>
<p>Pods use the <code>kube-dns</code> Service for name resolution. Kubernetes uses destination NAT (DNAT) to redirect <code>kube-dns</code> traffic from nodes to CoreDNS backend pods. As you scale the CoreDNS Deployment, <code>kube-proxy</code> updates iptables rules and chains on nodes to redirect DNS traffic to CoreDNS pods. Propagating new endpoints when you scale up and deleting rules when you scale down CoreDNS can take between 1 to 10 seconds depending on the size of the cluster. </p>
<p>This propagation delay can cause DNS lookup failures when a CoreDNS pod gets terminated yet the node’s iptables rules haven’t been updated. In this scenario, the node may continue to send DNS queries to a terminated CoreDNS Pod. </p>
<p>You can reduce DNS lookup failures by setting a <a href="https://coredns.io/plugins/health/">lameduck</a> duration in your CoreDNS pods. While in lameduck mode, CoreDNS will continue to respond to in-flight requests. Setting a lameduck duration will delay the CoreDNS shutdown process, allowing nodes the time they need to update their iptables rules and chains. </p>
<p>We recommend setting CoreDNS lameduck duration to 30 seconds. </p>
<h2 id="scalability-docs-cluster-services-coredns-readiness-probe">CoreDNS readiness probe<a class="headerlink" href="#scalability-docs-cluster-services-coredns-readiness-probe" title="Permanent link">&para;</a></h2>
<p>We recommend using <code>/ready</code> instead of <code>/health</code> for CoreDNS's readiness probe.</p>
<p>In alignment with the earlier recommendation to set the lameduck duration to 30 seconds, providing ample time for the node's iptables rules to be updated before pod termination, employing <code>/ready</code> instead of <code>/health</code> for the CoreDNS readiness probe ensures that the CoreDNS pod is fully prepared at startup to promptly respond to DNS requests.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#scalability-docs-cluster-services-__codelineno-2-1"></a><span class="nt">readinessProbe</span><span class="p">:</span>
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#scalability-docs-cluster-services-__codelineno-2-2"></a><span class="w">  </span><span class="nt">httpGet</span><span class="p">:</span>
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#scalability-docs-cluster-services-__codelineno-2-3"></a><span class="w">    </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/ready</span>
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#scalability-docs-cluster-services-__codelineno-2-4"></a><span class="w">    </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8181</span>
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#scalability-docs-cluster-services-__codelineno-2-5"></a><span class="w">    </span><span class="nt">scheme</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">HTTP</span>
</code></pre></div>
<p>For more information about the CoreDNS Ready plugin please refer to <a href="https://coredns.io/plugins/ready/">https://coredns.io/plugins/ready/</a></p>
<h2 id="scalability-docs-cluster-services-logging-and-monitoring-agents">Logging and monitoring agents<a class="headerlink" href="#scalability-docs-cluster-services-logging-and-monitoring-agents" title="Permanent link">&para;</a></h2>
<p>Logging and monitoring agents can add significant load to your cluster control plane because the agents query the API server to enrich logs and metrics with workload metadata. The agent on a node only has access to the local node resources to see things like container and process name. Querying the API server it can add more details such as Kubernetes deployment name and labels. This can be extremely helpful for troubleshooting but detrimental to scaling.</p>
<p>Because there are so many different options for logging and monitoring we cannot show examples for every provider. With <a href="https://docs.fluentbit.io/manual/pipeline/filters/kubernetes">fluentbit</a> we recommend enabling Use_Kubelet to fetch metadata from the local kubelet instead of the Kubernetes API Server and set <code>Kube_Meta_Cache_TTL</code> to a number that reduces repeated calls when data can be cached (e.g. 60).</p>
<p>Scaling monitoring and logging has two general options:</p>
<ul>
<li>Disable integrations</li>
<li>Sampling and filtering</li>
</ul>
<p>Disabling integrations is often not an option because you lose log metadata. This eliminates the API scaling problem, but it will introduce other issues by not having the required metadata when needed.</p>
<p>Sampling and filtering reduces the number of metrics and logs that are collected. This will lower the amount of requests to the Kubernetes API, and it will reduce the amount of storage needed for the metrics and logs that are collected. Reducing the storage costs will lower the cost for the overall system.</p>
<p>The ability to configure sampling depends on the agent software and can be implemented at different points of ingestion. It’s important to add sampling as close to the agent as possible because that is likely where the API server calls happen. Contact your provider to find out more about sampling support.</p>
<p>If you are using CloudWatch and CloudWatch Logs you can add agent filtering using patterns <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html">described in the documentation</a>.</p>
<p>To avoid losing logs and metrics you should send your data to a system that can buffer data in case of an outage on the receiving endpoint. With fluentbit you can use <a href="https://docs.fluentbit.io/manual/pipeline/outputs/firehose">Amazon Kinesis Data Firehose</a> to temporarily keep data which can reduce the chance of overloading your final data storage location.</p></section><section class="print-page" id="scalability-docs-workloads"><h1 id="scalability-docs-workloads-workloads">Workloads<a class="headerlink" href="#scalability-docs-workloads-workloads" title="Permanent link">&para;</a></h1>
<p>Workloads have an impact on how large your cluster can scale. Workloads that use the Kubernetes APIs heavily will limit the total amount of workloads you can have in a single cluster, but there are some defaults you can change to help reduce the load.</p>
<p>Workloads in a Kubernetes cluster have access to features that integrate with the Kubernetes API (e.g. Secrets and ServiceAccounts), but these features are not always required and should be disabled if they’re not being used. Limiting workload access and dependence on the Kubernetes control plane will increase the number of workloads you can run in the cluster and improve the security of your clusters by removing unnecessary access to workloads and implementing least privilege practices. Please read the <a href="https://aws.github.io/aws-eks-best-practices/security/docs/">security best practices</a> for more information.</p>
<h2 id="scalability-docs-workloads-use-ipv6-for-pod-networking">Use IPv6 for pod networking<a class="headerlink" href="#scalability-docs-workloads-use-ipv6-for-pod-networking" title="Permanent link">&para;</a></h2>
<p>You cannot transition a VPC from IPv4 to IPv6 so enabling IPv6 before provisioning a cluster is important. If you enable IPv6 in a VPC it does not mean you have to use it and if your pods and services use IPv6 you can still route traffic to and from IPv4 addresses. Please see the <a href="https://aws.github.io/aws-eks-best-practices/networking/index/">EKS networking best practices</a> for more information.</p>
<p>Using <a href="https://docs.aws.amazon.com/eks/latest/userguide/cni-ipv6.html">IPv6 in your cluster</a> avoids some of the most common cluster and workload scaling limits. IPv6 avoids IP address exhaustion where pods and nodes cannot be created because no IP address is available. It also has per node performance improvements because pods receive IP addresses faster by reducing the number of ENI attachments per node. You can achieve similar node performance by using <a href="https://aws.github.io/aws-eks-best-practices/networking/prefix-mode/">IPv4 prefix mode in the VPC CNI</a>, but you still need to make sure you have enough IP addresses available in the VPC.</p>
<h2 id="scalability-docs-workloads-limit-number-of-services-per-namespace">Limit number of services per namespace<a class="headerlink" href="#scalability-docs-workloads-limit-number-of-services-per-namespace" title="Permanent link">&para;</a></h2>
<p>The maximum number of <a href="https://github.com/kubernetes/community/blob/master/sig-scalability/configs-and-limits/thresholds.md">services in a namespaces is 5,000 and the maximum number of services in a cluster is 10,000</a>. To help organize workloads and services, increase performance, and to avoid cascading impact for namespace scoped resources we recommend limiting the number of services per namespace to 500.</p>
<p>The number of IP tables rules that are created per node with kube-proxy grows with the total number of services in the cluster. Generating thousands of IP tables rules and routing packets through those rules have a performance impact on the nodes and add network latency.</p>
<p>Create Kubernetes namespaces that encompass a single application environment so long as the number of services per namespace is under 500. This will keep service discovery small enough to avoid service discovery limits and can also help you avoid service naming collisions. Applications environments (e.g. dev, test, prod) should use separate EKS clusters instead of namespaces.</p>
<h2 id="scalability-docs-workloads-understand-elastic-load-balancer-quotas">Understand Elastic Load Balancer Quotas<a class="headerlink" href="#scalability-docs-workloads-understand-elastic-load-balancer-quotas" title="Permanent link">&para;</a></h2>
<p>When creating your services consider what type of load balancing you will use (e.g. Network Load Balancer (NLB) or Application Load Balancer (ALB)). Each load balancer type provides different functionality and have <a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-limits.html">different quotas</a>. Some of the default quotas can be adjusted, but there are some quota maximums which cannot be changed. To view your account quotas and usage view the <a href="http://console.aws.amazon.com/servicequotas">Service Quotas dashboard</a> in the AWS console.</p>
<p>For example, the default ALB targets is 1000. If you have a service with more than 1000 endpoints you will need to increase the quota or split the service across multiple ALBs or use Kubernetes Ingress. The default NLB targets is 3000, but is limited to 500 targets per AZ. If your cluster runs more than 500 pods for an NLB service you will need to use multiple AZs or request a quota limit increase.</p>
<p>An alternative to using a load balancer coupled to a service is to use an <a href="https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/">ingress controller</a>. The AWS Load Balancer controller can create ALBs for ingress resources, but you may consider running a dedicated controller in your cluster. An in-cluster ingress controller allows you to expose multiple Kubernetes services from a single load balancer by running a reverse proxy inside your cluster. Controllers have different features such as support for the <a href="https://gateway-api.sigs.k8s.io/">Gateway API</a> which may have benefits depending on how many and how large your workloads are.</p>
<h2 id="scalability-docs-workloads-use-route-53-global-accelerator-or-cloudfront">Use Route 53, Global Accelerator, or CloudFront<a class="headerlink" href="#scalability-docs-workloads-use-route-53-global-accelerator-or-cloudfront" title="Permanent link">&para;</a></h2>
<p>To make a service using multiple load balancers available as a single endpoint you need to use <a href="https://aws.amazon.com/cloudfront/">Amazon CloudFront</a>, <a href="https://aws.amazon.com/global-accelerator/">AWS Global Accelerator</a>, or <a href="https://aws.amazon.com/route53/">Amazon Route 53</a> to expose all of the load balancers as a single, customer facing endpoint. Each options has different benefits and can be used separately or together depending on your needs.</p>
<p>Route 53 can expose multiple load balancers under a common name and can send traffic to each of them based on the weight assigned. You can read more about <a href="https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-values-weighted.html#rrsets-values-weighted-weight">DNS weights in the documentation</a> and you can read how to implement them with the <a href="https://github.com/kubernetes-sigs/external-dns">Kubernetes external DNS controller</a> in the <a href="https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.4/guide/integrations/external_dns/#usage">AWS Load Balancer Controller documentation</a>.</p>
<p>Global Accelerator can route workloads to the nearest region based on request IP address. This may be useful for workloads that are deployed to multiple regions, but it does not improve routing to a single cluster in a single region. Using Route 53 in combination with the Global Accelerator has additional benefits such as health checking and automatic failover if an AZ is not available. You can see an example of using Global Accelerator with Route 53 in <a href="https://aws.amazon.com/blogs/containers/operating-a-multi-regional-stateless-application-using-amazon-eks/">this blog post</a>.</p>
<p>CloudFront can be use with Route 53 and Global Accelerator or by itself to route traffic to multiple destinations. CloudFront caches assets being served from the origin sources which may reduce bandwidth requirements depending on what you are serving.</p>
<h2 id="scalability-docs-workloads-use-endpointslices-instead-of-endpoints">Use EndpointSlices instead of Endpoints<a class="headerlink" href="#scalability-docs-workloads-use-endpointslices-instead-of-endpoints" title="Permanent link">&para;</a></h2>
<p>When discovering pods that match a service label you should use <a href="https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/">EndpointSlices</a> instead of Endpoints. Endpoints were a simple way to expose services at small scales but large services that automatically scale or have updates causes a lot of traffic on the Kubernetes control plane. EndpointSlices have automatic grouping which enable things like topology aware hints.</p>
<p>Not all controllers use EndpointSlices by default. You should verify your controller settings and enable it if needed. For the <a href="https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.4/deploy/configurations/#controller-command-line-flags">AWS Load Balancer Controller</a> you should enable the <code>--enable-endpoint-slices</code> optional flag to use EndpointSlices.</p>
<h2 id="scalability-docs-workloads-use-immutable-and-external-secrets-if-possible">Use immutable and external secrets if possible<a class="headerlink" href="#scalability-docs-workloads-use-immutable-and-external-secrets-if-possible" title="Permanent link">&para;</a></h2>
<p>The kubelet keeps a cache of the current keys and values for the Secrets that are used in volumes for pods on that node. The kubelet sets a watch on the Secrets to detect changes. As the cluster scales, the growing number of watches can negatively impact the API server performance.</p>
<p>There are two strategies to reduce the number of watches on Secrets:</p>
<ul>
<li>For applications that don’t need access to Kubernetes resources, you can disable auto-mounting service account secrets by setting automountServiceAccountToken: false</li>
<li>If your application’s secrets are static and will not be modified in the future, mark the <a href="https://kubernetes.io/docs/concepts/configuration/secret/#secret-immutable">secret as immutable</a>. The kubelet does not maintain an API watch for immutable secrets.</li>
</ul>
<p>To disable automatically mounting a service account to pods you can use the following setting in your workload. You can override these settings if specific workloads need a service account.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#scalability-docs-workloads-__codelineno-0-1"></a>apiVersion: v1
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#scalability-docs-workloads-__codelineno-0-2"></a>kind: ServiceAccount
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#scalability-docs-workloads-__codelineno-0-3"></a>metadata:
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#scalability-docs-workloads-__codelineno-0-4"></a>  name: app
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#scalability-docs-workloads-__codelineno-0-5"></a>automountServiceAccountToken: true
</code></pre></div>
<p>Monitor the number of secrets in the cluster before it exceeds the limit of 10,000. You can see a total count of secrets in a cluster with the following command. You should monitor this limit through your cluster monitoring tooling.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#scalability-docs-workloads-__codelineno-1-1"></a>kubectl get secrets -A | wc -l
</code></pre></div>
<p>You should set up monitoring to alert a cluster admin before this limit is reached. Consider using external secrets management options such as <a href="https://aws.amazon.com/kms/">AWS Key Management Service (AWS KMS)</a> or <a href="https://www.vaultproject.io/">Hashicorp Vault</a> with the <a href="https://secrets-store-csi-driver.sigs.k8s.io/">Secrets Store CSI driver</a>.</p>
<h2 id="scalability-docs-workloads-limit-deployment-history">Limit Deployment history<a class="headerlink" href="#scalability-docs-workloads-limit-deployment-history" title="Permanent link">&para;</a></h2>
<p>Pods can be slow when creating, updating, or deleting because old objects are still tracked in the cluster. You can reduce the <code>revisionHistoryLimit</code> of <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#clean-up-policy">deployments</a> to cleanup older ReplicaSets which will lower to total amount of objects tracked by the Kubernetes Controller Manager. The default history limit for Deployments in 10.</p>
<p>If your cluster creates a lot of job objects through CronJobs or other mechanisms you should use the <a href="https://kubernetes.io/docs/concepts/workloads/controllers/ttlafterfinished/"><code>ttlSecondsAfterFinished</code> setting</a> to automatically clean up old pods in the cluster. This will remove successfully executed jobs from the job history after a specified amount of time.</p>
<h2 id="scalability-docs-workloads-disable-enableservicelinks-by-default">Disable enableServiceLinks by default<a class="headerlink" href="#scalability-docs-workloads-disable-enableservicelinks-by-default" title="Permanent link">&para;</a></h2>
<p>When a Pod runs on a Node, the kubelet adds a set of environment variables for each active Service. Linux processes have a maximum size for their environment which can be reached if you have too many services in your namespace. The number of services per namespace should not exceed 5,000. After this, the number of service environment variables outgrows shell limits, causing Pods to crash on startup. </p>
<p>There are other reasons pods should not use service environment variables for service discovery. Environment variable name clashes, leaking service names, and total environment size are a few. You should use CoreDNS for discovering service endpoints.</p>
<h2 id="scalability-docs-workloads-limit-dynamic-admission-webhooks-per-resource">Limit dynamic admission webhooks per resource<a class="headerlink" href="#scalability-docs-workloads-limit-dynamic-admission-webhooks-per-resource" title="Permanent link">&para;</a></h2>
<p><a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/">Dynamic Admission Webhooks</a> include admission webhooks and mutating webhooks. They are API endpoints not part of the Kubernetes Control Plane that are called in sequence when a resource is sent to the Kubernetes API. Each webhook has a default timeout of 10 seconds and can increase the amount of time an API request takes if you have multiple webhooks or any of them timeout.</p>
<p>Make sure your webhooks are highly available—especially during an AZ incident—and the <a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#failure-policy">failurePolicy</a> is set properly to reject the resource or ignore the failure. Do not call webhooks when not needed by allowing --dry-run kubectl commands to bypass the webhook.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#scalability-docs-workloads-__codelineno-2-1"></a>apiVersion: admission.k8s.io/v1
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#scalability-docs-workloads-__codelineno-2-2"></a>kind: AdmissionReview
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#scalability-docs-workloads-__codelineno-2-3"></a>request:
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#scalability-docs-workloads-__codelineno-2-4"></a>  dryRun: False
</code></pre></div>
<p>Mutating webhooks can modify resources in frequent succession. If you have 5 mutating webhooks and deploy 50 resources etcd will store all versions of each resource until compaction runs—every 5 minutes—to remove old versions of modified resources. In this scenario when etcd removes superseded resources there will be 200 resource version removed from etcd and depending on the size of the resources may use considerable space on the etcd host until defragmentation runs every 15 minutes.</p>
<p>This defragmentation may cause pauses in etcd which could have other affects on the Kubernetes API and controllers. You should avoid frequent modification of large resources or modifying hundreds of resources in quick succession.</p></section><section class="print-page" id="scalability-docs-scaling_theory"><h1 id="scalability-docs-scaling_theory-kubernetes-scaling-theory">Kubernetes Scaling Theory<a class="headerlink" href="#scalability-docs-scaling_theory-kubernetes-scaling-theory" title="Permanent link">&para;</a></h1>
<h2 id="scalability-docs-scaling_theory-nodes-vs-churn-rate">Nodes vs. Churn Rate<a class="headerlink" href="#scalability-docs-scaling_theory-nodes-vs-churn-rate" title="Permanent link">&para;</a></h2>
<p>Often when we discuss the scalability of Kubernetes, we do so in terms of how many nodes there are in a single cluster. Interestingly, this is seldom the most useful metric for understanding scalability. For example, a 5,000 node cluster with a large but fixed number of pods would not put a great deal of stress on the control plane after the initial setup. However, if we took a 1,000 node cluster and tried creating 10,000 short lived jobs in less than a minute, it would put a great deal of sustained pressure on the control plane. </p>
<p>Simply using the number of nodes to understand scaling can be misleading. It’s better to think in terms of the rate of change that occurs within a specific period of time (let’s use a 5 minute interval for this discussion, as this is what Prometheus queries typically use by default). Let’s explore why framing the problem in terms of the rate of change can give us a better idea of what to tune to achieve our desired scale. </p>
<h2 id="scalability-docs-scaling_theory-thinking-in-queries-per-second">Thinking in Queries Per Second<a class="headerlink" href="#scalability-docs-scaling_theory-thinking-in-queries-per-second" title="Permanent link">&para;</a></h2>
<p>Kubernetes has a number of protection mechanisms for each component - the Kubelet, Scheduler, Kube Controller Manager, and API server - to prevent overwhelming the next link in the Kubernetes chain. For example, the Kubelet has a flag to throttle calls to the API server at a certain rate. These protection mechanisms are generally, but not always, expressed in terms of queries allowed on a per second basis or QPS. </p>
<p>Great care must be taken when changing these QPS settings. Removing one bottleneck, such as the queries per second on a Kubelet will have an impact on other down stream components. This can and will overwhelm the system above a certain rate, so understanding and monitoring each part of the service chain is key to successfully scaling workloads on Kubernetes.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The API server has a more complex system with introduction of API Priority and Fairness which we will discuss separately.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Caution, some metrics seem like the right fit but are in fact measuring something else. As an example, <code>kubelet_http_inflight_requests</code> relates to just the metrics server in Kubelet, not the number of requests from Kubelet to apiserver requests. This could cause us to misconfigure the QPS flag on the Kubelet. A query on audit logs for a particular Kubelet would be a more reliable way to check metrics.</p>
</div>
<h2 id="scalability-docs-scaling_theory-scaling-distributed-components">Scaling Distributed Components<a class="headerlink" href="#scalability-docs-scaling_theory-scaling-distributed-components" title="Permanent link">&para;</a></h2>
<p>Since EKS is a managed service, let’s split the Kubernetes components into two categories: AWS managed components which include etcd, Kube Controller Manager, and the Scheduler (on the left part of diagram), and customer configurable components such as the Kubelet, Container Runtime, and the various operators that call AWS APIs such as the Networking and Storage drivers (on the right part of diagram). We leave the API server in the middle even though it is AWS managed, as the settings for API Priority and Fairness can be configured by customers. </p>
<p><img alt="Kubernetes components" src="../scalability/images/k8s-components.png" /></p>
<h2 id="scalability-docs-scaling_theory-upstream-and-downstream-bottlenecks">Upstream and Downstream Bottlenecks<a class="headerlink" href="#scalability-docs-scaling_theory-upstream-and-downstream-bottlenecks" title="Permanent link">&para;</a></h2>
<p>As we monitor each service, it’s important to look at metrics in both directions to look for bottlenecks. Let’s learn how to do this by using Kubelet as an example. Kubelet talks both to the API server and the container runtime; <strong>how</strong> and <strong>what</strong> do we need to monitor to detect whether either component is experiencing an issue?</p>
<h3 id="scalability-docs-scaling_theory-how-many-pods-per-node">How many Pods per Node<a class="headerlink" href="#scalability-docs-scaling_theory-how-many-pods-per-node" title="Permanent link">&para;</a></h3>
<p>When we look at scaling numbers, such as how many pods can run on a node, we could take the 110 pods per node that upstream supports at face value. </p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>https://kubernetes.io/docs/setup/best-practices/cluster-large/</p>
</div>
<p>However, your workload is likely more complex than what was tested in a scalability test in Upstream. To ensure we can service the number of pods we want to run in production, let’s make sure that the Kubelet is “keeping up” with the Containerd runtime. </p>
<p><img alt="Keeping up" src="../scalability/images/keeping-up.png" /></p>
<p>To oversimplify, the Kubelet is getting the status of the pods from the container runtime (in our case Containerd). What if we had too many pods changing status too quickly? If the rate of change is too high, requests [to the container runtime] can timeout.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Kubernetes is constantly evolving, this subsystem is currently undergoing changes. https://github.com/kubernetes/enhancements/issues/3386</p>
</div>
<p><img alt="Flow" src="../scalability/images/flow.png" />
<img alt="PLEG duration" src="../scalability/images/PLEG-duration.png" /></p>
<p>In the graph above, we see a flat line indicating we have just hit the timeout value for the pod lifecycle event generation duration metric. If you would like to see this in your own cluster you could use the following PromQL syntax.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#scalability-docs-scaling_theory-__codelineno-0-1"></a>increase(kubelet_pleg_relist_duration_seconds_bucket{instance=&quot;$instance&quot;}[$__rate_interval])
</code></pre></div>
<p>If we witness this timeout behavior, we know we pushed the node over the limit it was capable of. We need to fix the cause of the timeout before proceeding further. This could be achieved by reducing the number of pods per node, or looking for errors that might be causing a high volume of retries (thus effecting the churn rate). The important take-away is that metrics are the best way to understand if a node is able to handle the churn rate of the pods assigned vs. using a fixed number. </p>
<h2 id="scalability-docs-scaling_theory-scale-by-metrics">Scale by Metrics<a class="headerlink" href="#scalability-docs-scaling_theory-scale-by-metrics" title="Permanent link">&para;</a></h2>
<p>While the concept of using metrics to optimize systems is an old one, it’s often overlooked as people begin their Kubernetes journey. Instead of focusing on specific numbers (i.e. 110 pods per node), we focus our efforts on finding the metrics that help us find bottlenecks in our system. Understanding the right thresholds for these metrics can give us a high degree of confidence our system is optimally configured.  </p>
<h3 id="scalability-docs-scaling_theory-the-impact-of-changes">The Impact of Changes<a class="headerlink" href="#scalability-docs-scaling_theory-the-impact-of-changes" title="Permanent link">&para;</a></h3>
<p>A common pattern that could get us into trouble is focusing on the first metric or log error that looks suspect. When we saw that the Kubelet was timing out earlier, we could try random things, such as increasing the per second rate that the Kubelet is allowed to send, etc. However, it is wise to look at the whole picture of everything downstream of the error we find first. <em>Make each change with purpose and backed by data</em>.</p>
<p>Downstream of the Kubelet would be the Containerd runtime (pod errors), DaemonSets such as the storage driver (CSI) and the network driver (CNI) that talk to the EC2 API, etc. </p>
<p><img alt="Flow add-ons" src="../scalability/images/flow-addons.png" /></p>
<p>Let’s continue our earlier example of the Kubelet not keeping up with the runtime. There are a number of points where we could bin pack a node so densely that it triggers errors. </p>
<p><img alt="Bottlenecks" src="../scalability/images/bottlenecks.png" /></p>
<p>When designing the right node size for our workloads these are easy-to-overlook signals that might be putting unnecessary pressure on the system thus limiting both our scale and performance.</p>
<h3 id="scalability-docs-scaling_theory-the-cost-of-unnecessary-errors">The Cost of Unnecessary Errors<a class="headerlink" href="#scalability-docs-scaling_theory-the-cost-of-unnecessary-errors" title="Permanent link">&para;</a></h3>
<p>Kubernetes controllers excel at retrying when error conditions arise, however this comes at a cost. These retries can increase the pressure on components such as the Kube Controller Manager. It is an important tenant of scale testing to monitor for such errors. </p>
<p>When fewer errors are occurring, it is easier spot issues in the system. By periodically ensuring that our clusters are error free before major operations (such as upgrades) we can simplify troubleshooting logs when unforeseen events happen.</p>
<h4 id="scalability-docs-scaling_theory-expanding-our-view">Expanding Our View<a class="headerlink" href="#scalability-docs-scaling_theory-expanding-our-view" title="Permanent link">&para;</a></h4>
<p>In large scale clusters with 1,000’s of nodes we don’t want to look for bottlenecks individually. In PromQL we can find the highest values in a data set using a function called topk; K being a variable we place the number of items we want. Here we use three nodes to get an idea whether all of the the Kubelets in the cluster are saturated. We have been looking at latency up to this point, now let’s see if the Kubelet is discarding events. </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#scalability-docs-scaling_theory-__codelineno-1-1"></a>topk(3, increase(kubelet_pleg_discard_events{}[$__rate_interval]))
</code></pre></div>
<p>Breaking this statement down. </p>
<ul>
<li>We use the Grafana variable <code>$__rate_interval</code> to ensure it gets the four samples it needs. This bypasses a complex topic in monitoring with a simple variable. </li>
<li><code>topk</code> give us just the top results and the number 3 limits those results to three. This is a useful function for cluster wide metrics.</li>
<li><code>{}</code> tell us there are no filters, normally you would put the job name of whatever the scraping rule, however since these names vary we will leave it blank. </li>
</ul>
<h4 id="scalability-docs-scaling_theory-splitting-the-problem-in-half">Splitting the Problem in Half<a class="headerlink" href="#scalability-docs-scaling_theory-splitting-the-problem-in-half" title="Permanent link">&para;</a></h4>
<p>To address a bottleneck in the system, we will take the approach of finding a metric that shows us there is a problem upstream or downstream as this allows us to split the problem in half. It will also be a core tenet of how we display our metrics data. </p>
<p>A good place to start with this process is the API server, as it allow us to see if there’s a problem with a client application or the Control Plane.</p></section><section class="print-page" id="scalability-docs-kcp_monitoring"><h1 id="scalability-docs-kcp_monitoring-control-plane-monitoring">Control Plane Monitoring<a class="headerlink" href="#scalability-docs-kcp_monitoring-control-plane-monitoring" title="Permanent link">&para;</a></h1>
<h2 id="scalability-docs-kcp_monitoring-api-server">API Server<a class="headerlink" href="#scalability-docs-kcp_monitoring-api-server" title="Permanent link">&para;</a></h2>
<p>When looking at our API server it’s important to remember that one of its functions is to throttle inbound requests to prevent overloading the control plane. What can seem like a bottleneck at the API server level might actually be protecting it from more serious issues. We need to factor in the pros and cons of increasing the volume of requests moving through the system. To make a determination if the API server values should be increased, here is small sampling of the things we need to be mindful of:</p>
<ol>
<li>What is the latency of requests moving through the system?</li>
<li>Is that latency the API server itself, or something “downstream” like etcd?</li>
<li>Is the API server queue depth a factor in this latency?</li>
<li>Are the API Priority and Fairness (APF) queues setup correctly for the API call patterns we want?</li>
</ol>
<h2 id="scalability-docs-kcp_monitoring-where-is-the-issue">Where is the issue?<a class="headerlink" href="#scalability-docs-kcp_monitoring-where-is-the-issue" title="Permanent link">&para;</a></h2>
<p>To start, we can use the metric for API latency to give us insight into how long it’s taking the API server to service requests. Let’s use the below PromQL and Grafana heatmap to display this data.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#scalability-docs-kcp_monitoring-__codelineno-0-1"></a>max(increase(apiserver_request_duration_seconds_bucket{subresource!=&quot;status&quot;,subresource!=&quot;token&quot;,subresource!=&quot;scale&quot;,subresource!=&quot;/healthz&quot;,subresource!=&quot;binding&quot;,subresource!=&quot;proxy&quot;,verb!=&quot;WATCH&quot;}[$__rate_interval])) by (le)
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>For an in depth write up on how to monitor the API server with the API dashboard used in this article, please see the following <a href="https://aws.amazon.com/blogs/containers/troubleshooting-amazon-eks-api-servers-with-prometheus/">blog</a></p>
</div>
<p><img alt="API request duration heatmap" src="../scalability/images/api-request-duration.png" /></p>
<p>These requests are all under the one second mark, which is a good indication that the control plane is handling requests in a timely fashion.  But what if that was not the case?</p>
<p>The format we are using in the above API Request Duration is a heatmap. What’s nice about the heatmap format, is that it tells us the timeout value for the API by default (60 sec). However, what we really need to know is at what threshold should this value be of concern before we reach the timeout threshold. For a rough guideline of what acceptable thresholds are we can use the upstream Kubernetes SLO, which can be found <a href="https://github.com/kubernetes/community/blob/master/sig-scalability/slos/slos.md#steady-state-slisslos">here</a></p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Notice the max function on this statement? When using metrics that are aggregating multiple servers (by default two API servers on EKS) it’s important not to average those servers together.</p>
</div>
<h3 id="scalability-docs-kcp_monitoring-asymmetrical-traffic-patterns">Asymmetrical traffic patterns<a class="headerlink" href="#scalability-docs-kcp_monitoring-asymmetrical-traffic-patterns" title="Permanent link">&para;</a></h3>
<p>What if one API server [pod] was lightly loaded, and the other heavily loaded? If we averaged those two numbers together we might misinterpret what was happening. For example, here we have three API servers but all of the load is on one of these API servers. As a rule anything that has multiple servers such as etcd and API servers should be broken out when investing scale and performance issues.</p>
<p><img alt="Total inflight requests" src="../scalability/images/inflight-requests.png" /></p>
<p>With the move to API Priority and Fairness the total number of requests on the system is only one factor to check to see if the API server is oversubscribed. Since the system now works off a series of queues, we must look to see if any of these queues are full and if the traffic for that queue is getting dropped. </p>
<p>Let’s look at these queues with the following query:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#scalability-docs-kcp_monitoring-__codelineno-1-1"></a>max without(instance)(apiserver_flowcontrol_request_concurrency_limit{})
</code></pre></div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more information on how API A&amp;F works please see the following <a href="https://aws.github.io/aws-eks-best-practices/scalability/docs/control-plane/#api-priority-and-fairness">best practices guide</a></p>
</div>
<p>Here we see the seven different priority groups that come by default on the cluster</p>
<p><img alt="Shared concurrency" src="../scalability/images/shared-concurrency.png" /></p>
<p>Next we want to see what percentage of that priority group is being used, so that we can understand if a certain priority level is being saturated. Throttling requests in the workload-low level might be desirable, however drops in a leader election level would not be. </p>
<p>The API Priority and Fairness (APF) system has a number of complex options, some of those options can have unintended consequences. A common issue we see in the field is increasing the queue depth to the point it starts adding unnecessary latency. We can monitor this problem by using the <code>apiserver_flowcontrol_current_inqueue_request</code> metric. We can check for drops using the <code>apiserver_flowcontrol_rejected_requests_total</code>. These metrics will be a non-zero value if any bucket exceeds its concurrency. </p>
<p><img alt="Requests in use" src="../scalability/images/requests-in-use.png" /></p>
<p>Increasing the queue depth can make the API Server a significant source of latency and should be done with care. We recommend being judicious with the number of queues created. For example, the number of shares on a EKS system is 600, if we create too many queues, this can reduce the shares in important queues that need the throughput such as the leader-election queue or system queue. Creating too many extra queues can make it more difficult to size theses queues correctly. </p>
<p>To focus on a simple impactful change you can make in APF we simply take shares from underutilized buckets and increase the size of buckets that are at their max usage. By intelligently redistributing the shares among these buckets, you can make drops less likely. </p>
<p>For more information, visit <a href="https://aws.github.io/aws-eks-best-practices/scalability/docs/control-plane/#api-priority-and-fairness">API Priority and Fairness settings</a> in the EKS Best Practices Guide.</p>
<h3 id="scalability-docs-kcp_monitoring-api-vs-etcd-latency">API vs. etcd latency<a class="headerlink" href="#scalability-docs-kcp_monitoring-api-vs-etcd-latency" title="Permanent link">&para;</a></h3>
<p>How can we use the metrics/logs of the API server to determine whether there’s a problem with API server, or a problem that’s upstream/downstream of the API server, or a combination of both. To understand this better, lets look at how API Server and etcd can be related, and how easy it can be to troubleshoot the wrong system.</p>
<p>In the below chart we see API server latency, but we also see much of this latency is correlated to the etcd server due to the bars in the graph showing most of the latency at the etcd level. If there is 15 secs of etcd latency at the same time there is 20 seconds of API server latency, then the majority of the latency is actually at the etcd level.</p>
<p>By looking at the whole flow, we see that it’s wise to not focus solely on the API Server, but also look for signals that indicate that etcd is under duress (i.e. slow apply counters increasing). Being able to quickly move to the right problem area with just a glance is what makes a dashboard powerful. </p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The dashboard in section can be found at https://github.com/RiskyAdventure/Troubleshooting-Dashboards/blob/main/api-troubleshooter.json</p>
</div>
<p><img alt="ETCD duress" src="../scalability/images/etcd-duress.png" /></p>
<h3 id="scalability-docs-kcp_monitoring-control-plane-vs-client-side-issues">Control plane vs. Client side issues<a class="headerlink" href="#scalability-docs-kcp_monitoring-control-plane-vs-client-side-issues" title="Permanent link">&para;</a></h3>
<p>In this chart we are looking for the API calls that took the most time to complete for that period. In this case we see a custom resource (CRD) is calling a APPLY function that is the most latent call during the 05:40 time frame. </p>
<p><img alt="Slowest requests" src="../scalability/images/slowest-requests.png" /></p>
<p>Armed with this data we can use an Ad-Hoc PromQL or a CloudWatch Insights query to pull LIST requests from the audit log during that time frame to see which application this might be.</p>
<h3 id="scalability-docs-kcp_monitoring-finding-the-source-with-cloudwatch">Finding the Source with CloudWatch<a class="headerlink" href="#scalability-docs-kcp_monitoring-finding-the-source-with-cloudwatch" title="Permanent link">&para;</a></h3>
<p>Metrics are best used to find the problem area we want to look at and narrow both the timeframe and the search parameters of the problem. Once we have this data we want to transition to logs for more detailed times and errors. To do this we will turn our logs into metrics using <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html">CloudWatch Logs Insights</a>. </p>
<p>For example, to investigate the issue above, we will use the following CloudWatch Logs Insights query to pull the userAgent and requestURI so that we can pin down which application is causing this latency. </p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>An appropriate Count needs to be used as to not pull normal List/Resync behavior on a Watch.</p>
</div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#scalability-docs-kcp_monitoring-__codelineno-2-1"></a>fields *@timestamp*, *@message*
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#scalability-docs-kcp_monitoring-__codelineno-2-2"></a>| filter *@logStream* like &quot;kube-apiserver-audit&quot;
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#scalability-docs-kcp_monitoring-__codelineno-2-3"></a>| filter ispresent(requestURI)
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#scalability-docs-kcp_monitoring-__codelineno-2-4"></a>| filter verb = &quot;list&quot;
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#scalability-docs-kcp_monitoring-__codelineno-2-5"></a>| parse requestReceivedTimestamp /\d+-\d+-(?&lt;StartDay&gt;\d+)T(?&lt;StartHour&gt;\d+):(?&lt;StartMinute&gt;\d+):(?&lt;StartSec&gt;\d+).(?&lt;StartMsec&gt;\d+)Z/
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#scalability-docs-kcp_monitoring-__codelineno-2-6"></a>| parse stageTimestamp /\d+-\d+-(?&lt;EndDay&gt;\d+)T(?&lt;EndHour&gt;\d+):(?&lt;EndMinute&gt;\d+):(?&lt;EndSec&gt;\d+).(?&lt;EndMsec&gt;\d+)Z/
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#scalability-docs-kcp_monitoring-__codelineno-2-7"></a>| fields (StartHour * 3600 + StartMinute * 60 + StartSec + StartMsec / 1000000) as StartTime, (EndHour * 3600 + EndMinute * 60 + EndSec + EndMsec / 1000000) as EndTime, (EndTime - StartTime) as DeltaTime
<a id="__codelineno-2-8" name="__codelineno-2-8" href="#scalability-docs-kcp_monitoring-__codelineno-2-8"></a>| stats avg(DeltaTime) as AverageDeltaTime, count(*) as CountTime by requestURI, userAgent
<a id="__codelineno-2-9" name="__codelineno-2-9" href="#scalability-docs-kcp_monitoring-__codelineno-2-9"></a>| filter CountTime &gt;=50
<a id="__codelineno-2-10" name="__codelineno-2-10" href="#scalability-docs-kcp_monitoring-__codelineno-2-10"></a>| sort AverageDeltaTime desc
</code></pre></div>
<p>Using this query we found two different agents running a large number of high latency list operations. Splunk and CloudWatch agent. Armed with the data, we can make a decision to remove, update, or replace this controller with another project. </p>
<p><img alt="Query results" src="../scalability/images/query-results.png" /></p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>For more details on this subject please see the following <a href="https://aws.amazon.com/blogs/containers/troubleshooting-amazon-eks-api-servers-with-prometheus/">blog</a></p>
</div>
<h2 id="scalability-docs-kcp_monitoring-scheduler">Scheduler<a class="headerlink" href="#scalability-docs-kcp_monitoring-scheduler" title="Permanent link">&para;</a></h2>
<p>Since the EKS control plane instances are run in separate AWS account we will not be able to scrape those components for metrics (The API server being the exception). However, since we have access to the audit logs for these components, we can turn those logs into metrics to see if any of the sub-systems are causing a scaling bottleneck. Let’s use CloudWatch Logs Insights to see how many unscheduled pods are in the scheduler queue.</p>
<h3 id="scalability-docs-kcp_monitoring-unscheduled-pods-in-the-scheduler-log">Unscheduled pods in the scheduler log<a class="headerlink" href="#scalability-docs-kcp_monitoring-unscheduled-pods-in-the-scheduler-log" title="Permanent link">&para;</a></h3>
<p>If we had access to scrape the scheduler metrics directly on a self managed Kubernetes (such as Kops) we would use the following PromQL to understand the scheduler backlog.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#scalability-docs-kcp_monitoring-__codelineno-3-1"></a>max without(instance)(scheduler_pending_pods)
</code></pre></div>
<p>Since we do not have access to the above metric in EKS, we will use the below CloudWatch Logs Insights query to see the backlog by checking for how many pods were unable to unscheduled during a particular time frame. Then we could dive further into into the messages at the peak time frame to understand the nature of the bottleneck. For example, nodes not spinning up fast enough, or the rate limiter in the scheduler itself. </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#scalability-docs-kcp_monitoring-__codelineno-4-1"></a>fields timestamp, pod, err, *@message*
<a id="__codelineno-4-2" name="__codelineno-4-2" href="#scalability-docs-kcp_monitoring-__codelineno-4-2"></a>| filter *@logStream* like &quot;scheduler&quot;
<a id="__codelineno-4-3" name="__codelineno-4-3" href="#scalability-docs-kcp_monitoring-__codelineno-4-3"></a>| filter *@message* like &quot;Unable to schedule pod&quot;
<a id="__codelineno-4-4" name="__codelineno-4-4" href="#scalability-docs-kcp_monitoring-__codelineno-4-4"></a>| parse *@message*  /^.(?&lt;date&gt;\d{4})\s+(?&lt;timestamp&gt;\d+:\d+:\d+\.\d+)\s+\S*\s+\S+\]\s\&quot;(.*?)\&quot;\s+pod=(?&lt;pod&gt;\&quot;(.*?)\&quot;)\s+err=(?&lt;err&gt;\&quot;(.*?)\&quot;)/
<a id="__codelineno-4-5" name="__codelineno-4-5" href="#scalability-docs-kcp_monitoring-__codelineno-4-5"></a>| count(*) as count by pod, err
<a id="__codelineno-4-6" name="__codelineno-4-6" href="#scalability-docs-kcp_monitoring-__codelineno-4-6"></a>| sort count desc
</code></pre></div>
<p>Here we see the errors from the scheduler saying the pod did not deploy because the storage PVC was unavailable. </p>
<p><img alt="CloudWatch Logs query" src="../scalability/images/cwl-query.png" /></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Audit logging must be turned on the control plane to enable this function. It is also a best practice to limit the log retention as to not drive up cost over time unnecessarily. An example for turning on all logging functions using the EKSCTL tool below.  </p>
</div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#scalability-docs-kcp_monitoring-__codelineno-5-1"></a><span class="nt">cloudWatch</span><span class="p">:</span>
<a id="__codelineno-5-2" name="__codelineno-5-2" href="#scalability-docs-kcp_monitoring-__codelineno-5-2"></a><span class="w">  </span><span class="nt">clusterLogging</span><span class="p">:</span>
<a id="__codelineno-5-3" name="__codelineno-5-3" href="#scalability-docs-kcp_monitoring-__codelineno-5-3"></a><span class="w">    </span><span class="nt">enableTypes</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;*&quot;</span><span class="p p-Indicator">]</span>
<a id="__codelineno-5-4" name="__codelineno-5-4" href="#scalability-docs-kcp_monitoring-__codelineno-5-4"></a><span class="w">    </span><span class="nt">logRetentionInDays</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10</span>
</code></pre></div>
<h2 id="scalability-docs-kcp_monitoring-kube-controller-manager">Kube Controller Manager<a class="headerlink" href="#scalability-docs-kcp_monitoring-kube-controller-manager" title="Permanent link">&para;</a></h2>
<p>Kube Controller Manager, like all other controllers, has limits on how many operations it can do at once. Let’s review what some of those flags are by looking at a KOPS configuration where we can set these parameters.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#scalability-docs-kcp_monitoring-__codelineno-6-1"></a><span class="w">  </span><span class="nt">kubeControllerManager</span><span class="p">:</span>
<a id="__codelineno-6-2" name="__codelineno-6-2" href="#scalability-docs-kcp_monitoring-__codelineno-6-2"></a><span class="w">    </span><span class="nt">concurrentEndpointSyncs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span>
<a id="__codelineno-6-3" name="__codelineno-6-3" href="#scalability-docs-kcp_monitoring-__codelineno-6-3"></a><span class="w">    </span><span class="nt">concurrentReplicasetSyncs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span>
<a id="__codelineno-6-4" name="__codelineno-6-4" href="#scalability-docs-kcp_monitoring-__codelineno-6-4"></a><span class="w">    </span><span class="nt">concurrentNamespaceSyncs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10</span>
<a id="__codelineno-6-5" name="__codelineno-6-5" href="#scalability-docs-kcp_monitoring-__codelineno-6-5"></a><span class="w">    </span><span class="nt">concurrentServiceaccountTokenSyncs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span>
<a id="__codelineno-6-6" name="__codelineno-6-6" href="#scalability-docs-kcp_monitoring-__codelineno-6-6"></a><span class="w">    </span><span class="nt">concurrentServiceSyncs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span>
<a id="__codelineno-6-7" name="__codelineno-6-7" href="#scalability-docs-kcp_monitoring-__codelineno-6-7"></a><span class="w">    </span><span class="nt">concurrentResourceQuotaSyncs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span>
<a id="__codelineno-6-8" name="__codelineno-6-8" href="#scalability-docs-kcp_monitoring-__codelineno-6-8"></a><span class="w">    </span><span class="nt">concurrentGcSyncs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">20</span>
<a id="__codelineno-6-9" name="__codelineno-6-9" href="#scalability-docs-kcp_monitoring-__codelineno-6-9"></a><span class="w">    </span><span class="nt">kubeAPIBurst</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">20</span>
<a id="__codelineno-6-10" name="__codelineno-6-10" href="#scalability-docs-kcp_monitoring-__codelineno-6-10"></a><span class="w">    </span><span class="nt">kubeAPIQPS</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;30&quot;</span>
</code></pre></div>
<p>These controllers have queues that fill up during times of high churn on a cluster. In this case we see the replicaset set controller has a large backlog in its queue. </p>
<p><img alt="Queues" src="../scalability/images/queues.png" /></p>
<p>We have two different ways of addressing such a situation. If running self managed we could simply increase the concurrent goroutines, however this would have an impact on etcd by processing more data in the KCM. The other option would be to reduce the number of replicaset objects using <code>.spec.revisionHistoryLimit</code> on the deployment to reduce the number of replicaset objects we can rollback, thus reducing the pressure on this controller. </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#scalability-docs-kcp_monitoring-__codelineno-7-1"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-7-2" name="__codelineno-7-2" href="#scalability-docs-kcp_monitoring-__codelineno-7-2"></a><span class="w">  </span><span class="nt">revisionHistoryLimit</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span>
</code></pre></div>
<p>Other Kubernetes features can be tuned or turned off to reduce pressure in high churn rate systems. For example, if the application in our pods doesn’t need to speak to the k8s API directly then turning off the projected secret into those pods would decrease the load on ServiceaccountTokenSyncs. This is the more desirable way to address such issues if possible.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1" href="#scalability-docs-kcp_monitoring-__codelineno-8-1"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Pod</span>
<a id="__codelineno-8-2" name="__codelineno-8-2" href="#scalability-docs-kcp_monitoring-__codelineno-8-2"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-8-3" name="__codelineno-8-3" href="#scalability-docs-kcp_monitoring-__codelineno-8-3"></a><span class="w">  </span><span class="nt">automountServiceAccountToken</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
</code></pre></div>
<p>In systems where we can’t get access to the metrics, we can again look at the logs to detect contention. If we wanted to see the number of requests being being processed on a per controller or an aggregate level we would use the following CloudWatch Logs Insights Query. </p>
<h3 id="scalability-docs-kcp_monitoring-total-volume-processed-by-the-kcm">Total Volume Processed by the KCM<a class="headerlink" href="#scalability-docs-kcp_monitoring-total-volume-processed-by-the-kcm" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-9-1" name="__codelineno-9-1" href="#scalability-docs-kcp_monitoring-__codelineno-9-1"></a># Query to count API qps coming from kube-controller-manager, split by controller type.
<a id="__codelineno-9-2" name="__codelineno-9-2" href="#scalability-docs-kcp_monitoring-__codelineno-9-2"></a># If you&#39;re seeing values close to 20/sec for any particular controller, it&#39;s most likely seeing client-side API throttling.
<a id="__codelineno-9-3" name="__codelineno-9-3" href="#scalability-docs-kcp_monitoring-__codelineno-9-3"></a>fields @timestamp, @logStream, @message
<a id="__codelineno-9-4" name="__codelineno-9-4" href="#scalability-docs-kcp_monitoring-__codelineno-9-4"></a>| filter @logStream like /kube-apiserver-audit/
<a id="__codelineno-9-5" name="__codelineno-9-5" href="#scalability-docs-kcp_monitoring-__codelineno-9-5"></a>| filter userAgent like /kube-controller-manager/
<a id="__codelineno-9-6" name="__codelineno-9-6" href="#scalability-docs-kcp_monitoring-__codelineno-9-6"></a># Exclude lease-related calls (not counted under kcm qps)
<a id="__codelineno-9-7" name="__codelineno-9-7" href="#scalability-docs-kcp_monitoring-__codelineno-9-7"></a>| filter requestURI not like &quot;apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager&quot;
<a id="__codelineno-9-8" name="__codelineno-9-8" href="#scalability-docs-kcp_monitoring-__codelineno-9-8"></a># Exclude API discovery calls (not counted under kcm qps)
<a id="__codelineno-9-9" name="__codelineno-9-9" href="#scalability-docs-kcp_monitoring-__codelineno-9-9"></a>| filter requestURI not like &quot;?timeout=32s&quot;
<a id="__codelineno-9-10" name="__codelineno-9-10" href="#scalability-docs-kcp_monitoring-__codelineno-9-10"></a># Exclude watch calls (not counted under kcm qps)
<a id="__codelineno-9-11" name="__codelineno-9-11" href="#scalability-docs-kcp_monitoring-__codelineno-9-11"></a>| filter verb != &quot;watch&quot;
<a id="__codelineno-9-12" name="__codelineno-9-12" href="#scalability-docs-kcp_monitoring-__codelineno-9-12"></a># If you want to get counts of API calls coming from a specific controller, uncomment the appropriate line below:
<a id="__codelineno-9-13" name="__codelineno-9-13" href="#scalability-docs-kcp_monitoring-__codelineno-9-13"></a># | filter user.username like &quot;system:serviceaccount:kube-system:job-controller&quot;
<a id="__codelineno-9-14" name="__codelineno-9-14" href="#scalability-docs-kcp_monitoring-__codelineno-9-14"></a># | filter user.username like &quot;system:serviceaccount:kube-system:cronjob-controller&quot;
<a id="__codelineno-9-15" name="__codelineno-9-15" href="#scalability-docs-kcp_monitoring-__codelineno-9-15"></a># | filter user.username like &quot;system:serviceaccount:kube-system:deployment-controller&quot;
<a id="__codelineno-9-16" name="__codelineno-9-16" href="#scalability-docs-kcp_monitoring-__codelineno-9-16"></a># | filter user.username like &quot;system:serviceaccount:kube-system:replicaset-controller&quot;
<a id="__codelineno-9-17" name="__codelineno-9-17" href="#scalability-docs-kcp_monitoring-__codelineno-9-17"></a># | filter user.username like &quot;system:serviceaccount:kube-system:horizontal-pod-autoscaler&quot;
<a id="__codelineno-9-18" name="__codelineno-9-18" href="#scalability-docs-kcp_monitoring-__codelineno-9-18"></a># | filter user.username like &quot;system:serviceaccount:kube-system:persistent-volume-binder&quot;
<a id="__codelineno-9-19" name="__codelineno-9-19" href="#scalability-docs-kcp_monitoring-__codelineno-9-19"></a># | filter user.username like &quot;system:serviceaccount:kube-system:endpointslice-controller&quot;
<a id="__codelineno-9-20" name="__codelineno-9-20" href="#scalability-docs-kcp_monitoring-__codelineno-9-20"></a># | filter user.username like &quot;system:serviceaccount:kube-system:endpoint-controller&quot;
<a id="__codelineno-9-21" name="__codelineno-9-21" href="#scalability-docs-kcp_monitoring-__codelineno-9-21"></a># | filter user.username like &quot;system:serviceaccount:kube-system:generic-garbage-controller&quot;
<a id="__codelineno-9-22" name="__codelineno-9-22" href="#scalability-docs-kcp_monitoring-__codelineno-9-22"></a>| stats count(*) as count by user.username
<a id="__codelineno-9-23" name="__codelineno-9-23" href="#scalability-docs-kcp_monitoring-__codelineno-9-23"></a>| sort count desc
</code></pre></div>
<p>The key takeaway here is when looking into scalability issues, to look at every step in the path (API, scheduler, KCM, etcd) before moving to the detailed troubleshooting phase. Often in production you will find that it takes adjustments to more than one part of Kubernetes to allow the system to work at its most performant. It’s easy to inadvertently troubleshoot what is just a symptom (such as a node timeout) of a much larger bottle neck. </p>
<h2 id="scalability-docs-kcp_monitoring-etcd">ETCD<a class="headerlink" href="#scalability-docs-kcp_monitoring-etcd" title="Permanent link">&para;</a></h2>
<p>etcd uses a memory mapped file to store key value pairs efficiently. There is a protection mechanism to set the size of this memory space available set commonly at the 2, 4, and 8GB limits. Fewer objects in the database means less clean up etcd needs to do when objects are updated and older versions needs to be cleaned out. This process of cleaning old versions of an object out is referred to as compaction. After a number of compaction operations, there is a subsequent process that recovers usable space space called defragging that happens above a certain threshold or on a fixed schedule of time. </p>
<p>There are a couple user related items we can do to limit the number of objects in Kubernetes and thus reduce the impact of both the compaction and de-fragmentation process. For example, Helm keeps a high <code>revisionHistoryLimit</code>. This keeps older objects such as ReplicaSets on the system to be able to do rollbacks. By setting the history limits down to 2 we can reduce the the number of objects (like ReplicaSets) from ten to two which in turn would put less load on the system. </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-10-1" name="__codelineno-10-1" href="#scalability-docs-kcp_monitoring-__codelineno-10-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">apps/v1</span>
<a id="__codelineno-10-2" name="__codelineno-10-2" href="#scalability-docs-kcp_monitoring-__codelineno-10-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deployment</span>
<a id="__codelineno-10-3" name="__codelineno-10-3" href="#scalability-docs-kcp_monitoring-__codelineno-10-3"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-10-4" name="__codelineno-10-4" href="#scalability-docs-kcp_monitoring-__codelineno-10-4"></a><span class="w">  </span><span class="nt">revisionHistoryLimit</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span>
</code></pre></div>
<p>From a monitoring standpoint, if system latency spikes occur in a set pattern separated by hours, checking to see if this defragmentation process is the source can be helpful. We can see this by using CloudWatch Logs.</p>
<p>If you want to see start/end times of defrag use the following query:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-11-1" name="__codelineno-11-1" href="#scalability-docs-kcp_monitoring-__codelineno-11-1"></a>fields *@timestamp*, *@message*
<a id="__codelineno-11-2" name="__codelineno-11-2" href="#scalability-docs-kcp_monitoring-__codelineno-11-2"></a>| filter *@logStream* like /etcd-manager/
<a id="__codelineno-11-3" name="__codelineno-11-3" href="#scalability-docs-kcp_monitoring-__codelineno-11-3"></a>| filter *@message* like /defraging|defraged/
<a id="__codelineno-11-4" name="__codelineno-11-4" href="#scalability-docs-kcp_monitoring-__codelineno-11-4"></a>| sort *@timestamp* asc
</code></pre></div>
<p><img alt="Defrag query" src="../scalability/images/defrag.png" /></p></section><section class="print-page" id="scalability-docs-node_efficiency"><h1 id="scalability-docs-node_efficiency-node-and-workload-efficiency">Node and Workload Efficiency<a class="headerlink" href="#scalability-docs-node_efficiency-node-and-workload-efficiency" title="Permanent link">&para;</a></h1>
<p>Being efficient with our workloads and nodes reduces complexity/cost while increasing performance and scale. There are many factors to consider when planning this efficiency, and it’s easiest to think in terms of trade offs vs. one best practice setting for each feature. Let’s explore these tradeoffs in depth in the following section.     </p>
<h2 id="scalability-docs-node_efficiency-node-selection">Node Selection<a class="headerlink" href="#scalability-docs-node_efficiency-node-selection" title="Permanent link">&para;</a></h2>
<p>Using node sizes that are slightly larger (4-12xlarge) increases the available space that we have for running pods due to the fact it reduces the percentage of the node used for “overhead” such as <a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/">DaemonSets</a> and <a href="https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/">Reserves</a> for system components. In the diagram below we see the difference between the usable space on a 2xlarge vs. a 8xlarge system with just a moderate number of DaemonSets. </p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Since k8s scales horizontally as a general rule, for most applications it does not make sense to take the performance impact of NUMA sizes nodes, thus the recommendation of a range below that node size.</p>
</div>
<p><img alt="Node size" src="../scalability/images/node-size.png" /></p>
<p>Large nodes sizes allow us to have a higher percentage of usable space per node. However, this model can be taken to to the extreme by packing the node with so many pods that it causes errors or saturates the node. Monitoring node saturation is key to successfully using larger node sizes. </p>
<p>Node selection is rarely a one-size-fits-all proposition. Often it is best to split workloads with dramatically different churn rates into different node groups. Small batch workloads with a high churn rate would be best served by the the 4xlarge family of instances, while a large scale application such as Kafka which takes 8 vCPU and has a low churn rate would be better served by the 12xlarge family.   </p>
<p><img alt="Churn rate" src="../scalability/images/churn-rate.png" /></p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Another factor to consider with very large node sizes is since CGROUPS do not hide the total number of vCPU from the containerized application. Dynamic runtimes can often spawn an unintentional number of OS threads, creating latency that is difficult to troubleshoot. For these application <a href="https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/#static-policy">CPU pinning</a> is recommend. For a deeper exploration of topic please see the following video https://www.youtube.com/watch?v=NqtfDy_KAqg</p>
</div>
<h2 id="scalability-docs-node_efficiency-node-bin-packing">Node Bin-packing<a class="headerlink" href="#scalability-docs-node_efficiency-node-bin-packing" title="Permanent link">&para;</a></h2>
<h3 id="scalability-docs-node_efficiency-kubernetes-vs-linux-rules">Kubernetes vs. Linux Rules<a class="headerlink" href="#scalability-docs-node_efficiency-kubernetes-vs-linux-rules" title="Permanent link">&para;</a></h3>
<p>There are two sets of rules we need to be mindful of when dealing with workloads on Kubernetes. The rules of the Kubernetes Scheduler, which uses the request value to schedule pods on a node, and then what happens after the pod is scheduled, which is the realm of Linux, not Kubernetes.</p>
<p>After Kubernetes scheduler is finished, a new set of rules takes over, the Linux Completely Fair Scheduler (CFS). The key take away is that Linux CFS doesn’t have a the concept of a core. We will discuss why thinking in cores can lead to major problems with optimizing workloads for scale.  </p>
<h3 id="scalability-docs-node_efficiency-thinking-in-cores">Thinking in Cores<a class="headerlink" href="#scalability-docs-node_efficiency-thinking-in-cores" title="Permanent link">&para;</a></h3>
<p>The confusion starts because the Kubernetes scheduler does have the concept of cores. From a Kubernetes scheduler perspective if we looked at a node with 4 NGINX pods, each with a request of one core set, the node would look like this.</p>
<p><img alt="" src="../scalability/images/cores-1.png" /></p>
<p>However, let’s do a thought experiment on how different this looks from a Linux CFS perspective. The most important thing to remember when using the Linux CFS system is: busy containers (CGROUPS) are the only containers that count toward the share system. In this case, only the first container is busy so it is allowed to use all 4 cores on the node.</p>
<p><img alt="" src="../scalability/images/cores-2.png" /></p>
<p>Why does this matter? Let’s say we ran our performance testing in a development cluster where an NGINX application was the only busy container on that node. When we move the app to production, the following would happen: the NGINX application wants 4 vCPU of resources however, because all the other pods on the node are busy, our app’s performance is constrained. </p>
<p><img alt="" src="../scalability/images/cores-3.png" /></p>
<p>This situation would lead us to add more containers unnecessarily because we were not allowing our applications scale to their “sweet spot“. Let's explore this important concept of a ”sweet spot“ in a bit more detail.</p>
<h3 id="scalability-docs-node_efficiency-application-right-sizing">Application right sizing<a class="headerlink" href="#scalability-docs-node_efficiency-application-right-sizing" title="Permanent link">&para;</a></h3>
<p>Each application has a certain point where it can not take anymore traffic. Going above this point can increase processing times and even drop traffic when pushed well beyond this point. This is known as the application’s saturation point. To avoid scaling issues, we should attempt to scale the application <strong>before</strong> it reaches its saturation point. Let’s call this point the sweet spot. </p>
<p><img alt="The sweet spot" src="../scalability/images/sweet-spot.png" /></p>
<p>We need to test each of our applications to understand its sweet spot. There will be no universal guidance here as each application is different. During this testing we are trying to understand the best metric that shows our applications saturation point. Oftentimes, utilization metrics are used to indicate an application is saturated but this can quickly lead to scaling issues (We will explore this topic in detail in a later section). Once we have this “sweet spot“ we can use it to efficiently scale our workloads.</p>
<p>Conversely, what would happen if we scale up well before the sweet spot and created unnecessary pods? Let’s explore that in the next section. </p>
<h3 id="scalability-docs-node_efficiency-pod-sprawl">Pod sprawl<a class="headerlink" href="#scalability-docs-node_efficiency-pod-sprawl" title="Permanent link">&para;</a></h3>
<p>To see how creating unnecessary pods could quickly get out of hand, let's look at the first example on the left. The correct vertical scale of this container takes up about two vCPUs worth of utilization when handling 100 requests a second. However, If we were to under-provision the requests value by setting requests to half a core, we would now need 4 pods for each one pods we actually needed. Exacerbating this problem further, if our <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">HPA</a> was set at the default of 50% CPU, those pods would scale half empty, creating an 8:1 ratio. </p>
<p><img alt="" src="../scalability/images/scaling-ratio.png" /></p>
<p>Scaling this problem up we can quickly see how this can get out of hand. A deployment of ten pods whose sweet spot was set incorrectly could quickly spiral to 80 pods and the additional infrastructure needed to run them. </p>
<p><img alt="" src="../scalability/images/bad-sweetspot.png" /></p>
<p>Now that we understand the impact of not allowing applications to operate in their sweet spot, let’s return to the node level and ask why this difference between the Kubernetes scheduler and Linux CFS so important?</p>
<p>When scaling up and down with HPA, we can have a scenario where we have a lot of space to allocate more pods. This would be a bad decision because the node depicted on the left is already at 100% CPU utilization. In a unrealistic but theoretically possible scenario, we could have the other extreme where our node is completely full, yet our CPU utilization is zero. </p>
<p><img alt="" src="../scalability/images/hpa-utilization.png" /></p>
<h3 id="scalability-docs-node_efficiency-setting-requests">Setting Requests<a class="headerlink" href="#scalability-docs-node_efficiency-setting-requests" title="Permanent link">&para;</a></h3>
<p>It would tempting to set the request at the “sweet spot” value for that application, however this would cause inefficiencies as pictured in the diagram below.  Here we have set the request value to 2 vCPU, however the average utilization of these pods runs only 1 CPU most of the time. This setting would cause us to waste 50% of our CPU cycles, which would be unacceptable. </p>
<p><img alt="" src="../scalability/images/requests-1.png" /></p>
<p>This bring us to the complex answer to problem. Container utilization cannot be thought of in a vacuum; one must take into account the other applications running on the node. In the following example containers that are bursty in nature are mixed in with two low CPU utilization containers that might be memory constrained. In this way we allow the containers to hit their sweet spot without taxing the node.   </p>
<p><img alt="" src="../scalability/images/requests-2.png" /></p>
<p>The important concept to take away from all this is that using Kubernetes scheduler concept of cores to understand Linux container performance can lead to poor decision making as they are not related. </p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Linux CFS has its strong points. This is especially true for I/O based workloads. However, if your application uses full cores without sidecars, and has no I/O requirements, CPU pinning can remove a great deal of complexity from this process and is encouraged with those caveats.</p>
</div>
<h2 id="scalability-docs-node_efficiency-utilization-vs-saturation">Utilization vs. Saturation<a class="headerlink" href="#scalability-docs-node_efficiency-utilization-vs-saturation" title="Permanent link">&para;</a></h2>
<p>A common mistake in application scaling is only using CPU utilization for your scaling metric. In complex applications this is almost always a poor indicator that an application is actually saturated with requests. In the example on the left, we see all of our requests are actually hitting the web server, so CPU utilization is tracking well with saturation. </p>
<p>In real world applications, it’s likely that some of those requests will be getting serviced by a database layer or an authentication layer, etc. In this more common case, notice CPU is not tracking with saturation as the request is being serviced by other entities. In this case CPU is a very poor indicator for saturation.</p>
<p><img alt="" src="../scalability/images/util-vs-saturation-1.png" /></p>
<p>Using the wrong metric in application performance is the number one reason for unnecessary and unpredictable scaling in Kubernetes. Great care must be taken in picking the correct saturation metric for the type of application that you're using. It is important to note that there is not a one size fits all recommendation that can be given. Depending on the language used and the type of application in question, there is a diverse set of metrics for saturation.</p>
<p>We might think this problem is only with CPU Utilization, however other common metrics such as request per second can also fall into the exact same problem as discussed above.  Notice the request can also go to DB layers, auth layers, not being directly serviced by our web server, thus it’s a poor metric for true saturation of the web server itself.</p>
<p><img alt="" src="../scalability/images/util-vs-saturation-2.png" /></p>
<p>Unfortunately there are no easy answers when it comes to picking the right saturation metric. Here are some guidelines to take into consideration: </p>
<ul>
<li>Understand your language runtime - languages with multiple OS threads will react differently than single threaded applications, thus impacting the node differently.</li>
<li>Understand the correct vertical scale - how much buffer do you want in your applications vertical scale before scaling a new pod?  </li>
<li>What metrics truly reflect the saturation of your application - The saturation metric for a Kafka Producer would be quite different than a complex web application. </li>
<li>How do all the other applications on the node effect each other - Application performance is not done in a vacuum the other workloads on the node have a major impact.</li>
</ul>
<p>To close out this section, it would be easy to dismiss the above as overly complex and unnecessary. It can often be the case that we are experiencing an issue but we are unaware of the true nature of the problem because we are looking at the wrong metrics. In the next section we will look at how that could happen. </p>
<h3 id="scalability-docs-node_efficiency-node-saturation">Node Saturation<a class="headerlink" href="#scalability-docs-node_efficiency-node-saturation" title="Permanent link">&para;</a></h3>
<p>Now that we have explored application saturation, let’s look at this same concept from a node point of view. Let’s take two CPUs that are 100% utilized to see the difference between utilization vs. saturation. </p>
<p>The vCPU on the left is 100% utilized, however no other tasks are waiting to run on this vCPU, so in a purely theoretical sense, this is quite efficient. Meanwhile, we have 20 single threaded applications waiting to get processed by a vCPU in the second example. All 20 applications now will experience some type of latency while they're waiting their turn to be processed by the vCPU. In other words, the vCPU on the right is saturated.  </p>
<p>Not only would we not see this problem if we where just looking at utilization, but we might attribute this latency to something unrelated such as networking which would lead us down the wrong path. </p>
<p><img alt="" src="../scalability/images/node-saturation.png" /></p>
<p>It is important to view saturation metrics, not just utilization metrics when increasing the total number of pods running on a node at any given time as we can easily miss the fact we have over-saturated a node. For this task we can use pressure stall information metrics as seen in the below chart.</p>
<p>PromQL - Stalled I/O</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#scalability-docs-node_efficiency-__codelineno-0-1"></a>topk(3, ((irate(node_pressure_io_stalled_seconds_total[1m])) * 100))
</code></pre></div>
<p><img alt="" src="../scalability/images/stalled-io.png" /></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more on Pressure stall metrics, see https://facebookmicrosites.github.io/psi/docs/overview*</p>
</div>
<p>With these metrics we can tell if threads are waiting on CPU, or even if every thread on the box is stalled waiting on resource like memory or I/O. For example, we could see what percentage every thread on the instance was stalled waiting on I/O over the period of 1 min.  </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#scalability-docs-node_efficiency-__codelineno-1-1"></a>topk(3, ((irate(node_pressure_io_stalled_seconds_total[1m])) * 100))
</code></pre></div>
<p>Using this metric, we can see in the above chart every thread on the box was stalled 45% of the time waiting on I/O at the high water mark, meaning we were throwing away all of those CPU cycles in that minute. Understanding that this is happening can help us reclaim a significant amount of vCPU time, thus making scaling more efficient. </p>
<h3 id="scalability-docs-node_efficiency-hpa-v2">HPA V2<a class="headerlink" href="#scalability-docs-node_efficiency-hpa-v2" title="Permanent link">&para;</a></h3>
<p>It is recommended to use the autoscaling/v2 version of the HPA API. The older versions of the HPA API could get stuck scaling in certain edge cases. It was also limited to pods only doubling during each scaling step, which created issues for small deployments that needed to scale rapidly.  </p>
<p>Autoscaling/v2 allows us more flexibility to include multiple criteria to scale on and allows us a great deal of flexibility when using custom and external metrics (non K8s metrics).</p>
<p>As an example, we can scaling on the highest of three values (see below). We scale if the average utilization of all the pods are over 50%, if custom metrics the packets per second of the ingress exceed an average of 1,000, or ingress object exceeds 10K request per second.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is just to show the flexibility of the auto-scaling API, we recommend against overly complex rules that can be difficult to troubleshoot in production. </p>
</div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#scalability-docs-node_efficiency-__codelineno-2-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">autoscaling/v2</span>
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#scalability-docs-node_efficiency-__codelineno-2-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">HorizontalPodAutoscaler</span>
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#scalability-docs-node_efficiency-__codelineno-2-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#scalability-docs-node_efficiency-__codelineno-2-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">php-apache</span>
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#scalability-docs-node_efficiency-__codelineno-2-5"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#scalability-docs-node_efficiency-__codelineno-2-6"></a><span class="w">  </span><span class="nt">scaleTargetRef</span><span class="p">:</span>
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#scalability-docs-node_efficiency-__codelineno-2-7"></a><span class="w">    </span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">apps/v1</span>
<a id="__codelineno-2-8" name="__codelineno-2-8" href="#scalability-docs-node_efficiency-__codelineno-2-8"></a><span class="w">    </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deployment</span>
<a id="__codelineno-2-9" name="__codelineno-2-9" href="#scalability-docs-node_efficiency-__codelineno-2-9"></a><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">php-apache</span>
<a id="__codelineno-2-10" name="__codelineno-2-10" href="#scalability-docs-node_efficiency-__codelineno-2-10"></a><span class="w">  </span><span class="nt">minReplicas</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<a id="__codelineno-2-11" name="__codelineno-2-11" href="#scalability-docs-node_efficiency-__codelineno-2-11"></a><span class="w">  </span><span class="nt">maxReplicas</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10</span>
<a id="__codelineno-2-12" name="__codelineno-2-12" href="#scalability-docs-node_efficiency-__codelineno-2-12"></a><span class="w">  </span><span class="nt">metrics</span><span class="p">:</span>
<a id="__codelineno-2-13" name="__codelineno-2-13" href="#scalability-docs-node_efficiency-__codelineno-2-13"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Resource</span>
<a id="__codelineno-2-14" name="__codelineno-2-14" href="#scalability-docs-node_efficiency-__codelineno-2-14"></a><span class="w">    </span><span class="nt">resource</span><span class="p">:</span>
<a id="__codelineno-2-15" name="__codelineno-2-15" href="#scalability-docs-node_efficiency-__codelineno-2-15"></a><span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">cpu</span>
<a id="__codelineno-2-16" name="__codelineno-2-16" href="#scalability-docs-node_efficiency-__codelineno-2-16"></a><span class="w">      </span><span class="nt">target</span><span class="p">:</span>
<a id="__codelineno-2-17" name="__codelineno-2-17" href="#scalability-docs-node_efficiency-__codelineno-2-17"></a><span class="w">        </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Utilization</span>
<a id="__codelineno-2-18" name="__codelineno-2-18" href="#scalability-docs-node_efficiency-__codelineno-2-18"></a><span class="w">        </span><span class="nt">averageUtilization</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">50</span>
<a id="__codelineno-2-19" name="__codelineno-2-19" href="#scalability-docs-node_efficiency-__codelineno-2-19"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Pods</span>
<a id="__codelineno-2-20" name="__codelineno-2-20" href="#scalability-docs-node_efficiency-__codelineno-2-20"></a><span class="w">    </span><span class="nt">pods</span><span class="p">:</span>
<a id="__codelineno-2-21" name="__codelineno-2-21" href="#scalability-docs-node_efficiency-__codelineno-2-21"></a><span class="w">      </span><span class="nt">metric</span><span class="p">:</span>
<a id="__codelineno-2-22" name="__codelineno-2-22" href="#scalability-docs-node_efficiency-__codelineno-2-22"></a><span class="w">        </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">packets-per-second</span>
<a id="__codelineno-2-23" name="__codelineno-2-23" href="#scalability-docs-node_efficiency-__codelineno-2-23"></a><span class="w">      </span><span class="nt">target</span><span class="p">:</span>
<a id="__codelineno-2-24" name="__codelineno-2-24" href="#scalability-docs-node_efficiency-__codelineno-2-24"></a><span class="w">        </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">AverageValue</span>
<a id="__codelineno-2-25" name="__codelineno-2-25" href="#scalability-docs-node_efficiency-__codelineno-2-25"></a><span class="w">        </span><span class="nt">averageValue</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1k</span>
<a id="__codelineno-2-26" name="__codelineno-2-26" href="#scalability-docs-node_efficiency-__codelineno-2-26"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Object</span>
<a id="__codelineno-2-27" name="__codelineno-2-27" href="#scalability-docs-node_efficiency-__codelineno-2-27"></a><span class="w">    </span><span class="nt">object</span><span class="p">:</span>
<a id="__codelineno-2-28" name="__codelineno-2-28" href="#scalability-docs-node_efficiency-__codelineno-2-28"></a><span class="w">      </span><span class="nt">metric</span><span class="p">:</span>
<a id="__codelineno-2-29" name="__codelineno-2-29" href="#scalability-docs-node_efficiency-__codelineno-2-29"></a><span class="w">        </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">requests-per-second</span>
<a id="__codelineno-2-30" name="__codelineno-2-30" href="#scalability-docs-node_efficiency-__codelineno-2-30"></a><span class="w">      </span><span class="nt">describedObject</span><span class="p">:</span>
<a id="__codelineno-2-31" name="__codelineno-2-31" href="#scalability-docs-node_efficiency-__codelineno-2-31"></a><span class="w">        </span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">networking.k8s.io/v1</span>
<a id="__codelineno-2-32" name="__codelineno-2-32" href="#scalability-docs-node_efficiency-__codelineno-2-32"></a><span class="w">        </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Ingress</span>
<a id="__codelineno-2-33" name="__codelineno-2-33" href="#scalability-docs-node_efficiency-__codelineno-2-33"></a><span class="w">        </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">main-route</span>
<a id="__codelineno-2-34" name="__codelineno-2-34" href="#scalability-docs-node_efficiency-__codelineno-2-34"></a><span class="w">      </span><span class="nt">target</span><span class="p">:</span>
<a id="__codelineno-2-35" name="__codelineno-2-35" href="#scalability-docs-node_efficiency-__codelineno-2-35"></a><span class="w">        </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Value</span>
<a id="__codelineno-2-36" name="__codelineno-2-36" href="#scalability-docs-node_efficiency-__codelineno-2-36"></a><span class="w">        </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10k</span>
</code></pre></div>
<p>However, we learned the danger of using such metrics for complex web applications. In this case we would be better served by using custom or external metric that accurately reflects the saturation of our application vs. the utilization. HPAv2 allows for this by having the ability to scale according to any metric, however we still need to find and export that metric to Kubernetes for use.</p>
<p>For example, we can look at the active thread queue count in Apache. This often creates a “smoother” scaling profile (more on that term soon). If a thread is active, it doesn’t matter if that thread is waiting on a database layer or servicing a request locally, if all of the applications threads are being used, it’s a great indication that application is saturated. </p>
<p>We can use this thread exhaustion as a signal to create a new pod with a fully available thread pool. This also gives us control over how big a buffer we want in the application to absorb during times of heavy traffic. For example, if we had a total thread pool of 10, scaling at 4 threads used vs. 8 threads used would have a major impact on the buffer we have available when scaling the application. A setting of 4 would make sense for an application that needs to rapidly scale under heavy load, where a setting of 8 would be more efficient with our resources if we had plenty of time to scale due to the number of requests increasing slowly vs. sharply over time. </p>
<p><img alt="" src="../scalability/images/thread-pool.png" /></p>
<p>What do we mean by the term “smooth” when it comes to scaling? Notice the below chart where we are using CPU as a metric. The pods in this deployment are spiking in a short period for from 50 pods, all the way up to 250 pods only to immediately scale down again. This is highly inefficient scaling is the leading cause on churn on clusters.</p>
<p><img alt="" src="../scalability/images/spiky-scaling.png" /></p>
<p>Notice how after we change to a metric that reflects the correct sweet spot of our application (mid-part of chart), we are able to scale smoothly. Our scaling is now efficient, and our pods are allowed to fully scale with the headroom we provided by adjusting requests settings. Now a smaller group of pods are doing the work the hundreds of pods were doing before.  Real world data shows that this is the number one factor in scalability of Kubernetes clusters. </p>
<p><img alt="" src="../scalability/images/smooth-scaling.png" /></p>
<p>The key takeaway is CPU utilization is only one dimension of both application and node performance. Using CPU utilization as a sole health indicator for our nodes and applications creates problems in scaling, performance and cost which are all tightly linked concepts. The more performant the application and nodes are, the less that you need to scale, which in turn lowers your costs. </p>
<p>Finding and using the correct saturation metrics for scaling your particular application also allows you to monitor and alarm on the true bottlenecks for that application. If this critical step is skipped, reports of performance problems will be difficult, if not impossible, to understand.  </p>
<h2 id="scalability-docs-node_efficiency-setting-cpu-limits">Setting CPU Limits<a class="headerlink" href="#scalability-docs-node_efficiency-setting-cpu-limits" title="Permanent link">&para;</a></h2>
<p>To round out this section on misunderstood topics, we will cover CPU limits. In short, limits are metadata associated with the container that has a counter that resets every 100ms. This helps Linux keep track of how many CPU resources are used node-wide by a specific container in a 100ms period of time. </p>
<p><img alt="CPU limits" src="../scalability/images/cpu-limits.png" /></p>
<p>A common error with setting limits is assuming that the application is single threaded and only running on it’s “assigned“ vCPU. In the above section we learned that CFS doesn’t assign cores, and in reality a container running large thread pools will schedule on all available vCPU’s on the box. </p>
<p>If 64 OS threads are running across 64 available cores (from a Linux node perspective) we will make the total bill of used CPU time in a 100ms period quite large after the time running on all of those 64 cores are added up. Since this might only occur during a garbage collection process it can be quite easy to miss something like this. This is why it is necessary to use metrics to ensure we have the correct usage over time before attempting to set a limit. </p>
<p>Fortunately, we have a way to see exactly how much vCPU is being used by all the threads in a application. We will use the metric <code>container_cpu_usage_seconds_total</code> for this purpose. </p>
<p>Since throttling logic happens every 100ms and this metric is a per second metric, we will PromQL to match this 100ms period. If you would like to dive deep into this PromQL statement work please see the following <a href="https://aws.amazon.com/blogs/containers/using-prometheus-to-avoid-disasters-with-kubernetes-cpu-limits/">blog</a>.</p>
<p>PromQL query: </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#scalability-docs-node_efficiency-__codelineno-3-1"></a>topk(3, max by (pod, container)(rate(container_cpu_usage_seconds_total{image!=&quot;&quot;, instance=&quot;$instance&quot;}[$__rate_interval]))) / 10
</code></pre></div>
<p><img alt="" src="../scalability/images/cpu-1.png" /></p>
<p>Once we feel we have the right value, we can put the limit in production. It then becomes necessary to see if our application is being throttled due to something unexpected. We can do this by looking at  <code>container_cpu_throttled_seconds_total</code></p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#scalability-docs-node_efficiency-__codelineno-4-1"></a>topk(3, max by (pod, container)(rate(container_cpu_cfs_throttled_seconds_total{image!=``&quot;&quot;``, instance=``&quot;$instance&quot;``}[$__rate_interval]))) / 10
</code></pre></div>
<p><img alt="" src="../scalability/images/cpu-2.png" /></p>
<h3 id="scalability-docs-node_efficiency-memory">Memory<a class="headerlink" href="#scalability-docs-node_efficiency-memory" title="Permanent link">&para;</a></h3>
<p>The memory allocation is another example where it is easy to confuse Kubernetes scheduling behavior for Linux CGroup behavior. This is a more nuanced topic as there have been major changes in the way that CGroup v2 handles memory in Linux and Kubernetes has changed its syntax to reflect this; read this <a href="https://kubernetes.io/blog/2021/11/26/qos-memory-resources/">blog</a> for further details.</p>
<p>Unlike CPU requests, memory requests go unused after the scheduling process completes. This is because we can not compress memory in CGroup v1 the same way we can with CPU. That leaves us with just memory limits, which are designed to act as a fail safe for memory leaks by terminating the pod completely. This is an all or nothing style proposition, however we have now been given new ways to address this problem.</p>
<p>First, it is important to understand that setting the right amount of memory for containers is not a straightforward as it appears. The file system in Linux will use memory as a cache to improve performance. This cache will grow over time, and it can be hard to know how much memory is just nice to have for the cache but can be reclaimed without a significant impact to application performance. This often results in misinterpreting memory usage.</p>
<p>Having the ability to “compress” memory was one of the primary drivers behind CGroup v2. For more history on why CGroup V2 was necessary, please see Chris Down’s <a href="https://www.youtube.com/watch?v=kPMZYoRxtmg">presentation</a> at LISA21 where he covers why being unable to set the minimum memory correctly was one of the reasons that drove him to create CGroup v2  and pressure stall metrics. </p>
<p>Fortunately, Kubernetes now has the concept of <code>memory.min</code> and <code>memory.high</code> under <code>requests.memory</code>. This gives us the option of aggressive releasing this cached memory for other containers to use. Once the container hits the memory high limit, the kernel can aggressively reclaim that container’s memory up to the value set at <code>memory.min</code>. Thus giving us more flexibility when a node comes under memory pressure.</p>
<p>The key question becomes, what value to set <code>memory.min</code> to? This is where memory pressure stall metrics come into play. We can use these metrics to detect memory “thrashing” at a container level. Then we can use controllers such as <a href="https://facebookmicrosites.github.io/cgroup2/docs/fbtax-results.html">fbtax</a> to detect the correct values for <code>memory.min</code> by looking for this memory thrashing, and dynamically set the <code>memory.min</code> value to this setting. </p>
<h3 id="scalability-docs-node_efficiency-summary">Summary<a class="headerlink" href="#scalability-docs-node_efficiency-summary" title="Permanent link">&para;</a></h3>
<p>To sum up the section, it is easy to conflate the following concepts: </p>
<ul>
<li>Utilization and Saturation  </li>
<li>Linux performance rules with Kubernetes Scheduler logic</li>
</ul>
<p>Great care must be taken to keep these concepts separated. Performance and scale are linked on a deep level. Unnecessary scaling creates performance problems, which in turn creates scaling problems. </p></section><h1 class='nav-section-title-end'>Ended: Scalability</h1><section class="print-page" id="upgrades"><h1 id="upgrades-best-practices-for-cluster-upgrades">Best Practices for Cluster Upgrades<a class="headerlink" href="#upgrades-best-practices-for-cluster-upgrades" title="Permanent link">&para;</a></h1>
<p>This guide shows cluster administrators how to plan and execute their Amazon EKS upgrade strategy. It also describes how to upgrade self-managed nodes, managed node groups, Karpenter nodes, and Fargate nodes. It does not include guidance on EKS Anywhere, self-managed Kubernetes, AWS Outposts, or AWS Local Zones. </p>
<h2 id="upgrades-overview">Overview<a class="headerlink" href="#upgrades-overview" title="Permanent link">&para;</a></h2>
<p>A Kubernetes version encompasses both the control plane and the data plane. To ensure smooth operation, both the control plane and the data plane should run the same <a href="https://kubernetes.io/releases/version-skew-policy/#supported-versions">Kubernetes minor version, such as 1.24</a>. While AWS manages and upgrades the control plane, updating the worker nodes in the data plane is your responsibility.</p>
<ul>
<li><strong>Control plane</strong> — The version of the control plane is determined by the Kubernetes API server. In Amazon EKS clusters, AWS takes care of managing this component. Control plane upgrades can be initiated via the AWS API. </li>
<li><strong>Data plane</strong> — The data plane version is associated with the Kubelet versions running on your individual nodes. It's possible to have nodes in the same cluster running different versions. You can check the versions of all nodes by running <code>kubectl get nodes</code>.</li>
</ul>
<h2 id="upgrades-before-upgrading">Before Upgrading<a class="headerlink" href="#upgrades-before-upgrading" title="Permanent link">&para;</a></h2>
<p>If you're planning to upgrade your Kubernetes version in Amazon EKS, there are a few important policies, tools, and procedures you should put in place before starting an upgrade. </p>
<ul>
<li><strong>Understand Deprecation Policies</strong> — Gain a deep understanding of how the  <a href="https://kubernetes.io/docs/reference/using-api/deprecation-policy/">Kubernetes deprecation policy</a> works. Be aware of any upcoming changes that may affect your existing applications. Newer versions of Kubernetes often phase out certain APIs and features, potentially causing issues for running applications.</li>
<li><strong>Review Kubernetes Change Log</strong> — Thoroughly review the <a href="https://github.com/kubernetes/kubernetes/tree/master/CHANGELOG">Kubernetes change log</a> alongside <a href="https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html">Amazon EKS Kubernetes versions</a> to understand any possible impact to your cluster, such as breaking changes that may affect your workloads.</li>
<li><strong>Assess Cluster Add-Ons Compatibility</strong> — Amazon EKS doesn't automatically update an add-on when new versions are released or after you update your cluster to a new Kubernetes minor version. Review <a href="https://docs.aws.amazon.com/eks/latest/userguide/managing-add-ons.html#updating-an-add-on">Updating an add-on</a> to understand the compatibility of any existing cluster add-ons with the cluster version you intend to upgrade to.</li>
<li><strong>Enable Control Plane Logging</strong> — Enable <a href="https://docs.aws.amazon.com/eks/latest/userguide/control-plane-logs.html">control plane logging</a> to capture logs, errors, or issues that can arise during the upgrade process. Consider reviewing these logs for any anomalies. Test cluster upgrades in a non-production environment, or integrate automated tests into your continuous integration workflow to assess version compatibility with your applications, controllers, and custom integrations.</li>
<li><strong>Explore eksctl for Cluster Management</strong> — Consider using <a href="https://eksctl.io/">eksctl</a> to manage your EKS cluster. It provides you with the ability to <a href="https://eksctl.io/usage/cluster-upgrade/">update the control plane, manage add-ons, and handle worker node updates</a> out-of-the-box. </li>
<li><strong>Opt for Managed Node Groups or EKS on Fargate</strong> — Streamline and automate worker node upgrades by using <a href="https://docs.aws.amazon.com/eks/latest/userguide/managed-node-groups.html">EKS managed node groups</a> or <a href="https://docs.aws.amazon.com/eks/latest/userguide/fargate.html">EKS on Fargate</a>. These options simplify the process and reduce manual intervention.</li>
<li><strong>Utilize kubectl Convert Plugin</strong> — Leverage the <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/#install-kubectl-convert-plugin">kubectl convert plugin</a> to facilitate the <a href="https://kubernetes.io/docs/tasks/tools/included/kubectl-convert-overview/">conversion of Kubernetes manifest files</a> between different API versions. This can help ensure that your configurations remain compatible with the new Kubernetes version.</li>
</ul>
<h2 id="upgrades-keep-your-cluster-up-to-date">Keep your cluster up-to-date<a class="headerlink" href="#upgrades-keep-your-cluster-up-to-date" title="Permanent link">&para;</a></h2>
<p>Staying current with Kubernetes updates is paramount for a secure and efficient EKS environment, reflecting the shared responsibility model in Amazon EKS. By integrating these strategies into your operational workflow, you're positioning yourself to maintain up-to-date, secure clusters that take full advantage of the latest features and improvements. Tactics:</p>
<ul>
<li><strong>Supported Version Policy</strong> — Aligned with the Kubernetes community, Amazon EKS typically provides three active Kubernetes versions while deprecating a fourth version each year. Deprecation notices are issued at least 60 days before a version reaches its end-of-support date. For more details, refer to the <a href="https://aws.amazon.com/eks/eks-version-faq/">EKS Version FAQ</a>.</li>
<li><strong>Auto-Upgrade Policy</strong> — We strongly recommend staying in sync with Kubernetes updates in your EKS cluster. Kubernetes community support, including bug fixes and security patches, typically ceases for versions older than one year. Deprecated versions may also lack vulnerability reporting, posing a potential risk. Failure to proactively upgrade before a version's end-of-life triggers an automatic upgrade, which could disrupt your workloads and systems. For additional information, consult the <a href="https://aws.amazon.com/eks/eks-version-support-policy/">EKS Version Support Policy</a>.</li>
<li><strong>Create Upgrade Runbooks</strong> — Establish a well-documented process for managing upgrades. As part of your proactive approach, develop runbooks and specialized tools tailored to your upgrade process. This not only enhances your preparedness but also simplifies complex transitions. Make it a standard practice to upgrade your clusters at least once a year. This practice aligns you with ongoing technological advancements, thereby boosting the efficiency and security of your environment.</li>
</ul>
<h2 id="upgrades-review-the-eks-release-calendar">Review the EKS release calendar<a class="headerlink" href="#upgrades-review-the-eks-release-calendar" title="Permanent link">&para;</a></h2>
<p><a href="https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html#kubernetes-release-calendar">Review the EKS Kubernetes release calendar</a> to learn when new versions are coming, and when support for specific versions end. Generally, EKS releases three minor versions of Kubernetes annually, and each minor version is supported for about 14 months. </p>
<p>Additionally, review the upstream <a href="https://kubernetes.io/releases/">Kubernetes release information</a>.</p>
<h2 id="upgrades-understand-how-the-shared-responsibility-model-applies-to-cluster-upgrades">Understand how the shared responsibility model applies to cluster upgrades<a class="headerlink" href="#upgrades-understand-how-the-shared-responsibility-model-applies-to-cluster-upgrades" title="Permanent link">&para;</a></h2>
<p>You are responsible for initiating upgrade for both cluster control plane as well as the data plane. <a href="https://docs.aws.amazon.com/eks/latest/userguide/update-cluster.html">Learn how to initiate an upgrade.</a> When you initiate a cluster upgrade, AWS manages upgrading the cluster control plane. You are responsible for upgrading the data plane, including Fargate pods and <a href="#upgrades-upgrade-add-ons-and-components-using-the-kubernetes-api">other add-ons.</a> You must validate and plan upgrades for workloads running on your cluster to ensure their availability and operations are not impacted after cluster upgrade</p>
<h2 id="upgrades-upgrade-clusters-in-place">Upgrade clusters in-place<a class="headerlink" href="#upgrades-upgrade-clusters-in-place" title="Permanent link">&para;</a></h2>
<p>EKS supports an in-place cluster upgrade strategy. This maintains cluster resources, and keeps cluster configuration consistent (e.g., API endpoint, OIDC, ENIs, load balancers). This is less disruptive for cluster users, and it will use the existing workloads and resources in the cluster without requiring you to redeploy workloads or migrate external resources (e.g., DNS, storage).</p>
<p>When performing an in-place cluster upgrade, it is important to note that only one minor version upgrade can be executed at a time (e.g., from 1.24 to 1.25). </p>
<p>This means that if you need to update multiple versions, a series of sequential upgrades will be required. Planning sequential upgrades is more complicated, and has a higher risk of downtime. In this situation, <a href="#upgrades-evaluate-bluegreen-clusters-as-an-alternative-to-in-place-cluster-upgrades">evaluate a blue/green cluster upgrade strategy.</a></p>
<h2 id="upgrades-upgrade-your-control-plane-and-data-plane-in-sequence">Upgrade your control plane and data plane in sequence<a class="headerlink" href="#upgrades-upgrade-your-control-plane-and-data-plane-in-sequence" title="Permanent link">&para;</a></h2>
<p>To upgrade a cluster you will need to take the following actions:</p>
<ol>
<li><a href="#upgrades-use-the-eks-documentation-to-create-an-upgrade-checklist">Review the Kubernetes and EKS release notes.</a></li>
<li><a href="#upgrades-backup-the-cluster-before-upgrading">Take a backup of the cluster. (optional)</a></li>
<li><a href="#upgrades-identify-and-remediate-removed-api-usage-before-upgrading-the-control-plane">Identify and remediate deprecated and removed API usage in your workloads.</a></li>
<li><a href="#upgrades-track-the-version-skew-of-nodes-ensure-managed-node-groups-are-on-the-same-version-as-the-control-plane-before-upgrading">Ensure Managed Node Groups, if used, are on the same Kubernetes version as the control plane.</a> EKS managed node groups and nodes created by EKS Fargate Profiles only support 1 minor version skew between the control plane and data plane.</li>
<li><a href="https://docs.aws.amazon.com/eks/latest/userguide/update-cluster.html">Upgrade the cluster control plane using the AWS console or cli.</a></li>
<li><a href="#upgrades-upgrade-add-ons-and-components-using-the-kubernetes-api">Review add-on compatibility.</a> Upgrade your Kubernetes add-ons and custom controllers, as required. </li>
<li><a href="https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html">Update kubectl.</a></li>
<li><a href="https://docs.aws.amazon.com/eks/latest/userguide/update-managed-node-group.html">Upgrade the cluster data plane.</a>  Upgrade your nodes to the same Kubernetes minor version as your upgraded cluster. </li>
</ol>
<h2 id="upgrades-use-the-eks-documentation-to-create-an-upgrade-checklist">Use the EKS Documentation to create an upgrade checklist<a class="headerlink" href="#upgrades-use-the-eks-documentation-to-create-an-upgrade-checklist" title="Permanent link">&para;</a></h2>
<p>The EKS Kubernetes <a href="https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html">version documentation</a> includes a detailed list of changes for each version. Build a checklist for each upgrade. </p>
<p>For specific EKS version upgrade guidance, review the documentation for notable changes and considerations for each version.</p>
<ul>
<li><a href="https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html#kubernetes-1.27">EKS 1.27</a></li>
<li><a href="https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html#kubernetes-1.26">EKS 1.26</a></li>
<li><a href="https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html#kubernetes-1.25">EKS 1.25</a></li>
<li><a href="https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html#kubernetes-1.24">EKS 1.24</a></li>
<li><a href="https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html#kubernetes-1.23">EKS 1.23</a></li>
<li><a href="https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html#kubernetes-1.22">EKS 1.22</a></li>
</ul>
<h2 id="upgrades-upgrade-add-ons-and-components-using-the-kubernetes-api">Upgrade add-ons and components using the Kubernetes API<a class="headerlink" href="#upgrades-upgrade-add-ons-and-components-using-the-kubernetes-api" title="Permanent link">&para;</a></h2>
<p>Before you upgrade a cluster, you should understand what versions of Kubernetes components you are using. Inventory cluster components, and identify components that use the Kubernetes API directly. This includes critical cluster components such as monitoring and logging agents, cluster autoscalers, container storage drivers (e.g. <a href="https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html">EBS CSI</a>, <a href="https://docs.aws.amazon.com/eks/latest/userguide/efs-csi.html">EFS CSI</a>), ingress controllers, and any other workloads or add-ons that rely on the Kubernetes API directly. </p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Critical cluster components are often installed in a <code>*-system</code> namespace</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#upgrades-__codelineno-0-1"></a>kubectl get ns | grep &#39;-system&#39;
</code></pre></div>
</div>
<p>Once you have identified components that rely the Kubernetes API, check their documentation for version compatibility and upgrade requirements. For example, see the <a href="https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.4/deploy/installation/">AWS Load Balancer Controller</a> documentation for version compatibility. Some components may need to be upgraded or configuration changed before proceeding with a cluster upgrade. Some critical components to check include <a href="https://github.com/coredns/coredns">CoreDNS</a>, <a href="https://kubernetes.io/docs/concepts/overview/components/#kube-proxy">kube-proxy</a>, <a href="https://github.com/aws/amazon-vpc-cni-k8s">VPC CNI</a>, and storage drivers. </p>
<p>Clusters often contain many workloads that use the Kubernetes API and are required for workload functionality such as ingress controllers, continuous delivery systems, and monitoring tools. When you upgrade an EKS cluster, you must also upgrade your add-ons and third-party tools to make sure they are compatible.</p>
<p>See the following examples of common add-ons and their relevant upgrade documentation:</p>
<ul>
<li><strong>Amazon VPC CNI:</strong> For the recommended version of the Amazon VPC CNI add-on for each cluster version, see <a href="https://docs.aws.amazon.com/eks/latest/userguide/managing-vpc-cni.html">Updating the Amazon VPC CNI plugin for Kubernetes self-managed add-on</a>. <strong>When installed as an Amazon EKS Add-on, it can only be upgraded one minor version at a time.</strong></li>
<li><strong>kube-proxy:</strong> See <a href="https://docs.aws.amazon.com/eks/latest/userguide/managing-kube-proxy.html">Updating the Kubernetes kube-proxy self-managed add-on</a>.</li>
<li><strong>CoreDNS:</strong> See <a href="https://docs.aws.amazon.com/eks/latest/userguide/managing-coredns.html">Updating the CoreDNS self-managed add-on</a>.</li>
<li><strong>AWS Load Balancer Controller:</strong> The AWS Load Balancer Controller needs to be compatible with the EKS version you have deployed. See the <a href="https://docs.aws.amazon.com/eks/latest/userguide/aws-load-balancer-controller.html">installation guide</a> for more information. </li>
<li><strong>Amazon Elastic Block Store (Amazon EBS) Container Storage Interface (CSI) driver:</strong> For installation and upgrade information, see <a href="https://docs.aws.amazon.com/eks/latest/userguide/managing-ebs-csi.html">Managing the Amazon EBS CSI driver as an Amazon EKS add-on</a>.</li>
<li><strong>Amazon Elastic File System (Amazon EFS) Container Storage Interface (CSI) driver:</strong> For installation and upgrade information, see <a href="https://docs.aws.amazon.com/eks/latest/userguide/efs-csi.html">Amazon EFS CSI driver</a>.</li>
<li><strong>Kubernetes Metrics Server:</strong> For more information, see <a href="https://kubernetes-sigs.github.io/metrics-server/">metrics-server</a> on GitHub.</li>
<li><strong>Kubernetes Cluster Autoscaler</strong><strong>:</strong> To upgrade the version of Kubernetes Cluster Autoscaler, change the version of the image in the deployment. The Cluster Autoscaler is tightly coupled with the Kubernetes scheduler. You will always need to upgrade it when you upgrade the cluster. Review the <a href="https://github.com/kubernetes/autoscaler/releases">GitHub releases</a> to find the address of the latest release corresponding to your Kubernetes minor version.</li>
<li><strong>Karpenter:</strong> For installation and upgrade information, see the <a href="https://karpenter.sh/v0.27.3/faq/#which-versions-of-kubernetes-does-karpenter-support">Karpenter documentation.</a></li>
</ul>
<h2 id="upgrades-verify-basic-eks-requirements-before-upgrading">Verify basic EKS requirements before upgrading<a class="headerlink" href="#upgrades-verify-basic-eks-requirements-before-upgrading" title="Permanent link">&para;</a></h2>
<p>AWS requires certain resources in your account to complete the upgrade process. If these resources aren’t present, the cluster cannot be upgraded. A control plane upgrade requires the following resources:</p>
<ol>
<li>Available IP addresses: Amazon EKS requires up to five available IP addresses from the subnets you specified when you created the cluster in order to update the cluster. If not, update your cluster configuration to include new cluster subnets prior to performing the version update.</li>
<li>EKS IAM role: The control plane IAM role is still present in the account with the necessary permissions.</li>
<li>If your cluster has secret encryption enabled, then make sure that the cluster IAM role has permission to use the AWS Key Management Service (AWS KMS) key.</li>
</ol>
<h3 id="upgrades-verify-available-ip-addresses">Verify available IP addresses<a class="headerlink" href="#upgrades-verify-available-ip-addresses" title="Permanent link">&para;</a></h3>
<p>To update the cluster, Amazon EKS requires up to five available IP addresses from the subnets that you specified when you created your cluster.</p>
<p>To verify that your subnets have enough IP addresses to upgrade the cluster you can run the following command:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#upgrades-__codelineno-1-1"></a>CLUSTER=&lt;cluster name&gt;
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#upgrades-__codelineno-1-2"></a>aws ec2 describe-subnets --subnet-ids \
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#upgrades-__codelineno-1-3"></a>  $(aws eks describe-cluster --name ${CLUSTER} \
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#upgrades-__codelineno-1-4"></a>  --query &#39;cluster.resourcesVpcConfig.subnetIds&#39; \
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#upgrades-__codelineno-1-5"></a>  --output text) \
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#upgrades-__codelineno-1-6"></a>  --query &#39;Subnets[*].[SubnetId,AvailabilityZone,AvailableIpAddressCount]&#39; \
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#upgrades-__codelineno-1-7"></a>  --output table
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#upgrades-__codelineno-1-8"></a>
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#upgrades-__codelineno-1-9"></a>----------------------------------------------------
<a id="__codelineno-1-10" name="__codelineno-1-10" href="#upgrades-__codelineno-1-10"></a>|                  DescribeSubnets                 |
<a id="__codelineno-1-11" name="__codelineno-1-11" href="#upgrades-__codelineno-1-11"></a>+---------------------------+--------------+-------+
<a id="__codelineno-1-12" name="__codelineno-1-12" href="#upgrades-__codelineno-1-12"></a>|  subnet-067fa8ee8476abbd6 |  us-east-1a  |  8184 |
<a id="__codelineno-1-13" name="__codelineno-1-13" href="#upgrades-__codelineno-1-13"></a>|  subnet-0056f7403b17d2b43 |  us-east-1b  |  8153 |
<a id="__codelineno-1-14" name="__codelineno-1-14" href="#upgrades-__codelineno-1-14"></a>|  subnet-09586f8fb3addbc8c |  us-east-1a  |  8120 |
<a id="__codelineno-1-15" name="__codelineno-1-15" href="#upgrades-__codelineno-1-15"></a>|  subnet-047f3d276a22c6bce |  us-east-1b  |  8184 |
<a id="__codelineno-1-16" name="__codelineno-1-16" href="#upgrades-__codelineno-1-16"></a>+---------------------------+--------------+-------+
</code></pre></div>
<p>The <a href="https://github.com/aws/amazon-vpc-cni-k8s/blob/master/cmd/cni-metrics-helper/README.md">VPC CNI Metrics Helper</a> may be used to create a CloudWatch dashboard for VPC metrics. 
Amazon EKS recommends updating the cluster subnets using the "UpdateClusterConfiguration" API prior to beginning a Kubernetes version upgrade if you are running out of IP addresses in the subnets initially specified during cluster creation. Please verify that the new subnets you will be provided:</p>
<ul>
<li>belong to same set of AZs that are selected during cluster creation. </li>
<li>belong to the same VPC provided during cluster creation</li>
</ul>
<p>Please consider associating additional CIDR blocks if the IP addresses in the existing VPC CIDR block run out. AWS enables the association of additional CIDR blocks with your existing cluster VPC, effectively expanding your IP address pool. This expansion can be accomplished by introducing additional private IP ranges (RFC 1918) or, if necessary, public IP ranges (non-RFC 1918). You must add new VPC CIDR blocks and allow VPC refresh to complete before Amazon EKS can use the new CIDR. After that, you can update the subnets based on the newly set up CIDR blocks to the VPC.</p>
<h3 id="upgrades-verify-eks-iam-role">Verify EKS IAM role<a class="headerlink" href="#upgrades-verify-eks-iam-role" title="Permanent link">&para;</a></h3>
<p>To verify that the IAM role is available and has the correct assume role policy in your account you can run the following commands:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#upgrades-__codelineno-2-1"></a>CLUSTER=&lt;cluster name&gt;
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#upgrades-__codelineno-2-2"></a>ROLE_ARN=$(aws eks describe-cluster --name ${CLUSTER} \
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#upgrades-__codelineno-2-3"></a>  --query &#39;cluster.roleArn&#39; --output text)
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#upgrades-__codelineno-2-4"></a>aws iam get-role --role-name ${ROLE_ARN##*/} \
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#upgrades-__codelineno-2-5"></a>  --query &#39;Role.AssumeRolePolicyDocument&#39;
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#upgrades-__codelineno-2-6"></a>
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#upgrades-__codelineno-2-7"></a>{
<a id="__codelineno-2-8" name="__codelineno-2-8" href="#upgrades-__codelineno-2-8"></a>    &quot;Version&quot;: &quot;2012-10-17&quot;,
<a id="__codelineno-2-9" name="__codelineno-2-9" href="#upgrades-__codelineno-2-9"></a>    &quot;Statement&quot;: [
<a id="__codelineno-2-10" name="__codelineno-2-10" href="#upgrades-__codelineno-2-10"></a>        {
<a id="__codelineno-2-11" name="__codelineno-2-11" href="#upgrades-__codelineno-2-11"></a>            &quot;Effect&quot;: &quot;Allow&quot;,
<a id="__codelineno-2-12" name="__codelineno-2-12" href="#upgrades-__codelineno-2-12"></a>            &quot;Principal&quot;: {
<a id="__codelineno-2-13" name="__codelineno-2-13" href="#upgrades-__codelineno-2-13"></a>                &quot;Service&quot;: &quot;eks.amazonaws.com&quot;
<a id="__codelineno-2-14" name="__codelineno-2-14" href="#upgrades-__codelineno-2-14"></a>            },
<a id="__codelineno-2-15" name="__codelineno-2-15" href="#upgrades-__codelineno-2-15"></a>            &quot;Action&quot;: &quot;sts:AssumeRole&quot;
<a id="__codelineno-2-16" name="__codelineno-2-16" href="#upgrades-__codelineno-2-16"></a>        }
<a id="__codelineno-2-17" name="__codelineno-2-17" href="#upgrades-__codelineno-2-17"></a>    ]
<a id="__codelineno-2-18" name="__codelineno-2-18" href="#upgrades-__codelineno-2-18"></a>}
</code></pre></div>
<h2 id="upgrades-migrate-to-eks-add-ons">Migrate to EKS Add-ons<a class="headerlink" href="#upgrades-migrate-to-eks-add-ons" title="Permanent link">&para;</a></h2>
<p>Amazon EKS automatically installs add-ons such as the Amazon VPC CNI plugin for Kubernetes, <code>kube-proxy</code>, and CoreDNS for every cluster. Add-ons may be self-managed, or installed as Amazon EKS Add-ons. Amazon EKS Add-ons is an alternate way to manage add-ons using the EKS API. </p>
<p>You can use Amazon EKS Add-ons to update versions with a single command. For Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#upgrades-__codelineno-3-1"></a>aws eks update-addon —cluster-name my-cluster —addon-name vpc-cni —addon-version version-number \
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#upgrades-__codelineno-3-2"></a>--service-account-role-arn arn:aws:iam::111122223333:role/role-name —configuration-values &#39;{}&#39; —resolve-conflicts PRESERVE
</code></pre></div>
<p>Check if you have any EKS Add-ons with:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#upgrades-__codelineno-4-1"></a>aws eks list-addons --cluster-name &lt;cluster name&gt;
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>EKS Add-ons are not automatically upgraded during a control plane upgrade. You must initiate EKS add-on updates, and select the desired version. </p>
<ul>
<li>You are responsible for selecting a compatible version from all available versions. <a href="#upgrades-upgrade-add-ons-and-components-using-the-kubernetes-api">Review the guidance on add-on version compatibility.</a></li>
<li>Amazon EKS Add-ons may only be upgraded one minor version at a time. </li>
</ul>
</div>
<p><a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-add-ons.html">Learn more about what components are available as EKS Add-ons, and how to get started.</a></p>
<p><a href="https://aws.amazon.com/blogs/containers/amazon-eks-add-ons-advanced-configuration/">Learn how to supply a custom configuration to an EKS Add-on.</a></p>
<h2 id="upgrades-identify-and-remediate-removed-api-usage-before-upgrading-the-control-plane">Identify and remediate removed API usage before upgrading the control plane<a class="headerlink" href="#upgrades-identify-and-remediate-removed-api-usage-before-upgrading-the-control-plane" title="Permanent link">&para;</a></h2>
<p>You should identify API usage of removed APIs before upgrading your EKS control plane. To do that we recommend using tools that can check a running cluster or static, rendered Kubernetes manifest files. </p>
<p>Running the check against static manifest files is generally more accurate. If run against live clusters, these tools may return false positives. </p>
<p>A deprecated Kubernetes API does not mean the API has been removed. You should check the <a href="https://kubernetes.io/docs/reference/using-api/deprecation-policy/">Kubernetes Deprecation Policy</a> to understand how API removal affects your workloads.</p>
<h3 id="upgrades-kube-no-trouble">Kube-no-trouble<a class="headerlink" href="#upgrades-kube-no-trouble" title="Permanent link">&para;</a></h3>
<p><a href="https://github.com/doitintl/kube-no-trouble">Kube-no-trouble</a> is an open source command line utility with the command <code>kubent</code>. When you run <code>kubent</code> without any arguments it will use your current KubeConfig context and scan the cluster and print a report with what APIs will be deprecated and removed. </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#upgrades-__codelineno-5-1"></a>kubent
<a id="__codelineno-5-2" name="__codelineno-5-2" href="#upgrades-__codelineno-5-2"></a>
<a id="__codelineno-5-3" name="__codelineno-5-3" href="#upgrades-__codelineno-5-3"></a>4:17PM INF &gt;&gt;&gt; Kube No Trouble `kubent` &lt;&lt;&lt;
<a id="__codelineno-5-4" name="__codelineno-5-4" href="#upgrades-__codelineno-5-4"></a>4:17PM INF version 0.7.0 (git sha d1bb4e5fd6550b533b2013671aa8419d923ee042)
<a id="__codelineno-5-5" name="__codelineno-5-5" href="#upgrades-__codelineno-5-5"></a>4:17PM INF Initializing collectors and retrieving data
<a id="__codelineno-5-6" name="__codelineno-5-6" href="#upgrades-__codelineno-5-6"></a>4:17PM INF Target K8s version is 1.24.8-eks-ffeb93d
<a id="__codelineno-5-7" name="__codelineno-5-7" href="#upgrades-__codelineno-5-7"></a>4:l INF Retrieved 93 resources from collector name=Cluster
<a id="__codelineno-5-8" name="__codelineno-5-8" href="#upgrades-__codelineno-5-8"></a>4:17PM INF Retrieved 16 resources from collector name=&quot;Helm v3&quot;
<a id="__codelineno-5-9" name="__codelineno-5-9" href="#upgrades-__codelineno-5-9"></a>4:17PM INF Loaded ruleset name=custom.rego.tmpl
<a id="__codelineno-5-10" name="__codelineno-5-10" href="#upgrades-__codelineno-5-10"></a>4:17PM INF Loaded ruleset name=deprecated-1-16.rego
<a id="__codelineno-5-11" name="__codelineno-5-11" href="#upgrades-__codelineno-5-11"></a>4:17PM INF Loaded ruleset name=deprecated-1-22.rego
<a id="__codelineno-5-12" name="__codelineno-5-12" href="#upgrades-__codelineno-5-12"></a>4:17PM INF Loaded ruleset name=deprecated-1-25.rego
<a id="__codelineno-5-13" name="__codelineno-5-13" href="#upgrades-__codelineno-5-13"></a>4:17PM INF Loaded ruleset name=deprecated-1-26.rego
<a id="__codelineno-5-14" name="__codelineno-5-14" href="#upgrades-__codelineno-5-14"></a>4:17PM INF Loaded ruleset name=deprecated-future.rego
<a id="__codelineno-5-15" name="__codelineno-5-15" href="#upgrades-__codelineno-5-15"></a>__________________________________________________________________________________________
<a id="__codelineno-5-16" name="__codelineno-5-16" href="#upgrades-__codelineno-5-16"></a>&gt;&gt;&gt; Deprecated APIs removed in 1.25 &lt;&lt;&lt;
<a id="__codelineno-5-17" name="__codelineno-5-17" href="#upgrades-__codelineno-5-17"></a>------------------------------------------------------------------------------------------
<a id="__codelineno-5-18" name="__codelineno-5-18" href="#upgrades-__codelineno-5-18"></a>KIND                NAMESPACE     NAME             API_VERSION      REPLACE_WITH (SINCE)
<a id="__codelineno-5-19" name="__codelineno-5-19" href="#upgrades-__codelineno-5-19"></a>PodSecurityPolicy   &lt;undefined&gt;   eks.privileged   policy/v1beta1   &lt;removed&gt; (1.21.0)
</code></pre></div>
<p>It can also be used to scan static manifest files and helm packages. It is recommended to run <code>kubent</code> as part of a continuous integration (CI) process to identify issues before manifests are deployed. Scanning manifests is also more accurate than scanning live clusters. </p>
<p>Kube-no-trouble provides a sample <a href="https://github.com/doitintl/kube-no-trouble/blob/master/docs/k8s-sa-and-role-example.yaml">Service Account and Role</a> with the appropriate permissions for scanning the cluster. </p>
<h3 id="upgrades-pluto">Pluto<a class="headerlink" href="#upgrades-pluto" title="Permanent link">&para;</a></h3>
<p>Another option is <a href="https://pluto.docs.fairwinds.com/">pluto</a> which is similar to <code>kubent</code> because it supports scanning a live cluster, manifest files, helm charts and has a GitHub Action you can include in your CI process.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#upgrades-__codelineno-6-1"></a>pluto detect-all-in-cluster
<a id="__codelineno-6-2" name="__codelineno-6-2" href="#upgrades-__codelineno-6-2"></a>
<a id="__codelineno-6-3" name="__codelineno-6-3" href="#upgrades-__codelineno-6-3"></a>NAME             KIND                VERSION          REPLACEMENT   REMOVED   DEPRECATED   REPL AVAIL  
<a id="__codelineno-6-4" name="__codelineno-6-4" href="#upgrades-__codelineno-6-4"></a>eks.privileged   PodSecurityPolicy   policy/v1beta1                 false     true         true
</code></pre></div>
<h3 id="upgrades-resources">Resources<a class="headerlink" href="#upgrades-resources" title="Permanent link">&para;</a></h3>
<p>To verify that your cluster don't use deprecated APIs before the upgrade, you should monitor:</p>
<ul>
<li>metric <code>apiserver_requested_deprecated_apis</code> since Kubernetes v1.19:</li>
</ul>
<div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#upgrades-__codelineno-7-1"></a>kubectl get --raw /metrics | grep apiserver_requested_deprecated_apis
<a id="__codelineno-7-2" name="__codelineno-7-2" href="#upgrades-__codelineno-7-2"></a>
<a id="__codelineno-7-3" name="__codelineno-7-3" href="#upgrades-__codelineno-7-3"></a>apiserver_requested_deprecated_apis{group=&quot;policy&quot;,removed_release=&quot;1.25&quot;,resource=&quot;podsecuritypolicies&quot;,subresource=&quot;&quot;,version=&quot;v1beta1&quot;} 1
</code></pre></div>
<ul>
<li>events in the <a href="https://docs.aws.amazon.com/eks/latest/userguide/control-plane-logs.html">audit logs</a> with <code>k8s.io/deprecated</code> set to <code>true</code>:</li>
</ul>
<div class="highlight"><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1" href="#upgrades-__codelineno-8-1"></a>CLUSTER=&quot;&lt;cluster_name&gt;&quot;
<a id="__codelineno-8-2" name="__codelineno-8-2" href="#upgrades-__codelineno-8-2"></a>QUERY_ID=$(aws logs start-query \
<a id="__codelineno-8-3" name="__codelineno-8-3" href="#upgrades-__codelineno-8-3"></a> --log-group-name /aws/eks/${CLUSTER}/cluster \
<a id="__codelineno-8-4" name="__codelineno-8-4" href="#upgrades-__codelineno-8-4"></a> --start-time $(date -u --date=&quot;-30 minutes&quot; &quot;+%s&quot;) # or date -v-30M &quot;+%s&quot; on MacOS \
<a id="__codelineno-8-5" name="__codelineno-8-5" href="#upgrades-__codelineno-8-5"></a> --end-time $(date &quot;+%s&quot;) \
<a id="__codelineno-8-6" name="__codelineno-8-6" href="#upgrades-__codelineno-8-6"></a> --query-string &#39;fields @message | filter `annotations.k8s.io/deprecated`=&quot;true&quot;&#39; \
<a id="__codelineno-8-7" name="__codelineno-8-7" href="#upgrades-__codelineno-8-7"></a> --query queryId --output text)
<a id="__codelineno-8-8" name="__codelineno-8-8" href="#upgrades-__codelineno-8-8"></a>
<a id="__codelineno-8-9" name="__codelineno-8-9" href="#upgrades-__codelineno-8-9"></a>echo &quot;Query started (query id: $QUERY_ID), please hold ...&quot; &amp;&amp; sleep 5 # give it some time to query
<a id="__codelineno-8-10" name="__codelineno-8-10" href="#upgrades-__codelineno-8-10"></a>
<a id="__codelineno-8-11" name="__codelineno-8-11" href="#upgrades-__codelineno-8-11"></a>aws logs get-query-results --query-id $QUERY_ID
</code></pre></div>
<p>Which will output lines if deprecated APIs are in use:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-9-1" name="__codelineno-9-1" href="#upgrades-__codelineno-9-1"></a>{
<a id="__codelineno-9-2" name="__codelineno-9-2" href="#upgrades-__codelineno-9-2"></a>    &quot;results&quot;: [
<a id="__codelineno-9-3" name="__codelineno-9-3" href="#upgrades-__codelineno-9-3"></a>        [
<a id="__codelineno-9-4" name="__codelineno-9-4" href="#upgrades-__codelineno-9-4"></a>            {
<a id="__codelineno-9-5" name="__codelineno-9-5" href="#upgrades-__codelineno-9-5"></a>                &quot;field&quot;: &quot;@message&quot;,
<a id="__codelineno-9-6" name="__codelineno-9-6" href="#upgrades-__codelineno-9-6"></a>                &quot;value&quot;: &quot;{\&quot;kind\&quot;:\&quot;Event\&quot;,\&quot;apiVersion\&quot;:\&quot;audit.k8s.io/v1\&quot;,\&quot;level\&quot;:\&quot;Request\&quot;,\&quot;auditID\&quot;:\&quot;8f7883c6-b3d5-42d7-967a-1121c6f22f01\&quot;,\&quot;stage\&quot;:\&quot;ResponseComplete\&quot;,\&quot;requestURI\&quot;:\&quot;/apis/policy/v1beta1/podsecuritypolicies?allowWatchBookmarks=true\\u0026resourceVersion=4131\\u0026timeout=9m19s\\u0026timeoutSeconds=559\\u0026watch=true\&quot;,\&quot;verb\&quot;:\&quot;watch\&quot;,\&quot;user\&quot;:{\&quot;username\&quot;:\&quot;system:apiserver\&quot;,\&quot;uid\&quot;:\&quot;8aabfade-da52-47da-83b4-46b16cab30fa\&quot;,\&quot;groups\&quot;:[\&quot;system:masters\&quot;]},\&quot;sourceIPs\&quot;:[\&quot;::1\&quot;],\&quot;userAgent\&quot;:\&quot;kube-apiserver/v1.24.16 (linux/amd64) kubernetes/af930c1\&quot;,\&quot;objectRef\&quot;:{\&quot;resource\&quot;:\&quot;podsecuritypolicies\&quot;,\&quot;apiGroup\&quot;:\&quot;policy\&quot;,\&quot;apiVersion\&quot;:\&quot;v1beta1\&quot;},\&quot;responseStatus\&quot;:{\&quot;metadata\&quot;:{},\&quot;code\&quot;:200},\&quot;requestReceivedTimestamp\&quot;:\&quot;2023-10-04T12:36:11.849075Z\&quot;,\&quot;stageTimestamp\&quot;:\&quot;2023-10-04T12:45:30.850483Z\&quot;,\&quot;annotations\&quot;:{\&quot;authorization.k8s.io/decision\&quot;:\&quot;allow\&quot;,\&quot;authorization.k8s.io/reason\&quot;:\&quot;\&quot;,\&quot;k8s.io/deprecated\&quot;:\&quot;true\&quot;,\&quot;k8s.io/removed-release\&quot;:\&quot;1.25\&quot;}}&quot;
<a id="__codelineno-9-7" name="__codelineno-9-7" href="#upgrades-__codelineno-9-7"></a>            },
<a id="__codelineno-9-8" name="__codelineno-9-8" href="#upgrades-__codelineno-9-8"></a>[...]
</code></pre></div>
<h2 id="upgrades-update-kubernetes-workloads-use-kubectl-convert-to-update-manifests">Update Kubernetes workloads. Use kubectl-convert to update manifests<a class="headerlink" href="#upgrades-update-kubernetes-workloads-use-kubectl-convert-to-update-manifests" title="Permanent link">&para;</a></h2>
<p>After you have identified what workloads and manifests need to be updated, you may need to change the resource type in your manifest files (e.g. PodSecurityPolicies to PodSecurityStandards). This will require updating the resource specification and additional research depending on what resource is being replaced.</p>
<p>If the resource type is staying the same but API version needs to be updated you can use the <code>kubectl-convert</code> command to automatically convert your manifest files.  For example, to convert an older Deployment to <code>apps/v1</code>. For more information, see <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/#install-kubectl-convert-plugin">Install kubectl convert plugin</a>on the Kubernetes website.</p>
<p><code>kubectl-convert -f &lt;file&gt; --output-version &lt;group&gt;/&lt;version&gt;</code></p>
<h2 id="upgrades-configure-poddisruptionbudgets-and-topologyspreadconstraints-to-ensure-availability-of-your-workloads-while-the-data-plane-is-upgraded">Configure PodDisruptionBudgets and topologySpreadConstraints to ensure availability of your workloads while the data plane is upgraded<a class="headerlink" href="#upgrades-configure-poddisruptionbudgets-and-topologyspreadconstraints-to-ensure-availability-of-your-workloads-while-the-data-plane-is-upgraded" title="Permanent link">&para;</a></h2>
<p>Ensure your workloads have the proper <a href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#pod-disruption-budgets">PodDisruptionBudgets</a> and <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints">topologySpreadConstraints</a> to ensure availability of your workloads while the data plane is upgraded. Not every workload requires the same level of availability so you need to validate the scale and requirements of your workload.</p>
<p>Make sure workloads are spread in multiple Availability Zones and on multiple hosts with topology spreads will give a higher level of confidence that workloads will migrate to the new data plane automatically without incident. </p>
<p>Here is an example workload that will always have 80% of replicas available and spread replicas across zones and hosts</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-10-1" name="__codelineno-10-1" href="#upgrades-__codelineno-10-1"></a>apiVersion: policy/v1
<a id="__codelineno-10-2" name="__codelineno-10-2" href="#upgrades-__codelineno-10-2"></a>kind: PodDisruptionBudget
<a id="__codelineno-10-3" name="__codelineno-10-3" href="#upgrades-__codelineno-10-3"></a>metadata:
<a id="__codelineno-10-4" name="__codelineno-10-4" href="#upgrades-__codelineno-10-4"></a>  name: myapp
<a id="__codelineno-10-5" name="__codelineno-10-5" href="#upgrades-__codelineno-10-5"></a>spec:
<a id="__codelineno-10-6" name="__codelineno-10-6" href="#upgrades-__codelineno-10-6"></a>  minAvailable: &quot;80%&quot;
<a id="__codelineno-10-7" name="__codelineno-10-7" href="#upgrades-__codelineno-10-7"></a>  selector:
<a id="__codelineno-10-8" name="__codelineno-10-8" href="#upgrades-__codelineno-10-8"></a>    matchLabels:
<a id="__codelineno-10-9" name="__codelineno-10-9" href="#upgrades-__codelineno-10-9"></a>      app: myapp
<a id="__codelineno-10-10" name="__codelineno-10-10" href="#upgrades-__codelineno-10-10"></a>---
<a id="__codelineno-10-11" name="__codelineno-10-11" href="#upgrades-__codelineno-10-11"></a>apiVersion: apps/v1
<a id="__codelineno-10-12" name="__codelineno-10-12" href="#upgrades-__codelineno-10-12"></a>kind: Deployment
<a id="__codelineno-10-13" name="__codelineno-10-13" href="#upgrades-__codelineno-10-13"></a>metadata:
<a id="__codelineno-10-14" name="__codelineno-10-14" href="#upgrades-__codelineno-10-14"></a>  name: myapp
<a id="__codelineno-10-15" name="__codelineno-10-15" href="#upgrades-__codelineno-10-15"></a>spec:
<a id="__codelineno-10-16" name="__codelineno-10-16" href="#upgrades-__codelineno-10-16"></a>  replicas: 10
<a id="__codelineno-10-17" name="__codelineno-10-17" href="#upgrades-__codelineno-10-17"></a>  selector:
<a id="__codelineno-10-18" name="__codelineno-10-18" href="#upgrades-__codelineno-10-18"></a>    matchLabels:
<a id="__codelineno-10-19" name="__codelineno-10-19" href="#upgrades-__codelineno-10-19"></a>      app: myapp
<a id="__codelineno-10-20" name="__codelineno-10-20" href="#upgrades-__codelineno-10-20"></a>  template:
<a id="__codelineno-10-21" name="__codelineno-10-21" href="#upgrades-__codelineno-10-21"></a>    metadata:
<a id="__codelineno-10-22" name="__codelineno-10-22" href="#upgrades-__codelineno-10-22"></a>      labels:
<a id="__codelineno-10-23" name="__codelineno-10-23" href="#upgrades-__codelineno-10-23"></a>        app: myapp
<a id="__codelineno-10-24" name="__codelineno-10-24" href="#upgrades-__codelineno-10-24"></a>    spec:
<a id="__codelineno-10-25" name="__codelineno-10-25" href="#upgrades-__codelineno-10-25"></a>      containers:
<a id="__codelineno-10-26" name="__codelineno-10-26" href="#upgrades-__codelineno-10-26"></a>      - image: public.ecr.aws/eks-distro/kubernetes/pause:3.2
<a id="__codelineno-10-27" name="__codelineno-10-27" href="#upgrades-__codelineno-10-27"></a>        name: myapp
<a id="__codelineno-10-28" name="__codelineno-10-28" href="#upgrades-__codelineno-10-28"></a>        resources:
<a id="__codelineno-10-29" name="__codelineno-10-29" href="#upgrades-__codelineno-10-29"></a>          requests:
<a id="__codelineno-10-30" name="__codelineno-10-30" href="#upgrades-__codelineno-10-30"></a>            cpu: &quot;1&quot;
<a id="__codelineno-10-31" name="__codelineno-10-31" href="#upgrades-__codelineno-10-31"></a>            memory: 256M
<a id="__codelineno-10-32" name="__codelineno-10-32" href="#upgrades-__codelineno-10-32"></a>      topologySpreadConstraints:
<a id="__codelineno-10-33" name="__codelineno-10-33" href="#upgrades-__codelineno-10-33"></a>      - labelSelector:
<a id="__codelineno-10-34" name="__codelineno-10-34" href="#upgrades-__codelineno-10-34"></a>          matchLabels:
<a id="__codelineno-10-35" name="__codelineno-10-35" href="#upgrades-__codelineno-10-35"></a>            app: host-zone-spread
<a id="__codelineno-10-36" name="__codelineno-10-36" href="#upgrades-__codelineno-10-36"></a>        maxSkew: 2
<a id="__codelineno-10-37" name="__codelineno-10-37" href="#upgrades-__codelineno-10-37"></a>        topologyKey: kubernetes.io/hostname
<a id="__codelineno-10-38" name="__codelineno-10-38" href="#upgrades-__codelineno-10-38"></a>        whenUnsatisfiable: DoNotSchedule
<a id="__codelineno-10-39" name="__codelineno-10-39" href="#upgrades-__codelineno-10-39"></a>      - labelSelector:
<a id="__codelineno-10-40" name="__codelineno-10-40" href="#upgrades-__codelineno-10-40"></a>          matchLabels:
<a id="__codelineno-10-41" name="__codelineno-10-41" href="#upgrades-__codelineno-10-41"></a>            app: host-zone-spread
<a id="__codelineno-10-42" name="__codelineno-10-42" href="#upgrades-__codelineno-10-42"></a>        maxSkew: 2
<a id="__codelineno-10-43" name="__codelineno-10-43" href="#upgrades-__codelineno-10-43"></a>        topologyKey: topology.kubernetes.io/zone
<a id="__codelineno-10-44" name="__codelineno-10-44" href="#upgrades-__codelineno-10-44"></a>        whenUnsatisfiable: DoNotSchedule
</code></pre></div>
<p><a href="https://aws.amazon.com/resilience-hub/">AWS Resilience Hub</a> has added Amazon Elastic Kubernetes Service (Amazon EKS) as a supported resource. Resilience Hub provides a single place to define, validate, and track the resilience of your applications so that you can avoid unnecessary downtime caused by software, infrastructure, or operational disruptions.</p>
<h2 id="upgrades-use-managed-node-groups-or-karpenter-to-simplify-data-plane-upgrades">Use Managed Node Groups or Karpenter to simplify data plane upgrades<a class="headerlink" href="#upgrades-use-managed-node-groups-or-karpenter-to-simplify-data-plane-upgrades" title="Permanent link">&para;</a></h2>
<p>Managed Node Groups and Karpenter both simplify node upgrades, but they take different approaches.</p>
<p>Managed node groups automate the provisioning and lifecycle management of nodes. This means that you can create, automatically update, or terminate nodes with a single operation.</p>
<p>In the default configuration, Karpenter automatically creates new nodes using the latest compatible EKS Optimized AMI. As EKS releases updated EKS Optimized AMIs or the cluster is upgraded, Karpenter will automatically start using these images. <a href="#upgrades-enable-node-expiry-for-karpenter-managed-nodes">Karpenter also implements Node Expiry to update nodes.</a></p>
<p><a href="https://karpenter.sh/docs/concepts/node-templates/">Karpenter can be configured to use custom AMIs.</a> If you use custom AMIs with Karpenter, you are responsible for the version of kubelet. </p>
<h2 id="upgrades-confirm-version-compatibility-with-existing-nodes-and-the-control-plane">Confirm version compatibility with existing nodes and the control plane<a class="headerlink" href="#upgrades-confirm-version-compatibility-with-existing-nodes-and-the-control-plane" title="Permanent link">&para;</a></h2>
<p>Before proceeding with a Kubernetes upgrade in Amazon EKS, it's vital to ensure compatibility between your managed node groups, self-managed nodes, and the control plane. Compatibility is determined by the Kubernetes version you are using, and it varies based on different scenarios. Tactics:</p>
<ul>
<li><strong>Kubernetes v1.28+</strong> — **** Starting from Kubernetes version 1.28 and onwards, there's a more lenient version policy for core components. Specifically, the supported skew between the Kubernetes API server and the kubelet has been extended by one minor version, going from n-2 to n-3. For example, if your EKS control plane version is 1.28, you can safely use kubelet versions as old as 1.25. This version skew is supported across <a href="https://docs.aws.amazon.com/eks/latest/userguide/fargate.html">AWS Fargate</a>, <a href="https://docs.aws.amazon.com/eks/latest/userguide/managed-node-groups.html">managed node groups</a>, and <a href="https://docs.aws.amazon.com/eks/latest/userguide/worker.html">self-managed nodes</a>. We highly recommend keeping your <a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-amis.html">Amazon Machine Image (AMI)</a> versions up-to-date for security reasons. Older kubelet versions might pose security risks due to potential Common Vulnerabilities and Exposures (CVEs), which could outweigh the benefits of using older kubelet versions.</li>
<li><strong>Kubernetes &lt; v1.28</strong> — If you are using a version older than v1.28, the supported skew between the API server and the kubelet is n-2. For example, if your EKS version is 1.27, the oldest kubelet version you can use is 1.25. This version skew is applicable across <a href="https://docs.aws.amazon.com/eks/latest/userguide/fargate.html">AWS Fargate</a>, <a href="https://docs.aws.amazon.com/eks/latest/userguide/managed-node-groups.html">managed node groups</a>, and <a href="https://docs.aws.amazon.com/eks/latest/userguide/worker.html">self-managed nodes</a>.</li>
</ul>
<h2 id="upgrades-enable-node-expiry-for-karpenter-managed-nodes">Enable node expiry for Karpenter managed nodes<a class="headerlink" href="#upgrades-enable-node-expiry-for-karpenter-managed-nodes" title="Permanent link">&para;</a></h2>
<p>One way Karpenter implements node upgrades is using the concept of node expiry. This reduces the planning required for node upgrades. When you set a value for <strong>ttlSecondsUntilExpired </strong>in your provisioner, this activates node expiry. After nodes reach the defined age in seconds, they’re safely drained and deleted. This is true even if they’re in use, allowing you to replace nodes with newly provisioned upgraded instances. When a node is replaced, Karpenter uses the latest EKS-optimized AMIs. For more information, see <a href="https://karpenter.sh/docs/concepts/deprovisioning/#methods">Deprovisioning</a> on the Karpenter website.</p>
<p>Karpenter doesn’t automatically add jitter to this value. To prevent excessive workload disruption, define a <a href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/">pod disruption budget</a>, as shown in Kubernetes documentation.</p>
<p>If you configure <strong>ttlSecondsUntilExpired </strong>on a provisioner, this applies to existing nodes associated with the provisioner.</p>
<h2 id="upgrades-use-drift-feature-for-karpenter-managed-nodes">Use Drift feature for Karpenter managed nodes<a class="headerlink" href="#upgrades-use-drift-feature-for-karpenter-managed-nodes" title="Permanent link">&para;</a></h2>
<p><a href="https://karpenter.sh/docs/concepts/deprovisioning/#drift">Karpenter's Drift feature</a> can automatically upgrade the Karpenter-provisioned nodes to stay in-sync with the EKS control plane. Karpenter Drift currently needs to be enabled using a <a href="https://karpenter.sh/docs/concepts/settings/#feature-gates">feature gate</a>. Karpenter's default configuration uses the latest EKS-Optimized AMI for the same major and minor version as the EKS cluster's control plane.</p>
<p>After an EKS Cluster upgrade completes, Karpenter's Drift feature will detect that the Karpenter-provisioned nodes are using EKS-Optimized AMIs for the previous cluster version, and automatically cordon, drain, and replace those nodes. To support pods moving to new nodes, follow Kubernetes best practices by setting appropriate pod <a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/">resource quotas</a>, and using <a href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/">pod disruption budgets</a> (PDB). Karpenter's deprovisioning will pre-spin up replacement nodes based on the pod resource requests, and will respect the PDBs when deprovisioning nodes.</p>
<h2 id="upgrades-use-eksctl-to-automate-upgrades-for-self-managed-node-groups">Use eksctl to automate upgrades for self-managed node groups<a class="headerlink" href="#upgrades-use-eksctl-to-automate-upgrades-for-self-managed-node-groups" title="Permanent link">&para;</a></h2>
<p>Self managed node groups are EC2 instances that were deployed in your account and attached to the cluster outside of the EKS service. These are usually deployed and managed by some form of automation tooling. To upgrade self-managed node groups you should refer to your tools documentation.</p>
<p>For example, eksctl supports <a href="https://eksctl.io/usage/managing-nodegroups/#deleting-and-draining">deleting and draining self-managed nodes.</a> </p>
<p>Some common tools include:</p>
<ul>
<li><a href="https://eksctl.io/usage/nodegroup-upgrade/">eksctl</a></li>
<li><a href="https://kops.sigs.k8s.io/operations/updates_and_upgrades/">kOps</a></li>
<li><a href="https://aws-ia.github.io/terraform-aws-eks-blueprints/node-groups/#self-managed-node-groups">EKS Blueprints</a></li>
</ul>
<h2 id="upgrades-backup-the-cluster-before-upgrading">Backup the cluster before upgrading<a class="headerlink" href="#upgrades-backup-the-cluster-before-upgrading" title="Permanent link">&para;</a></h2>
<p>New versions of Kubernetes introduce significant changes to your Amazon EKS cluster. After you upgrade a cluster, you can’t downgrade it.</p>
<p><a href="https://velero.io/">Velero</a> is an community supported open-source tool that can be used to take backups of existing clusters and apply the backups to a new cluster.</p>
<p>Note that you can only create new clusters for Kubernetes versions currently supported by EKS. If the version your cluster is currently running is still supported and an upgrade fails, you can create a new cluster with the original version and restore the data plane. Note that AWS resources, including IAM, are not included in the backup by Velero. These resources would need to be recreated. </p>
<h2 id="upgrades-restart-fargate-deployments-after-upgrading-the-control-plane">Restart Fargate deployments after upgrading the control plane<a class="headerlink" href="#upgrades-restart-fargate-deployments-after-upgrading-the-control-plane" title="Permanent link">&para;</a></h2>
<p>To upgrade Fargate data plane nodes you need to redeploy the workloads. You can identify which workloads are running on fargate nodes by listing all pods with the <code>-o wide</code> option. Any node name that begins with <code>fargate-</code> will need to be redeployed in the cluster.</p>
<h2 id="upgrades-evaluate-bluegreen-clusters-as-an-alternative-to-in-place-cluster-upgrades">Evaluate Blue/Green Clusters as an alternative to in-place cluster upgrades<a class="headerlink" href="#upgrades-evaluate-bluegreen-clusters-as-an-alternative-to-in-place-cluster-upgrades" title="Permanent link">&para;</a></h2>
<p>Some customers prefer to do a blue/green upgrade strategy. This can have benefits, but also includes downsides that should be considered.</p>
<p>Benefits include:</p>
<ul>
<li>Possible to change multiple EKS versions at once (e.g. 1.23 to 1.25)</li>
<li>Able to switch back to the old cluster</li>
<li>Creates a new cluster which may be managed with newer systems (e.g. terraform)</li>
<li>Workloads can be migrated individually</li>
</ul>
<p>Some downsides include:</p>
<ul>
<li>API endpoint and OIDC change which requires updating consumers (e.g. kubectl and CI/CD)</li>
<li>Requires 2 clusters to be run in parallel during the migration, which can be expensive and limit region capacity</li>
<li>More coordination is needed if workloads depend on each other to be migrated together</li>
<li>Load balancers and external DNS cannot easily span multiple clusters</li>
</ul>
<p>While this strategy is possible to do, it is more expensive than an in-place upgrade and requires more time for coordination and workload migrations. It may be required in some situations and should be planned carefully.</p>
<p>With high degrees of automation and declarative systems like GitOps, this may be easier to do. You will need to take additional precautions for stateful workloads so data is backed up and migrated to new clusters.</p>
<p>Review these blogs posts for more information:</p>
<ul>
<li><a href="https://aws.amazon.com/blogs/containers/kubernetes-cluster-upgrade-the-blue-green-deployment-strategy/">Kubernetes cluster upgrade: the blue-green deployment strategy</a></li>
<li><a href="https://aws.amazon.com/blogs/containers/blue-green-or-canary-amazon-eks-clusters-migration-for-stateless-argocd-workloads/">Blue/Green or Canary Amazon EKS clusters migration for stateless ArgoCD workloads</a></li>
</ul>
<h2 id="upgrades-track-planned-major-changes-in-the-kubernetes-project-think-ahead">Track planned major changes in the Kubernetes project — Think ahead<a class="headerlink" href="#upgrades-track-planned-major-changes-in-the-kubernetes-project-think-ahead" title="Permanent link">&para;</a></h2>
<p>Don’t look only at the next version. Review new versions of Kubernetes as they are released, and identify major changes. For example, some applications directly used the docker API, and support for Container Runtime Interface (CRI) for Docker (also known as Dockershim) was removed in Kubernetes <code>1.24</code>. This kind of change requires more time to prepare for. </p>
<p>Review all documented changes for the version that you’re upgrading to, and note any required upgrade steps. Also, note any requirements or procedures that are specific to Amazon EKS managed clusters.</p>
<ul>
<li><a href="https://github.com/kubernetes/kubernetes/tree/master/CHANGELOG">Kubernetes changelog</a></li>
</ul>
<h2 id="upgrades-specific-guidance-on-feature-removals">Specific Guidance on Feature Removals<a class="headerlink" href="#upgrades-specific-guidance-on-feature-removals" title="Permanent link">&para;</a></h2>
<h3 id="upgrades-removal-of-dockershim-in-125-use-detector-for-docker-socket-dds">Removal of Dockershim in 1.25 - Use Detector for Docker Socket (DDS)<a class="headerlink" href="#upgrades-removal-of-dockershim-in-125-use-detector-for-docker-socket-dds" title="Permanent link">&para;</a></h3>
<p>The EKS Optimized AMI for 1.25 no longer includes support for Dockershim. If you have a dependency on Dockershim, e.g. you are mounting the Docker socket, you will need to remove those dependencies before upgrading your worker nodes to 1.25. </p>
<p>Find instances where you have a dependency on the Docker socket before upgrading to 1.25. We recommend using <a href="https://github.com/aws-containers/kubectl-detector-for-docker-socket">Detector for Docker Socket (DDS), a kubectl plugin.</a>. </p>
<h3 id="upgrades-removal-of-podsecuritypolicy-in-125-migrate-to-pod-security-standards-or-a-policy-as-code-solution">Removal of PodSecurityPolicy in 1.25 - Migrate to Pod Security Standards or a policy-as-code solution<a class="headerlink" href="#upgrades-removal-of-podsecuritypolicy-in-125-migrate-to-pod-security-standards-or-a-policy-as-code-solution" title="Permanent link">&para;</a></h3>
<p><code>PodSecurityPolicy</code> was <a href="https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/">deprecated in Kubernetes 1.21</a>, and has been removed in Kubernetes 1.25. If you are using PodSecurityPolicy in your cluster, then you must migrate to the built-in Kubernetes Pod Security Standards (PSS) or to a policy-as-code solution before upgrading your cluster to version 1.25 to avoid interruptions to your workloads. </p>
<p>AWS published a <a href="https://docs.aws.amazon.com/eks/latest/userguide/pod-security-policy-removal-faq.html">detailed FAQ in the EKS documentation.</a></p>
<p>Review the <a href="https://aws.github.io/aws-eks-best-practices/security/docs/pods/#pod-security-standards-pss-and-pod-security-admission-psa">Pod Security Standards (PSS) and Pod Security Admission (PSA)</a> best practices. </p>
<p>Review the <a href="https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/">PodSecurityPolicy Deprecation blog post</a> on the Kubernetes website.</p>
<h3 id="upgrades-deprecation-of-in-tree-storage-driver-in-123-migrate-to-container-storage-interface-csi-drivers">Deprecation of In-Tree Storage Driver in 1.23 - Migrate to Container Storage Interface (CSI) Drivers<a class="headerlink" href="#upgrades-deprecation-of-in-tree-storage-driver-in-123-migrate-to-container-storage-interface-csi-drivers" title="Permanent link">&para;</a></h3>
<p>The Container Storage Interface (CSI) was designed to help Kubernetes replace its existing, in-tree storage driver mechanisms. The Amazon EBS container storage interface (CSI) migration feature is enabled by default in Amazon EKS <code>1.23</code> and later clusters. If you have pods running on a version <code>1.22</code> or earlier cluster, then you must install the <a href="https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html">Amazon EBS CSI driver</a> before updating your cluster to version <code>1.23</code> to avoid service interruption. </p>
<p>Review the <a href="https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi-migration-faq.html">Amazon EBS CSI migration frequently asked questions</a>.</p>
<h2 id="upgrades-additional-resources">Additional Resources<a class="headerlink" href="#upgrades-additional-resources" title="Permanent link">&para;</a></h2>
<h3 id="upgrades-clowdhaus-eks-upgrade-guidance">ClowdHaus EKS Upgrade Guidance<a class="headerlink" href="#upgrades-clowdhaus-eks-upgrade-guidance" title="Permanent link">&para;</a></h3>
<p><a href="https://clowdhaus.github.io/eksup/">ClowdHaus EKS Upgrade Guidance</a> is a CLI to aid in upgrading Amazon EKS clusters. It can analyze a cluster for any potential issues to remediate prior to upgrade. </p>
<h3 id="upgrades-gonogo">GoNoGo<a class="headerlink" href="#upgrades-gonogo" title="Permanent link">&para;</a></h3>
<p><a href="https://github.com/FairwindsOps/GoNoGo">GoNoGo</a> is an alpha-stage tool to determine the upgrade confidence of your cluster add-ons. </p></section>
                        <h1 class='nav-section-title' id='section-cost-optimization'>
                            Cost Optimization <a class='headerlink' href='#section-cost-optimization' title='Permanent link'>↵</a>
                        </h1>
                        <section class="print-page" id="cost_optimization-cfm_framework"><h1 id="cost_optimization-cfm_framework-cost-optimization-introduction">Cost Optimization - Introduction<a class="headerlink" href="#cost_optimization-cfm_framework-cost-optimization-introduction" title="Permanent link">&para;</a></h1>
<p>AWS Cloud Economics is a discipline that helps customers increase efficiency and reduce their costs through the adoption of modern compute technologies like Amazon EKS. The discipline recommends following a methodology called the “Cloud Financial Management (CFM) framework” which consists of 4 pillars: </p>
<p><img alt="CFM Framework" src="../images/cfm_framework.png" /></p>
<h2 id="cost_optimization-cfm_framework-the-see-pillar-measurement-and-accountability">The See pillar: Measurement and accountability<a class="headerlink" href="#cost_optimization-cfm_framework-the-see-pillar-measurement-and-accountability" title="Permanent link">&para;</a></h2>
<p>The See pillar is a foundational set of activities and technologies that define how to measure, monitor and create accountability for cloud spend. It is often referred to as “Observability”, “Instrumentation”, or “Telemetry”. The capabilities and limitations of the “Observability” infrastructure dictate what can be optimized. Obtaining a clear picture of your costs is a critical first step in cost optimization as you need to know where you are starting from. This type of visibility will also guide the types of activities you will need to do to further optimize your environment.  </p>
<p>Here is a brief overview of our best practices for the See pillar:</p>
<ul>
<li>Define and maintain a tagging strategy for your workloads. <ul>
<li>Use <a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-using-tags.html#tag-resources-for-billing">Instance Tagging</a>, tagging EKS clusters allows you to see individual cluster costs and allocate them in your Cost &amp; Usage Reports. </li>
</ul>
</li>
<li>Establish reporting and monitoring of EKS usage by using technologies like <a href="https://docs.kubecost.com/install-and-configure/install/provider-installations/aws-eks-cost-monitoring">Kubecost</a>. <ul>
<li><a href="https://wellarchitectedlabs.com/cost/200_labs/200_enterprise_dashboards/">Enable Cloud Intelligence Dashboards</a>, by having resources properly tagged and using visualizations, you can measure and estimate costs.</li>
</ul>
</li>
<li>Allocate cloud costs to applications, Lines of Business (LoBs), and revenue streams.</li>
<li>Define, measure, and circulate efficiency/value KPIs with business stakeholders. For example, create a “unit metric” KPI that measures the cost per transaction, e.g. a ride sharing services might have a KPI for “cost per ride”.  </li>
</ul>
<p>For more details on the recommended technologies and activities associated with this pillar, please see the <a href="#cost_optimization-cost_opt_observability">Cost Optimization - Observability</a> section of this guide. </p>
<h2 id="cost_optimization-cfm_framework-the-save-pillar-cost-optimization">The Save pillar: Cost optimization<a class="headerlink" href="#cost_optimization-cfm_framework-the-save-pillar-cost-optimization" title="Permanent link">&para;</a></h2>
<p>This pillar is based on the technologies and capabilities developed in the “See” pillar. The following activities typically fall under this pillar: </p>
<ul>
<li>Identify and eliminate waste in your environment. </li>
<li>Architect and design for cost efficiency.</li>
<li>Choose the best purchasing option, e.g. on-demand instances vs Spot instances.</li>
<li>Adapt as services evolve: as AWS services evolve, the way to efficiently use those services may change. Be willing to adapt to account for these changes. </li>
</ul>
<p>Since these activities are operational, they are highly dependent on your environment’s characteristics. Ask yourself, what are the main drivers of costs? What business value do your different environments provide? What purchasing options and infrastructure choices, e.g. instance family types, are best suited for each environment?  </p>
<p>Below is a prioritized list of the most common cost drivers for EKS clusters:</p>
<ol>
<li><strong>Compute costs:</strong> Combining multiple types of instance families, purchasing options, and balancing scalability with availability require careful consideration. For further information, see the recommendations in the <a href="#cost_optimization-cost_opt_compute">Cost Optimization - Compute</a> section of this guide. </li>
<li><strong>Networking costs:</strong> using 3 AZs for EKS clusters can potentially increase inter-AZ traffic costs. For our recommendations on how to balance HA requirements with keeping network traffic costs down, please consult the <a href="#cost_optimization-cost_opt_networking">Cost Optimization - Networking</a> section of this guide. </li>
<li><strong>Storage costs:</strong> Depending on the stateful/stateless nature of the workloads in the EKS clusters, and how the different storage types are used, storage can be considered as part of the workload. For considerations relating to EKS storage costs, please consult the <a href="#cost_optimization-cost_opt_storage">Cost Optimization - Storage</a> section of this guide.</li>
</ol>
<h2 id="cost_optimization-cfm_framework-the-plan-pillar-planning-and-forecasting">The Plan pillar:  Planning and forecasting<a class="headerlink" href="#cost_optimization-cfm_framework-the-plan-pillar-planning-and-forecasting" title="Permanent link">&para;</a></h2>
<p>Once the recommendations in the See pillar are implemented, clusters are optimized on an on-going basis. As experience is gained in operating clusters efficiently, planning and forecasting activities can focus on:</p>
<ul>
<li>Budgeting and forecasting cloud costs dynamically. </li>
<li>Quantifying the business value delivered by EKS container services.</li>
<li>Integrating EKS cluster cost management with IT financial management planning. </li>
</ul>
<h2 id="cost_optimization-cfm_framework-the-run-pillar">The Run pillar<a class="headerlink" href="#cost_optimization-cfm_framework-the-run-pillar" title="Permanent link">&para;</a></h2>
<p>Cost optimization is a continuous process and involves a flywheel of incremental improvements: </p>
<p><img alt="Cost optimization flywheel" src="../images/flywheel.png" /></p>
<p>Securing executive sponsorship for these types of activities is crucial for integrating EKS cluster optimization into the organization’s “FinOps” efforts. It allows stakeholder alignment through a shared understanding of EKS cluster costs, implementation of EKS cluster cost guardrails, and ensuring that the tooling, automation, and activities evolve with the organization’s needs. </p>
<h2 id="cost_optimization-cfm_framework-references">References<a class="headerlink" href="#cost_optimization-cfm_framework-references" title="Permanent link">&para;</a></h2>
<ul>
<li><a href="https://aws.amazon.com/aws-cost-management/">AWS Cloud Economics, Cloud Financial Management</a></li>
</ul></section><section class="print-page" id="cost_optimization-cost_opt_compute"><h1 id="cost_optimization-cost_opt_compute-cost-optimization-compute-and-autoscaling">Cost Optimization - Compute and Autoscaling<a class="headerlink" href="#cost_optimization-cost_opt_compute-cost-optimization-compute-and-autoscaling" title="Permanent link">&para;</a></h1>
<p>As a developer, you'll make estimates about your application’s resource requirements, e.g. CPU and memory, but if you’re not continually adjusting them they may become outdated which could increase your costs and worsen performance and reliability. Continually adjusting an application's resource requirements is more important than getting them right the first time.</p>
<p>The best practices mentioned below will help you build and operate cost-aware workloads that achieve business outcomes while minimizing costs and allowing your organization to maximize its return on investment. A high level order of importance for optimizing your cluster compute costs are:</p>
<ol>
<li>Right-size workloads</li>
<li>Reduce unused capacity</li>
<li>Optimize compute capacity types (e.g. Spot) and accelerators (e.g. GPUs)</li>
</ol>
<h2 id="cost_optimization-cost_opt_compute-right-size-your-workloads">Right-size your workloads<a class="headerlink" href="#cost_optimization-cost_opt_compute-right-size-your-workloads" title="Permanent link">&para;</a></h2>
<p>In most EKS clusters, the bulk of cost come from the EC2 instances that are used to run your containerized workloads. You will not be able to right-size your compute resources without understanding your workloads requirements. This is why it is essential that you use the appropriate requests and limits and make adjustments to those settings as necessary. In addition, dependencies, such as instance size and storage selection, may effect workload performance which can have a variety of unintended consequences on costs and reliability.</p>
<p><em>Requests</em> should align with the actual utilization. If a container's requests are too high there will be unused capacity which is a large factor in total cluster costs. Each container in a pod, e.g. application and sidecars, should have their own requests and limits set to make sure the aggregate pod limits are as accurate as possible.</p>
<p>Utilize tools such as <a href="https://www.youtube.com/watch?v=DfmQWYiwFDk">Goldilocks</a>, <a href="https://www.youtube.com/watch?v=uITOzpf82RY">KRR</a>, and <a href="https://aws.amazon.com/blogs/containers/aws-and-kubecost-collaborate-to-deliver-cost-monitoring-for-eks-customers/">Kubecost</a> which estimate resource requests and limits for your containers. Depending on the nature of the applications, performance/cost requirements, and complexity you need to evaluate which metrics are best to scale on, at what point your application performance degrades (saturation point), and how to tweak request and limits accordingly. Please refer to <a href="https://aws.github.io/aws-eks-best-practices/scalability/docs/node_efficiency/#application-right-sizing">Application right sizing</a> for further guidance on this topic.</p>
<p>We recommend using the Horizontal Pod Autoscaler (HPA) to control how many replicas of your application should be running, the Vertical Pod Autoscaler (VPA) to adjust how many requests and limits your application needs per replica, and a node autoscaler like <a href="http://karpenter.sh/">Karpenter</a> or <a href="https://github.com/kubernetes/autoscaler">Cluster Autoscaler</a> to continually adjust the total number of nodes in your cluster. Cost optimization techniques using Karpenter and Cluster Autoscaler are documented in a later section of this document.</p>
<p>The Vertical Pod Autoscaler can adjust the requests and limits assigned to containers so workloads run optimally. You should run the VPA in auditing mode so it does not automatically make changes and restart your pods. It will suggest changes based on observed metrics. With any changes that affect production workloads you should review and test those changes first in a non-production environment because these can have impact on your application’s reliability and performance.</p>
<h2 id="cost_optimization-cost_opt_compute-reduce-consumption">Reduce consumption<a class="headerlink" href="#cost_optimization-cost_opt_compute-reduce-consumption" title="Permanent link">&para;</a></h2>
<p>The best way to save money is to provision fewer resources. One way to do that is to adjust workloads based on their current requirements. You should start any cost optimization efforts with making sure your workloads define their requirements and scale dynamically. This will require getting metrics from your applications and setting configurations such as <a href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/"><code>PodDisruptionBudgets</code></a> and <a href="https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.5/deploy/pod_readiness_gate/">Pod Readiness Gates</a> to make sure your application can safely scale up and down dynamically.</p>
<p>The Horizontal Pod Autoscaler is a flexible workload autoscaler that can adjust how many replicas are needed to meet the performance and reliability requirements of your application. It has a flexible model for defining when to scale up and down based on various metrics such as CPU, memory, or custom metrics e.g. queue depth, number of connections to a pod, etc.</p>
<p>The Kubernetes Metrics Server enables scaling in response to built-in metrics like CPU and memory usage, but if you want to scale based on other metrics, such as Amazon CloudWatch or SQS queue depth, you should consider event driven autoscaling projects such as <a href="https://keda.sh/">KEDA</a>. Please refer to <a href="https://aws.amazon.com/blogs/mt/proactive-autoscaling-of-kubernetes-workloads-with-keda-using-metrics-ingested-into-amazon-cloudwatch/">this blog post</a> on how to use KEDA with CloudWatch metrics. If you are unsure, which metrics to monitor and scale based on, check out the <a href="https://aws-observability.github.io/observability-best-practices/guides/#monitor-what-matters">best practices on monitoring metrics that matters</a>.</p>
<p>Reducing workload consumption creates excess capacity in a cluster and with proper autoscaling configuration allows you to scale down nodes automatically and reduce your total spend. We recommend you do not try to optimize compute capacity manually. The Kubernetes scheduler and node autoscalers were designed to handle this process for you.</p>
<h2 id="cost_optimization-cost_opt_compute-reduce-unused-capacity">Reduce unused capacity<a class="headerlink" href="#cost_optimization-cost_opt_compute-reduce-unused-capacity" title="Permanent link">&para;</a></h2>
<p>After you have determined the correct size for applications, reducing excess requests, you can begin to reduce the provisioned compute capacity. You should be able to do this dynamically if you have taken the time to correctly size your workloads from the sections above. There are two primary node autoscalers used with Kubernetes in AWS.</p>
<h3 id="cost_optimization-cost_opt_compute-karpenter-and-cluster-autoscaler">Karpenter and Cluster Autoscaler<a class="headerlink" href="#cost_optimization-cost_opt_compute-karpenter-and-cluster-autoscaler" title="Permanent link">&para;</a></h3>
<p>Both Karpenter and the Kubernetes Cluster Autoscaler will scale the number of nodes in your cluster as pods are created or removed and compute requirements change. The primary goal of both is the same, but Karpenter takes a different approach for node management provisioning and de-provisioning which can help reduce costs and optimize cluster wide usage.</p>
<p>As clusters grow in size and the variety of workloads increases it becomes more difficult to pre-configure node groups and instances. Just like with workload requests it’s important to set an initial baseline and continually adjust as needed.</p>
<h3 id="cost_optimization-cost_opt_compute-cluster-autoscaler-priority-expander">Cluster Autoscaler Priority Expander<a class="headerlink" href="#cost_optimization-cost_opt_compute-cluster-autoscaler-priority-expander" title="Permanent link">&para;</a></h3>
<p>The Kubernetes Cluster Autoscaler works by scaling groups of nodes — called a node group — up and down as applications scale up and down. If you are not dynamically scaling workloads then the Cluster Autoscaler will not help you save money. The Cluster Autoscaler requires a cluster admin to create node groups ahead of time for workloads to consume. The node groups need to configured to use instances that have the same "profile", i.e. roughly the same amount of CPU and memory.</p>
<p>You can have multiple node groups and the Cluster Autoscaler can be configured to set priority scaling levels and each node group can contain different sized nodes. Node groups can have different capacity types and the priority expander can be used to scale less expensive groups first.</p>
<p>Below is an example of a snippet of cluster configuration that uses a `ConfigMap`` to prioritize reserved capacity before using on-demand instances.  You can use the same technique to prioritize Graviton or Spot Instances over other types.  </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#cost_optimization-cost_opt_compute-__codelineno-0-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">eksctl.io/v1alpha5</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#cost_optimization-cost_opt_compute-__codelineno-0-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ClusterConfig</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#cost_optimization-cost_opt_compute-__codelineno-0-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#cost_optimization-cost_opt_compute-__codelineno-0-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">my-cluster</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#cost_optimization-cost_opt_compute-__codelineno-0-5"></a><span class="nt">managedNodeGroups</span><span class="p">:</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#cost_optimization-cost_opt_compute-__codelineno-0-6"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">managed-ondemand</span>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#cost_optimization-cost_opt_compute-__codelineno-0-7"></a><span class="w">    </span><span class="nt">minSize</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#cost_optimization-cost_opt_compute-__codelineno-0-8"></a><span class="w">    </span><span class="nt">maxSize</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">7</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#cost_optimization-cost_opt_compute-__codelineno-0-9"></a><span class="w">    </span><span class="nt">instanceType</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">m5.xlarge</span>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#cost_optimization-cost_opt_compute-__codelineno-0-10"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">managed-reserved</span>
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#cost_optimization-cost_opt_compute-__codelineno-0-11"></a><span class="w">    </span><span class="nt">minSize</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span>
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#cost_optimization-cost_opt_compute-__codelineno-0-12"></a><span class="w">    </span><span class="nt">maxSize</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10</span>
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#cost_optimization-cost_opt_compute-__codelineno-0-13"></a><span class="w">    </span><span class="nt">instanceType</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">c5.2xlarge</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#cost_optimization-cost_opt_compute-__codelineno-1-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#cost_optimization-cost_opt_compute-__codelineno-1-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ConfigMap</span>
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#cost_optimization-cost_opt_compute-__codelineno-1-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#cost_optimization-cost_opt_compute-__codelineno-1-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">cluster-autoscaler-priority-expander</span>
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#cost_optimization-cost_opt_compute-__codelineno-1-5"></a><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">kube-system</span>
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#cost_optimization-cost_opt_compute-__codelineno-1-6"></a><span class="nt">data</span><span class="p">:</span>
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#cost_optimization-cost_opt_compute-__codelineno-1-7"></a><span class="w">  </span><span class="nt">priorities</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|-</span>
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#cost_optimization-cost_opt_compute-__codelineno-1-8"></a><span class="w">    </span><span class="no">10:</span>
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#cost_optimization-cost_opt_compute-__codelineno-1-9"></a><span class="w">      </span><span class="no">- .*ondemand.*</span>
<a id="__codelineno-1-10" name="__codelineno-1-10" href="#cost_optimization-cost_opt_compute-__codelineno-1-10"></a><span class="w">    </span><span class="no">50:</span>
<a id="__codelineno-1-11" name="__codelineno-1-11" href="#cost_optimization-cost_opt_compute-__codelineno-1-11"></a><span class="w">      </span><span class="no">- .*reserved.*</span>
</code></pre></div>
<p>Using node groups can help the underlying compute resources do the expected thing by default, e.g. spread nodes across AZs, but not all workloads have the same requirements or expectations and it’s better to let applications declare their requirements explicitly. For more information about Cluster Autoscaler, please see the <a href="https://aws.github.io/aws-eks-best-practices/cluster-autoscaling/">best practices section</a>.</p>
<h3 id="cost_optimization-cost_opt_compute-descheduler">Descheduler<a class="headerlink" href="#cost_optimization-cost_opt_compute-descheduler" title="Permanent link">&para;</a></h3>
<p>The Cluster Autoscaler can add and remove node capacity from a cluster based on new pods needing to be scheduled or nodes being underutilized. It does not take a wholistic view of pod placement after it has been scheduled to a node. If you are using the Cluster Autoscaler you should also look at the <a href="https://github.com/kubernetes-sigs/descheduler">Kubernetes descheduler</a> to avoid wasting capacity in your cluster.</p>
<p>If you have 10 nodes in a cluster and each node is 60% utilized you are not using 40% of the provisioned capacity in the cluster. With the Cluster Autoscaler you can set the utilization threashold per node to 60%, but that would only try to scale down a single node after utilization dropped below 60%.</p>
<p>With the descheduler it can look at cluster capacity and utilization after pods have been scheduled or nodes have been added to the cluster. It attempts to keep the total capacity of the cluster above a specified threshold. It can also remove pods based on node taints or new nodes that join the cluster to make sure pods are running in their optimal compute environment. Note that, descheduler does not schedule replacement of evicted pods but relies on the default scheduler for that.</p>
<h3 id="cost_optimization-cost_opt_compute-karpenter-consolidation">Karpenter Consolidation<a class="headerlink" href="#cost_optimization-cost_opt_compute-karpenter-consolidation" title="Permanent link">&para;</a></h3>
<p>Karpenter takes a “groupless” approach to node management. This approach is more flexible for different workload types and requires less up front configuration for cluster administrators. Instead of pre-defining groups and scaling each group as workloads need, Karpenter uses provisioners and node templates to define broadly what type of EC2 instances can be created and settings about the instances as they are created.</p>
<p>Bin packing is the practice of utilizing more of the instance’s resources by packing more workloads onto fewer, optimally sized, instances. While this helps to reduce your compute costs by only provisioning resources your workloads use, it has a trade-off. It can take longer to start new workloads because capacity has to be added to the cluster, especially during large scaling events. Consider the balance between cost optimization, performance, and availability when setting up bin packing. </p>
<p>Karpenter can continuously monitor and binpack to improve instance resource utilization and lower your compute costs. Karpenter can also select a more cost efficient worker node for your workload. This can be achieved by turning on “consolidation” flag to true in the provisioner (sample code snippet below).  The example below shows an example provisioner that enables consolidation. At the time of writing this guide, Karpenter won’t replace a running Spot instance with a cheaper Spot instance. For further details on Karpenter consolidation, refer to <a href="https://aws.amazon.com/blogs/containers/optimizing-your-kubernetes-compute-costs-with-karpenter-consolidation/">this blog</a>.  </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#cost_optimization-cost_opt_compute-__codelineno-2-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">karpenter.sh/v1alpha5</span>
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#cost_optimization-cost_opt_compute-__codelineno-2-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Provisioner</span>
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#cost_optimization-cost_opt_compute-__codelineno-2-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#cost_optimization-cost_opt_compute-__codelineno-2-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">enable-binpacking</span>
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#cost_optimization-cost_opt_compute-__codelineno-2-5"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#cost_optimization-cost_opt_compute-__codelineno-2-6"></a><span class="w">  </span><span class="nt">consolidation</span><span class="p">:</span>
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#cost_optimization-cost_opt_compute-__codelineno-2-7"></a><span class="w">    </span><span class="nt">enabled</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</code></pre></div>
<p>For workloads that might not be interruptible e.g. long running batch jobs without checkpointing, consider annotating pods with the <code>do-not-evict</code> annotation. By opting pods out of eviction, you are telling Karpenter that it should not voluntarily remove nodes containing this pod. However, if a <code>do-not-evict</code> pod is added to a node while the node is draining, the remaining pods will still evict, but that pod will block termination until it is removed. In either case, the node will be cordoned to prevent additional work from being scheduled on the node. Below is an example showing how set the annotation:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#cost_optimization-cost_opt_compute-__codelineno-3-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#cost_optimization-cost_opt_compute-__codelineno-3-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Pod</span>
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#cost_optimization-cost_opt_compute-__codelineno-3-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-3-4" name="__codelineno-3-4" href="#cost_optimization-cost_opt_compute-__codelineno-3-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">label-demo</span>
<a id="__codelineno-3-5" name="__codelineno-3-5" href="#cost_optimization-cost_opt_compute-__codelineno-3-5"></a><span class="w">  </span><span class="nt">labels</span><span class="p">:</span>
<a id="__codelineno-3-6" name="__codelineno-3-6" href="#cost_optimization-cost_opt_compute-__codelineno-3-6"></a><span class="w">    </span><span class="nt">environment</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">production</span>
<a id="__codelineno-3-7" name="__codelineno-3-7" href="#cost_optimization-cost_opt_compute-__codelineno-3-7"></a><span class="w">  </span><span class="nt">annotations</span><span class="p">:</span><span class="w">  </span>
<a id="__codelineno-3-8" name="__codelineno-3-8" href="#cost_optimization-cost_opt_compute-__codelineno-3-8"></a><span class="hll"><span class="w">    </span><span class="s">&quot;karpenter.sh/do-not-evict&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="s">&quot;true&quot;</span>
</span><a id="__codelineno-3-9" name="__codelineno-3-9" href="#cost_optimization-cost_opt_compute-__codelineno-3-9"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-3-10" name="__codelineno-3-10" href="#cost_optimization-cost_opt_compute-__codelineno-3-10"></a><span class="w">  </span><span class="nt">containers</span><span class="p">:</span>
<a id="__codelineno-3-11" name="__codelineno-3-11" href="#cost_optimization-cost_opt_compute-__codelineno-3-11"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nginx</span>
<a id="__codelineno-3-12" name="__codelineno-3-12" href="#cost_optimization-cost_opt_compute-__codelineno-3-12"></a><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nginx</span>
<a id="__codelineno-3-13" name="__codelineno-3-13" href="#cost_optimization-cost_opt_compute-__codelineno-3-13"></a><span class="w">    </span><span class="nt">ports</span><span class="p">:</span>
<a id="__codelineno-3-14" name="__codelineno-3-14" href="#cost_optimization-cost_opt_compute-__codelineno-3-14"></a><span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">80</span>
</code></pre></div>
<h3 id="cost_optimization-cost_opt_compute-remove-under-utilized-nodes-by-adjusting-cluster-autoscaler-parameters">Remove under-utilized nodes by adjusting Cluster Autoscaler parameters<a class="headerlink" href="#cost_optimization-cost_opt_compute-remove-under-utilized-nodes-by-adjusting-cluster-autoscaler-parameters" title="Permanent link">&para;</a></h3>
<p>Node utilization is defined as the sum of requested resources divided by capacity. By default <code>scale-down-utilization-threshold</code> is set to 50%. This parameter can be used along with and <code>scale-down-unneeded-time</code>, which determines how long a node should be unneeded before it is eligible for scale down — the default is 10 minutes. Pods still running on a node that was scaled down will get scheduled on other nodes by kube-scheduler.  Adjusting these settings can help remove nodes that are underutilized, but it’s important you test these values first so you don’t force the cluster to scale down prematurely.</p>
<p>You can prevent scale down from happening by ensuring that pods that are expensive to evict are protected by a label recognized by the Cluster Autoscaler. To do this, ensure that pods that are expensive to evict have the annotation <code>cluster-autoscaler.kubernetes.io/safe-to-evict=false</code>. Below is an example yaml to set the annotation:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#cost_optimization-cost_opt_compute-__codelineno-4-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<a id="__codelineno-4-2" name="__codelineno-4-2" href="#cost_optimization-cost_opt_compute-__codelineno-4-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Pod</span>
<a id="__codelineno-4-3" name="__codelineno-4-3" href="#cost_optimization-cost_opt_compute-__codelineno-4-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-4-4" name="__codelineno-4-4" href="#cost_optimization-cost_opt_compute-__codelineno-4-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">label-demo</span>
<a id="__codelineno-4-5" name="__codelineno-4-5" href="#cost_optimization-cost_opt_compute-__codelineno-4-5"></a><span class="w">  </span><span class="nt">labels</span><span class="p">:</span>
<a id="__codelineno-4-6" name="__codelineno-4-6" href="#cost_optimization-cost_opt_compute-__codelineno-4-6"></a><span class="w">    </span><span class="nt">environment</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">production</span>
<a id="__codelineno-4-7" name="__codelineno-4-7" href="#cost_optimization-cost_opt_compute-__codelineno-4-7"></a><span class="w">  </span><span class="nt">annotations</span><span class="p">:</span><span class="w">  </span>
<a id="__codelineno-4-8" name="__codelineno-4-8" href="#cost_optimization-cost_opt_compute-__codelineno-4-8"></a><span class="hll"><span class="w">    </span><span class="s">&quot;cluster-autoscaler.kubernetes.io/safe-to-evict&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="s">&quot;false&quot;</span>
</span><a id="__codelineno-4-9" name="__codelineno-4-9" href="#cost_optimization-cost_opt_compute-__codelineno-4-9"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-4-10" name="__codelineno-4-10" href="#cost_optimization-cost_opt_compute-__codelineno-4-10"></a><span class="w">  </span><span class="nt">containers</span><span class="p">:</span>
<a id="__codelineno-4-11" name="__codelineno-4-11" href="#cost_optimization-cost_opt_compute-__codelineno-4-11"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nginx</span>
<a id="__codelineno-4-12" name="__codelineno-4-12" href="#cost_optimization-cost_opt_compute-__codelineno-4-12"></a><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nginx</span>
<a id="__codelineno-4-13" name="__codelineno-4-13" href="#cost_optimization-cost_opt_compute-__codelineno-4-13"></a><span class="w">    </span><span class="nt">ports</span><span class="p">:</span>
<a id="__codelineno-4-14" name="__codelineno-4-14" href="#cost_optimization-cost_opt_compute-__codelineno-4-14"></a><span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">80</span>
</code></pre></div>
<h3 id="cost_optimization-cost_opt_compute-tag-nodes-with-cluster-autoscaler-and-karpenter">Tag nodes with Cluster Autoscaler and Karpenter<a class="headerlink" href="#cost_optimization-cost_opt_compute-tag-nodes-with-cluster-autoscaler-and-karpenter" title="Permanent link">&para;</a></h3>
<p>AWS resource <a href="https://docs.aws.amazon.com/tag-editor/latest/userguide/tagging.html">tags</a> are used to organize your resources, and to track your AWS costs on a detailed level. They do not directly correlate with Kubernetes labels for cost tracking. It’s recommended to start with Kubernetes resource labeling and utilize tools like <a href="https://aws.amazon.com/blogs/containers/aws-and-kubecost-collaborate-to-deliver-cost-monitoring-for-eks-customers/">Kubecost</a> to get infrastructure cost reporting based on Kubernetes labels on pods, namespaces etc.</p>
<p>Worker nodes need to have tags to show billing information in AWS Cost Explorer. With Cluster Autoscaler, tag your worker nodes inside a managed node group using <a href="https://docs.aws.amazon.com/eks/latest/userguide/launch-templates.html">launch template</a>. For self managed node groups, tag your instances using <a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-tagging.html">EC2 auto scaling group</a>. For instances provisioned by Karpenter, tag them using <a href="https://karpenter.sh/v0.29/concepts/node-templates/#spectags">spec.tags in the node template</a>.</p>
<h3 id="cost_optimization-cost_opt_compute-multi-tenant-clusters">Multi-tenant clusters<a class="headerlink" href="#cost_optimization-cost_opt_compute-multi-tenant-clusters" title="Permanent link">&para;</a></h3>
<p>When working on clusters that are shared by different teams you may not have visibility to other workloads running on the same node. While resource requests can help isolate some “noisy neighbor” concerns, such as CPU sharing, they may not isolate all resource boundaries such as disk I/O exhaustion. Not every consumable resource by a workload can be isolated or limited. Workloads that consume shared resources at higher rates than other workloads should be isolated through node <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">taints and tolerations</a>. Another advanced technique for such workload is <a href="https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/#static-policy">CPU pinning</a> which ensures exclusive CPU instead of shared CPU for the container.</p>
<p>Isolating workloads at a node level can be more expensive, but it may be possible to schedule <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/#besteffort">BestEffort</a> jobs or take advantage of additional savings by using <a href="https://aws.amazon.com/ec2/pricing/reserved-instances/">Reserved Instances</a>, <a href="https://aws.amazon.com/ec2/graviton/">Graviton processors</a>, or <a href="https://aws.amazon.com/ec2/spot/">Spot</a>.</p>
<p>Shared clusters may also have cluster level resource constraints such as IP exhaustion, Kubernetes service limits, or API scaling requests. You should review the <a href="https://aws.github.io/aws-eks-best-practices/scalability/docs/control-plane/">scalability best practices guide</a> to make sure your clusters avoid these limits.</p>
<p>You can isolate resources at a namespace or Karpenter provisioner level. <a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/">Resource Quotas</a> provide a way to set limits on how many resources workloads in a namespace can consume. This can be a good initial guard rail but it should be continually evaluated to make sure it doesn’t artificially restrict workloads from scaling.</p>
<p>Karpenter provisioners can <a href="https://karpenter.sh/docs/concepts/provisioners/#speclimitsresources">set limits on some of the consumable resources</a> in a cluster (e.g. CPU, GPU), but you will need to configure tenant applications to use the appropriate provisioner. This can prevent a single provisioner from creating too many nodes in a cluster, but it should be continually evaluated to make sure the limit isn’t set too low and in turn, prevent workloads from scaling.</p>
<h3 id="cost_optimization-cost_opt_compute-scheduled-autoscaling">Scheduled Autoscaling<a class="headerlink" href="#cost_optimization-cost_opt_compute-scheduled-autoscaling" title="Permanent link">&para;</a></h3>
<p>You may have the need to scale down your clusters on weekends and off hours. This is particularly relevant for test and non-production clusters where you want to scale down to zero when they are not in use. Solutions like <a href="https://github.com/kubecost/cluster-turndown">cluster-turndown</a> and <a href="https://codeberg.org/hjacobs/kube-downscaler">kube-downscaler</a> can scale down the replicas to zero based on a cron schedule.    </p>
<h2 id="cost_optimization-cost_opt_compute-optimize-compute-capacity-types">Optimize compute capacity types<a class="headerlink" href="#cost_optimization-cost_opt_compute-optimize-compute-capacity-types" title="Permanent link">&para;</a></h2>
<p>After optimizing the total capacity of compute in your cluster and utilizing bin packing, you should look at what type of compute you have provisioned in your clusters and how you pay for those resources. AWS has <a href="https://aws.amazon.com/savingsplans/compute-pricing/">Compute Savings plans</a> that can reduce the cost for your compute which we will categorize into the following capacity types:</p>
<ul>
<li>Spot</li>
<li>Savings Plans</li>
<li>On-Demand</li>
<li>Fargate</li>
</ul>
<p>Each capacity type has different trade-offs for management overhead, availability, and long term commitments and you will need to decide which is right for your environment. No environment should rely on a single capacity type and you can mix multiple run types in a single cluster to optimize specific workload requirements and cost.</p>
<h3 id="cost_optimization-cost_opt_compute-spot">Spot<a class="headerlink" href="#cost_optimization-cost_opt_compute-spot" title="Permanent link">&para;</a></h3>
<p>The <a href="https://aws.amazon.com/ec2/spot/">spot</a> capacity type provisions EC2 instances from spare capacity in an Availability Zone. Spot offers the largest discounts—up to 90% — but those instances can be interrupted when they are needed elsewhere. Additionally, there may not always be capacity to provision new Spot instances and existing Spot instances can be reclaimed with a <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-interruptions.html">2 minute interruption notice</a>. If your application has a long startup or shutdown process, Spot instances may not be the best option.</p>
<p>Spot compute should use a wide variety of instance types to reduce the likelihood of not having spot capacity available. Instance interruptions need to be handled to safely shutdown nodes. Nodes provisioned with Karpenter or part of a Managed Node Group automatically support <a href="https://aws.github.io/aws-eks-best-practices/karpenter/#enable-interruption-handling-when-using-spot">instance interruption notifications</a>. If you are using self-managed nodes you will need to run the <a href="https://github.com/aws/aws-node-termination-handler">node termination handler</a> separately to gracefully shutdown spot instances.</p>
<p>It is possible to balance spot and on-demand instances in a single cluster. With Karpenter you can create <a href="https://karpenter.sh/docs/concepts/scheduling/#on-demandspot-ratio-split">weighted provisioners</a> to achieve a balance of different capacity types. With Cluster Autoscaler you can create <a href="https://aws.amazon.com/blogs/containers/amazon-eks-now-supports-provisioning-and-managing-ec2-spot-instances-in-managed-node-groups/">mixed node groups with spot and on-demand or reserved instances</a>.</p>
<p>Here is an example of using Karpenter to prioritize Spot instances ahead of On-Demand instances. When creating a provisioner, you can specify either Spot, On-Demand, or both (as shown below). When you specify both, and if the pod does not explicitly specify whether it needs to use Spot or On-Demand, then Karpenter prioritizes Spot when provisioning a node with <a href="https://aws.amazon.com/blogs/compute/introducing-price-capacity-optimized-allocation-strategy-for-ec2-spot-instances/">price-capacity-optimization allocation strategy</a> .</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#cost_optimization-cost_opt_compute-__codelineno-5-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">karpenter.sh/v1alpha5</span>
<a id="__codelineno-5-2" name="__codelineno-5-2" href="#cost_optimization-cost_opt_compute-__codelineno-5-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Provisioner</span>
<a id="__codelineno-5-3" name="__codelineno-5-3" href="#cost_optimization-cost_opt_compute-__codelineno-5-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-5-4" name="__codelineno-5-4" href="#cost_optimization-cost_opt_compute-__codelineno-5-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">spot-prioritized</span>
<a id="__codelineno-5-5" name="__codelineno-5-5" href="#cost_optimization-cost_opt_compute-__codelineno-5-5"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-5-6" name="__codelineno-5-6" href="#cost_optimization-cost_opt_compute-__codelineno-5-6"></a><span class="w">  </span><span class="nt">requirements</span><span class="p">:</span>
<a id="__codelineno-5-7" name="__codelineno-5-7" href="#cost_optimization-cost_opt_compute-__codelineno-5-7"></a><span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;karpenter.sh/capacity-type&quot;</span><span class="w"> </span>
<a id="__codelineno-5-8" name="__codelineno-5-8" href="#cost_optimization-cost_opt_compute-__codelineno-5-8"></a><span class="w">      </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">In</span>
<a id="__codelineno-5-9" name="__codelineno-5-9" href="#cost_optimization-cost_opt_compute-__codelineno-5-9"></a><span class="hll"><span class="w">        </span><span class="l l-Scalar l-Scalar-Plain">values</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;spot&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;on-demand&quot;</span><span class="p p-Indicator">]</span>
</span></code></pre></div>
<h3 id="cost_optimization-cost_opt_compute-savings-plans-reserved-instances-and-aws-edp">Savings Plans, Reserved Instances, and AWS EDP<a class="headerlink" href="#cost_optimization-cost_opt_compute-savings-plans-reserved-instances-and-aws-edp" title="Permanent link">&para;</a></h3>
<p>You can reduce your compute spend by using a <a href="https://aws.amazon.com/savingsplans/compute-pricing/">compute savings plan</a>. Savings plans offer reduced prices for a 1 or 3 year commitment of compute usage. The usage can apply to EC2 instances in an EKS cluster but also applies to any compute usage such as Lambda and Fargate. With savings plans you can reduce costs and still pick any EC2 instance type during your commitment period.</p>
<p>Compute savings plan can reduce your EC2 cost by up to 66% without requiring commitments on what instance types, families, or regions you want to use. Savings are automatically applied to instances as you use them.</p>
<p>EC2 Instance Savings Plans provides up to 72% savings on compute with a commitment of usage in a specific region and EC2 family, e.g. instances from the C family. You can shift usage to any AZ within the region, use any generation of the instance family, e.g. c5 or c6, and use any size of instance within the family. The discount will automatically be applied for any instance in your account that matches the savings plan criteria.</p>
<p><a href="https://aws.amazon.com/ec2/pricing/reserved-instances/">Reserved Instances</a> are similar to EC2 Instance Savings Plan but they also guarantee capacity in an Availability Zone or Region and reduce cost—up to 72% — over on-demand instances. Once you calculate how much reserved capacity you will need you can select how long you would like to reserve them for (1 year or 3 years). The discounts will automatically be applied as you run those EC2 instances in your account.</p>
<p>Customers also have the option to enroll in an Enterprise Agreement with AWS. Enterprise Agreements give customers the option to tailor agreements that best suit their needs. Customers can enjoy discounts on the pricing based on AWS EDP (Enterprise Discount Program). For additional information on Enterprise Agreements please contact your AWS sales representative. </p>
<h3 id="cost_optimization-cost_opt_compute-on-demand">On-Demand<a class="headerlink" href="#cost_optimization-cost_opt_compute-on-demand" title="Permanent link">&para;</a></h3>
<p>On-Demand EC2 instances have the benefit of availability without interruptions — compared to spot — and no long term commitments — compared to savings plans. If you are looking to reduce costs in a cluster you should reduce your usage of on-demand EC2 instances.</p>
<p>After optimizing your workload requirements you should be able to calculate a minimum and maximum capacity for your clusters. This number may change over time but rarely goes down. Consider using a Savings Plan for everything under the minimum, and spot for capacity that will not affect your application’s availability. Anything else that may not be continuously used or is required for availability can use on-demand.</p>
<p>As mentioned in this section, the best way to reduce your usage is to consume fewer resources and utilize the resources you provision to the fullest extent possible. With the Cluster Autoscaler you can remove underutilized nodes with the <code>scale-down-utilization-threshold</code> setting. With Karpenter it is recommended to enable consolidation.</p>
<p>To manually identify EC2 instance types that can be used with your workloads you should use <a href="https://github.com/aws/amazon-ec2-instance-selector">ec2-instance-selector</a> which can show instances that are available in each region as well as instances compatible with EKS. Example usage for instances with x86 process architecture, 4 Gb of memory, 2 vCPUs and available in the us-east-1 region.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#cost_optimization-cost_opt_compute-__codelineno-6-1"></a>ec2-instance-selector<span class="w"> </span>--memory<span class="w"> </span><span class="m">4</span><span class="w"> </span>--vcpus<span class="w"> </span><span class="m">2</span><span class="w"> </span>--cpu-architecture<span class="w"> </span>x86_64<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-6-2" name="__codelineno-6-2" href="#cost_optimization-cost_opt_compute-__codelineno-6-2"></a><span class="w">  </span>-r<span class="w"> </span>us-east-1<span class="w"> </span>--service<span class="w"> </span>eks
<a id="__codelineno-6-3" name="__codelineno-6-3" href="#cost_optimization-cost_opt_compute-__codelineno-6-3"></a>c5.large
<a id="__codelineno-6-4" name="__codelineno-6-4" href="#cost_optimization-cost_opt_compute-__codelineno-6-4"></a>c5a.large
<a id="__codelineno-6-5" name="__codelineno-6-5" href="#cost_optimization-cost_opt_compute-__codelineno-6-5"></a>c5ad.large
<a id="__codelineno-6-6" name="__codelineno-6-6" href="#cost_optimization-cost_opt_compute-__codelineno-6-6"></a>c5d.large
<a id="__codelineno-6-7" name="__codelineno-6-7" href="#cost_optimization-cost_opt_compute-__codelineno-6-7"></a>c6a.large
<a id="__codelineno-6-8" name="__codelineno-6-8" href="#cost_optimization-cost_opt_compute-__codelineno-6-8"></a>c6i.large
<a id="__codelineno-6-9" name="__codelineno-6-9" href="#cost_optimization-cost_opt_compute-__codelineno-6-9"></a>t2.medium
<a id="__codelineno-6-10" name="__codelineno-6-10" href="#cost_optimization-cost_opt_compute-__codelineno-6-10"></a>t3.medium
<a id="__codelineno-6-11" name="__codelineno-6-11" href="#cost_optimization-cost_opt_compute-__codelineno-6-11"></a>t3a.medium
</code></pre></div>
<p>For non-production environments you can automatically have clusters scaled down during unused hours such as night and weekends. The kubecost project <a href="https://github.com/kubecost/cluster-turndown">cluster-turndown</a> is an example of a controller that can automatically scale your cluster down based on a set schedule.</p>
<h3 id="cost_optimization-cost_opt_compute-fargate-compute">Fargate compute<a class="headerlink" href="#cost_optimization-cost_opt_compute-fargate-compute" title="Permanent link">&para;</a></h3>
<p>Fargate compute is a fully managed compute option for EKS clusters. It provides pod isolation by scheduling one pod per node in a Kubernetes cluster. It allows you to size your compute nodes to the CPU and RAM requirements of your workload to tightly control workload usage in a cluster.</p>
<p>Fargate can scale workloads as small as .25 vCPU with 0.5 GB memory and as large as 16 vCPU with 120 GB memory. There are limits on how many <a href="https://docs.aws.amazon.com/eks/latest/userguide/fargate-pod-configuration.html">pod size variations</a> are available and you will need to understand how your workload best fits into a Fargate configuration. For example, if your workload requires 1 vCPU with 0.5 GB of memory the smallest Fargate pod will be 1 vCPU with 2 GB of memory.</p>
<p>While Fargate has many benefits such as no EC2 instance or operating system management, it may require more compute capacity than traditional EC2 instances due to the fact that every deployed pod is isolated as a separate node in the cluster. This requires more duplication for things like the Kubelet, logging agents, and any DaemonSets you would typically deploy to a node. DaemonSets are not supported in Fargate and they will need to be converted into pod “sidecars“ and run alongside the application.</p>
<p>Fargate cannot benefit from bin packing or CPU over provisioning because the boundary for the workload is a node which is not burstable or shareable between workloads. Fargate will save you EC2 instance management time which itself has a cost, but CPU and memory costs may be more expensive than other EC2 capacity types. Fargate pods can take advantage of compute savings plan to reduce the on-demand cost.</p>
<h2 id="cost_optimization-cost_opt_compute-optimize-compute-usage">Optimize Compute Usage<a class="headerlink" href="#cost_optimization-cost_opt_compute-optimize-compute-usage" title="Permanent link">&para;</a></h2>
<p>Another way to save money on your compute infrastructure is to use more efficient compute for the workload. This can come from more performant general purpose compute like <a href="https://aws.amazon.com/ec2/graviton/">Graviton processors</a> which are up to 20% cheaper and 60% more energy efficient than x86—or workload specific accelerators such as GPUs and <a href="https://aws.amazon.com/ec2/instance-types/f1/">FPGAs</a>. You will need to build containers that can <a href="https://aws.amazon.com/blogs/containers/how-to-build-your-containers-for-arm-and-save-with-graviton-and-spot-instances-on-amazon-ecs/">run on arm architecture</a> and <a href="https://aws.amazon.com/blogs/compute/running-gpu-accelerated-kubernetes-workloads-on-p3-and-p2-ec2-instances-with-amazon-eks/">set up nodes with the right accelerators</a> for your workloads.</p>
<p>EKS has the ability to run clusters with mixed architecture (e.g. amd64 and arm64) and if your containers are compiled for multiple architectures you can take advantage of Graviton processors with Karpenter by allowing both architectures in your provisioner. To keep consistent performance, however, it is recommended you keep each workload on a single compute architecture and only use different architecture if there is no additional capacity available.</p>
<p>Provisioners can be configured with multiple architectures and workloads can also request specific architectures in their workload specification.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#cost_optimization-cost_opt_compute-__codelineno-7-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">karpenter.sh/v1alpha5</span>
<a id="__codelineno-7-2" name="__codelineno-7-2" href="#cost_optimization-cost_opt_compute-__codelineno-7-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Provisioner</span>
<a id="__codelineno-7-3" name="__codelineno-7-3" href="#cost_optimization-cost_opt_compute-__codelineno-7-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-7-4" name="__codelineno-7-4" href="#cost_optimization-cost_opt_compute-__codelineno-7-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">default</span>
<a id="__codelineno-7-5" name="__codelineno-7-5" href="#cost_optimization-cost_opt_compute-__codelineno-7-5"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-7-6" name="__codelineno-7-6" href="#cost_optimization-cost_opt_compute-__codelineno-7-6"></a><span class="w">  </span><span class="nt">requirements</span><span class="p">:</span>
<a id="__codelineno-7-7" name="__codelineno-7-7" href="#cost_optimization-cost_opt_compute-__codelineno-7-7"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;kubernetes.io/arch&quot;</span>
<a id="__codelineno-7-8" name="__codelineno-7-8" href="#cost_optimization-cost_opt_compute-__codelineno-7-8"></a><span class="w">    </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">In</span>
<a id="__codelineno-7-9" name="__codelineno-7-9" href="#cost_optimization-cost_opt_compute-__codelineno-7-9"></a><span class="w">    </span><span class="nt">values</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;arm64&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;amd64&quot;</span><span class="p p-Indicator">]</span>
</code></pre></div>
<p>With Cluster Autoscaler you will need to create a node group for Graviton instances and set <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">node tolerations on your workload</a> to utilize the new capacity.</p>
<p>GPUs and FPGAs can greatly increase the performance for your workload, but the workload will need to be optimized to use the accelerator. Many workload types for machine learning and artificial intelligence can use GPUs for compute and instances can be added to a cluster and mounted into a workload using resource requests.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1" href="#cost_optimization-cost_opt_compute-__codelineno-8-1"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-8-2" name="__codelineno-8-2" href="#cost_optimization-cost_opt_compute-__codelineno-8-2"></a><span class="w">  </span><span class="nt">template</span><span class="p">:</span>
<a id="__codelineno-8-3" name="__codelineno-8-3" href="#cost_optimization-cost_opt_compute-__codelineno-8-3"></a><span class="w">    </span><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-8-4" name="__codelineno-8-4" href="#cost_optimization-cost_opt_compute-__codelineno-8-4"></a><span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">containers</span><span class="p">:</span>
<a id="__codelineno-8-5" name="__codelineno-8-5" href="#cost_optimization-cost_opt_compute-__codelineno-8-5"></a><span class="w">      </span><span class="l l-Scalar l-Scalar-Plain">...</span>
<a id="__codelineno-8-6" name="__codelineno-8-6" href="#cost_optimization-cost_opt_compute-__codelineno-8-6"></a><span class="w">      </span><span class="nt">resources</span><span class="p">:</span>
<a id="__codelineno-8-7" name="__codelineno-8-7" href="#cost_optimization-cost_opt_compute-__codelineno-8-7"></a><span class="w">          </span><span class="nt">limits</span><span class="p">:</span>
<a id="__codelineno-8-8" name="__codelineno-8-8" href="#cost_optimization-cost_opt_compute-__codelineno-8-8"></a><span class="w">            </span><span class="nt">nvidia.com/gpu</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;1&quot;</span>
</code></pre></div>
<p>Some GPU hardware can be shared across multiple workloads so a single GPU can be provisioned and used. To see how to configure workload GPU sharing see the <a href="https://aws.amazon.com/blogs/opensource/virtual-gpu-device-plugin-for-inference-workload-in-kubernetes/">virtual GPU device plugin</a> for more information. You can also refer to the following blogs: </p>
<ul>
<li><a href="https://aws.amazon.com/blogs/containers/gpu-sharing-on-amazon-eks-with-nvidia-time-slicing-and-accelerated-ec2-instances/">GPU sharing on Amazon EKS with NVIDIA time-slicing and accelerated EC2 instances</a></li>
<li><a href="https://aws.amazon.com/blogs/containers/maximizing-gpu-utilization-with-nvidias-multi-instance-gpu-mig-on-amazon-eks-running-more-pods-per-gpu-for-enhanced-performance/">Maximizing GPU utilization with NVIDIA’s Multi-Instance GPU (MIG) on Amazon EKS: Running more pods per GPU for enhanced performance</a></li>
</ul></section><section class="print-page" id="cost_optimization-cost_opt_networking"><h1 id="cost_optimization-cost_opt_networking-cost-optimization-networking">Cost Optimization - Networking<a class="headerlink" href="#cost_optimization-cost_opt_networking-cost-optimization-networking" title="Permanent link">&para;</a></h1>
<p>Architecting systems for high availability (HA) is a best practice in order to accomplish resilience and fault-tolerance. In practice, this means spreading your workloads and the underlying infrastructure across multiple Availability Zones (AZs) in a given AWS Region. Ensuring these characteristics are in place for your Amazon EKS environment will enhance the overall reliability of your system. In conjunction with this, your EKS environments will likely also be composed of a variety of constructs (i.e. VPCs), components (i.e. ELBs), and integrations (i.e. ECR and other container registries). </p>
<p>The combination of highly available systems and other use-case specific components can play a significant role in how data is transferred and processed. This will in turn have an impact on the costs incurred due to data transfer and processing. </p>
<p>The practices detailed below will help you design and optimize your EKS environments in order to achieve cost-effectiveness for different domains and use cases.</p>
<h2 id="cost_optimization-cost_opt_networking-pod-to-pod-communication">Pod to Pod Communication<a class="headerlink" href="#cost_optimization-cost_opt_networking-pod-to-pod-communication" title="Permanent link">&para;</a></h2>
<p>Depending on your setup, network communication and data transfer between Pods can have a significant impact on the overall cost of running Amazon EKS workloads. This section will cover different concepts and approaches to mitigating the costs tied to inter-pod communication, while considering highly available (HA) architectures, application performance and resilience. </p>
<h3 id="cost_optimization-cost_opt_networking-restricting-traffic-to-an-availability-zone">Restricting Traffic to an Availability Zone<a class="headerlink" href="#cost_optimization-cost_opt_networking-restricting-traffic-to-an-availability-zone" title="Permanent link">&para;</a></h3>
<p>Frequent egress cross-zone traffic (traffic distributed between AZs) can have a major impact on your network-related costs. Below are some strategies on how to control the amount of cross-zone traffic between Pods in your EKS cluster. </p>
<p><em>If you want granular visibility into the amount of cross-zone traffic between Pods in your cluster (such as the amount of data transferred in bytes), <a href="https://aws.amazon.com/blogs/containers/getting-visibility-into-your-amazon-eks-cross-az-pod-to-pod-network-bytes/">refer to this post</a>.</em></p>
<p><strong>Using Topology Aware Routing (formerly known as Topology Aware Hints)</strong></p>
<p><img alt="Topology aware routing" src="../images/topo_aware_routing.png" /></p>
<p>When using topology aware routing, it's important to understand how Services, EndpointSlices and the <code>kube-proxy</code> work together when routing traffic. As the diagram above depicts, Services are the stable network abstraction layer that receive traffic destined for your Pods. When a Service is created, multiple EndpointSlices are created. Each EndpointSlice has a list of endpoints containing a subset of Pod addresses along with the nodes they're running on and any additional topology information. <code>kube-proxy</code> is a daemonset that runs on every node in your cluster and also fulfills a role of internal routing, but it does so based on what it consumes from the created EndpointSlices.</p>
<p>When <a href="https://kubernetes.io/docs/concepts/services-networking/topology-aware-routing/"><em>topology aware routing</em></a> is enabled and implemented on a Kubernetes Service, the EndpointSlice controller will proportionally allocate endpoints to the different zones that your cluster is spread across. For each of those endpoints, the EndpointSlice controller will also set a <em>hint</em> for the zone. <em>Hints</em> describe which zone an endpoint should serve traffic for. <code>kube-proxy</code> will then route traffic from a zone to an endpoint based on the <em>hints</em> that get applied. </p>
<p>The diagram below shows how EndpointSlices with hints are organized in such a way that <code>kube-proxy</code> can know what destination they should go to based on their zonal point of origin. Without hints, there is no such allocation or organization and traffic will be proxied to different zonal destinations regardless of where it’s coming from. </p>
<p><img alt="Endpoint Slice" src="../images/endpoint_slice.png" /></p>
<p>In some cases, the EndpointSlice controller may apply a <em>hint</em> for a different zone, meaning the endpoint could end up serving traffic originating from a different zone. The reason for this is to try and maintain an even distribution of traffic between endpoints in different zones.</p>
<p>Below is a code snippet on how to enable <em>topology aware routing</em> for a Service. </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#cost_optimization-cost_opt_networking-__codelineno-0-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#cost_optimization-cost_opt_networking-__codelineno-0-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Service</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#cost_optimization-cost_opt_networking-__codelineno-0-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#cost_optimization-cost_opt_networking-__codelineno-0-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">orders-service</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#cost_optimization-cost_opt_networking-__codelineno-0-5"></a><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ecommerce</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#cost_optimization-cost_opt_networking-__codelineno-0-6"></a><span class="hll"><span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">annotations</span><span class="p p-Indicator">:</span>
</span><a id="__codelineno-0-7" name="__codelineno-0-7" href="#cost_optimization-cost_opt_networking-__codelineno-0-7"></a><span class="hll"><span class="w">      </span><span class="nt">service.kubernetes.io/topology-mode</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Auto</span>
</span><a id="__codelineno-0-8" name="__codelineno-0-8" href="#cost_optimization-cost_opt_networking-__codelineno-0-8"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#cost_optimization-cost_opt_networking-__codelineno-0-9"></a><span class="w">  </span><span class="nt">selector</span><span class="p">:</span>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#cost_optimization-cost_opt_networking-__codelineno-0-10"></a><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">orders</span>
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#cost_optimization-cost_opt_networking-__codelineno-0-11"></a><span class="w">  </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ClusterIP</span>
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#cost_optimization-cost_opt_networking-__codelineno-0-12"></a><span class="w">  </span><span class="nt">ports</span><span class="p">:</span>
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#cost_optimization-cost_opt_networking-__codelineno-0-13"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">TCP</span>
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#cost_optimization-cost_opt_networking-__codelineno-0-14"></a><span class="w">    </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">3003</span>
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#cost_optimization-cost_opt_networking-__codelineno-0-15"></a><span class="w">    </span><span class="nt">targetPort</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">3003</span>
</code></pre></div>
<p>The screenshot below shows the result of the EndpointSlice controller having successfully applied a hint to an endpoint for a Pod replica running in the AZ <code>eu-west-1a</code>. </p>
<p><img alt="Slice shell" src="../images/slice_shell.png" /></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It’s important to note that topology aware routing is still in <strong>beta</strong>. Also, this feature is more predictable when workloads are widely and evenly distributed across the cluster topology. Therefore, it is highly recommended to use it in conjunction with scheduling constraints that increase the availability of an application such as <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/">pod topology spread constraints</a>.</p>
</div>
<p><strong>Using Autoscalers: Provision Nodes to a Specific AZ</strong></p>
<p><em>We strongly recommend</em> running your workloads in highly available environments across multiple AZs. This improves the reliability of your applications, especially when there is an incident of an issue with an AZ. In the case you're willing to sacrifice reliability for the sake of reducing their network-related costs, you can restrict your nodes to a single AZ. </p>
<p>To run all your Pods in the same AZ, either provision the worker nodes in the same AZ or schedule the Pods on the worker nodes running on the same AZ. To provision nodes within a single AZ, define a node group with subnets belonging to the same AZ with <a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler">Cluster Autoscaler (CA)</a>. For <a href="https://karpenter.sh/">Karpenter,</a> use “<a href="http://topology.kubernetes.io/zone%E2%80%9D"><em>topology.kubernetes.io/zone”</em></a> and specify the AZ where you’d like to create the worker nodes. For example, the below Karpenter provisioner snippet provisions the nodes in the us-west-2a AZ.</p>
<p><strong>Karpenter</strong></p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#cost_optimization-cost_opt_networking-__codelineno-1-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">karpenter.sh/v1alpha5</span>
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#cost_optimization-cost_opt_networking-__codelineno-1-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Provisioner</span>
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#cost_optimization-cost_opt_networking-__codelineno-1-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#cost_optimization-cost_opt_networking-__codelineno-1-4"></a><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">single-az</span>
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#cost_optimization-cost_opt_networking-__codelineno-1-5"></a><span class="hll"><span class="nt">spec</span><span class="p">:</span>
</span><a id="__codelineno-1-6" name="__codelineno-1-6" href="#cost_optimization-cost_opt_networking-__codelineno-1-6"></a><span class="hll"><span class="w">  </span><span class="nt">requirements</span><span class="p">:</span>
</span><a id="__codelineno-1-7" name="__codelineno-1-7" href="#cost_optimization-cost_opt_networking-__codelineno-1-7"></a><span class="hll"><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;topology.kubernetes.io/zone“</span>
</span><a id="__codelineno-1-8" name="__codelineno-1-8" href="#cost_optimization-cost_opt_networking-__codelineno-1-8"></a><span class="hll"><span class="w">    </span><span class="s">operator:</span><span class="nv"> </span><span class="s">In</span>
</span><a id="__codelineno-1-9" name="__codelineno-1-9" href="#cost_optimization-cost_opt_networking-__codelineno-1-9"></a><span class="hll"><span class="w">    </span><span class="s">values:</span><span class="nv"> </span><span class="s">[&quot;</span><span class="l l-Scalar l-Scalar-Plain">us-west-2a&quot;]</span>
</span></code></pre></div>
<p><strong>Cluster Autoscaler (CA)</strong></p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#cost_optimization-cost_opt_networking-__codelineno-2-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">eksctl.io/v1alpha5</span>
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#cost_optimization-cost_opt_networking-__codelineno-2-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ClusterConfig</span>
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#cost_optimization-cost_opt_networking-__codelineno-2-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#cost_optimization-cost_opt_networking-__codelineno-2-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">my-ca-cluster</span>
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#cost_optimization-cost_opt_networking-__codelineno-2-5"></a><span class="w">  </span><span class="nt">region</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">us-east-1</span>
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#cost_optimization-cost_opt_networking-__codelineno-2-6"></a><span class="w">  </span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;1.21&quot;</span>
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#cost_optimization-cost_opt_networking-__codelineno-2-7"></a><span class="hll"><span class="nt">availabilityZones</span><span class="p">:</span>
</span><a id="__codelineno-2-8" name="__codelineno-2-8" href="#cost_optimization-cost_opt_networking-__codelineno-2-8"></a><span class="hll"><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">us-east-1a</span>
</span><a id="__codelineno-2-9" name="__codelineno-2-9" href="#cost_optimization-cost_opt_networking-__codelineno-2-9"></a><span class="nt">managedNodeGroups</span><span class="p">:</span>
<a id="__codelineno-2-10" name="__codelineno-2-10" href="#cost_optimization-cost_opt_networking-__codelineno-2-10"></a><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">managed-nodes</span>
<a id="__codelineno-2-11" name="__codelineno-2-11" href="#cost_optimization-cost_opt_networking-__codelineno-2-11"></a><span class="w">  </span><span class="nt">labels</span><span class="p">:</span>
<a id="__codelineno-2-12" name="__codelineno-2-12" href="#cost_optimization-cost_opt_networking-__codelineno-2-12"></a><span class="w">    </span><span class="nt">role</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">managed-nodes</span>
<a id="__codelineno-2-13" name="__codelineno-2-13" href="#cost_optimization-cost_opt_networking-__codelineno-2-13"></a><span class="w">  </span><span class="nt">instanceType</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">t3.medium</span>
<a id="__codelineno-2-14" name="__codelineno-2-14" href="#cost_optimization-cost_opt_networking-__codelineno-2-14"></a><span class="w">  </span><span class="nt">minSize</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<a id="__codelineno-2-15" name="__codelineno-2-15" href="#cost_optimization-cost_opt_networking-__codelineno-2-15"></a><span class="w">  </span><span class="nt">maxSize</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10</span>
<a id="__codelineno-2-16" name="__codelineno-2-16" href="#cost_optimization-cost_opt_networking-__codelineno-2-16"></a><span class="w">  </span><span class="nt">desiredCapacity</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<a id="__codelineno-2-17" name="__codelineno-2-17" href="#cost_optimization-cost_opt_networking-__codelineno-2-17"></a><span class="nn">...</span>
</code></pre></div>
<p><strong>Using Pod Assignment and Node Affinity</strong></p>
<p>Alternatively, if you have worker nodes running in multiple AZs, each node would have the label <em><a href="http://topology.kubernetes.io/zone%E2%80%9D">topology.kubernetes.io/zone</a></em> with the value of its AZ (such as us-west-2a or us-west-2b). You can utilize <code>nodeSelector</code> or <code>nodeAffinity</code> to schedule Pods to the nodes in a single AZ. For example, the following manifest file will schedule the Pod inside a node running in AZ us-west-2a.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#cost_optimization-cost_opt_networking-__codelineno-3-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#cost_optimization-cost_opt_networking-__codelineno-3-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Pod</span>
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#cost_optimization-cost_opt_networking-__codelineno-3-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-3-4" name="__codelineno-3-4" href="#cost_optimization-cost_opt_networking-__codelineno-3-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nginx</span>
<a id="__codelineno-3-5" name="__codelineno-3-5" href="#cost_optimization-cost_opt_networking-__codelineno-3-5"></a><span class="w">  </span><span class="nt">labels</span><span class="p">:</span>
<a id="__codelineno-3-6" name="__codelineno-3-6" href="#cost_optimization-cost_opt_networking-__codelineno-3-6"></a><span class="w">    </span><span class="nt">env</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">test</span>
<a id="__codelineno-3-7" name="__codelineno-3-7" href="#cost_optimization-cost_opt_networking-__codelineno-3-7"></a><span class="hll"><span class="nt">spec</span><span class="p">:</span>
</span><a id="__codelineno-3-8" name="__codelineno-3-8" href="#cost_optimization-cost_opt_networking-__codelineno-3-8"></a><span class="hll"><span class="w">  </span><span class="nt">nodeSelector</span><span class="p">:</span>
</span><a id="__codelineno-3-9" name="__codelineno-3-9" href="#cost_optimization-cost_opt_networking-__codelineno-3-9"></a><span class="hll"><span class="w">    </span><span class="nt">topology.kubernetes.io/zone</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">us-west-2a</span>
</span><a id="__codelineno-3-10" name="__codelineno-3-10" href="#cost_optimization-cost_opt_networking-__codelineno-3-10"></a><span class="w">  </span><span class="nt">containers</span><span class="p">:</span>
<a id="__codelineno-3-11" name="__codelineno-3-11" href="#cost_optimization-cost_opt_networking-__codelineno-3-11"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nginx</span>
<a id="__codelineno-3-12" name="__codelineno-3-12" href="#cost_optimization-cost_opt_networking-__codelineno-3-12"></a><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nginx</span><span class="w"> </span>
<a id="__codelineno-3-13" name="__codelineno-3-13" href="#cost_optimization-cost_opt_networking-__codelineno-3-13"></a><span class="w">    </span><span class="nt">imagePullPolicy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">IfNotPresent</span>
</code></pre></div>
<h3 id="cost_optimization-cost_opt_networking-restricting-traffic-to-a-node">Restricting Traffic to a Node<a class="headerlink" href="#cost_optimization-cost_opt_networking-restricting-traffic-to-a-node" title="Permanent link">&para;</a></h3>
<p>There are cases where restricting traffic at a zonal level isn’t sufficient. Apart from reducing costs, you may have the added requirement of reducing network latency between certain applications that have frequent inter-communication. In order to achieve optimal network performance and reduce costs, you need a way to restrict traffic to a specific node. For example, Microservice A should always talk to Microservice B on Node 1, even in highly available (HA) setups. Having Microservice A on Node 1 talk to Microservice B on Node 2 may have a negative impact on the desired performance for applications of this nature, especially if Node 2 is in a separate AZ altogether. </p>
<p><strong>Using the Service Internal Traffic Policy</strong></p>
<p>In order to restrict Pod network traffic to a node, you can make use of the <em><a href="https://kubernetes.io/docs/concepts/services-networking/service-traffic-policy/">Service internal traffic policy</a></em>. By default, traffic sent to a workload’s Service will be randomly distributed across the different generated endpoints. So in a HA architecture, that means traffic from Microservice A could go to any replica of Microservice B on any given node across the different AZs. However, with the Service's internal traffic policy set to <code>Local</code>, traffic will be restricted to endpoints on the node that the traffic originated from. This policy dictates the exclusive use of node-local endpoints. By implication, your network traffic-related costs for that workload will be lower than if the distribution was cluster wide. Also, the latency will be lower, making your application more performant. </p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It’s important to note that this feature cannot be combined with topology aware routing in Kubernetes.</p>
</div>
<p><img alt="Local internal traffic" src="../images/local_traffic.png" /></p>
<p>Below is a code snippet on how to set the <em>internal traffic policy</em> for a Service. </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#cost_optimization-cost_opt_networking-__codelineno-4-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<a id="__codelineno-4-2" name="__codelineno-4-2" href="#cost_optimization-cost_opt_networking-__codelineno-4-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Service</span>
<a id="__codelineno-4-3" name="__codelineno-4-3" href="#cost_optimization-cost_opt_networking-__codelineno-4-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-4-4" name="__codelineno-4-4" href="#cost_optimization-cost_opt_networking-__codelineno-4-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">orders-service</span>
<a id="__codelineno-4-5" name="__codelineno-4-5" href="#cost_optimization-cost_opt_networking-__codelineno-4-5"></a><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ecommerce</span>
<a id="__codelineno-4-6" name="__codelineno-4-6" href="#cost_optimization-cost_opt_networking-__codelineno-4-6"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-4-7" name="__codelineno-4-7" href="#cost_optimization-cost_opt_networking-__codelineno-4-7"></a><span class="w">  </span><span class="nt">selector</span><span class="p">:</span>
<a id="__codelineno-4-8" name="__codelineno-4-8" href="#cost_optimization-cost_opt_networking-__codelineno-4-8"></a><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">orders</span>
<a id="__codelineno-4-9" name="__codelineno-4-9" href="#cost_optimization-cost_opt_networking-__codelineno-4-9"></a><span class="w">  </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ClusterIP</span>
<a id="__codelineno-4-10" name="__codelineno-4-10" href="#cost_optimization-cost_opt_networking-__codelineno-4-10"></a><span class="w">  </span><span class="nt">ports</span><span class="p">:</span>
<a id="__codelineno-4-11" name="__codelineno-4-11" href="#cost_optimization-cost_opt_networking-__codelineno-4-11"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">TCP</span>
<a id="__codelineno-4-12" name="__codelineno-4-12" href="#cost_optimization-cost_opt_networking-__codelineno-4-12"></a><span class="w">    </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">3003</span>
<a id="__codelineno-4-13" name="__codelineno-4-13" href="#cost_optimization-cost_opt_networking-__codelineno-4-13"></a><span class="w">    </span><span class="nt">targetPort</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">3003</span>
<a id="__codelineno-4-14" name="__codelineno-4-14" href="#cost_optimization-cost_opt_networking-__codelineno-4-14"></a><span class="hll"><span class="w">  </span><span class="nt">internalTrafficPolicy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Local</span>
</span></code></pre></div>
<p>To avoid unexpected behaviour from your application due to traffic drops, you should consider the following approaches:</p>
<ul>
<li>Run enough replicas for each of the communicating Pods</li>
<li>Have a relatively even spread of Pods using <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/">topology spread constraints</a></li>
<li>Make use of <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity">pod-affinity rules</a> for co-location of communicating Pods</li>
</ul>
<p>In this example, you have 2 replicas of Microservice A and 3 replicas of Microservice B. If Microservice A has its replicas spread between Nodes 1 and 2, and Microservice B has all 3 of its replicas on Node 3, then they won't be able to communicate because of the <code>Local</code> internal traffic policy. When there are no available node-local endpoints the traffic is dropped. </p>
<p><img alt="node-local_no_peer" src="../images/no_node_local_1.png" /></p>
<p>If Microservice B does have 2 of its 3 replicas on Nodes 1 and 2, then there will be communication between the peer applications. But you would still have an isolated replica of Microservice B without any peer replica to communicate with. </p>
<p><img alt="node-local_with_peer" src="../images/no_node_local_2.png" /></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In some scenarios, an isolated replica like the one depicted in the above diagram may not be a cause for concern if it still serves a purpose (such as serving requests from external incoming traffic).</p>
</div>
<p><strong>Using the Service Internal Traffic Policy with Topology Spread Constraints</strong></p>
<p>Using the <em>internal traffic policy</em> in conjunction with <em>topology spread constraints</em> can be useful to ensure that you have the right number of replicas for communicating microservices on different nodes. </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#cost_optimization-cost_opt_networking-__codelineno-5-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">apps/v1</span>
<a id="__codelineno-5-2" name="__codelineno-5-2" href="#cost_optimization-cost_opt_networking-__codelineno-5-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deployment</span>
<a id="__codelineno-5-3" name="__codelineno-5-3" href="#cost_optimization-cost_opt_networking-__codelineno-5-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-5-4" name="__codelineno-5-4" href="#cost_optimization-cost_opt_networking-__codelineno-5-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">express-test</span>
<a id="__codelineno-5-5" name="__codelineno-5-5" href="#cost_optimization-cost_opt_networking-__codelineno-5-5"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-5-6" name="__codelineno-5-6" href="#cost_optimization-cost_opt_networking-__codelineno-5-6"></a><span class="w">  </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">6</span>
<a id="__codelineno-5-7" name="__codelineno-5-7" href="#cost_optimization-cost_opt_networking-__codelineno-5-7"></a><span class="w">  </span><span class="nt">selector</span><span class="p">:</span>
<a id="__codelineno-5-8" name="__codelineno-5-8" href="#cost_optimization-cost_opt_networking-__codelineno-5-8"></a><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span>
<a id="__codelineno-5-9" name="__codelineno-5-9" href="#cost_optimization-cost_opt_networking-__codelineno-5-9"></a><span class="w">      </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">express-test</span>
<a id="__codelineno-5-10" name="__codelineno-5-10" href="#cost_optimization-cost_opt_networking-__codelineno-5-10"></a><span class="w">  </span><span class="nt">template</span><span class="p">:</span>
<a id="__codelineno-5-11" name="__codelineno-5-11" href="#cost_optimization-cost_opt_networking-__codelineno-5-11"></a><span class="w">    </span><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-5-12" name="__codelineno-5-12" href="#cost_optimization-cost_opt_networking-__codelineno-5-12"></a><span class="w">      </span><span class="nt">labels</span><span class="p">:</span>
<a id="__codelineno-5-13" name="__codelineno-5-13" href="#cost_optimization-cost_opt_networking-__codelineno-5-13"></a><span class="w">        </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">express-test</span>
<a id="__codelineno-5-14" name="__codelineno-5-14" href="#cost_optimization-cost_opt_networking-__codelineno-5-14"></a><span class="w">        </span><span class="nt">tier</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">backend</span>
<a id="__codelineno-5-15" name="__codelineno-5-15" href="#cost_optimization-cost_opt_networking-__codelineno-5-15"></a><span class="w">    </span><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-5-16" name="__codelineno-5-16" href="#cost_optimization-cost_opt_networking-__codelineno-5-16"></a><span class="hll"><span class="w">      </span><span class="nt">topologySpreadConstraints</span><span class="p">:</span>
</span><a id="__codelineno-5-17" name="__codelineno-5-17" href="#cost_optimization-cost_opt_networking-__codelineno-5-17"></a><span class="hll"><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">maxSkew</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
</span><a id="__codelineno-5-18" name="__codelineno-5-18" href="#cost_optimization-cost_opt_networking-__codelineno-5-18"></a><span class="hll"><span class="w">        </span><span class="nt">topologyKey</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;topology.kubernetes.io/zone&quot;</span>
</span><a id="__codelineno-5-19" name="__codelineno-5-19" href="#cost_optimization-cost_opt_networking-__codelineno-5-19"></a><span class="hll"><span class="w">        </span><span class="nt">whenUnsatisfiable</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ScheduleAnyway</span>
</span><a id="__codelineno-5-20" name="__codelineno-5-20" href="#cost_optimization-cost_opt_networking-__codelineno-5-20"></a><span class="hll"><span class="w">        </span><span class="nt">labelSelector</span><span class="p">:</span>
</span><a id="__codelineno-5-21" name="__codelineno-5-21" href="#cost_optimization-cost_opt_networking-__codelineno-5-21"></a><span class="hll"><span class="w">          </span><span class="nt">matchLabels</span><span class="p">:</span>
</span><a id="__codelineno-5-22" name="__codelineno-5-22" href="#cost_optimization-cost_opt_networking-__codelineno-5-22"></a><span class="hll"><span class="w">            </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">express-test</span>
</span></code></pre></div>
<p><strong>Using the Service Internal Traffic Policy with Pod Affinity Rules</strong></p>
<p>Another approach is to make use of Pod affinity rules when using the Service internal traffic policy. With Pod affinity, you can influence the scheduler to co-locate certain Pods because of their frequent communication. By applying strict scheduling constraints (<code>requiredDuringSchedulingIgnoredDuringExecution</code>) on certain Pods, this will give you better results for Pod co-location when the Scheduler is placing Pods on nodes.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#cost_optimization-cost_opt_networking-__codelineno-6-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">apps/v1</span>
<a id="__codelineno-6-2" name="__codelineno-6-2" href="#cost_optimization-cost_opt_networking-__codelineno-6-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deployment</span>
<a id="__codelineno-6-3" name="__codelineno-6-3" href="#cost_optimization-cost_opt_networking-__codelineno-6-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-6-4" name="__codelineno-6-4" href="#cost_optimization-cost_opt_networking-__codelineno-6-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">graphql</span>
<a id="__codelineno-6-5" name="__codelineno-6-5" href="#cost_optimization-cost_opt_networking-__codelineno-6-5"></a><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ecommerce</span>
<a id="__codelineno-6-6" name="__codelineno-6-6" href="#cost_optimization-cost_opt_networking-__codelineno-6-6"></a><span class="w">  </span><span class="nt">labels</span><span class="p">:</span>
<a id="__codelineno-6-7" name="__codelineno-6-7" href="#cost_optimization-cost_opt_networking-__codelineno-6-7"></a><span class="w">    </span><span class="nt">app.kubernetes.io/version</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;0.1.6&quot;</span>
<a id="__codelineno-6-8" name="__codelineno-6-8" href="#cost_optimization-cost_opt_networking-__codelineno-6-8"></a><span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">...</span>
<a id="__codelineno-6-9" name="__codelineno-6-9" href="#cost_optimization-cost_opt_networking-__codelineno-6-9"></a><span class="w">    </span><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-6-10" name="__codelineno-6-10" href="#cost_optimization-cost_opt_networking-__codelineno-6-10"></a><span class="w">      </span><span class="nt">serviceAccountName</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">graphql-service-account</span>
<a id="__codelineno-6-11" name="__codelineno-6-11" href="#cost_optimization-cost_opt_networking-__codelineno-6-11"></a><span class="hll"><span class="w">      </span><span class="nt">affinity</span><span class="p">:</span>
</span><a id="__codelineno-6-12" name="__codelineno-6-12" href="#cost_optimization-cost_opt_networking-__codelineno-6-12"></a><span class="hll"><span class="w">        </span><span class="nt">podAffinity</span><span class="p">:</span>
</span><a id="__codelineno-6-13" name="__codelineno-6-13" href="#cost_optimization-cost_opt_networking-__codelineno-6-13"></a><span class="hll"><span class="w">          </span><span class="nt">requiredDuringSchedulingIgnoredDuringExecution</span><span class="p">:</span>
</span><a id="__codelineno-6-14" name="__codelineno-6-14" href="#cost_optimization-cost_opt_networking-__codelineno-6-14"></a><span class="hll"><span class="w">          </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">labelSelector</span><span class="p">:</span>
</span><a id="__codelineno-6-15" name="__codelineno-6-15" href="#cost_optimization-cost_opt_networking-__codelineno-6-15"></a><span class="hll"><span class="w">              </span><span class="nt">matchExpressions</span><span class="p">:</span>
</span><a id="__codelineno-6-16" name="__codelineno-6-16" href="#cost_optimization-cost_opt_networking-__codelineno-6-16"></a><span class="hll"><span class="w">              </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">app</span>
</span><a id="__codelineno-6-17" name="__codelineno-6-17" href="#cost_optimization-cost_opt_networking-__codelineno-6-17"></a><span class="hll"><span class="w">                </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">In</span>
</span><a id="__codelineno-6-18" name="__codelineno-6-18" href="#cost_optimization-cost_opt_networking-__codelineno-6-18"></a><span class="hll"><span class="w">                </span><span class="nt">values</span><span class="p">:</span>
</span><a id="__codelineno-6-19" name="__codelineno-6-19" href="#cost_optimization-cost_opt_networking-__codelineno-6-19"></a><span class="hll"><span class="w">                </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">orders</span>
</span><a id="__codelineno-6-20" name="__codelineno-6-20" href="#cost_optimization-cost_opt_networking-__codelineno-6-20"></a><span class="hll"><span class="w">            </span><span class="nt">topologyKey</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;kubernetes.io/hostname&quot;</span>
</span></code></pre></div>
<h2 id="cost_optimization-cost_opt_networking-load-balancer-to-pod-communication">Load Balancer to Pod Communication<a class="headerlink" href="#cost_optimization-cost_opt_networking-load-balancer-to-pod-communication" title="Permanent link">&para;</a></h2>
<p>EKS workloads are typically fronted by a load balancer that distributes traffic to the relevant Pods in your EKS cluster. Your architecture may comprise internal and/or external facing load balancers. Depending on your architecture and network traffic configurations, the communication between load balancers and Pods can contribute a significant amount to data transfer charges.</p>
<p>You can use the <a href="https://kubernetes-sigs.github.io/aws-load-balancer-controller">AWS Load Balancer Controller</a> to automatically manage the creation of ELB resources (ALB and NLB). The data transfer charges you incur in such setups will depend on the path taken by the network traffic. The AWS Load Balancer Controller supports two network traffic modes, <em>instance mode</em>, and <em>ip mode</em>.</p>
<p>When using <em>instance mode</em>, a NodePort will be opened on each node in your EKS cluster. The load balancer will then proxy traffic evenly across the nodes. If a node has the destination Pod running on it, then there will be no data transfer costs incurred. However, if the destination Pod is on a separate node and in a different AZ than the NodePort receiving the traffic, then there will be an extra network hop from the kube-proxy to the destination Pod. In such a scenario, there will be cross-AZ data transfer charges. Because of the even distribution of traffic across the nodes, it is highly likely that there will be additional data transfer charges associated with cross-zone network traffic hops from kube-proxies to the relevant destination Pods.</p>
<p>The diagram below depicts a network path for traffic flowing from the load balancer to the NodePort, and subsequently from the <code>kube-proxy</code> to the destination Pod on a separate node in a different AZ. This is an example of the <em>instance mode</em> setting. </p>
<p><img alt="LB to Pod" src="../images/lb_2_pod.png" /></p>
<p>When using <em>ip mode</em>, network traffic is proxied from the load balancer directly to the destination Pod. As a result, there are <em>no data transfer charges</em> involved in this approach. </p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>It is recommended that you set your load balancer to <em>ip traffic mode</em> to reduce data transfer charges. For this setup, it’s also important to make sure that your load balancer is deployed across all the subnets in your VPC. </p>
</div>
<p>The diagram below depicts network paths for traffic flowing from the load balancer to Pods in the network <em>ip mode</em>. </p>
<p><img alt="IP mode" src="../images/ip_mode.png" /></p>
<h2 id="cost_optimization-cost_opt_networking-data-transfer-from-container-registry">Data Transfer from Container Registry<a class="headerlink" href="#cost_optimization-cost_opt_networking-data-transfer-from-container-registry" title="Permanent link">&para;</a></h2>
<h3 id="cost_optimization-cost_opt_networking-amazon-ecr">Amazon ECR<a class="headerlink" href="#cost_optimization-cost_opt_networking-amazon-ecr" title="Permanent link">&para;</a></h3>
<p>Data transfer into the Amazon ECR private registry is free. <em>In-region data transfer incurs no cost</em>, but data transfer out to the internet and across regions will be charged at Internet Data Transfer rates on both sides of the transfer. </p>
<p>You should utilize ECRs built-in <a href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/replication.html">image replication feature</a> to replicate the relevant container images into the same region as your workloads. This way the replication would be charged once, and all the same region (intra-region) image pulls would be free.</p>
<p>You can further reduce data transfer costs associated with pulling images from ECR (data transfer out) by <em>using <a href="https://docs.aws.amazon.com/whitepapers/latest/aws-privatelink/what-are-vpc-endpoints.html">Interface VPC Endpoints</a> to connect to the in-region ECR repositories</em>. The alternative approach of connecting to ECR’s public AWS endpoint (via a NAT Gateway and an Internet Gateway) will incur higher data processing and transfer costs. The next section will cover reducing data transfer costs between your workloads and AWS Services in greater detail. </p>
<p>If you’re running workloads with especially large images, you can build your own custom Amazon Machine Images (AMIs) with pre-cached container images. This can reduce the initial image pull time and potential data transfer costs from a container registry to the EKS worker nodes. </p>
<h2 id="cost_optimization-cost_opt_networking-data-transfer-to-internet-aws-services">Data Transfer to Internet &amp; AWS Services<a class="headerlink" href="#cost_optimization-cost_opt_networking-data-transfer-to-internet-aws-services" title="Permanent link">&para;</a></h2>
<p>It's a common practice to integrate Kubernetes workloads with other AWS services or third-party tools and platforms via the Internet. The underlying network infrastructure used to route traffic to and from the relevant destination can impact the costs incurred in the data transfer process.</p>
<h3 id="cost_optimization-cost_opt_networking-using-nat-gateways">Using NAT Gateways<a class="headerlink" href="#cost_optimization-cost_opt_networking-using-nat-gateways" title="Permanent link">&para;</a></h3>
<p>NAT Gateways are network components that perform network address translation (NAT). The diagram below depicts Pods in an EKS cluster communicating with other AWS services (Amazon ECR, DynamoDB, and S3), and third-party platforms. In this example, the Pods are running in private subnets in separate AZs. To send and receive traffic from the Internet, a NAT Gateway is deployed to the public subnet of one AZ, allowing any resources with private IP addresses to share a single public IP address to access the Internet. This NAT Gateway in turn communicates with the Internet Gateway component, allowing for packets to be sent to their final destination.</p>
<p><img alt="NAT Gateway" src="../images/nat_gw.png" /></p>
<p>When using NAT Gateways for such use cases, <em>you can minimize the data transfer costs by deploying a NAT Gateway in each AZ</em>. This way, traffic routed to the Internet will go through the NAT Gateway in the same AZ, avoiding inter-AZ data transfer. However, even though you’ll save on the cost of inter-AZ data transfer, the implication of this setup is that you’ll incur the cost of an additional NAT Gateway in your architecture. </p>
<p>This recommended approach is depicted in the diagram below.</p>
<p><img alt="Recommended approach" src="../images/recommended_approach.png" /></p>
<h3 id="cost_optimization-cost_opt_networking-using-vpc-endpoints">Using VPC Endpoints<a class="headerlink" href="#cost_optimization-cost_opt_networking-using-vpc-endpoints" title="Permanent link">&para;</a></h3>
<p>To further reduce costs in such architectures, <em>you should use <a href="https://docs.aws.amazon.com/whitepapers/latest/aws-privatelink/what-are-vpc-endpoints.html">VPC Endpoints</a> to establish connectivity between your workloads and AWS services</em>. VPC Endpoints allow you to access AWS services from within a VPC without data/network packets traversing the Internet. All traffic is internal and stays within the AWS network. There are two types of VPC Endpoints: Interface VPC Endpoints (<a href="https://docs.aws.amazon.com/vpc/latest/privatelink/aws-services-privatelink-support.html">supported by many AWS services</a>) and Gateway VPC Endpoints (only supported by S3 and DynamoDB).</p>
<p><strong>Gateway VPC Endpoints</strong></p>
<p><em>There are no hourly or data transfer costs associated with Gateway VPC Endpoints</em>. When using Gateway VPC Endpoints, it's important to note that they are not extendable across VPC boundaries. They can't be used in VPC peering, VPN networking, or via Direct Connect.</p>
<p><strong>Interface VPC Endpoint</strong></p>
<p>VPC Endpoints have an <a href="https://aws.amazon.com/privatelink/pricing/">hourly charge</a> and, depending on the AWS service, may or may not have an additional charge associated with data processing via the underlying ENI. To reduce inter-AZ data transfer costs related to Interface VPC Endpoints, you can create a VPC Endpoint in each AZ. You can create multiple VPC Endpoints in the same VPC even if they're pointing to the same AWS service.</p>
<p>The diagram below shows Pods communicating with AWS services via VPC Endpoints.</p>
<p><img alt="VPC Endpoints" src="../images/vpc_endpoints.png" /></p>
<h2 id="cost_optimization-cost_opt_networking-data-transfer-between-vpcs">Data Transfer between VPCs<a class="headerlink" href="#cost_optimization-cost_opt_networking-data-transfer-between-vpcs" title="Permanent link">&para;</a></h2>
<p>In some cases, you may have workloads in distinct VPCs (within the same AWS region) that need to communicate with each other. This can be accomplished by allowing traffic to traverse the public internet through Internet Gateways attached to the respective VPCs. Such communication can be enabled by deploying infrastructure components like EC2 instances, NAT Gateways or NAT instances in public subnets. However, a setup including these components will incur charges for processing/transferring data in and out of the VPCs. If the traffic to and from the separate VPCs is moving across AZs, then there will be an additional charge in the transfer of data. The diagram below depicts a setup that uses NAT Gateways and Internet Gateways to establish communication between workloads in different VPCs. </p>
<p><img alt="Between VPCs" src="../images/between_vpcs.png" /></p>
<h3 id="cost_optimization-cost_opt_networking-vpc-peering-connections">VPC Peering Connections<a class="headerlink" href="#cost_optimization-cost_opt_networking-vpc-peering-connections" title="Permanent link">&para;</a></h3>
<p>To reduce costs for such use cases, you can make use of <a href="https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html">VPC Peering</a>. With a VPC Peering connection, there are no data transfer charges for network traffic that stays within the same AZ. If traffic crosses AZs, there will be a cost incurred. Nonetheless, the VPC Peering approach is recommended for cost-effective communication between workloads in separate VPCs within the same AWS region. However, it’s important to note that VPC peering is primarily effective for 1:1 VPC connectivity because it doesn’t allow for transitive networking. </p>
<p>The diagram below is a high-level representation of workloads communication via a VPC peering connection. </p>
<p><img alt="Peering" src="../images/peering.png" /></p>
<h3 id="cost_optimization-cost_opt_networking-transitive-networking-connections">Transitive Networking Connections<a class="headerlink" href="#cost_optimization-cost_opt_networking-transitive-networking-connections" title="Permanent link">&para;</a></h3>
<p>As pointed out in the previous section, VPC Peering connections do not allow for transitive networking connectivity. If you want to connect 3 or more VPCs with transitive networking requirements, then you should use a <a href="https://docs.aws.amazon.com/vpc/latest/tgw/what-is-transit-gateway.html">Transit Gateway</a> (TGW). This will enable you to overcome the limits of VPC Peering or any operational overhead associated with having multiple VPC Peering connections between multiple VPCs. You are <a href="https://aws.amazon.com/transit-gateway/pricing/">billed on an hourly basis</a> and for data sent to the TGW. <em>There is no destination cost associated with inter-AZ traffic that flows through the TGW.</em></p>
<p>The diagram below shows inter-AZ traffic flowing through a TGW between workloads in different VPCs but within the same AWS region.</p>
<p><img alt="Transitive" src="../images/transititive.png" /></p>
<h2 id="cost_optimization-cost_opt_networking-using-a-service-mesh">Using a Service Mesh<a class="headerlink" href="#cost_optimization-cost_opt_networking-using-a-service-mesh" title="Permanent link">&para;</a></h2>
<p>Service meshes offer powerful networking capabilities that can be used to reduce network related costs in your EKS cluster environments. However, you should carefully consider the operational tasks and complexity that a service mesh will introduce to your environment if you adopt one. </p>
<h3 id="cost_optimization-cost_opt_networking-restricting-traffic-to-availability-zones">Restricting Traffic to Availability Zones<a class="headerlink" href="#cost_optimization-cost_opt_networking-restricting-traffic-to-availability-zones" title="Permanent link">&para;</a></h3>
<p><strong>Using Istio’s Locality Weighted Distribution</strong></p>
<p>Istio enables you to apply network policies to traffic <em>after</em> routing occurs. This is done using <a href="https://istio.io/latest/docs/reference/config/networking/destination-rule/">Destination Rules</a> such as <a href="https://istio.io/latest/docs/tasks/traffic-management/locality-load-balancing/distribute/">locality weighted distribution</a>. Using this feature, you can control the weight (expressed as a percentage) of traffic that can go to a certain destination based on its origin. The source of this traffic can either be from an external (or public facing) load balancer or a Pod within the cluster itself. When all the Pod endpoints are available, the locality will be selected based on a weighted round-robin load balancing algorithm. In the case that certain endpoints are unhealthy or unavailable, <a href="https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/upstream/load_balancing/locality_weight.html">the locality weight will be automatically adjusted</a> to reflect this change in the available endpoints. </p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Before implementing locality weighted distribution, you should start by understanding your network traffic patterns and the implications that the Destination Rule policy may have on your application’s behaviour. As such, it’s important to have distributed tracing mechanisms in place with tools such as <a href="https://aws.amazon.com/xray/">AWS X-Ray</a> or <a href="https://www.jaegertracing.io/">Jaeger</a>. </p>
</div>
<p>The Istio Destination Rules detailed above can also be applied to manage traffic from a load balancer to Pods in your EKS cluster. Locality weighted distribution rules can be applied to a Service that receives traffic from a highly available load balancer (specifically the Ingress Gateway). These rules allow you to control how much traffic goes where based on its zonal origin - the load balancer in this case. If configured correctly, less egress cross-zone traffic will be incurred compared to a load balancer that distributes traffic evenly or randomly to Pod replicas in different AZs. </p>
<p>Below is a code block example of a Destination Rule resource in Istio. As can be seen below, this resource specifies weighted configurations for incoming traffic from 3 different AZs in the <code>eu-west-1</code> region. These configurations declare that a majority of the incoming traffic (70% in this case) from a given AZ should be proxied to a destination in the same AZ from which it originates. </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#cost_optimization-cost_opt_networking-__codelineno-7-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">networking.istio.io/v1beta1</span>
<a id="__codelineno-7-2" name="__codelineno-7-2" href="#cost_optimization-cost_opt_networking-__codelineno-7-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">DestinationRule</span>
<a id="__codelineno-7-3" name="__codelineno-7-3" href="#cost_optimization-cost_opt_networking-__codelineno-7-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-7-4" name="__codelineno-7-4" href="#cost_optimization-cost_opt_networking-__codelineno-7-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">express-test-dr</span>
<a id="__codelineno-7-5" name="__codelineno-7-5" href="#cost_optimization-cost_opt_networking-__codelineno-7-5"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-7-6" name="__codelineno-7-6" href="#cost_optimization-cost_opt_networking-__codelineno-7-6"></a><span class="w">  </span><span class="nt">host</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">express-test.default.svc.cluster.local</span>
<a id="__codelineno-7-7" name="__codelineno-7-7" href="#cost_optimization-cost_opt_networking-__codelineno-7-7"></a><span class="hll"><span class="w">  </span><span class="nt">trafficPolicy</span><span class="p">:</span>
</span><a id="__codelineno-7-8" name="__codelineno-7-8" href="#cost_optimization-cost_opt_networking-__codelineno-7-8"></a><span class="hll"><span class="w">    </span><span class="nt">loadBalancer</span><span class="p">:</span><span class="w">                        </span>
</span><a id="__codelineno-7-9" name="__codelineno-7-9" href="#cost_optimization-cost_opt_networking-__codelineno-7-9"></a><span class="hll"><span class="w">      </span><span class="nt">localityLbSetting</span><span class="p">:</span>
</span><a id="__codelineno-7-10" name="__codelineno-7-10" href="#cost_optimization-cost_opt_networking-__codelineno-7-10"></a><span class="hll"><span class="w">        </span><span class="nt">distribute</span><span class="p">:</span>
</span><a id="__codelineno-7-11" name="__codelineno-7-11" href="#cost_optimization-cost_opt_networking-__codelineno-7-11"></a><span class="hll"><span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">from</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">eu-west-1/eu-west-1a/</span><span class="w">    </span>
</span><a id="__codelineno-7-12" name="__codelineno-7-12" href="#cost_optimization-cost_opt_networking-__codelineno-7-12"></a><span class="w">          </span><span class="nt">to</span><span class="p">:</span>
<a id="__codelineno-7-13" name="__codelineno-7-13" href="#cost_optimization-cost_opt_networking-__codelineno-7-13"></a><span class="w">            </span><span class="s">&quot;eu-west-1/eu-west-1a/*&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">70</span><span class="w"> </span>
<a id="__codelineno-7-14" name="__codelineno-7-14" href="#cost_optimization-cost_opt_networking-__codelineno-7-14"></a><span class="w">            </span><span class="s">&quot;eu-west-1/eu-west-1b/*&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">20</span>
<a id="__codelineno-7-15" name="__codelineno-7-15" href="#cost_optimization-cost_opt_networking-__codelineno-7-15"></a><span class="w">            </span><span class="s">&quot;eu-west-1/eu-west-1c/*&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10</span>
<a id="__codelineno-7-16" name="__codelineno-7-16" href="#cost_optimization-cost_opt_networking-__codelineno-7-16"></a><span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">from</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">eu-west-1/eu-west-1b/*</span><span class="w">    </span>
<a id="__codelineno-7-17" name="__codelineno-7-17" href="#cost_optimization-cost_opt_networking-__codelineno-7-17"></a><span class="w">          </span><span class="nt">to</span><span class="p">:</span>
<a id="__codelineno-7-18" name="__codelineno-7-18" href="#cost_optimization-cost_opt_networking-__codelineno-7-18"></a><span class="w">            </span><span class="s">&quot;eu-west-1/eu-west-1a/*&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">20</span><span class="w"> </span>
<a id="__codelineno-7-19" name="__codelineno-7-19" href="#cost_optimization-cost_opt_networking-__codelineno-7-19"></a><span class="w">            </span><span class="s">&quot;eu-west-1/eu-west-1b/*&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">70</span>
<a id="__codelineno-7-20" name="__codelineno-7-20" href="#cost_optimization-cost_opt_networking-__codelineno-7-20"></a><span class="w">            </span><span class="s">&quot;eu-west-1/eu-west-1c/*&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10</span>
<a id="__codelineno-7-21" name="__codelineno-7-21" href="#cost_optimization-cost_opt_networking-__codelineno-7-21"></a><span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">from</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">eu-west-1/eu-west-1c/*</span><span class="w">    </span>
<a id="__codelineno-7-22" name="__codelineno-7-22" href="#cost_optimization-cost_opt_networking-__codelineno-7-22"></a><span class="w">          </span><span class="nt">to</span><span class="p">:</span>
<a id="__codelineno-7-23" name="__codelineno-7-23" href="#cost_optimization-cost_opt_networking-__codelineno-7-23"></a><span class="w">            </span><span class="s">&quot;eu-west-1/eu-west-1a/*&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">20</span><span class="w"> </span>
<a id="__codelineno-7-24" name="__codelineno-7-24" href="#cost_optimization-cost_opt_networking-__codelineno-7-24"></a><span class="w">            </span><span class="s">&quot;eu-west-1/eu-west-1b/*&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10</span>
<a id="__codelineno-7-25" name="__codelineno-7-25" href="#cost_optimization-cost_opt_networking-__codelineno-7-25"></a><span class="w">            </span><span class="s">&quot;eu-west-1/eu-west-1c/*&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">70**</span>
<a id="__codelineno-7-26" name="__codelineno-7-26" href="#cost_optimization-cost_opt_networking-__codelineno-7-26"></a><span class="w">    </span><span class="nt">connectionPool</span><span class="p">:</span>
<a id="__codelineno-7-27" name="__codelineno-7-27" href="#cost_optimization-cost_opt_networking-__codelineno-7-27"></a><span class="w">      </span><span class="nt">http</span><span class="p">:</span>
<a id="__codelineno-7-28" name="__codelineno-7-28" href="#cost_optimization-cost_opt_networking-__codelineno-7-28"></a><span class="w">        </span><span class="nt">http2MaxRequests</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10</span>
<a id="__codelineno-7-29" name="__codelineno-7-29" href="#cost_optimization-cost_opt_networking-__codelineno-7-29"></a><span class="w">        </span><span class="nt">maxRequestsPerConnection</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10</span>
<a id="__codelineno-7-30" name="__codelineno-7-30" href="#cost_optimization-cost_opt_networking-__codelineno-7-30"></a><span class="w">    </span><span class="nt">outlierDetection</span><span class="p">:</span>
<a id="__codelineno-7-31" name="__codelineno-7-31" href="#cost_optimization-cost_opt_networking-__codelineno-7-31"></a><span class="w">      </span><span class="nt">consecutiveGatewayErrors</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<a id="__codelineno-7-32" name="__codelineno-7-32" href="#cost_optimization-cost_opt_networking-__codelineno-7-32"></a><span class="w">      </span><span class="nt">interval</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1m</span>
<a id="__codelineno-7-33" name="__codelineno-7-33" href="#cost_optimization-cost_opt_networking-__codelineno-7-33"></a><span class="w">      </span><span class="nt">baseEjectionTime</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">30s</span>
</code></pre></div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The minimum weight that can be distributed destination is 1%. The reason for this is to maintain failover regions and zones in the case that the endpoints in the main destination become unhealthy or unavailable.</p>
</div>
<p>The diagram below depicts a scenario in which there is a highly available load balancer in the <em>eu-west-1</em> region and locality weighted distribution is applied. The Destination Rule policy for this diagram is configured to send 60% of traffic coming from <em>eu-west-1a</em> to Pods in the same AZ, whereas 40% of the traffic from <em>eu-west-1a</em> should go to Pods in eu-west-1b. </p>
<p><img alt="Istio Traffic Control" src="../images/istio-traffic-control.png" /></p>
<h3 id="cost_optimization-cost_opt_networking-restricting-traffic-to-availability-zones-and-nodes">Restricting Traffic to Availability Zones and Nodes<a class="headerlink" href="#cost_optimization-cost_opt_networking-restricting-traffic-to-availability-zones-and-nodes" title="Permanent link">&para;</a></h3>
<p><strong>Using the Service Internal Traffic Policy with Istio</strong></p>
<p>To mitigate network costs associated with <em>external</em> incoming traffic and <em>internal</em> traffic between Pods, you can combine Istio’s Destination Rules and the Kubernetes Service <em>internal traffic policy</em>.  The way to combine Istio destination rules with the service internal traffic policy will largely depend on 3 things:</p>
<ul>
<li>The role of the microservices</li>
<li>Network traffic patterns across the microservices</li>
<li>How the microservices should be deployed across the Kubernetes cluster topology</li>
</ul>
<p>The diagram below shows what the network flow would look like in the case of a nested request and how the aforementioned policies would control the traffic.</p>
<p><img alt="External and Internal traffic policy" src="../images/external-and-internal-traffic-policy.png" /></p>
<ol>
<li>The end user makes a request to <strong>APP A,</strong> which in turn makes a nested request to <strong>APP C</strong>. This request is first sent to a highly available load balancer, which has instances in AZ 1 and AZ 2 as the above diagram shows.</li>
<li>The external incoming request is then routed to the correct destination by the Istio Virtual Service.</li>
<li>After the request is routed, the Istio Destination Rule controls how much traffic goes to the respective AZs based on where it originated from (AZ 1 or AZ 2). </li>
<li>The traffic then goes to the Service for <strong>APP A</strong>, and is then proxied to the respective Pod endpoints. As shown in the diagram, 80% of the incoming traffic is sent to Pod endpoints in AZ 1, and 20% of the incoming traffic is sent to AZ 2.</li>
<li><strong>APP A</strong> then makes an internal request to <strong>APP C</strong>. <strong>APP C</strong>'s Service has an internal traffic policy enabled (<code>internalTrafficPolicy``: Local</code>). </li>
<li>The internal request from <strong>APP A</strong> (on <em>NODE 1</em>) to <strong>APP C</strong> is successful because of the available node-local endpoint for <strong>APP C</strong>. </li>
<li>The internal request from <strong>APP A</strong> (on <em>NODE 3) to</em> <strong>APP C</strong> fails because there are no available <em>node-local endpoints</em> for <strong>APP C</strong>. As the diagram shows, APP C has no replicas on NODE 3. **** </li>
</ol>
<p>The screenshots below are captured from a live example of this approach. The first set of screenshots demonstrate a successful external request to a <code>graphql</code> and a successful nested request from the <code>graphql</code> to a co-located <code>orders</code> replica on the node <code>ip-10-0-0-151.af-south-1.compute.internal</code>. </p>
<p><img alt="Before" src="../images/before.png" />
<img alt="Before results" src="../images/before-results.png" /></p>
<p>With Istio, you can verify and export the statistics of any <a href="https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/intro/terminology">upstream clusters</a> and endpoints that your proxies are aware of. This can help provide a picture of the network flow as well as the share of distribution among the services of a workload. Continuing with the same example, the <code>orders</code> endpoints that the <code>graphql</code> proxy is aware of can be obtained using the following command:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1" href="#cost_optimization-cost_opt_networking-__codelineno-8-1"></a>kubectl<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>-it<span class="w"> </span>deploy/graphql<span class="w"> </span>-n<span class="w"> </span>ecommerce<span class="w"> </span>-c<span class="w"> </span>istio-proxy<span class="w"> </span>--<span class="w"> </span>curl<span class="w"> </span>localhost:15000/clusters<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>orders<span class="w"> </span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-9-1" name="__codelineno-9-1" href="#cost_optimization-cost_opt_networking-__codelineno-9-1"></a>...
<a id="__codelineno-9-2" name="__codelineno-9-2" href="#cost_optimization-cost_opt_networking-__codelineno-9-2"></a>orders-service.ecommerce.svc.cluster.local::10.0.1.33:3003::**rq_error::0**
<a id="__codelineno-9-3" name="__codelineno-9-3" href="#cost_optimization-cost_opt_networking-__codelineno-9-3"></a>orders-service.ecommerce.svc.cluster.local::10.0.1.33:3003::**rq_success::119**
<a id="__codelineno-9-4" name="__codelineno-9-4" href="#cost_optimization-cost_opt_networking-__codelineno-9-4"></a>orders-service.ecommerce.svc.cluster.local::10.0.1.33:3003::**rq_timeout::0**
<a id="__codelineno-9-5" name="__codelineno-9-5" href="#cost_optimization-cost_opt_networking-__codelineno-9-5"></a>orders-service.ecommerce.svc.cluster.local::10.0.1.33:3003::**rq_total::119**
<a id="__codelineno-9-6" name="__codelineno-9-6" href="#cost_optimization-cost_opt_networking-__codelineno-9-6"></a>orders-service.ecommerce.svc.cluster.local::10.0.1.33:3003::**health_flags::healthy**
<a id="__codelineno-9-7" name="__codelineno-9-7" href="#cost_optimization-cost_opt_networking-__codelineno-9-7"></a>orders-service.ecommerce.svc.cluster.local::10.0.1.33:3003::**region::af-south-1**
<a id="__codelineno-9-8" name="__codelineno-9-8" href="#cost_optimization-cost_opt_networking-__codelineno-9-8"></a>orders-service.ecommerce.svc.cluster.local::10.0.1.33:3003::**zone::af-south-1b**
<a id="__codelineno-9-9" name="__codelineno-9-9" href="#cost_optimization-cost_opt_networking-__codelineno-9-9"></a>...
</code></pre></div>
<p>In this case, the <code>graphql</code> proxy is only aware of the <code>orders</code> endpoint for the replica that it shares a node with. If you remove the <code>internalTrafficPolicy: Local</code> setting from the orders Service, and re-run a command like the one above, then the results will return all the endpoints of the replicas spread across the different nodes. Furthermore, by examining the <code>rq_total</code> for the respective endpoints, you'll notice a relatively even share in network distribution. Consequently, if the endpoints are associated with upstream services running in different AZs, then this network distribution across zones will result in higher costs.</p>
<p>As mentioned in a previous section above, you can co-locate frequently communicating Pods by making use of pod-affinity.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-10-1" name="__codelineno-10-1" href="#cost_optimization-cost_opt_networking-__codelineno-10-1"></a><span class="nn">...</span>
<a id="__codelineno-10-2" name="__codelineno-10-2" href="#cost_optimization-cost_opt_networking-__codelineno-10-2"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-10-3" name="__codelineno-10-3" href="#cost_optimization-cost_opt_networking-__codelineno-10-3"></a><span class="nn">...</span>
<a id="__codelineno-10-4" name="__codelineno-10-4" href="#cost_optimization-cost_opt_networking-__codelineno-10-4"></a><span class="w">  </span><span class="nt">template</span><span class="p">:</span>
<a id="__codelineno-10-5" name="__codelineno-10-5" href="#cost_optimization-cost_opt_networking-__codelineno-10-5"></a><span class="w">    </span><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-10-6" name="__codelineno-10-6" href="#cost_optimization-cost_opt_networking-__codelineno-10-6"></a><span class="w">      </span><span class="nt">labels</span><span class="p">:</span>
<a id="__codelineno-10-7" name="__codelineno-10-7" href="#cost_optimization-cost_opt_networking-__codelineno-10-7"></a><span class="w">        </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">graphql</span>
<a id="__codelineno-10-8" name="__codelineno-10-8" href="#cost_optimization-cost_opt_networking-__codelineno-10-8"></a><span class="w">        </span><span class="nt">role</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">api</span>
<a id="__codelineno-10-9" name="__codelineno-10-9" href="#cost_optimization-cost_opt_networking-__codelineno-10-9"></a><span class="w">        </span><span class="nt">workload</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ecommerce</span>
<a id="__codelineno-10-10" name="__codelineno-10-10" href="#cost_optimization-cost_opt_networking-__codelineno-10-10"></a><span class="w">    </span><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-10-11" name="__codelineno-10-11" href="#cost_optimization-cost_opt_networking-__codelineno-10-11"></a><span class="hll"><span class="w">      </span><span class="nt">affinity</span><span class="p">:</span>
</span><a id="__codelineno-10-12" name="__codelineno-10-12" href="#cost_optimization-cost_opt_networking-__codelineno-10-12"></a><span class="hll"><span class="w">        </span><span class="nt">podAffinity</span><span class="p">:</span>
</span><a id="__codelineno-10-13" name="__codelineno-10-13" href="#cost_optimization-cost_opt_networking-__codelineno-10-13"></a><span class="hll"><span class="w">          </span><span class="nt">requiredDuringSchedulingIgnoredDuringExecution</span><span class="p">:</span>
</span><a id="__codelineno-10-14" name="__codelineno-10-14" href="#cost_optimization-cost_opt_networking-__codelineno-10-14"></a><span class="hll"><span class="w">          </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">labelSelector</span><span class="p">:</span>
</span><a id="__codelineno-10-15" name="__codelineno-10-15" href="#cost_optimization-cost_opt_networking-__codelineno-10-15"></a><span class="hll"><span class="w">              </span><span class="nt">matchExpressions</span><span class="p">:</span>
</span><a id="__codelineno-10-16" name="__codelineno-10-16" href="#cost_optimization-cost_opt_networking-__codelineno-10-16"></a><span class="hll"><span class="w">              </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">app</span>
</span><a id="__codelineno-10-17" name="__codelineno-10-17" href="#cost_optimization-cost_opt_networking-__codelineno-10-17"></a><span class="hll"><span class="w">                </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">In</span>
</span><a id="__codelineno-10-18" name="__codelineno-10-18" href="#cost_optimization-cost_opt_networking-__codelineno-10-18"></a><span class="hll"><span class="w">                </span><span class="nt">values</span><span class="p">:</span>
</span><a id="__codelineno-10-19" name="__codelineno-10-19" href="#cost_optimization-cost_opt_networking-__codelineno-10-19"></a><span class="hll"><span class="w">                </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">orders</span>
</span><a id="__codelineno-10-20" name="__codelineno-10-20" href="#cost_optimization-cost_opt_networking-__codelineno-10-20"></a><span class="hll"><span class="w">            </span><span class="nt">topologyKey</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;kubernetes.io/hostname&quot;</span>
</span><a id="__codelineno-10-21" name="__codelineno-10-21" href="#cost_optimization-cost_opt_networking-__codelineno-10-21"></a><span class="w">      </span><span class="nt">nodeSelector</span><span class="p">:</span>
<a id="__codelineno-10-22" name="__codelineno-10-22" href="#cost_optimization-cost_opt_networking-__codelineno-10-22"></a><span class="w">        </span><span class="nt">managedBy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">karpenter</span>
<a id="__codelineno-10-23" name="__codelineno-10-23" href="#cost_optimization-cost_opt_networking-__codelineno-10-23"></a><span class="w">        </span><span class="nt">billing-team</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ecommerce</span>
<a id="__codelineno-10-24" name="__codelineno-10-24" href="#cost_optimization-cost_opt_networking-__codelineno-10-24"></a><span class="nn">...</span>
</code></pre></div>
<p>When the <code>graphql</code> and <code>orders</code> replicas don't co-exist on the same node (<code>ip-10-0-0-151.af-south-1.compute.internal</code>), the first request to <code>graphql</code> is successful as noted by the <code>200 response code</code> in the Postman screenshot below, whereas the second nested request from <code>graphql</code> to <code>orders</code> fails with a <code>503 response code</code>. </p>
<p><img alt="After" src="../images/after.png" />
<img alt="After results" src="../images/after-results.png" /></p>
<h2 id="cost_optimization-cost_opt_networking-additional-resources">Additional Resources<a class="headerlink" href="#cost_optimization-cost_opt_networking-additional-resources" title="Permanent link">&para;</a></h2>
<ul>
<li><a href="https://aws.amazon.com/blogs/containers/addressing-latency-and-data-transfer-costs-on-eks-using-istio/">Addressing latency and data transfer costs on EKS using Istio</a></li>
<li><a href="https://aws.amazon.com/blogs/containers/exploring-the-effect-of-topology-aware-hints-on-network-traffic-in-amazon-elastic-kubernetes-service/">Exploring the effect of Topology Aware Hints on network traffic in Amazon Elastic Kubernetes Service</a></li>
<li><a href="https://aws.amazon.com/blogs/containers/getting-visibility-into-your-amazon-eks-cross-az-pod-to-pod-network-bytes/">Getting visibility into your Amazon EKS Cross-AZ pod to pod network bytes</a></li>
<li><a href="https://youtu.be/EkpdKVm9kQY">Optimize AZ Traffic with Istio</a></li>
<li><a href="https://youtu.be/KFgE_lNVfz4">Optimize AZ Traffic with Topology Aware Routing</a></li>
<li><a href="https://youtu.be/-uiF_zixEro">Optimize Kubernetes Cost &amp; Performance with Service Internal Traffic Policy</a></li>
<li><a href="https://youtu.be/edSgEe7Rihc">Optimize Kubernetes Cost &amp; Performance with Istio and Service Internal Traffic Policy</a></li>
<li><a href="https://aws.amazon.com/blogs/architecture/overview-of-data-transfer-costs-for-common-architectures/">Overview of Data Transfer Costs for Common Architectures</a> </li>
<li><a href="https://aws.amazon.com/blogs/containers/understanding-data-transfer-costs-for-aws-container-services/">Understanding data transfer costs for AWS container services</a></li>
</ul></section><section class="print-page" id="cost_optimization-cost_opt_storage"><h1 id="cost_optimization-cost_opt_storage-cost-optimization-storage">Cost Optimization - Storage<a class="headerlink" href="#cost_optimization-cost_opt_storage-cost-optimization-storage" title="Permanent link">&para;</a></h1>
<h2 id="cost_optimization-cost_opt_storage-overview">Overview<a class="headerlink" href="#cost_optimization-cost_opt_storage-overview" title="Permanent link">&para;</a></h2>
<p>There are scenarios where you may want to run applications that need to preserve data for a short or long term basis. For such use cases, volumes can be defined and mounted by Pods so that their containers can tap into different storage mechanisms. Kubernetes supports different types of <a href="https://kubernetes.io/docs/concepts/storage/volumes/">volumes</a> for ephemeral and persistent storage. The choice of storage largely depends on application requirements. For each approach, there are cost implications, and the practices detailed below which will help you accomplish cost efficiency for workloads needing some form of storage in your EKS environments. </p>
<h2 id="cost_optimization-cost_opt_storage-ephemeral-volumes">Ephemeral Volumes<a class="headerlink" href="#cost_optimization-cost_opt_storage-ephemeral-volumes" title="Permanent link">&para;</a></h2>
<p>Ephemeral volumes are for applications that require transient local volumes but don't require data to be persisted after restarts. Examples of this include requirements for scratch space, caching, and read-only input data like configuration data and secrets. You can find more details of Kubernetes ephemeral volumes <a href="https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/">here</a>. Most of ephemeral volumes (e.g. emptyDir, configMap, downwardAPI, secret, hostpath) are backed by locally-attached writable devices (usually the root disk) or RAM, so it's important to choose the most cost efficient and performant host volume. </p>
<h3 id="cost_optimization-cost_opt_storage-using-ebs-volumes">Using EBS Volumes<a class="headerlink" href="#cost_optimization-cost_opt_storage-using-ebs-volumes" title="Permanent link">&para;</a></h3>
<p><em>We recommend starting with <a href="https://aws.amazon.com/ebs/general-purpose/">gp3</a> as the host root volume.</em> It is the latest general purpose SSD volume offered by Amazon EBS and also offers a lower price (up to 20%) per GB compared to gp2 volumes. </p>
<h3 id="cost_optimization-cost_opt_storage-using-amazon-ec2-instance-stores">Using Amazon EC2 Instance Stores<a class="headerlink" href="#cost_optimization-cost_opt_storage-using-amazon-ec2-instance-stores" title="Permanent link">&para;</a></h3>
<p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html">Amazon EC2 instance stores</a> provide temporary block-level storage for your EC2 instances. The storage provided by EC2 instance stores is accessible through disks that are physically attached to the hosts. Unlike Amazon EBS, you can only attach instance store volumes when the instance is launched, and these volumes only exist during the lifetime of the instance. They cannot be detached and re-attached to other instances. You can learn more about Amazon EC2 instance stores <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html">here</a>. <em>There are no additional fees associated with an instance store volume.</em> This makes them (instance store volumes) <em>more cost efficient</em> than the the general EC2 instances with large EBS volumes. </p>
<p>To use local store volumes in Kubernetes, you should partition, configure, and format the disks <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-add-user-data.html">using the Amazon EC2 user-data</a> so that volumes can be mounted as a <a href="https://kubernetes.io/docs/concepts/storage/volumes/#hostpath">HostPath</a> in the pod spec. Alternatively, you can leverage the <a href="https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner">Local Persistent Volume Static Provisioner</a> to simplify local storage management. The Local Persistent Volume static provisioner allows you to access local instance store volumes through the standard Kubernetes PersistentVolumeClaim (PVC) interface. Furthermore, it will provision PersistentVolumes (PVs) that contains node affinity information to schedule Pods to the correct nodes. Although it uses Kubernetes PersistentVolumes, EC2 instance store volumes are ephemeral in nature. Data written to ephemeral disks is only available during the instance’s lifetime. When the instance is terminated, so is the data. Please refer to this <a href="https://aws.amazon.com/blogs/containers/eks-persistent-volumes-for-instance-store/">blog</a> for more details.</p>
<p>Keep in mind that when using Amazon EC2 instance store volumes, the total IOPS limit is shared with the host and it binds Pods to a specific host. You should thoroughly review your workload requirements before adopting Amazon EC2 instance store volumes.</p>
<h2 id="cost_optimization-cost_opt_storage-persistent-volumes">Persistent Volumes<a class="headerlink" href="#cost_optimization-cost_opt_storage-persistent-volumes" title="Permanent link">&para;</a></h2>
<p>Kubernetes is typically associated with running stateless applications. However, there are scenarios where you may want to run microservices that need to preserve persistent data or information from one request to the next. Databases are a common example for such use cases. However, Pods, and the containers or processes inside them, are ephemeral in nature. To persist data beyond the lifetime of a Pod, you can use PVs to define access to storage at a specific location that is independent from the Pod. <em>The costs associated with PVs is highly dependent on the type of storage being used and how applications are consuming it.</em> </p>
<p>There are different types of storage options that support Kubernetes PVs on Amazon EKS listed <a href="https://docs.aws.amazon.com/eks/latest/userguide/storage.html">here</a>. The storage options covered below are Amazon EBS, Amazon EFS, Amazon FSx for Lustre, Amazon FSx for NetApp ONTAP.</p>
<h3 id="cost_optimization-cost_opt_storage-amazon-elastic-block-store-ebs-volumes">Amazon Elastic Block Store (EBS) Volumes<a class="headerlink" href="#cost_optimization-cost_opt_storage-amazon-elastic-block-store-ebs-volumes" title="Permanent link">&para;</a></h3>
<p>Amazon EBS volumes can be consumed as Kubernetes PVs to provide block-level storage volumes. These are well suited for databases that rely on random reads &amp; writes and throughput-intensive applications that perform long, continuous reads and writes. <a href="https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html">The Amazon Elastic Block Store Container Storage Interface (CSI) driver</a> allows Amazon EKS clusters to manage the lifecycle of Amazon EBS volumes for persistent volumes. The Container Storage Interface enables and facilitates interaction between Kubernetes and a storage system. When a CSI driver is deployed to your EKS cluster, you can access it’s capabilities through the native Kubernetes storage resources such as Persistent Volumes (PVs), Persistent Volume Claims (PVCs) and Storage Classes (SCs). This <a href="https://github.com/kubernetes-sigs/aws-ebs-csi-driver/tree/master/examples/kubernetes">link</a> provides practical examples of how to interact with Amazon EBS volumes with Amazon EBS CSI driver.</p>
<h4 id="cost_optimization-cost_opt_storage-choosing-the-right-volume">Choosing the right volume<a class="headerlink" href="#cost_optimization-cost_opt_storage-choosing-the-right-volume" title="Permanent link">&para;</a></h4>
<p><em>We recommend using the latest generation of block storage (gp3) as it provides the right balance between price and performance</em>. It also allows you to scale volume IOPS and throughput independently of volume size without needing to provision additional block storage capacity. If you’re currently using gp2 volumes, we highly recommend migrating to gp3 volumes. This <a href="https://aws.amazon.com/blogs/containers/migrating-amazon-eks-clusters-from-gp2-to-gp3-ebs-volumes/">blog</a> explains how to migrate from <em>gp2</em> on <em>gp3</em> on Amazon EKS clusters. </p>
<p>When you have applications that require higher performance and need volumes larger than what a single <a href="https://aws.amazon.com/ebs/general-purpose/">gp3 volume can support</a>, you should consider using <a href="https://aws.amazon.com/ebs/provisioned-iops/">io2 block express</a>. This type of storage is ideal for your largest, most I/O intensive, and mission critical deployment such as SAP HANA or other large databases with low latency requirements. Keep in mind that an instance's EBS performance is bounded by the instance's performance limits, so not all the instances support io2 block express volumes. You can check the supported instance types and other considerations in this <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/provisioned-iops.html">doc</a>. </p>
<p><em>A single gp3 volume can support up to up to 16,000 max IOPS, 1,000 MiB/s max throughput, max 16TiB. The latest generation of Provisioned IOPS SSD volume that provides up to 256,000 IOPS, 4,000 MiB/s, throughput, and 64TiB.</em></p>
<p>Among these options, you should best tailor your storage performance and cost to the needs of your applications.</p>
<h4 id="cost_optimization-cost_opt_storage-monitor-and-optimize-over-time">Monitor and optimize over time<a class="headerlink" href="#cost_optimization-cost_opt_storage-monitor-and-optimize-over-time" title="Permanent link">&para;</a></h4>
<p>It's important to understand your application's baseline performance and monitor it for selected volumes to check if it's meeting your requirements/expectations or if it's over-provisioned (e.g. a scenario where provisioned IOPS are not being fully utilized). </p>
<p>Instead of allocating a large volume from the beginning, you can gradually increase the size of the volume as you accumulate data. You can dynamically re-size volumes using the <a href="https://github.com/kubernetes-sigs/aws-ebs-csi-driver/tree/master/examples/kubernetes/resizing">volume resizing</a> feature in the Amazon Elastic Block Store CSI driver (aws-ebs-csi-driver). <em>Keep in mind that you can only increase the EBS volume size.</em></p>
<p>To identify and remove any dangling EBS volumes, you can use <a href="https://docs.aws.amazon.com/awssupport/latest/user/cost-optimization-checks.html">AWS trusted advisor’s cost optimization category</a>. This feature helps you identify unattached volumes or volumes with very low write activity for a period of time. There is a cloud-native open-source, read-only tool called <a href="https://github.com/derailed/popeye">Popeye</a> that scans live Kubernetes clusters and reports potential issues with deployed resources and configurations. For example, it can scan for unused PVs and PVCs and check whether they are bound or whether there is any volume mount error.</p>
<p>For a deep dive on monitoring, please refer to the <a href="https://aws.github.io/aws-eks-best-practices/cost_optimization/cost_opt_observability/">EKS cost optimization observability guide</a>.  </p>
<p>One other option you can consider is the <a href="https://docs.aws.amazon.com/compute-optimizer/latest/ug/view-ebs-recommendations.html">AWS Compute Optimizer Amazon EBS volume recommendations</a>. This tool automatically identifies the optimal volume configuration and correct level of performance needed. For example, it can be used for optimal settings pertaining to provisioned IOPS, volume sizes, and types of EBS volumes based on the maximum utilization during the past 14 days. It also quantifies the potential monthly cost savings derived from its recommendations. You can review this <a href="https://aws.amazon.com/blogs/storage/cost-optimizing-amazon-ebs-volumes-using-aws-compute-optimizer/">blog</a> for more details.</p>
<h4 id="cost_optimization-cost_opt_storage-backup-retention-policy">Backup retention policy<a class="headerlink" href="#cost_optimization-cost_opt_storage-backup-retention-policy" title="Permanent link">&para;</a></h4>
<p>You can back up the data on your Amazon EBS volumes by taking point-in-time snapshots. The Amazon EBS CSI driver supports volume snapshots. You can learn how to create a snapshot and restore an EBS PV using the steps outlined <a href="https://github.com/kubernetes-sigs/aws-ebs-csi-driver/blob/master/examples/kubernetes/snapshot/README.md">here</a>. </p>
<p>Subsequent snapshots are incremental backups, meaning that only the blocks on the device that have changed after your most recent snapshot are saved. This minimizes the time required to create the snapshot and saves on storage costs by not duplicating data. However, growing the number of old EBS snapshots without a proper retention policy can cause unexpected costs when operating at scale. If you’re directly backing up Amazon EBS volumes through AWS API, you can leverage <a href="https://aws.amazon.com/ebs/data-lifecycle-manager/">Amazon Data Lifecycle Manager</a> (DLM) that provides an automated, policy-based lifecycle management solution for Amazon Elastic Block Store (EBS) Snapshots and EBS-backed Amazon Machine Images (AMIs). The console makes it easier to automate the creation, retention, and deletion of EBS Snapshots and AMIs. </p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>There is currently no way to make use of Amazon DLM via the Amazon EBS CSI driver.</p>
</div>
<p>In a Kubernetes environment, you can leverage an open-source tool called <a href="https://velero.io/">Velero</a> to backup your EBS Persistent Volumes. You can set a TTL flag when scheduling the job to expire backups. Here is a <a href="https://velero.io/docs/v1.12/how-velero-works/#set-a-backup-to-expire">guide</a> from Velero as an example. </p>
<h3 id="cost_optimization-cost_opt_storage-amazon-elastic-file-system-efs">Amazon Elastic File System (EFS)<a class="headerlink" href="#cost_optimization-cost_opt_storage-amazon-elastic-file-system-efs" title="Permanent link">&para;</a></h3>
<p><a href="https://aws.amazon.com/efs/">Amazon Elastic File System (EFS)</a> is a serverless, fully elastic file system that lets you share file data using standard file system interface and file system semantics for a broad spectrum of workloads and applications. Examples of workloads and applications include Wordpress and Drupal, developer tools like JIRA and Git, and shared notebook system such as Jupyter as well as home directories.</p>
<p>One of main benefits of Amazon EFS is that it can be mounted by multiple containers spread across multiple nodes and multiple availability zones. Another benefit is that you only pay for the storage you use. EFS file systems will automatically grow and shrink as you add and remove files which eliminates the need for capacity planning. </p>
<p>To use Amazon EFS in Kubernetes, you need to use the Amazon Elastic File System Container Storage Interface (CSI) Driver, <a href="https://github.com/kubernetes-sigs/aws-efs-csi-driver">aws-efs-csi-driver</a>. Currently, the driver can dynamically create <a href="https://docs.aws.amazon.com/efs/latest/ug/efs-access-points.html">access points</a>. However, the Amazon EFS file system has to be provisioned first and provided as an input to the Kubernetes storage class parameter. </p>
<h4 id="cost_optimization-cost_opt_storage-choosing-the-right-efs-storage-class">Choosing the right EFS storage class<a class="headerlink" href="#cost_optimization-cost_opt_storage-choosing-the-right-efs-storage-class" title="Permanent link">&para;</a></h4>
<p>Amazon EFS offers <a href="https://docs.aws.amazon.com/efs/latest/ug/storage-classes.html">four storage classes</a>. </p>
<p>Two standard storage classes:</p>
<ul>
<li>Amazon EFS Standard </li>
<li><a href="https://aws.amazon.com/blogs/aws/optimize-storage-cost-with-reduced-pricing-for-amazon-efs-infrequent-access/">Amazon EFS Standard-Infrequent Access</a> (EFS Standard-IA) </li>
</ul>
<p>Two one-zone storage classes: </p>
<ul>
<li><a href="https://aws.amazon.com/blogs/aws/new-lower-cost-one-zone-storage-classes-for-amazon-elastic-file-system/">Amazon EFS One Zone</a> </li>
<li>Amazon EFS One Zone-Infrequent Access (EFS One Zone-IA)</li>
</ul>
<p>The Infrequent Access (IA) storage classes are cost-optimized for files that are not accessed every day. With Amazon EFS lifecycle management, you can move files that have not been accessed for the duration of the lifecycle policy (7, 14, 30, 60, or 90 days) to the IA storage classes <em>which can reduce the storage cost by up to 92 percent compared to EFS Standard and EFS One Zone storage classes respectively</em>. </p>
<p>With EFS Intelligent-Tiering, lifecycle management monitors the access patterns of your file system and automatically move files to the most optimal storage class. </p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>aws-efs-csi-driver currently doesn’t have a control on changing storage classes, lifecycle management or Intelligent-Tiering. Those should be setup manually in the AWS console or through the EFS APIs.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>aws-efs-csi-driver isn’t compatible with Window-based container images.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>There is a known memory issue when <em>vol-metrics-opt-in</em> (to emit volume metrics) is enabled due to the <a href="https://github.com/kubernetes/kubernetes/blob/ee265c92fec40cd69d1de010b477717e4c142492/pkg/volume/util/fs/fs.go#L66">DiskUsage</a> function that consumes an amount of memory that is proportional to the size of your filesystem. <em>Currently, we recommend to disable the</em> <em><code>--vol-metrics-opt-in</code> option on large filesystems to avoid consuming too much memory. Here is a github issue <a href="https://github.com/kubernetes-sigs/aws-efs-csi-driver/issues/1104">link</a> for more details.</em></p>
</div>
<h3 id="cost_optimization-cost_opt_storage-amazon-fsx-for-lustre">Amazon FSx for Lustre<a class="headerlink" href="#cost_optimization-cost_opt_storage-amazon-fsx-for-lustre" title="Permanent link">&para;</a></h3>
<p>Lustre is a high-performance parallel file system commonly used in workloads requiring throughput up to hundreds of GB/s and sub-millisecond per-operation latencies. It’s used for scenarios such as machine learning training, financial modeling, HPC, and video processing. <a href="https://aws.amazon.com/fsx/lustre/">Amazon FSx for Lustre</a> provides a fully managed shared storage with the scalability and performance, seamlessly integrated with Amazon S3. </p>
<p>You can use Kubernetes persistent storage volumes backed by FSx for Lustre using the <a href="https://github.com/kubernetes-sigs/aws-fsx-csi-driver">FSx for Lustre CSI driver</a> from Amazon EKS or your self-managed Kubernetes cluster on AWS. See the <a href="https://docs.aws.amazon.com/eks/latest/userguide/fsx-csi.html">Amazon EKS documentation</a> for more details and examples. </p>
<h4 id="cost_optimization-cost_opt_storage-link-to-amazon-s3">Link to Amazon S3<a class="headerlink" href="#cost_optimization-cost_opt_storage-link-to-amazon-s3" title="Permanent link">&para;</a></h4>
<p>It's recommended to link a highly durable long-term data repository residing on Amazon S3 with your FSx for Lustre file system. Once linked, large datasets are lazy-loaded as needed from Amazon S3 to FSx for Lustre file systems. You can also run your analyses and your results back to S3, and then delete your [Lustre] file system. </p>
<h4 id="cost_optimization-cost_opt_storage-choosing-the-right-deployment-and-storage-options">Choosing the right deployment and storage options<a class="headerlink" href="#cost_optimization-cost_opt_storage-choosing-the-right-deployment-and-storage-options" title="Permanent link">&para;</a></h4>
<p>FSx for Lustre provides different deployment options. The first option is called <em>scratch</em> and it doesn’t replicate data, while the second option is called <em>persistent</em> which, as the name implies, persists data. </p>
<p>The first option (<em>scratch</em>) can be used <em>to reduce the cost of temporary shorter-term data processing.</em> The persistent deployment option <em>is designed for longer-term storage</em> that automatically replicates data within an AWS Availability Zone. It also supports both SSD and HDD storage. </p>
<p>You can configure the desired deployment type under parameters in the FSx for lustre filesystem’s Kubernetes StorageClass. Here is an <a href="https://github.com/kubernetes-sigs/aws-fsx-csi-driver/tree/master/examples/kubernetes/dynamic_provisioning#edit-storageclass">link</a> that provides sample templates.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For latency-sensitive workloads or workloads requiring the highest levels of IOPS/throughput, you should choose SSD storage. For throughput-focused workloads that aren’t latency-sensitive, you should choose HDD storage.</p>
</div>
<h4 id="cost_optimization-cost_opt_storage-enable-data-compression">Enable data compression<a class="headerlink" href="#cost_optimization-cost_opt_storage-enable-data-compression" title="Permanent link">&para;</a></h4>
<p>You can also enable data compression on your file system by specifying “LZ4” as the Data Compression Type. Once it’s enabled, all newly-written files will be automatically compressed on FSx for Lustre before they are written to disk and uncompressed when they are read. LZ4 data compression algorithm is lossless so the original data can be fully reconstructed from the compressed data. </p>
<p>You can configure the data compression type as LZ4 under parameters in the FSx for lustre filesystem’s Kubernetes StorageClass. Compression is disabled when the value is set to NONE, which is default. This <a href="https://github.com/kubernetes-sigs/aws-fsx-csi-driver/tree/master/examples/kubernetes/dynamic_provisioning#edit-storageclass">link</a> provides sample templates.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Amazon FSx for Lustre isn’t compatible with Window-based container images.</p>
</div>
<h3 id="cost_optimization-cost_opt_storage-amazon-fsx-for-netapp-ontap">Amazon FSx for NetApp ONTAP<a class="headerlink" href="#cost_optimization-cost_opt_storage-amazon-fsx-for-netapp-ontap" title="Permanent link">&para;</a></h3>
<p><a href="https://aws.amazon.com/fsx/netapp-ontap/">Amazon FSx for NetApp ONTAP</a> is a fully managed shared storage built on NetApp’s ONTAP file system. FSx for ONTAP provides feature-rich, fast, and flexible shared file storage that’s broadly accessible from Linux, Windows, and macOS compute instances running in AWS or on premises. </p>
<p>Amazon FSx for NetApp ONTAP supports two tiers of storage: <em>1/primary tier</em> and <em>2/capacity pool tier.</em> </p>
<p>The <em>primary tier</em> is a provisioned, high-performance SSD-based tier for active, latency-sensitive data. The fully elastic <em>capacity pool tier</em> is cost-optimized for infrequently accessed data, automatically scales as data is tiered to it, and offers virtually unlimited petabytes of capacity. You can enable data compression and deduplication on capacity pool storage and further reduce the amount of storage capacity your data consumes. NetApp’s native, policy-based FabricPool feature continually monitors data access patterns, automatically transferring data bidirectionally between storage tiers to optimize performance and cost.</p>
<p>NetApp's Astra Trident provides dynamic storage orchestration using a CSI driver which allows Amazon EKS clusters to manage the lifecycle of persistent volumes PVs backed by Amazon FSx for NetApp ONTAP file systems. To get started, see <a href="https://docs.netapp.com/us-en/trident/trident-use/trident-fsx.html">Use Astra Trident with Amazon FSx for NetApp ONTAP</a> in the Astra Trident documentation.</p>
<h2 id="cost_optimization-cost_opt_storage-other-considerations">Other considerations<a class="headerlink" href="#cost_optimization-cost_opt_storage-other-considerations" title="Permanent link">&para;</a></h2>
<h3 id="cost_optimization-cost_opt_storage-minimize-the-size-of-container-image">Minimize the size of container image<a class="headerlink" href="#cost_optimization-cost_opt_storage-minimize-the-size-of-container-image" title="Permanent link">&para;</a></h3>
<p>Once containers are deployed, container images are cached on the host as multiple layers. By reducing the size of images, the amount of storage required on the host can be reduced. </p>
<p>By using slimmed-down base images such as <a href="https://hub.docker.com/_/scratch">scratch</a> images or <a href="https://github.com/GoogleContainerTools/distroless">distroless</a> container images (that contain only your application and its runtime dependencies) from the beginning, <em>you can reduce storage cost in addition to other ancillary benefits such as a reducing the attack surface area and shorter image pull times.</em></p>
<p>You should also consider using open source tools, such as <a href="https://www.slim.ai/docs/quickstart">Slim.ai</a> that provides an easy, secure way to create minimal images.</p>
<p>Multiple layers of packages, tools, application dependencies, libraries can easily bloat the container image size. By using multi-stage builds, you can selectively copy artifacts from one stage to another, excluding everything that isn’t necessary from the final image. You can check more image-building best practices <a href="https://docs.docker.com/get-started/09_image_best/">here</a>. </p>
<p>Another thing to consider is how long to persist cached images. You may want to clean up the stale images from the image cache when a certain amount of disk is utilized. Doing so will help make sure you have enough space for the host’s operation. By default, the <a href="https://kubernetes.io/docs/reference/generated/kubelet">kubelet</a> performs garbage collection on unused images every five minutes and on unused containers every minute. </p>
<p><em>To configure options for unused container and image garbage collection, tune the kubelet using a <a href="https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/">configuration file</a> and change the parameters related to garbage collection using the <a href="https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/"><code>KubeletConfiguration</code></a> resource type.</em> </p>
<p>You can learn more about it in the Kubernetes <a href="https://kubernetes.io/docs/concepts/architecture/garbage-collection/#containers-images">documentation</a>. </p></section><section class="print-page" id="cost_optimization-cost_opt_observability"><h1 id="cost_optimization-cost_opt_observability-cost-optimization-observability">Cost Optimization - Observability<a class="headerlink" href="#cost_optimization-cost_opt_observability-cost-optimization-observability" title="Permanent link">&para;</a></h1>
<h2 id="cost_optimization-cost_opt_observability-introduction">Introduction<a class="headerlink" href="#cost_optimization-cost_opt_observability-introduction" title="Permanent link">&para;</a></h2>
<p>Observability tools help you efficiently detect, remediate and investigate your workloads. The cost of telemetry data naturally increases as your use of EKS increases. At times, it can be challenging to balance your operational needs and measuring what matters to your business and keeping observability costs in check. This guide focuses on cost optimization strategies for the three pillars of observability: logs, metrics and traces. Each of these best practices can be applied independently to fit your organization’s optimization goals. </p>
<h2 id="cost_optimization-cost_opt_observability-logging">Logging<a class="headerlink" href="#cost_optimization-cost_opt_observability-logging" title="Permanent link">&para;</a></h2>
<p>Logging plays a vital role in monitoring and troubleshooting the applications in your cluster. There are several strategies that can be employed to optimize logging costs. The best practice strategies listed below include examining your log retention policies to implement granular controls on how long log data is kept, sending log data to different storage options based on importance, and utilizing log filtering to narrow down the types of logs messages that are stored. Efficiently managing log telemetry can lead to cost savings for your environments.</p>
<h2 id="cost_optimization-cost_opt_observability-eks-control-plane">EKS Control Plane<a class="headerlink" href="#cost_optimization-cost_opt_observability-eks-control-plane" title="Permanent link">&para;</a></h2>
<h3 id="cost_optimization-cost_opt_observability-optimize-your-control-plane-logs">Optimize Your Control Plane Logs<a class="headerlink" href="#cost_optimization-cost_opt_observability-optimize-your-control-plane-logs" title="Permanent link">&para;</a></h3>
<p>The Kubernetes control plane is a <a href="https://kubernetes.io/docs/concepts/overview/components/#control-plane-components">set of components</a> that manage the clusters and these components send different types of information as log streams to a log group in <a href="https://aws.amazon.com/cloudwatch/">Amazon CloudWatch</a>. While there are benefits to enabling all control plane log types, you should be aware of the information in each log and the associated costs to storing all the log telemetry. You are charged for the standard <a href="https://aws.amazon.com/cloudwatch/pricing/">CloudWatch Logs data ingestion and storage costs for logs</a> sent to Amazon CloudWatch Logs from your clusters. Before enabling them, evaluate whether each log stream is necessary. </p>
<p>For example, in non-production clusters, selectively enable specific log types, such as the api server logs, only for analysis and deactivate afterward. But for production clusters, where you might not be able to reproduce events, and resolving issues requires more log information, then you can enable all log types. Further control plane cost optimization implementation details are in this <a href="https://aws.amazon.com/blogs/containers/understanding-and-cost-optimizing-amazon-eks-control-plane-logs/">blog</a> post. </p>
<h4 id="cost_optimization-cost_opt_observability-stream-logs-to-s3">Stream Logs to S3<a class="headerlink" href="#cost_optimization-cost_opt_observability-stream-logs-to-s3" title="Permanent link">&para;</a></h4>
<p>Another cost optimization best practice is streaming control plane logs to S3 via CloudWatch Logs subscriptions. Leveraging CloudWatch Logs <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Subscriptions.html">subscriptions</a> allows you to selectively forward logs to S3 which provides more cost efficient long term storage compared to retaining logs indefinitely in CloudWatch. For example, for production clusters, you can create a critical log group and leverage subscriptions to stream these logs to S3 after 15 days. This will ensure you have have quick access to the logs for analysis but also save on cost by moving logs to a more cost efficient storage.</p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>As of 9/5/2023 EKS logs are classified as Vended Logs in Amazon CloudWatch Logs. Vended Logs are specific AWS service logs natively published by AWS services on behalf of the customer and available at volume discount pricing. Please visit the <a href="https://aws.amazon.com/cloudwatch/pricing/">Amazon CloudWatch pricing page</a> to learn more about Vended Logs pricing.</p>
</div>
<h2 id="cost_optimization-cost_opt_observability-eks-data-plane">EKS Data Plane<a class="headerlink" href="#cost_optimization-cost_opt_observability-eks-data-plane" title="Permanent link">&para;</a></h2>
<h3 id="cost_optimization-cost_opt_observability-log-retention">Log Retention<a class="headerlink" href="#cost_optimization-cost_opt_observability-log-retention" title="Permanent link">&para;</a></h3>
<p>Amazon CloudWatch’s default retention policy is to keep logs indefinitely and never expire, incurring storage costs applicable to your AWS region. In order to reduce the storage costs, you can customize the retention policy for each log group based on your workload requirements.</p>
<p>In a development environment, a lengthy retention period may not be necessary. But in a production environment, you can set a longer retention policy to meet troubleshooting, compliance, and capacity planning requirements. For example, if you are running an e-commerce application during the peak holiday season the system is under heavier load and issues can arise that may not be immediately noticeable, you will want to set a longer log retention for detailed troubleshooting and post event analysis. </p>
<p>You can <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Working-with-log-groups-and-streams.html#SettingLogRetention">configure your retention periods</a> in the AWS CloudWatch console or <a href="https://docs.aws.amazon.com/cli/latest/reference/logs/put-retention-policy.html">AWS API</a> with the duration from 1 day to 10 years based on each log group. Having a flexible retention period can save log storage costs, while also maintaining critical logs.</p>
<h3 id="cost_optimization-cost_opt_observability-log-storage-options">Log Storage Options<a class="headerlink" href="#cost_optimization-cost_opt_observability-log-storage-options" title="Permanent link">&para;</a></h3>
<p>Storage is a large driver of observability costs therefore it is crucial to optimize your log storage strategy. Your strategies should align with your workloads requirements while maintaining performance and scalability. One strategy to reduce the costs of storing logs is to leverage AWS S3 buckets and its different storage tiers.</p>
<h4 id="cost_optimization-cost_opt_observability-forward-logs-directly-to-s3">Forward logs directly to S3<a class="headerlink" href="#cost_optimization-cost_opt_observability-forward-logs-directly-to-s3" title="Permanent link">&para;</a></h4>
<p>Consider forwarding less critical logs, such as development environments, directly to S3 instead of Cloudwatch. This can have an immediate impact on log storage costs. One option is to forward the logs straight to S3 using Fluentbit. You define this in the <code>[OUTPUT]</code> section, the destination where FluentBit transmits container logs for retention. Review additional configurations parameter <a href="https://docs.fluentbit.io/manual/pipeline/outputs/s3#worker-support">here</a>.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#cost_optimization-cost_opt_observability-__codelineno-0-1"></a>[OUTPUT]
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#cost_optimization-cost_opt_observability-__codelineno-0-2"></a>        Name eks_to_s3
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#cost_optimization-cost_opt_observability-__codelineno-0-3"></a>        Match application.* 
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#cost_optimization-cost_opt_observability-__codelineno-0-4"></a>        bucket $S3_BUCKET name
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#cost_optimization-cost_opt_observability-__codelineno-0-5"></a>        region us-east-2
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#cost_optimization-cost_opt_observability-__codelineno-0-6"></a>        store_dir /var/log/fluentbit
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#cost_optimization-cost_opt_observability-__codelineno-0-7"></a>        total_file_size 30M
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#cost_optimization-cost_opt_observability-__codelineno-0-8"></a>        upload_timeout 3m
</code></pre></div>
<h4 id="cost_optimization-cost_opt_observability-forward-logs-to-cloudwatch-only-for-short-term-analysis">Forward logs to CloudWatch only for short term analysis<a class="headerlink" href="#cost_optimization-cost_opt_observability-forward-logs-to-cloudwatch-only-for-short-term-analysis" title="Permanent link">&para;</a></h4>
<p>For more critical logs, such as a production environments where you might need to perform immediate analysis on the data, consider forwarding the logs to CloudWatch. You define this in the <code>[OUTPUT]</code> section, the destination where FluentBit transmits container logs for retention. Review additional configurations parameter <a href="https://docs.fluentbit.io/manual/pipeline/outputs/cloudwatch">here</a>.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#cost_optimization-cost_opt_observability-__codelineno-1-1"></a>[OUTPUT]
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#cost_optimization-cost_opt_observability-__codelineno-1-2"></a>        Name eks_to_cloudwatch_logs
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#cost_optimization-cost_opt_observability-__codelineno-1-3"></a>        Match application.*
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#cost_optimization-cost_opt_observability-__codelineno-1-4"></a>        region us-east-2
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#cost_optimization-cost_opt_observability-__codelineno-1-5"></a>        log_group_name fluent-bit-cloudwatch
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#cost_optimization-cost_opt_observability-__codelineno-1-6"></a>        log_stream_prefix from-fluent-bit-
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#cost_optimization-cost_opt_observability-__codelineno-1-7"></a>        auto_create_group On
</code></pre></div>
<p>However, this will not have an instant affect on your cost savings. For additional savings, you will have to export these logs to Amazon S3.</p>
<h4 id="cost_optimization-cost_opt_observability-export-to-amazon-s3-from-cloudwatch">Export to Amazon S3 from CloudWatch<a class="headerlink" href="#cost_optimization-cost_opt_observability-export-to-amazon-s3-from-cloudwatch" title="Permanent link">&para;</a></h4>
<p>For storing Amazon CloudWatch logs long term, we recommend exporting your Amazon EKS CloudWatch logs to Amazon Simple Storage Service (Amazon S3). You can forward the logs to Amazon S3 bucket by creating an export task via the <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3ExportTasksConsole.html">Console</a> or the API. After you have done so, Amazon S3 presents many options to further reduce cost. You can define your own <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html">Amazon S3 Lifecycle rules</a> to move your logs to a storage class that a fits your needs, or leverage the <a href="https://aws.amazon.com/s3/storage-classes/intelligent-tiering/">Amazon S3 Intelligent-Tiering</a> storage class to have AWS automatically move data to long-term storage based on your usage pattern. Please refer to this <a href="https://aws.amazon.com/blogs/containers/understanding-and-cost-optimizing-amazon-eks-control-plane-logs/">blog</a> for more details. For example, for your production environment logs reside in CloudWatch for more than 30 days then exported to Amazon S3 bucket. You can then use Amazon Athena to query the data in Amazon S3 bucket if you need to refer back to the logs at a later time.</p>
<h3 id="cost_optimization-cost_opt_observability-reduce-log-levels">Reduce Log Levels<a class="headerlink" href="#cost_optimization-cost_opt_observability-reduce-log-levels" title="Permanent link">&para;</a></h3>
<p>Practice selective logging for your application. Both your applications and nodes output logs by default. For your application logs, adjust the log levels to align with the criticality of the workload and environment. For example, the java application below is outputting <code>INFO</code> logs which is the typical default application configuration and depending on the code can result in a high volume of log data.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#cost_optimization-cost_opt_observability-__codelineno-2-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">org.apache.log4j.*</span><span class="p">;</span>
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#cost_optimization-cost_opt_observability-__codelineno-2-2"></a>
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#cost_optimization-cost_opt_observability-__codelineno-2-3"></a><span class="kd">public</span><span class="w"> </span><span class="kd">class</span> <span class="nc">LogClass</span><span class="w"> </span><span class="p">{</span>
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#cost_optimization-cost_opt_observability-__codelineno-2-4"></a><span class="w">   </span><span class="kd">private</span><span class="w"> </span><span class="kd">static</span><span class="w"> </span><span class="n">org</span><span class="p">.</span><span class="na">apache</span><span class="p">.</span><span class="na">log4j</span><span class="p">.</span><span class="na">Logger</span><span class="w"> </span><span class="n">log</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Logger</span><span class="p">.</span><span class="na">getLogger</span><span class="p">(</span><span class="n">LogClass</span><span class="p">.</span><span class="na">class</span><span class="p">);</span>
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#cost_optimization-cost_opt_observability-__codelineno-2-5"></a>
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#cost_optimization-cost_opt_observability-__codelineno-2-6"></a><span class="w">   </span><span class="kd">public</span><span class="w"> </span><span class="kd">static</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="nf">main</span><span class="p">(</span><span class="n">String</span><span class="o">[]</span><span class="w"> </span><span class="n">args</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#cost_optimization-cost_opt_observability-__codelineno-2-7"></a><span class="hll"><span class="w">      </span><span class="n">log</span><span class="p">.</span><span class="na">setLevel</span><span class="p">(</span><span class="n">Level</span><span class="p">.</span><span class="na">INFO</span><span class="p">);</span>
</span><a id="__codelineno-2-8" name="__codelineno-2-8" href="#cost_optimization-cost_opt_observability-__codelineno-2-8"></a>
<a id="__codelineno-2-9" name="__codelineno-2-9" href="#cost_optimization-cost_opt_observability-__codelineno-2-9"></a><span class="w">      </span><span class="n">log</span><span class="p">.</span><span class="na">debug</span><span class="p">(</span><span class="s">&quot;This is a DEBUG message, check this out!&quot;</span><span class="p">);</span>
<a id="__codelineno-2-10" name="__codelineno-2-10" href="#cost_optimization-cost_opt_observability-__codelineno-2-10"></a><span class="w">      </span><span class="n">log</span><span class="p">.</span><span class="na">info</span><span class="p">(</span><span class="s">&quot;This is an INFO message, nothing to see here!&quot;</span><span class="p">);</span>
<a id="__codelineno-2-11" name="__codelineno-2-11" href="#cost_optimization-cost_opt_observability-__codelineno-2-11"></a><span class="w">      </span><span class="n">log</span><span class="p">.</span><span class="na">warn</span><span class="p">(</span><span class="s">&quot;This is a WARN message, investigate this!&quot;</span><span class="p">);</span>
<a id="__codelineno-2-12" name="__codelineno-2-12" href="#cost_optimization-cost_opt_observability-__codelineno-2-12"></a><span class="w">      </span><span class="n">log</span><span class="p">.</span><span class="na">error</span><span class="p">(</span><span class="s">&quot;This is an ERROR message, check this out!&quot;</span><span class="p">);</span>
<a id="__codelineno-2-13" name="__codelineno-2-13" href="#cost_optimization-cost_opt_observability-__codelineno-2-13"></a><span class="w">      </span><span class="n">log</span><span class="p">.</span><span class="na">fatal</span><span class="p">(</span><span class="s">&quot;This is a FATAL message, investigate this!&quot;</span><span class="p">);</span>
<a id="__codelineno-2-14" name="__codelineno-2-14" href="#cost_optimization-cost_opt_observability-__codelineno-2-14"></a><span class="w">   </span><span class="p">}</span>
<a id="__codelineno-2-15" name="__codelineno-2-15" href="#cost_optimization-cost_opt_observability-__codelineno-2-15"></a><span class="p">}</span>
</code></pre></div>
<p>In a development environment, change your log level to <code>DEBUG</code>, as this can help you debug issues or catch potential ones before they get into production.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#cost_optimization-cost_opt_observability-__codelineno-3-1"></a><span class="w">      </span><span class="n">log</span><span class="p">.</span><span class="na">setLevel</span><span class="p">(</span><span class="n">Level</span><span class="p">.</span><span class="na">DEBUG</span><span class="p">);</span>
</code></pre></div>
<p>In a production environment, consider modifying your log level to <code>ERROR</code> or <code>FATAL</code>. This will output log only when your application has errors, reducing the log output and help you focus on important data about your application status.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#cost_optimization-cost_opt_observability-__codelineno-4-1"></a><span class="w">      </span><span class="n">log</span><span class="p">.</span><span class="na">setLevel</span><span class="p">(</span><span class="n">Level</span><span class="p">.</span><span class="na">ERROR</span><span class="p">);</span>
</code></pre></div>
<p>You can fine tune various Kubernetes components log levels. For example, if you are using <a href="https://bottlerocket.dev/">Bottlerocket</a> as your EKS Node operating system, there are configuration settings that allow you to adjust the kubelet process log level. A snippet of this configuration setting is below. Note the default <a href="https://github.com/bottlerocket-os/bottlerocket/blob/3f716bd68728f7fd825eb45621ada0972d0badbb/README.md?plain=1#L528">log level</a> of <strong>2</strong> which adjusts the logging verbosity of the <code>kubelet</code> process. </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#cost_optimization-cost_opt_observability-__codelineno-5-1"></a><span class="k">[settings.kubernetes]</span>
<a id="__codelineno-5-2" name="__codelineno-5-2" href="#cost_optimization-cost_opt_observability-__codelineno-5-2"></a><span class="hll"><span class="n">log-level</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;2&quot;</span>
</span><a id="__codelineno-5-3" name="__codelineno-5-3" href="#cost_optimization-cost_opt_observability-__codelineno-5-3"></a><span class="n">image-gc-high-threshold-percent</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;85&quot;</span>
<a id="__codelineno-5-4" name="__codelineno-5-4" href="#cost_optimization-cost_opt_observability-__codelineno-5-4"></a><span class="n">image-gc-low-threshold-percent</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;80&quot;</span>
</code></pre></div>
<p>For a development environment, you can set the log level greater than <strong>2</strong> in order to view additional events, this is good for debugging. For a production environment, you can set the level to <strong>0</strong> in order to view only critical events.</p>
<h3 id="cost_optimization-cost_opt_observability-leverage-filters">Leverage Filters<a class="headerlink" href="#cost_optimization-cost_opt_observability-leverage-filters" title="Permanent link">&para;</a></h3>
<p>When using a default EKS Fluentbit configuration to send container logs to Cloudwatch, FluentBit captures and send <strong>ALL</strong> application container logs enriched with Kubernetes metadata to Cloudwatch as shown in the <code>[INPUT]</code> configuration block below. </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#cost_optimization-cost_opt_observability-__codelineno-6-1"></a>    [INPUT]
<a id="__codelineno-6-2" name="__codelineno-6-2" href="#cost_optimization-cost_opt_observability-__codelineno-6-2"></a>        Name                tail
<a id="__codelineno-6-3" name="__codelineno-6-3" href="#cost_optimization-cost_opt_observability-__codelineno-6-3"></a>        Tag                 application.*
<a id="__codelineno-6-4" name="__codelineno-6-4" href="#cost_optimization-cost_opt_observability-__codelineno-6-4"></a>        Exclude_Path        /var/log/containers/cloudwatch-agent*, /var/log/containers/fluent-bit*, /var/log/containers/aws-node*, /var/log/containers/kube-proxy*
<a id="__codelineno-6-5" name="__codelineno-6-5" href="#cost_optimization-cost_opt_observability-__codelineno-6-5"></a>        Path                /var/log/containers/*.log
<a id="__codelineno-6-6" name="__codelineno-6-6" href="#cost_optimization-cost_opt_observability-__codelineno-6-6"></a>        Docker_Mode         On
<a id="__codelineno-6-7" name="__codelineno-6-7" href="#cost_optimization-cost_opt_observability-__codelineno-6-7"></a>        Docker_Mode_Flush   5
<a id="__codelineno-6-8" name="__codelineno-6-8" href="#cost_optimization-cost_opt_observability-__codelineno-6-8"></a>        Docker_Mode_Parser  container_firstline
<a id="__codelineno-6-9" name="__codelineno-6-9" href="#cost_optimization-cost_opt_observability-__codelineno-6-9"></a>        Parser              docker
<a id="__codelineno-6-10" name="__codelineno-6-10" href="#cost_optimization-cost_opt_observability-__codelineno-6-10"></a>        DB                  /var/fluent-bit/state/flb_container.db
<a id="__codelineno-6-11" name="__codelineno-6-11" href="#cost_optimization-cost_opt_observability-__codelineno-6-11"></a>        Mem_Buf_Limit       50MB
<a id="__codelineno-6-12" name="__codelineno-6-12" href="#cost_optimization-cost_opt_observability-__codelineno-6-12"></a>        Skip_Long_Lines     On
<a id="__codelineno-6-13" name="__codelineno-6-13" href="#cost_optimization-cost_opt_observability-__codelineno-6-13"></a>        Refresh_Interval    10
<a id="__codelineno-6-14" name="__codelineno-6-14" href="#cost_optimization-cost_opt_observability-__codelineno-6-14"></a>        Rotate_Wait         30
<a id="__codelineno-6-15" name="__codelineno-6-15" href="#cost_optimization-cost_opt_observability-__codelineno-6-15"></a>        storage.type        filesystem
<a id="__codelineno-6-16" name="__codelineno-6-16" href="#cost_optimization-cost_opt_observability-__codelineno-6-16"></a>        Read_from_Head      ${READ_FROM_HEAD}
</code></pre></div>
<p>The <code>[INPUT]</code> section above is ingesting all the container logs. This can generate a large amount of data that might not be necessary. Filtering out this data can reduce the amount of log data sent to CloudWatch therefore reducing your cost. You can apply a filter to you logs before it outputs to CloudWatch. Fluentbit defines this in the <code>[FILTER]</code> section. For example, filtering out the Kubernetes metadata from being appended to log events can reduce your log volume.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#cost_optimization-cost_opt_observability-__codelineno-7-1"></a>    [FILTER]
<a id="__codelineno-7-2" name="__codelineno-7-2" href="#cost_optimization-cost_opt_observability-__codelineno-7-2"></a>        Name                nest
<a id="__codelineno-7-3" name="__codelineno-7-3" href="#cost_optimization-cost_opt_observability-__codelineno-7-3"></a>        Match               application.*
<a id="__codelineno-7-4" name="__codelineno-7-4" href="#cost_optimization-cost_opt_observability-__codelineno-7-4"></a>        Operation           lift
<a id="__codelineno-7-5" name="__codelineno-7-5" href="#cost_optimization-cost_opt_observability-__codelineno-7-5"></a>        Nested_under        kubernetes
<a id="__codelineno-7-6" name="__codelineno-7-6" href="#cost_optimization-cost_opt_observability-__codelineno-7-6"></a>        Add_prefix          Kube.
<a id="__codelineno-7-7" name="__codelineno-7-7" href="#cost_optimization-cost_opt_observability-__codelineno-7-7"></a>
<a id="__codelineno-7-8" name="__codelineno-7-8" href="#cost_optimization-cost_opt_observability-__codelineno-7-8"></a>    [FILTER]
<a id="__codelineno-7-9" name="__codelineno-7-9" href="#cost_optimization-cost_opt_observability-__codelineno-7-9"></a>        Name                modify
<a id="__codelineno-7-10" name="__codelineno-7-10" href="#cost_optimization-cost_opt_observability-__codelineno-7-10"></a>        Match               application.*
<a id="__codelineno-7-11" name="__codelineno-7-11" href="#cost_optimization-cost_opt_observability-__codelineno-7-11"></a>        Remove              Kube.&lt;Metadata_1&gt;
<a id="__codelineno-7-12" name="__codelineno-7-12" href="#cost_optimization-cost_opt_observability-__codelineno-7-12"></a>        Remove              Kube.&lt;Metadata_2&gt;
<a id="__codelineno-7-13" name="__codelineno-7-13" href="#cost_optimization-cost_opt_observability-__codelineno-7-13"></a>        Remove              Kube.&lt;Metadata_3&gt;
<a id="__codelineno-7-14" name="__codelineno-7-14" href="#cost_optimization-cost_opt_observability-__codelineno-7-14"></a>
<a id="__codelineno-7-15" name="__codelineno-7-15" href="#cost_optimization-cost_opt_observability-__codelineno-7-15"></a>    [FILTER]
<a id="__codelineno-7-16" name="__codelineno-7-16" href="#cost_optimization-cost_opt_observability-__codelineno-7-16"></a>        Name                nest
<a id="__codelineno-7-17" name="__codelineno-7-17" href="#cost_optimization-cost_opt_observability-__codelineno-7-17"></a>        Match               application.*
<a id="__codelineno-7-18" name="__codelineno-7-18" href="#cost_optimization-cost_opt_observability-__codelineno-7-18"></a>        Operation           nest
<a id="__codelineno-7-19" name="__codelineno-7-19" href="#cost_optimization-cost_opt_observability-__codelineno-7-19"></a>        Wildcard            Kube.*
<a id="__codelineno-7-20" name="__codelineno-7-20" href="#cost_optimization-cost_opt_observability-__codelineno-7-20"></a>        Nested_under        kubernetes
<a id="__codelineno-7-21" name="__codelineno-7-21" href="#cost_optimization-cost_opt_observability-__codelineno-7-21"></a>        Remove_prefix       Kube.
</code></pre></div>
<h2 id="cost_optimization-cost_opt_observability-metrics">Metrics<a class="headerlink" href="#cost_optimization-cost_opt_observability-metrics" title="Permanent link">&para;</a></h2>
<p><a href="https://aws-observability.github.io/observability-best-practices/signals/metrics/">Metrics</a> provide valuable information regarding the performance of your system. By consolidating all system-related or available resource metrics in a centralized location, you gain the capability to compare and analyze performance data. This centralized approach enables you to make more informed strategic decisions, such as scaling up or scaling down resources. Additionally, metrics play a crucial role in assessing the health of resources, allowing you to take proactive measures when necessary. Generally observability costs scale with telemetry data collection and retention. Below are a few strategies you can implement to reduce the cost of metric telemetry: collecting only metrics that matter, reducing the cardinality of your telemetry data, and fine tuning the granularity of your telemetry data collection. </p>
<h3 id="cost_optimization-cost_opt_observability-monitor-what-matters-and-collect-only-what-you-need">Monitor what matters and collect only what you need<a class="headerlink" href="#cost_optimization-cost_opt_observability-monitor-what-matters-and-collect-only-what-you-need" title="Permanent link">&para;</a></h3>
<p>The first cost reduction strategy is to reduce the number of metrics you are collecting and in turn, reduce retention costs. </p>
<ol>
<li>Begin by working backwards from your and/or your stakeholder’s requirements to determine <a href="https://aws-observability.github.io/observability-best-practices/guides/#monitor-what-matters">the metrics that are most important</a>. Success metrics are different for everyone! Know what <em>good</em> looks like and measure for it.</li>
<li>Consider diving deep into the workloads you are supporting and identifying its Key Performance Indicators (KPIs) a.k.a 'Golden Signals'. These should align to business and stake-holder requirements. Calculating SLIs, SLOs, and SLAs using Amazon CloudWatch and Metric Math is crucial for managing service reliability. Follow the best practices outlined in this <a href="https://aws-observability.github.io/observability-best-practices/guides/operational/business/key-performance-indicators/#10-understanding-kpis-golden-signals">guide</a> to effectively monitor and maintain the performance of your EKS environment.  </li>
<li>Then continue through the different layers of infrastructure to <a href="https://aws-observability.github.io/observability-best-practices/signals/metrics/#correlate-with-operational-metric-data">connect and correlate</a> EKS cluster, node and additional infrastructure metrics to your workload KPIs. Store your business metrics and operational metrics in a system where you can correlate them together and draw conclusions based on observed impacts to both.</li>
<li>EKS exposes metrics from the control plane, cluster kube-state-metrics, pods, and nodes. The relevance of all these metrics is dependent on your needs, however it’s likely that you will not need every single metric across the different layers. You can use this <a href="https://aws-observability.github.io/observability-best-practices/guides/containers/oss/eks/best-practices-metrics-collection/">EKS essential metrics</a> guide as a baseline for monitoring the overall health of an EKS cluster and your workloads. </li>
</ol>
<p>Here is an example prometheus scrape config where we are using the <code>relabel_config</code> to keep only kubelet metrics and <code>metric_relabel_config</code> to drop all container metrics. </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1" href="#cost_optimization-cost_opt_observability-__codelineno-8-1"></a><span class="w">  </span><span class="nt">kubernetes_sd_configs</span><span class="p">:</span>
<a id="__codelineno-8-2" name="__codelineno-8-2" href="#cost_optimization-cost_opt_observability-__codelineno-8-2"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">role</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">endpoints</span>
<a id="__codelineno-8-3" name="__codelineno-8-3" href="#cost_optimization-cost_opt_observability-__codelineno-8-3"></a><span class="w">    </span><span class="nt">namespaces</span><span class="p">:</span>
<a id="__codelineno-8-4" name="__codelineno-8-4" href="#cost_optimization-cost_opt_observability-__codelineno-8-4"></a><span class="w">      </span><span class="nt">names</span><span class="p">:</span>
<a id="__codelineno-8-5" name="__codelineno-8-5" href="#cost_optimization-cost_opt_observability-__codelineno-8-5"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">kube-system</span>
<a id="__codelineno-8-6" name="__codelineno-8-6" href="#cost_optimization-cost_opt_observability-__codelineno-8-6"></a><span class="w">  </span><span class="nt">bearer_token_file</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/var/run/secrets/kubernetes.io/serviceaccount/token</span>
<a id="__codelineno-8-7" name="__codelineno-8-7" href="#cost_optimization-cost_opt_observability-__codelineno-8-7"></a><span class="w">  </span><span class="nt">tls_config</span><span class="p">:</span>
<a id="__codelineno-8-8" name="__codelineno-8-8" href="#cost_optimization-cost_opt_observability-__codelineno-8-8"></a><span class="w">    </span><span class="nt">insecure_skip_verify</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<a id="__codelineno-8-9" name="__codelineno-8-9" href="#cost_optimization-cost_opt_observability-__codelineno-8-9"></a><span class="w">  </span><span class="nt">relabel_configs</span><span class="p">:</span>
<a id="__codelineno-8-10" name="__codelineno-8-10" href="#cost_optimization-cost_opt_observability-__codelineno-8-10"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">source_labels</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">__meta_kubernetes_service_label_k8s_app</span><span class="p p-Indicator">]</span>
<a id="__codelineno-8-11" name="__codelineno-8-11" href="#cost_optimization-cost_opt_observability-__codelineno-8-11"></a><span class="w">    </span><span class="nt">regex</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">kubelet</span>
<a id="__codelineno-8-12" name="__codelineno-8-12" href="#cost_optimization-cost_opt_observability-__codelineno-8-12"></a><span class="w">    </span><span class="nt">action</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">keep</span>
<a id="__codelineno-8-13" name="__codelineno-8-13" href="#cost_optimization-cost_opt_observability-__codelineno-8-13"></a>
<a id="__codelineno-8-14" name="__codelineno-8-14" href="#cost_optimization-cost_opt_observability-__codelineno-8-14"></a><span class="w">  </span><span class="nt">metric_relabel_configs</span><span class="p">:</span>
<a id="__codelineno-8-15" name="__codelineno-8-15" href="#cost_optimization-cost_opt_observability-__codelineno-8-15"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">source_labels</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">__name__</span><span class="p p-Indicator">]</span>
<a id="__codelineno-8-16" name="__codelineno-8-16" href="#cost_optimization-cost_opt_observability-__codelineno-8-16"></a><span class="w">    </span><span class="nt">regex</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)</span>
<a id="__codelineno-8-17" name="__codelineno-8-17" href="#cost_optimization-cost_opt_observability-__codelineno-8-17"></a><span class="w">    </span><span class="nt">action</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">drop</span>
</code></pre></div>
<h3 id="cost_optimization-cost_opt_observability-reduce-cardinality-where-applicable">Reduce cardinality where applicable<a class="headerlink" href="#cost_optimization-cost_opt_observability-reduce-cardinality-where-applicable" title="Permanent link">&para;</a></h3>
<p>Cardinality refers to the uniqueness of the data values in combination with its dimensions (eg. prometheus labels) for a specific metrics set. High cardinality metrics have many dimensions and each dimension metric combination has higher uniqueness. Higher cardinality results in larger metric telemetry data size and storage needs which increases cost. </p>
<p>In the high cardinality example below, we see that the Metric, Latency, has Dimensions, RequestID, CustomerID, and Service and each Dimension has many unique values. Cardinality is the measure of the combination of the number of possible values per Dimension. In Prometheus, each set of unique dimensions/labels are consider as a new metric, therefore high cardinality means more metrics. </p>
<p><img alt="high cardinality" src="../images/high-cardinality.png" /></p>
<p>In EKS environments with many metrics and dimensions/labels per metric (Cluster, Namespace, Service, Pod, Container, etc), the cardinality tends to grow. In order to optimize cost, consider the cardinality of the metrics you are collecting carefully. For example, if you are aggregating a specific metric for visualization at the cluster level, then you can drop additional labels that are at a lower layer such as the namespace label. </p>
<p>In order to identify high cardinality metrics in prometheus you can run the following PROMQL query to determine which scrape targets have the highest number of metrics (cardinality): </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-9-1" name="__codelineno-9-1" href="#cost_optimization-cost_opt_observability-__codelineno-9-1"></a><span class="nv">topk_max</span><span class="o">(</span><span class="mi">5</span><span class="p">,</span><span class="w"> </span><span class="kr">max_over_time</span><span class="o">(</span><span class="nv">scrape_samples_scraped</span><span class="p">[</span><span class="s">1h</span><span class="p">]</span><span class="o">))</span>
</code></pre></div>
<p>and the following PROMQL query can help you determine which scrape targets have the highest metrics churn (how many new metrics series were created in a given scrape) rates :</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-10-1" name="__codelineno-10-1" href="#cost_optimization-cost_opt_observability-__codelineno-10-1"></a><span class="nv">topk_max</span><span class="o">(</span><span class="mi">5</span><span class="p">,</span><span class="w"> </span><span class="kr">max_over_time</span><span class="o">(</span><span class="nv">scrape_series_added</span><span class="p">[</span><span class="s">1h</span><span class="p">]</span><span class="o">))</span>
</code></pre></div>
<p>If you are using grafana you can use Grafana Lab’s Mimirtool to analyze your grafana dashboards and prometheus rules to identify unused high-cardinality metrics. Follow <a href="https://grafana.com/docs/grafana-cloud/account-management/billing-and-usage/control-prometheus-metrics-usage/usage-analysis-mimirtool/?pg=blog&amp;plcmt=body-txt#analyze-and-reduce-metrics-usage-with-grafana-mimirtool">this guide</a> on how to use the <code>mimirtool analyze</code> and <code>mimirtool analyze prometheus</code> commands to identify active metrics which are not referenced in your dashboards. </p>
<h3 id="cost_optimization-cost_opt_observability-consider-metric-granularity">Consider metric granularity<a class="headerlink" href="#cost_optimization-cost_opt_observability-consider-metric-granularity" title="Permanent link">&para;</a></h3>
<p>Collecting metrics at a higher granularity like every second vs every minute can have a big impact on how much telemetry is collected and stored which increases cost. Determine sensible scrape or metrics collection intervals that balance between enough granularity to see transient issues and low enough to be cost effective. Decrease granularity for metrics that are used for capacity planning and larger time window analysis. </p>
<p>Below is a snippet from the default AWS Distro for Opentelemetry (ADOT) EKS Addon Collector <a href="https://docs.aws.amazon.com/eks/latest/userguide/deploy-deployment.html">configuration</a>. </p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>the global prometheus scrape interval is set to 15s. This scrape interval can be increased resulting in a decrease in the amount of metric data collected in prometheus. </p>
</div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-11-1" name="__codelineno-11-1" href="#cost_optimization-cost_opt_observability-__codelineno-11-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">opentelemetry.io/v1alpha1</span>
<a id="__codelineno-11-2" name="__codelineno-11-2" href="#cost_optimization-cost_opt_observability-__codelineno-11-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">OpenTelemetryCollector</span>
<a id="__codelineno-11-3" name="__codelineno-11-3" href="#cost_optimization-cost_opt_observability-__codelineno-11-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-11-4" name="__codelineno-11-4" href="#cost_optimization-cost_opt_observability-__codelineno-11-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">my-collector-amp</span>
<a id="__codelineno-11-5" name="__codelineno-11-5" href="#cost_optimization-cost_opt_observability-__codelineno-11-5"></a>
<a id="__codelineno-11-6" name="__codelineno-11-6" href="#cost_optimization-cost_opt_observability-__codelineno-11-6"></a><span class="nn">...</span>
<a id="__codelineno-11-7" name="__codelineno-11-7" href="#cost_optimization-cost_opt_observability-__codelineno-11-7"></a>
<a id="__codelineno-11-8" name="__codelineno-11-8" href="#cost_optimization-cost_opt_observability-__codelineno-11-8"></a><span class="w">  </span><span class="nt">config</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<a id="__codelineno-11-9" name="__codelineno-11-9" href="#cost_optimization-cost_opt_observability-__codelineno-11-9"></a><span class="w">    </span><span class="no">extensions:</span>
<a id="__codelineno-11-10" name="__codelineno-11-10" href="#cost_optimization-cost_opt_observability-__codelineno-11-10"></a><span class="w">      </span><span class="no">sigv4auth:</span>
<a id="__codelineno-11-11" name="__codelineno-11-11" href="#cost_optimization-cost_opt_observability-__codelineno-11-11"></a><span class="w">        </span><span class="no">region: &quot;&lt;YOUR_AWS_REGION&gt;&quot;</span>
<a id="__codelineno-11-12" name="__codelineno-11-12" href="#cost_optimization-cost_opt_observability-__codelineno-11-12"></a><span class="w">        </span><span class="no">service: &quot;aps&quot;</span>
<a id="__codelineno-11-13" name="__codelineno-11-13" href="#cost_optimization-cost_opt_observability-__codelineno-11-13"></a>
<a id="__codelineno-11-14" name="__codelineno-11-14" href="#cost_optimization-cost_opt_observability-__codelineno-11-14"></a><span class="w">    </span><span class="no">receivers:</span>
<a id="__codelineno-11-15" name="__codelineno-11-15" href="#cost_optimization-cost_opt_observability-__codelineno-11-15"></a><span class="w">      </span><span class="no">#</span>
<a id="__codelineno-11-16" name="__codelineno-11-16" href="#cost_optimization-cost_opt_observability-__codelineno-11-16"></a><span class="w">      </span><span class="no"># Scrape configuration for the Prometheus Receiver</span>
<a id="__codelineno-11-17" name="__codelineno-11-17" href="#cost_optimization-cost_opt_observability-__codelineno-11-17"></a><span class="w">      </span><span class="no"># This is the same configuration used when Prometheus is installed using the community Helm chart</span>
<a id="__codelineno-11-18" name="__codelineno-11-18" href="#cost_optimization-cost_opt_observability-__codelineno-11-18"></a><span class="w">      </span><span class="no"># </span>
<a id="__codelineno-11-19" name="__codelineno-11-19" href="#cost_optimization-cost_opt_observability-__codelineno-11-19"></a><span class="w">      </span><span class="no">prometheus:</span>
<a id="__codelineno-11-20" name="__codelineno-11-20" href="#cost_optimization-cost_opt_observability-__codelineno-11-20"></a><span class="w">        </span><span class="no">config:</span>
<a id="__codelineno-11-21" name="__codelineno-11-21" href="#cost_optimization-cost_opt_observability-__codelineno-11-21"></a><span class="w">          </span><span class="no">global:</span>
<a id="__codelineno-11-22" name="__codelineno-11-22" href="#cost_optimization-cost_opt_observability-__codelineno-11-22"></a><span class="hll"><span class="w">  </span><span class="nt">scrape_interval</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s</span>
</span><a id="__codelineno-11-23" name="__codelineno-11-23" href="#cost_optimization-cost_opt_observability-__codelineno-11-23"></a><span class="w">            </span><span class="l l-Scalar l-Scalar-Plain">scrape_timeout</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10s</span>
</code></pre></div>
<h2 id="cost_optimization-cost_opt_observability-tracing">Tracing<a class="headerlink" href="#cost_optimization-cost_opt_observability-tracing" title="Permanent link">&para;</a></h2>
<p>The primary cost associated with tracing stem from trace storage generation. With tracing, the aim is to gather sufficient data to diagnose and understand performance aspects. However, as X-Ray traces costs are based on data forwarded to to X-Ray, erasing traces after it has been forward will not reduce your costs. Let’s review ways to lower your costs for tracing while maintaining data for you to perform proper analysis. </p>
<h3 id="cost_optimization-cost_opt_observability-apply-sampling-rules">Apply Sampling rules<a class="headerlink" href="#cost_optimization-cost_opt_observability-apply-sampling-rules" title="Permanent link">&para;</a></h3>
<p>The X-Ray sampling rate is conservative by default. Define sampling rules where you can control the amount of data that you gather. This will improve performance efficiency while reducing costs. By <a href="https://docs.aws.amazon.com/xray/latest/devguide/xray-console-sampling.html#xray-console-custom">decreasing the sampling rate</a>, you can collect traces from the request only what your workloads needs while maintaining a lower cost structure. </p>
<p>For example, you have java application that you want to debug the traces of all the requests for 1 problematic route.</p>
<p><strong>Configure via the SDK to load sampling rules from a JSON document</strong></p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-12-1" name="__codelineno-12-1" href="#cost_optimization-cost_opt_observability-__codelineno-12-1"></a><span class="p">{</span>
<a id="__codelineno-12-2" name="__codelineno-12-2" href="#cost_optimization-cost_opt_observability-__codelineno-12-2"></a><span class="nt">&quot;version&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span>
<a id="__codelineno-12-3" name="__codelineno-12-3" href="#cost_optimization-cost_opt_observability-__codelineno-12-3"></a><span class="w">  </span><span class="nt">&quot;rules&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<a id="__codelineno-12-4" name="__codelineno-12-4" href="#cost_optimization-cost_opt_observability-__codelineno-12-4"></a><span class="w">    </span><span class="p">{</span>
<a id="__codelineno-12-5" name="__codelineno-12-5" href="#cost_optimization-cost_opt_observability-__codelineno-12-5"></a><span class="nt">&quot;description&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;debug-eks&quot;</span><span class="p">,</span>
<a id="__codelineno-12-6" name="__codelineno-12-6" href="#cost_optimization-cost_opt_observability-__codelineno-12-6"></a><span class="w">      </span><span class="nt">&quot;host&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;*&quot;</span><span class="p">,</span>
<a id="__codelineno-12-7" name="__codelineno-12-7" href="#cost_optimization-cost_opt_observability-__codelineno-12-7"></a><span class="w">      </span><span class="nt">&quot;http_method&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;PUT&quot;</span><span class="p">,</span>
<a id="__codelineno-12-8" name="__codelineno-12-8" href="#cost_optimization-cost_opt_observability-__codelineno-12-8"></a><span class="w">      </span><span class="nt">&quot;url_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;/history/*&quot;</span><span class="p">,</span>
<a id="__codelineno-12-9" name="__codelineno-12-9" href="#cost_optimization-cost_opt_observability-__codelineno-12-9"></a><span class="w">      </span><span class="nt">&quot;fixed_target&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span>
<a id="__codelineno-12-10" name="__codelineno-12-10" href="#cost_optimization-cost_opt_observability-__codelineno-12-10"></a><span class="w">      </span><span class="nt">&quot;rate&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span>
<a id="__codelineno-12-11" name="__codelineno-12-11" href="#cost_optimization-cost_opt_observability-__codelineno-12-11"></a><span class="w">      </span><span class="nt">&quot;service_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;debug-eks&quot;</span>
<a id="__codelineno-12-12" name="__codelineno-12-12" href="#cost_optimization-cost_opt_observability-__codelineno-12-12"></a><span class="w">    </span><span class="p">}</span>
<a id="__codelineno-12-13" name="__codelineno-12-13" href="#cost_optimization-cost_opt_observability-__codelineno-12-13"></a><span class="w">  </span><span class="p">],</span>
<a id="__codelineno-12-14" name="__codelineno-12-14" href="#cost_optimization-cost_opt_observability-__codelineno-12-14"></a><span class="w">  </span><span class="nt">&quot;default&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<a id="__codelineno-12-15" name="__codelineno-12-15" href="#cost_optimization-cost_opt_observability-__codelineno-12-15"></a><span class="nt">&quot;fixed_target&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span>
<a id="__codelineno-12-16" name="__codelineno-12-16" href="#cost_optimization-cost_opt_observability-__codelineno-12-16"></a><span class="w">    </span><span class="nt">&quot;rate&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.1</span>
<a id="__codelineno-12-17" name="__codelineno-12-17" href="#cost_optimization-cost_opt_observability-__codelineno-12-17"></a><span class="w">  </span><span class="p">}</span>
<a id="__codelineno-12-18" name="__codelineno-12-18" href="#cost_optimization-cost_opt_observability-__codelineno-12-18"></a><span class="p">}</span>
</code></pre></div>
<p><strong>Via the Console</strong></p>
<p><img alt="console" src="../images/console.png" /></p>
<h3 id="cost_optimization-cost_opt_observability-apply-tail-sampling-with-aws-distro-for-opentelemetry-adot">Apply Tail Sampling with AWS Distro for OpenTelemetry (ADOT)<a class="headerlink" href="#cost_optimization-cost_opt_observability-apply-tail-sampling-with-aws-distro-for-opentelemetry-adot" title="Permanent link">&para;</a></h3>
<p>ADOT Tail Sampling allows you to control the volume of traces ingested in the service. However, Tail Sampling allows you to define the sampling policies after all the spans in the request have been completed instead of at the beginning. This further limits the amount of raw data transferred to CloudWatch, hence reducing cost.</p>
<p>For example, if you’re sampling 1% of traffic to a landing page and 10% of the requests to a payment page this might leave you with 300 traces for an 30 minute period. With an ADOT Tail Sampling rule of that filters specific errors, you could be left with 200 traces which decreases the number of traces stored. </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-13-1" name="__codelineno-13-1" href="#cost_optimization-cost_opt_observability-__codelineno-13-1"></a><span class="nt">processors</span><span class="p">:</span>
<a id="__codelineno-13-2" name="__codelineno-13-2" href="#cost_optimization-cost_opt_observability-__codelineno-13-2"></a><span class="w">  </span><span class="nt">groupbytrace</span><span class="p">:</span>
<a id="__codelineno-13-3" name="__codelineno-13-3" href="#cost_optimization-cost_opt_observability-__codelineno-13-3"></a><span class="w">    </span><span class="nt">wait_duration</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10s</span>
<a id="__codelineno-13-4" name="__codelineno-13-4" href="#cost_optimization-cost_opt_observability-__codelineno-13-4"></a><span class="w">    </span><span class="nt">num_traces</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">300</span><span class="w"> </span>
<a id="__codelineno-13-5" name="__codelineno-13-5" href="#cost_optimization-cost_opt_observability-__codelineno-13-5"></a><span class="hll"><span class="w">    </span><span class="nt">tail_sampling</span><span class="p">:</span>
</span><a id="__codelineno-13-6" name="__codelineno-13-6" href="#cost_optimization-cost_opt_observability-__codelineno-13-6"></a><span class="w">    </span><span class="nt">decision_wait</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1s</span><span class="w"> </span><span class="c1"># This value should be smaller than wait_duration</span>
<a id="__codelineno-13-7" name="__codelineno-13-7" href="#cost_optimization-cost_opt_observability-__codelineno-13-7"></a><span class="w">    </span><span class="nt">policies</span><span class="p">:</span>
<a id="__codelineno-13-8" name="__codelineno-13-8" href="#cost_optimization-cost_opt_observability-__codelineno-13-8"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">.....</span><span class="w"> </span><span class="c1"># Applicable policies**</span>
<a id="__codelineno-13-9" name="__codelineno-13-9" href="#cost_optimization-cost_opt_observability-__codelineno-13-9"></a><span class="w">  </span><span class="nt">batch/tracesampling</span><span class="p">:</span>
<a id="__codelineno-13-10" name="__codelineno-13-10" href="#cost_optimization-cost_opt_observability-__codelineno-13-10"></a><span class="w">    </span><span class="nt">timeout</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0s</span><span class="w"> </span><span class="c1"># No need to wait more since this will happen in previous processors</span>
<a id="__codelineno-13-11" name="__codelineno-13-11" href="#cost_optimization-cost_opt_observability-__codelineno-13-11"></a><span class="w">    </span><span class="nt">send_batch_max_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8196</span><span class="w"> </span><span class="c1"># This will still allow us to limit the size of the batches sent to subsequent exporters</span>
<a id="__codelineno-13-12" name="__codelineno-13-12" href="#cost_optimization-cost_opt_observability-__codelineno-13-12"></a>
<a id="__codelineno-13-13" name="__codelineno-13-13" href="#cost_optimization-cost_opt_observability-__codelineno-13-13"></a><span class="nt">service</span><span class="p">:</span>
<a id="__codelineno-13-14" name="__codelineno-13-14" href="#cost_optimization-cost_opt_observability-__codelineno-13-14"></a><span class="w">  </span><span class="nt">pipelines</span><span class="p">:</span>
<a id="__codelineno-13-15" name="__codelineno-13-15" href="#cost_optimization-cost_opt_observability-__codelineno-13-15"></a><span class="w">    </span><span class="nt">traces/tailsampling</span><span class="p">:</span>
<a id="__codelineno-13-16" name="__codelineno-13-16" href="#cost_optimization-cost_opt_observability-__codelineno-13-16"></a><span class="w">      </span><span class="nt">receivers</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">otlp</span><span class="p p-Indicator">]</span>
<a id="__codelineno-13-17" name="__codelineno-13-17" href="#cost_optimization-cost_opt_observability-__codelineno-13-17"></a><span class="w">      </span><span class="nt">processors</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">groupbytrace</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">tail_sampling</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">batch/tracesampling</span><span class="p p-Indicator">]</span>
<a id="__codelineno-13-18" name="__codelineno-13-18" href="#cost_optimization-cost_opt_observability-__codelineno-13-18"></a><span class="w">      </span><span class="nt">exporters</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">awsxray</span><span class="p p-Indicator">]</span>
</code></pre></div>
<h3 id="cost_optimization-cost_opt_observability-leverage-amazon-s3-storage-options">Leverage Amazon S3 Storage options<a class="headerlink" href="#cost_optimization-cost_opt_observability-leverage-amazon-s3-storage-options" title="Permanent link">&para;</a></h3>
<p>You should leverage AWS S3 bucket and its different storage classes to store the traces. Export traces to S3 before the retention period expires. Use Amazon S3 Lifecycle rules to move the trace data to the storage class that meets your requirements. </p>
<p>For example, if you have traces that are 90 days old, <a href="https://aws.amazon.com/s3/storage-classes/intelligent-tiering/">Amazon S3 Intelligent-Tiering</a> can automatically move the data to long-term storage based on your usage pattern. You can use <a href="https://aws.amazon.com/athena/">Amazon Athena</a> to query the data in Amazon S3 if you need to refer back to the traces at a later time. This can further reduce your cost for distributed tracing.</p>
<h2 id="cost_optimization-cost_opt_observability-additional-resources">Additional Resources:<a class="headerlink" href="#cost_optimization-cost_opt_observability-additional-resources" title="Permanent link">&para;</a></h2>
<ul>
<li><a href="https://aws-observability.github.io/observability-best-practices/guides/">Observability Best Practices Guide</a></li>
<li><a href="https://aws-observability.github.io/observability-best-practices/guides/containers/oss/eks/">Best Practices Metrics Collection</a></li>
<li><a href="https://www.youtube.com/watch?v=zZPzXEBW4P8">AWS re:Invent 2022 - Observability best practices at Amazon (COP343)</a></li>
<li><a href="https://www.youtube.com/watch?v=YiegAlC_yyc">AWS re:Invent 2022 - Observability: Best practices for modern applications (COP344)</a></li>
</ul></section><h1 class='nav-section-title-end'>Ended: Cost Optimization</h1></div>









  




                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "/", "features": ["tabs", "content.code.copy", "announce.dismiss", "header.autohide"], "search": "../assets/javascripts/workers/search.f886a092.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.d7c377c4.min.js"></script>
      
        <script src="../js/print-site.js"></script>
      
    
  </body>
</html>